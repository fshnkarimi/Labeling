{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Time_Series_Labelling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMURxUHNk5sf",
        "outputId": "861d32dd-0d36-41dd-e60c-76726d300285"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lrRB1spRjj2"
      },
      "source": [
        "Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXSg_jfxlaWw"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "dfs = pd.read_excel(\"000001data.xlsx\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a-OsIVVR1pz"
      },
      "source": [
        "Implementing Labelling algorithm "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqwGOVvDRqvH"
      },
      "source": [
        "#labelling algorithm X is price and w is our threshould\n",
        "def labeling(X,w):\n",
        "    n = len(X)\n",
        "    y = np.array([0 for i in range(n)])\n",
        "    FP=X[0]\n",
        "    xh = X[0]\n",
        "    xl = X[0]\n",
        "    HT = 0\n",
        "    LT = 0\n",
        "    cid = 0\n",
        "    FP_N = 0\n",
        "    for i in range(n):\n",
        "        if(X[i] > FP + X[0]*w):\n",
        "            xh,HT,FP_N,cid  = X[i],i,i,1\n",
        "            break\n",
        "        if(X[i] < FP - X[0]*w):\n",
        "            xh,HT,FP_N,cid  = X[i],i,i,-1\n",
        "            break\n",
        "    for i in range(FP_N+1,n):\n",
        "        if(cid > 0):\n",
        "            if(X[i]>xh):\n",
        "                xh,HT = X[i],i\n",
        "            if(X[i] < xh - xh*w and LT<= HT ):\n",
        "                for j in range(n):\n",
        "                    if(j>LT and j<=HT):\n",
        "                        y[j] = 1\n",
        "                xl,LT,cid = X[i],i,-1\n",
        "        if(cid < 0):\n",
        "            if(X[i]<xl):\n",
        "                xl,LT = X[i],i\n",
        "            if(X[i] > xl + xl*w and HT<= LT ):\n",
        "                for j in range(n):\n",
        "                    if(j>HT and j<=LT):\n",
        "                        y[j] = -1\n",
        "                xh,HT,cid = X[i],i,1\n",
        "    return y"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfIUNs4ER-ww"
      },
      "source": [
        "labelling data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zUQ57f_RrB1"
      },
      "source": [
        "label=labeling(dfs[\"closingprice\"],0.15)    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2raClgQCSNeh"
      },
      "source": [
        "Creat windows with size 11 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trtJyd9FSh5X"
      },
      "source": [
        "closing_price= dfs['closingprice']\n",
        "\n",
        "#windows size is 11\n",
        "expand_data = np.array([closing_price[i:i+11] for i in range(len(closing_price) - 10) ])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWXlg2f2So4Q"
      },
      "source": [
        "normalize Data \n",
        "\\begin{align*}\n",
        "        f_{ij} & = \\frac{x_{ij} - M^{\\lambda}_{i}}{M^{\\lambda}_{i}} ,x_{ij} \\in X\\\\\n",
        "        M^{\\lambda}_{s}& = \\frac{\\sum_{i=s}^{s+ \\lambda -1} x_i}{\\lambda} , x_i \\in x\n",
        "    \\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPiKNgIulg_Q"
      },
      "source": [
        "\n",
        "#normalizatin\n",
        "final_data_x = expand_data / np.mean(expand_data,axis = 1).reshape((len(expand_data) , 1))\n",
        "final_data_x = final_data_x - np.ones((len(final_data_x) ,1))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SZSPp6FTGA0"
      },
      "source": [
        "#finalizing data\n",
        "final_data_y = label[10:]\n",
        "#split data to test and train\n",
        "x_train_full = final_data_x[0:3011]\n",
        "y_train_full = final_data_y[0:3011]\n",
        "x_test= final_data_x[3011:]\n",
        "y_test = final_data_y[3011:]\n",
        "\n",
        "#(-1,1) --> (0,1)\n",
        "y_train_full = (y_train_full + 1)//2\n",
        "y_test = (y_test + 1)//2\n",
        "\n",
        "x_train_full =x_train_full.reshape(x_train_full.shape[0],x_train.shape[1] , 1)\n",
        "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1] , 1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT56qQryqZYx",
        "outputId": "76d6f564-2a4c-438e-9077-679fdd08bd54"
      },
      "source": [
        "closing_price"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDOBqAAqdWXr"
      },
      "source": [
        "import itertools as itt\n",
        "import numbers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from abc import abstractmethod\n",
        "from typing import Iterable, Tuple, List\n",
        "\n",
        "\n",
        "class BaseTimeSeriesCrossValidator:\n",
        "    \"\"\"\n",
        "    Abstract class for time series cross-validation.\n",
        "    Time series cross-validation requires each sample has a prediction time pred_time, at which the features are used to\n",
        "    predict the response, and an evaluation time eval_time, at which the response is known and the error can be\n",
        "    computed. Importantly, it means that unlike in standard sklearn cross-validation, the samples X, response y,\n",
        "    pred_times and eval_times must all be pandas dataframe/series having the same index. It is also assumed that the\n",
        "    samples are time-ordered with respect to the prediction time (i.e. pred_times is non-decreasing).\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_splits : int, default=10\n",
        "        Number of folds. Must be at least 2.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_splits=10):\n",
        "        if not isinstance(n_splits, numbers.Integral):\n",
        "            raise ValueError(f\"The number of folds must be of Integral type. {n_splits} of type {type(n_splits)}\"\n",
        "                             f\" was passed.\")\n",
        "        n_splits = int(n_splits)\n",
        "        if n_splits <= 1:\n",
        "            raise ValueError(f\"K-fold cross-validation requires at least one train/test split by setting n_splits = 2 \"\n",
        "                             f\"or more, got n_splits = {n_splits}.\")\n",
        "        self.n_splits = n_splits\n",
        "        self.pred_times = None\n",
        "        self.eval_times = None\n",
        "        self.indices = None\n",
        "\n",
        "    @abstractmethod\n",
        "    def split(self, X: pd.DataFrame, y: pd.Series = None,\n",
        "              pred_times: pd.Series = None, eval_times: pd.Series = None):\n",
        "        if not isinstance(X, pd.DataFrame) and not isinstance(X, pd.Series):\n",
        "            raise ValueError('X should be a pandas DataFrame/Series.')\n",
        "        if not isinstance(y, pd.Series) and y is not None:\n",
        "            raise ValueError('y should be a pandas Series.')\n",
        "        if not isinstance(pred_times, pd.Series):\n",
        "            raise ValueError('pred_times should be a pandas Series.')\n",
        "        if not isinstance(eval_times, pd.Series):\n",
        "            raise ValueError('eval_times should be a pandas Series.')\n",
        "        if y is not None and (X.index == y.index).sum() != len(y):\n",
        "            raise ValueError('X and y must have the same index')\n",
        "        if (X.index == pred_times.index).sum() != len(pred_times):\n",
        "            raise ValueError('X and pred_times must have the same index')\n",
        "        if (X.index == eval_times.index).sum() != len(eval_times):\n",
        "            raise ValueError('X and eval_times must have the same index')\n",
        "\n",
        "        self.pred_times = pred_times\n",
        "        self.eval_times = eval_times\n",
        "        self.indices = np.arange(X.shape[0])\n",
        "\n",
        "\n",
        "class PurgedWalkForwardCV(BaseTimeSeriesCrossValidator):\n",
        "  \"\"\"\n",
        "  Purged walk-forward cross-validation\n",
        "  As described in Advances in financial machine learning, Marcos Lopez de Prado, 2018.\n",
        "  The samples are decomposed into n_splits folds containing equal numbers of samples, without shuffling. In each cross\n",
        "  validation round, n_test_splits contiguous folds are used as the test set, while the train set consists in between\n",
        "  min_train_splits and max_train_splits immediately preceding folds.\n",
        "  Each sample should be tagged with a prediction time pred_time and an evaluation time eval_time. The split is such\n",
        "  that the intervals [pred_times, eval_times] associated to samples in the train and test set do not overlap. (The\n",
        "  overlapping samples are dropped.)\n",
        "  With split_by_times = True in the split method, it is also possible to split the samples in folds spanning equal\n",
        "  time intervals (using the prediction time as a time tag), instead of folds containing equal numbers of samples.\n",
        "  Parameters\n",
        "  ----------\n",
        "  n_splits : int, default=10\n",
        "      Number of folds. Must be at least 2.\n",
        "  n_test_splits : int, default = 1\n",
        "      Number of folds used in the test set. Must be at least 1.\n",
        "  min_train_splits: int, default = 2\n",
        "      Minimal number of folds to be used in the train set.\n",
        "  max_train_splits: int, default = None\n",
        "      Maximal number of folds to be used in the train set. If None, there is no upper limit.\n",
        "  \"\"\"\n",
        "  def __init__(self, n_splits=10, n_test_splits=1, min_train_splits=2, max_train_splits=None):\n",
        "      super().__init__(n_splits)\n",
        "      if not isinstance(n_test_splits, numbers.Integral):\n",
        "          raise ValueError(f\"The number of test folds must be of Integral type. {n_test_splits} of type \"\n",
        "                            f\"{type(n_test_splits)} was passed.\")\n",
        "      n_test_splits = int(n_test_splits)\n",
        "      if n_test_splits <= 0 or n_test_splits >= self.n_splits - 1:\n",
        "          raise ValueError(f\"K-fold cross-validation requires at least one train/test split by setting \"\n",
        "                            f\"n_test_splits between 1 and n_splits - 1, got n_test_splits = {n_test_splits}.\")\n",
        "      self.n_test_splits = n_test_splits\n",
        "\n",
        "      if not isinstance(min_train_splits, numbers.Integral):\n",
        "          raise ValueError(f\"The minimal number of train folds must be of Integral type. {min_train_splits} of type \"\n",
        "                            f\"{type(min_train_splits)} was passed.\")\n",
        "      min_train_splits = int(min_train_splits)\n",
        "      if min_train_splits <= 0 or min_train_splits >= self.n_splits - self.n_test_splits:\n",
        "          raise ValueError(f\"K-fold cross-validation requires at least one train/test split by setting \"\n",
        "                            f\"min_train_splits between 1 and n_splits - n_test_splits, got min_train_splits = \"\n",
        "                            f\"{min_train_splits}.\")\n",
        "      self.min_train_splits = min_train_splits\n",
        "\n",
        "      if max_train_splits is None:\n",
        "          max_train_splits = self.n_splits - self.n_test_splits\n",
        "      if not isinstance(max_train_splits, numbers.Integral):\n",
        "          raise ValueError(f\"The maximal number of train folds must be of Integral type. {max_train_splits} of type \"\n",
        "                            f\"{type(max_train_splits)} was passed.\")\n",
        "      max_train_splits = int(max_train_splits)\n",
        "      if max_train_splits <= 0 or max_train_splits > self.n_splits - self.n_test_splits:\n",
        "          raise ValueError(f\"K-fold cross-validation requires at least one train/test split by setting \"\n",
        "                            f\"max_train_split between 1 and n_splits - n_test_splits, got max_train_split = \"\n",
        "                            f\"{max_train_splits}.\")\n",
        "      self.max_train_splits = max_train_splits\n",
        "      self.fold_bounds = []\n",
        "\n",
        "  def split(self, X: pd.DataFrame, y: pd.Series = None, pred_times: pd.Series = None, eval_times: pd.Series = None,\n",
        "            split_by_time: bool = False) -> Iterable[Tuple[np.ndarray, np.ndarray]]:\n",
        "      \"\"\"\n",
        "      Yield the indices of the train and test sets.\n",
        "      Although the samples are passed in the form of a pandas dataframe, the indices returned are position indices,\n",
        "      not labels.\n",
        "      Parameters\n",
        "      ----------\n",
        "      X : pd.DataFrame, shape (n_samples, n_features), required\n",
        "          Samples. Only used to extract n_samples.\n",
        "      y : pd.Series, not used, inherited from _BaseKFold\n",
        "      pred_times : pd.Series, shape (n_samples,), required\n",
        "          Times at which predictions are made. pred_times.index has to coincide with X.index.\n",
        "      eval_times : pd.Series, shape (n_samples,), required\n",
        "          Times at which the response becomes available and the error can be computed. eval_times.index has to\n",
        "          coincide with X.index.\n",
        "      split_by_time: bool\n",
        "          If False, the folds contain an (approximately) equal number of samples. If True, the folds span identical\n",
        "          time intervals.\n",
        "      Returns\n",
        "      -------\n",
        "      train_indices: np.ndarray\n",
        "          A numpy array containing all the indices in the train set.\n",
        "      test_indices : np.ndarray\n",
        "          A numpy array containing all the indices in the test set.\n",
        "      \"\"\"\n",
        "    \n",
        "      super().split(X, y, pred_times, eval_times)\n",
        "\n",
        "      # Fold boundaries\n",
        "      self.fold_bounds = compute_fold_bounds(self, split_by_time)\n",
        "      \n",
        "      count_folds = 0\n",
        "      for fold_bound in self.fold_bounds:\n",
        "      \n",
        "        if count_folds < self.min_train_splits:\n",
        "            count_folds = count_folds + 1\n",
        "            continue\n",
        "        if self.n_splits - count_folds < self.n_test_splits:\n",
        "            break\n",
        "        # Computes the bounds of the test set, and the corresponding indices\n",
        "        test_indices = self.compute_test_set(fold_bound, count_folds)\n",
        "        # Computes the train set indices\n",
        "        train_indices = self.compute_train_set(fold_bound, count_folds)\n",
        "        \n",
        "        count_folds = count_folds + 1\n",
        "        yield train_indices, test_indices\n",
        "\n",
        "  def compute_train_set(self, fold_bound: int, count_folds: int) -> np.ndarray:\n",
        "      \"\"\"\n",
        "      Compute the position indices of samples in the train set.\n",
        "      Parameters\n",
        "      ----------\n",
        "      fold_bound : int\n",
        "          Bound between the train set and the test set.\n",
        "      count_folds : int\n",
        "          The number (starting at 0) of the first fold in the test set.\n",
        "      Returns\n",
        "      -------\n",
        "      train_indices: np.ndarray\n",
        "          A numpy array containing all the indices in the train set.\n",
        "      \"\"\"\n",
        "      if count_folds > self.max_train_splits:\n",
        "          start_train = self.fold_bounds[count_folds - self.max_train_splits]\n",
        "      else:\n",
        "          start_train = 0\n",
        "      train_indices = np.arange(start_train, fold_bound)\n",
        "      # Purge\n",
        "      train_indices = purge(self, train_indices, fold_bound, self.indices[-1])\n",
        "      return train_indices\n",
        "\n",
        "  def compute_test_set(self, fold_bound: int, count_folds: int) -> np.ndarray:\n",
        "      \"\"\"\n",
        "      Compute the indices of the samples in the test set.\n",
        "      Parameters\n",
        "      ----------\n",
        "      fold_bound : int\n",
        "          Bound between the train set and the test set.\n",
        "      count_folds : int\n",
        "          The number (starting at 0) of the first fold in the test set.\n",
        "      Returns\n",
        "      -------\n",
        "      test_indices: np.ndarray\n",
        "          A numpy array containing the test indices.\n",
        "      \"\"\"\n",
        "      if self.n_splits - count_folds > self.n_test_splits:\n",
        "          end_test = self.fold_bounds[count_folds + self.n_test_splits]\n",
        "      else:\n",
        "          end_test = self.indices[-1] + 1\n",
        "      return np.arange(fold_bound, end_test)\n",
        "\n",
        "\n",
        "class CombPurgedKFoldCV(BaseTimeSeriesCrossValidator):\n",
        "    \"\"\"\n",
        "    Purged and embargoed combinatorial cross-validation\n",
        "    As described in Advances in financial machine learning, Marcos Lopez de Prado, 2018.\n",
        "    The samples are decomposed into n_splits folds containing equal numbers of samples, without shuffling. In each cross\n",
        "    validation round, n_test_splits folds are used as the test set, while the other folds are used as the train set.\n",
        "    There are as many rounds as n_test_splits folds among the n_splits folds.\n",
        "    Each sample should be tagged with a prediction time pred_time and an evaluation time eval_time. The split is such\n",
        "    that the intervals [pred_times, eval_times] associated to samples in the train and test set do not overlap. (The\n",
        "    overlapping samples are dropped.) In addition, an \"embargo\" period is defined, giving the minimal time between an\n",
        "    evaluation time in the test set and a prediction time in the training set. This is to avoid, in the presence of\n",
        "    temporal correlation, a contamination of the test set by the train set.\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_splits : int, default=10\n",
        "        Number of folds. Must be at least 2.\n",
        "    n_test_splits : int, default=2\n",
        "        Number of folds used in the test set. Must be at least 1.\n",
        "    embargo_td : pd.Timedelta, default=0\n",
        "        Embargo period (see explanations above).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_splits=10, n_test_splits=2, embargo_td=0):\n",
        "        super().__init__(n_splits)\n",
        "        if not isinstance(n_test_splits, numbers.Integral):\n",
        "            raise ValueError(f\"The number of test folds must be of Integral type. {n_test_splits} of type \"\n",
        "                             f\"{type(n_test_splits)} was passed.\")\n",
        "        n_test_splits = int(n_test_splits)\n",
        "        if n_test_splits <= 0 or n_test_splits > self.n_splits - 1:\n",
        "            raise ValueError(f\"K-fold cross-validation requires at least one train/test split by setting \"\n",
        "                             f\"n_test_splits between 1 and n_splits - 1, got n_test_splits = {n_test_splits}.\")\n",
        "        self.n_test_splits = n_test_splits\n",
        "\n",
        "        if embargo_td < 0:\n",
        "            raise ValueError(f\"The embargo time should be positive, got embargo = {embargo_td}.\")\n",
        "        self.embargo_td = embargo_td\n",
        "\n",
        "    def split(self, X: pd.DataFrame, y: pd.Series = None,\n",
        "              pred_times: pd.Series = None, eval_times: pd.Series = None) -> Iterable[Tuple[np.ndarray, np.ndarray]]:\n",
        "        \"\"\"\n",
        "        Yield the indices of the train and test sets.\n",
        "        Although the samples are passed in the form of a pandas dataframe, the indices returned are position indices,\n",
        "        not labels.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pd.DataFrame, shape (n_samples, n_features), required\n",
        "            Samples. Only used to extract n_samples.\n",
        "        y : pd.Series, not used, inherited from _BaseKFold\n",
        "        pred_times : pd.Series, shape (n_samples,), required\n",
        "            Times at which predictions are made. pred_times.index has to coincide with X.index.\n",
        "        eval_times : pd.Series, shape (n_samples,), required\n",
        "            Times at which the response becomes available and the error can be computed. eval_times.index has to\n",
        "            coincide with X.index.\n",
        "        Returns\n",
        "        -------\n",
        "        train_indices: np.ndarray\n",
        "            A numpy array containing all the indices in the train set.\n",
        "        test_indices : np.ndarray\n",
        "            A numpy array containing all the indices in the test set.\n",
        "        \"\"\"\n",
        "        super().split(X, y, pred_times, eval_times)\n",
        "\n",
        "        # Fold boundaries\n",
        "        fold_bounds = [(fold[0], fold[-1] + 1) for fold in np.array_split(self.indices, self.n_splits)]\n",
        "        # List of all combinations of n_test_splits folds selected to become test sets\n",
        "        selected_fold_bounds = list(itt.combinations(fold_bounds, self.n_test_splits))\n",
        "        \n",
        "        # In order for the first round to have its whole test set at the end of the dataset\n",
        "        selected_fold_bounds.reverse()\n",
        "\n",
        "        for fold_bound_list in selected_fold_bounds:\n",
        "            # Computes the bounds of the test set, and the corresponding indices\n",
        "            test_fold_bounds, test_indices = self.compute_test_set(fold_bound_list)\n",
        "            # Computes the train set indices\n",
        "            train_indices = self.compute_train_set(test_fold_bounds, test_indices)\n",
        "\n",
        "            yield train_indices, test_indices\n",
        "\n",
        "    def compute_train_set(self, test_fold_bounds: List[Tuple[int, int]], test_indices: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the position indices of samples in the train set.\n",
        "        Parameters\n",
        "        ----------\n",
        "        test_fold_bounds : List of tuples of position indices\n",
        "            Each tuple records the bounds of a block of indices in the test set.\n",
        "        test_indices : np.ndarray\n",
        "            A numpy array containing all the indices in the test set.\n",
        "        Returns\n",
        "        -------\n",
        "        train_indices: np.ndarray\n",
        "            A numpy array containing all the indices in the train set.\n",
        "        \"\"\"\n",
        "        # As a first approximation, the train set is the complement of the test set\n",
        "        train_indices = np.setdiff1d(self.indices, test_indices)\n",
        "        # But we now have to purge and embargo\n",
        "        for test_fold_start, test_fold_end in test_fold_bounds:\n",
        "            # Purge\n",
        "            train_indices = purge(self, train_indices, test_fold_start, test_fold_end)\n",
        "            # Embargo\n",
        "            train_indices = embargo(self, train_indices, test_indices, test_fold_end)\n",
        "        return train_indices\n",
        "\n",
        "    def compute_test_set(self, fold_bound_list: List[Tuple[int, int]]) -> Tuple[List[Tuple[int, int]], np.ndarray]:\n",
        "        \"\"\"\n",
        "        Compute the indices of the samples in the test set.\n",
        "        Parameters\n",
        "        ----------\n",
        "        fold_bound_list: List of tuples of position indices\n",
        "            Each tuple records the bounds of the folds belonging to the test set.\n",
        "        Returns\n",
        "        -------\n",
        "        test_fold_bounds: List of tuples of position indices\n",
        "            Like fold_bound_list, but with the neighboring folds in the test set merged.\n",
        "        test_indices: np.ndarray\n",
        "            A numpy array containing the test indices.\n",
        "        \"\"\"\n",
        "        test_indices = np.empty(0)\n",
        "        test_fold_bounds = []\n",
        "        for fold_start, fold_end in fold_bound_list:\n",
        "            # Records the boundaries of the current test split\n",
        "            if not test_fold_bounds or fold_start != test_fold_bounds[-1][-1]:\n",
        "                test_fold_bounds.append((fold_start, fold_end))\n",
        "            # If the current test split is contiguous to the previous one, simply updates the endpoint\n",
        "            elif fold_start == test_fold_bounds[-1][-1]:\n",
        "                test_fold_bounds[-1] = (test_fold_bounds[-1][0], fold_end)\n",
        "            test_indices = np.union1d(test_indices, self.indices[fold_start:fold_end]).astype(int)\n",
        "        return test_fold_bounds, test_indices\n",
        "\n",
        "\n",
        "def compute_fold_bounds(cv: BaseTimeSeriesCrossValidator, split_by_time: bool) -> List[int]:\n",
        "    \"\"\"\n",
        "    Compute a list containing the fold (left) boundaries.\n",
        "    Parameters\n",
        "    ----------\n",
        "    cv: BaseTimeSeriesCrossValidator\n",
        "        Cross-validation object for which the bounds need to be computed.\n",
        "    split_by_time: bool\n",
        "        If False, the folds contain an (approximately) equal number of samples. If True, the folds span identical\n",
        "        time intervals.\n",
        "    \"\"\"\n",
        "    if split_by_time:\n",
        "        full_time_span = cv.pred_times.max() - cv.pred_times.min()\n",
        "        fold_time_span = full_time_span / cv.n_splits\n",
        "        fold_bounds_times = [cv.pred_times.iloc[0] + fold_time_span * n for n in range(cv.n_splits)]\n",
        "        return cv.pred_times.searchsorted(fold_bounds_times)\n",
        "    else:\n",
        "        return [fold[0] for fold in np.array_split(cv.indices, cv.n_splits)]\n",
        "\n",
        "\n",
        "def embargo(cv: BaseTimeSeriesCrossValidator, train_indices: np.ndarray,\n",
        "            test_indices: np.ndarray, test_fold_end: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Apply the embargo procedure to part of the train set.\n",
        "    This amounts to dropping the train set samples whose prediction time occurs within self.embargo_dt of the test\n",
        "    set sample evaluation times. This method applies the embargo only to the part of the training set immediately\n",
        "    following the end of the test set determined by test_fold_end.\n",
        "    Parameters\n",
        "    ----------\n",
        "    cv: Cross-validation class\n",
        "        Needs to have the attributes cv.pred_times, cv.eval_times, cv.embargo_dt and cv.indices.\n",
        "    train_indices: np.ndarray\n",
        "        A numpy array containing all the indices of the samples currently included in the train set.\n",
        "    test_indices : np.ndarray\n",
        "        A numpy array containing all the indices of the samples in the test set.\n",
        "    test_fold_end : int\n",
        "        Index corresponding to the end of a test set block.\n",
        "    Returns\n",
        "    -------\n",
        "    train_indices: np.ndarray\n",
        "        The same array, with the indices subject to embargo removed.\n",
        "    \"\"\"\n",
        "    if not hasattr(cv, 'embargo_td'):\n",
        "        raise ValueError(\"The passed cross-validation object should have a member cv.embargo_td defining the embargo\"\n",
        "                         \"time.\")\n",
        "    last_test_eval_time = cv.eval_times.iloc[cv.indices[:test_fold_end]].max()\n",
        "    min_train_index = len(cv.pred_times[cv.pred_times <= last_test_eval_time + cv.embargo_td])\n",
        "    if min_train_index < cv.indices.shape[0]:\n",
        "        allowed_indices = np.concatenate((cv.indices[:test_fold_end], cv.indices[min_train_index:]))\n",
        "        train_indices = np.intersect1d(train_indices, allowed_indices)\n",
        "    return train_indices\n",
        "\n",
        "\n",
        "def purge(cv: BaseTimeSeriesCrossValidator, train_indices: np.ndarray,\n",
        "          test_fold_start: int, test_fold_end: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Purge part of the train set.\n",
        "    Given a left boundary index test_fold_start of the test set, this method removes from the train set all the\n",
        "    samples whose evaluation time is posterior to the prediction time of the first test sample after the boundary.\n",
        "    Parameters\n",
        "    ----------\n",
        "    cv: Cross-validation class\n",
        "        Needs to have the attributes cv.pred_times, cv.eval_times and cv.indices.\n",
        "    train_indices: np.ndarray\n",
        "        A numpy array containing all the indices of the samples currently included in the train set.\n",
        "    test_fold_start : int\n",
        "        Index corresponding to the start of a test set block.\n",
        "    test_fold_end : int\n",
        "        Index corresponding to the end of the same test set block.\n",
        "    Returns\n",
        "    -------\n",
        "    train_indices: np.ndarray\n",
        "        A numpy array containing the train indices purged at test_fold_start.\n",
        "    \"\"\"\n",
        "    time_test_fold_start = cv.pred_times.iloc[test_fold_start]\n",
        "    # The train indices before the start of the test fold, purged.\n",
        "    train_indices_1 = np.intersect1d(train_indices, cv.indices[cv.eval_times < time_test_fold_start])\n",
        "    # The train indices after the end of the test fold.\n",
        "    train_indices_2 = np.intersect1d(train_indices, cv.indices[test_fold_end:])\n",
        "    \n",
        "\n",
        "    return np.concatenate((train_indices_1, train_indices_2))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr-SM9jf18St"
      },
      "source": [
        "n_splits=6\n",
        "n_test_splits=1\n",
        "time_gap = 50\n",
        "embargo_td = 0\n",
        "\n",
        "#input_size = len(diff_added_features)\n",
        "\n",
        "cpkf = PurgedWalkForwardCV(n_splits=n_splits, n_test_splits=n_test_splits)\n",
        "\n",
        "t1_ = closing_price.iloc[:3000].index\n",
        "t1 = pd.Series(t1_).shift(time_gap).fillna(0).astype(int)\n",
        "t2 = pd.Series(t1_).shift(-time_gap).fillna(1e12).astype(int)\n",
        "\n",
        "walk_forward_splits = list(cpkf.split(closing_price.iloc[:3000], pred_times=t1, eval_times=t2))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00bB72m3BjAq"
      },
      "source": [
        "n_splits=6\n",
        "n_test_splits=1\n",
        "time_gap = 50\n",
        "embargo_td = 0\n",
        "\n",
        "#input_size = len(diff_added_features)\n",
        "\n",
        "cpkf = CombPurgedKFoldCV(n_splits=n_splits, n_test_splits=n_test_splits, embargo_td=embargo_td)\n",
        "\n",
        "t1_ = closing_price.iloc[:3000].index\n",
        "t1 = pd.Series(t1_).shift(time_gap).fillna(0).astype(int)\n",
        "t2 = pd.Series(t1_).shift(-time_gap).fillna(1e12).astype(int)\n",
        "\n",
        "comb_purged_splits = list(cpkf.split(closing_price.iloc[:3000], pred_times=t1, eval_times=t2))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "zn7o7hdsKEsm",
        "outputId": "5d131857-1fe0-49cf-effe-64f37b68d151"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "cmap_data = plt.cm.Paired\n",
        "cmap_cv = plt.cm.coolwarm\n",
        "\n",
        "fig = plt.figure(figsize=(30,15))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "for ii, split in enumerate(comb_purged_splits):  \n",
        "    indices = np.array([np.nan] * len(closing_price.iloc[:10000]))\n",
        "    indices[split[0]] = 1\n",
        "    indices[split[1]] = 0\n",
        "\n",
        "    ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
        "                       c=indices, marker='_', lw=10, cmap=cmap_cv,\n",
        "                       vmin=-.2, vmax=1.2)\n",
        "\n",
        "yticklabels = list(range(len(comb_purged_splits)))\n",
        "\n",
        "ax.set(yticks=np.arange(len(comb_purged_splits)) + .5, yticklabels=yticklabels, xlabel='Sample index', ylabel=\"CV iteration\")\n",
        "ax.set_title('CombPurgedKFoldCV', fontsize=15)\n",
        "\n",
        "ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))], ['Training set', 'Testing set'], loc=(1.02, .8))\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2160x1080 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvDJ7NsucjYu"
      },
      "source": [
        "def window(df,index,label,step_size):\n",
        "  xtrain = np.array([df[i + step_size - 1:i+step_size*11:step_size] for i in index if (i-1 + 11*step_size) in index])\n",
        "  ytrain = np.array([label[i-1 + 11*step_size] for i in index if (i-1 + 11*step_size) in index])\n",
        "  return xtrain , ytrain"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZgt3ifW-nxj"
      },
      "source": [
        "data = closing_price[:3000]\n",
        "y_t = label[:3000]\n",
        "def prepare_data(data,y_t,splits,step_size):\n",
        "  x_train = []\n",
        "  y_train = []\n",
        "  x_val = []\n",
        "  y_val = []\n",
        "  for i,split in enumerate(splits):\n",
        "    x,y = window(data,split[0],label,step_size)\n",
        "    x = x / np.mean(x,axis = 1).reshape((len(x) , 1))\n",
        "    x = x - np.ones((len(x) ,1))\n",
        "    x =x.reshape(x.shape[0],x.shape[1] , 1)\n",
        "    x_train.append(x)\n",
        "    y = (y + 1)//2\n",
        "    y_train.append(y)\n",
        "    x,y = window(data,split[1],label,step_size)\n",
        "    x = x / np.mean(x,axis = 1).reshape((len(x) , 1))\n",
        "    x = x - np.ones((len(x) ,1))\n",
        "    x =x.reshape(x.shape[0],x.shape[1] , 1)\n",
        "    x_val.append(x)\n",
        "    y = (y + 1)//2\n",
        "    y_val.append(y)\n",
        "  return x_train,y_train,x_val,y_val"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfIBpl9YUTH2"
      },
      "source": [
        "Training data with 2 layer LSTM with hidden size 50 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8cNforDl1S5"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "def train_data(x_train,y_train,x_val ,y_val,flag,lr,layer_size, epoch):\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    model=Sequential()\n",
        "    model.add(tf.compat.v1.keras.layers.CuDNNLSTM(layer_size,return_sequences=True,input_shape=(11,1)))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(tf.compat.v1.keras.layers.CuDNNLSTM(layer_size))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    opt = tf.keras.optimizers.Adam(beta_1=0.9,beta_2=0.999,learning_rate=lr)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    model.save_weights('model.h5')\n",
        "    acc = []\n",
        "    for i,x in enumerate(x_train):\n",
        "      if (flag and i== len(x_train) - 1):\n",
        "        break\n",
        "      model.load_weights('model.h5')\n",
        "      hist = model.fit(x,y_train[i],validation_data=(x_val[i],y_val[i]),epochs=epoch,batch_size=64,verbose=1)\n",
        "      print(hist.history['val_accuracy'][9])\n",
        "      acc.append(hist.history['val_accuracy'][9])\n",
        "    return acc    "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlZ-2scoyWT0"
      },
      "source": [
        "x_train , y_train , x_val ,y_val = prepare_data(data, y_t,comb_purged_splits,1)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjeew-HiytAE",
        "outputId": "92789a47-6306-4906-dd9a-899a87ebf59a"
      },
      "source": [
        "acc = train_data(x_train , y_train , x_val ,y_val,True,0.02,50,30)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "38/38 [==============================] - 2s 13ms/step - loss: 0.6672 - accuracy: 0.6013 - val_loss: 0.6739 - val_accuracy: 0.5306\n",
            "Epoch 2/30\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.5884 - accuracy: 0.7029 - val_loss: 0.6367 - val_accuracy: 0.6531\n",
            "Epoch 3/30\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.5689 - accuracy: 0.7155 - val_loss: 0.5922 - val_accuracy: 0.6980\n",
            "Epoch 4/30\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.5519 - accuracy: 0.7318 - val_loss: 0.5741 - val_accuracy: 0.6816\n",
            "Epoch 5/30\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.5294 - accuracy: 0.7423 - val_loss: 0.5648 - val_accuracy: 0.7224\n",
            "Epoch 6/30\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.5268 - accuracy: 0.7402 - val_loss: 0.5711 - val_accuracy: 0.7163\n",
            "Epoch 7/30\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.5246 - accuracy: 0.7397 - val_loss: 0.5847 - val_accuracy: 0.7143\n",
            "Epoch 8/30\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.5165 - accuracy: 0.7364 - val_loss: 0.5606 - val_accuracy: 0.7143\n",
            "Epoch 9/30\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.5151 - accuracy: 0.7423 - val_loss: 0.6066 - val_accuracy: 0.6449\n",
            "Epoch 10/30\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.5171 - accuracy: 0.7418 - val_loss: 0.5677 - val_accuracy: 0.7306\n",
            "Epoch 11/30\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.5022 - accuracy: 0.7510 - val_loss: 0.5767 - val_accuracy: 0.7102\n",
            "Epoch 12/30\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.4894 - accuracy: 0.7536 - val_loss: 0.5926 - val_accuracy: 0.6939\n",
            "Epoch 13/30\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.4917 - accuracy: 0.7544 - val_loss: 0.5798 - val_accuracy: 0.7143\n",
            "Epoch 14/30\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.4789 - accuracy: 0.7674 - val_loss: 0.5986 - val_accuracy: 0.6878\n",
            "Epoch 15/30\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.4811 - accuracy: 0.7636 - val_loss: 0.5876 - val_accuracy: 0.6857\n",
            "Epoch 16/30\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.4964 - accuracy: 0.7611 - val_loss: 0.6139 - val_accuracy: 0.6857\n",
            "Epoch 17/30\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.4960 - accuracy: 0.7569 - val_loss: 0.5555 - val_accuracy: 0.7163\n",
            "Epoch 18/30\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.4853 - accuracy: 0.7644 - val_loss: 0.6045 - val_accuracy: 0.6673\n",
            "Epoch 19/30\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.4647 - accuracy: 0.7745 - val_loss: 0.5707 - val_accuracy: 0.7224\n",
            "Epoch 20/30\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.4693 - accuracy: 0.7774 - val_loss: 0.6075 - val_accuracy: 0.6714\n",
            "Epoch 21/30\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.4635 - accuracy: 0.7762 - val_loss: 0.5846 - val_accuracy: 0.6918\n",
            "Epoch 22/30\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.4504 - accuracy: 0.7862 - val_loss: 0.5981 - val_accuracy: 0.7184\n",
            "Epoch 23/30\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.4485 - accuracy: 0.7849 - val_loss: 0.6168 - val_accuracy: 0.6592\n",
            "Epoch 24/30\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.4345 - accuracy: 0.7962 - val_loss: 0.6678 - val_accuracy: 0.6612\n",
            "Epoch 25/30\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.4403 - accuracy: 0.7954 - val_loss: 0.6620 - val_accuracy: 0.6796\n",
            "Epoch 26/30\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.4155 - accuracy: 0.8075 - val_loss: 0.6034 - val_accuracy: 0.6898\n",
            "Epoch 27/30\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.4244 - accuracy: 0.8067 - val_loss: 0.6612 - val_accuracy: 0.6776\n",
            "Epoch 28/30\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.4702 - accuracy: 0.7808 - val_loss: 0.6595 - val_accuracy: 0.6673\n",
            "Epoch 29/30\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.4138 - accuracy: 0.8059 - val_loss: 0.6429 - val_accuracy: 0.6714\n",
            "Epoch 30/30\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.4083 - accuracy: 0.8126 - val_loss: 0.6318 - val_accuracy: 0.6776\n",
            "0.7306122183799744\n",
            "Epoch 1/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.6947 - accuracy: 0.5048 - val_loss: 0.6939 - val_accuracy: 0.4735\n",
            "Epoch 2/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.6258 - accuracy: 0.6662 - val_loss: 0.6419 - val_accuracy: 0.6735\n",
            "Epoch 3/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5554 - accuracy: 0.7289 - val_loss: 0.6166 - val_accuracy: 0.6837\n",
            "Epoch 4/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5349 - accuracy: 0.7320 - val_loss: 0.6149 - val_accuracy: 0.6878\n",
            "Epoch 5/30\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.5187 - accuracy: 0.7504 - val_loss: 0.6451 - val_accuracy: 0.5510\n",
            "Epoch 6/30\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.5181 - accuracy: 0.7487 - val_loss: 0.6031 - val_accuracy: 0.6776\n",
            "Epoch 7/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5040 - accuracy: 0.7456 - val_loss: 0.6095 - val_accuracy: 0.6878\n",
            "Epoch 8/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4939 - accuracy: 0.7561 - val_loss: 0.6072 - val_accuracy: 0.6735\n",
            "Epoch 9/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5000 - accuracy: 0.7579 - val_loss: 0.6472 - val_accuracy: 0.6327\n",
            "Epoch 10/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4933 - accuracy: 0.7583 - val_loss: 0.6021 - val_accuracy: 0.6857\n",
            "Epoch 11/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4936 - accuracy: 0.7610 - val_loss: 0.6228 - val_accuracy: 0.6735\n",
            "Epoch 12/30\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.4917 - accuracy: 0.7658 - val_loss: 0.6029 - val_accuracy: 0.6959\n",
            "Epoch 13/30\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.4802 - accuracy: 0.7697 - val_loss: 0.6058 - val_accuracy: 0.6837\n",
            "Epoch 14/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4730 - accuracy: 0.7658 - val_loss: 0.5979 - val_accuracy: 0.7102\n",
            "Epoch 15/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4611 - accuracy: 0.7794 - val_loss: 0.6228 - val_accuracy: 0.6878\n",
            "Epoch 16/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4742 - accuracy: 0.7737 - val_loss: 0.6519 - val_accuracy: 0.6714\n",
            "Epoch 17/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4765 - accuracy: 0.7654 - val_loss: 0.6180 - val_accuracy: 0.6776\n",
            "Epoch 18/30\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.4667 - accuracy: 0.7754 - val_loss: 0.6258 - val_accuracy: 0.6755\n",
            "Epoch 19/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4647 - accuracy: 0.7803 - val_loss: 0.6212 - val_accuracy: 0.6776\n",
            "Epoch 20/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4530 - accuracy: 0.7820 - val_loss: 0.6143 - val_accuracy: 0.6714\n",
            "Epoch 21/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4563 - accuracy: 0.7820 - val_loss: 0.6202 - val_accuracy: 0.6776\n",
            "Epoch 22/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4365 - accuracy: 0.7956 - val_loss: 0.6074 - val_accuracy: 0.6918\n",
            "Epoch 23/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4426 - accuracy: 0.7939 - val_loss: 0.6380 - val_accuracy: 0.6673\n",
            "Epoch 24/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4369 - accuracy: 0.7904 - val_loss: 0.6191 - val_accuracy: 0.6837\n",
            "Epoch 25/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4130 - accuracy: 0.8044 - val_loss: 0.6480 - val_accuracy: 0.6633\n",
            "Epoch 26/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4148 - accuracy: 0.8075 - val_loss: 0.6768 - val_accuracy: 0.6429\n",
            "Epoch 27/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4269 - accuracy: 0.7947 - val_loss: 0.6341 - val_accuracy: 0.6816\n",
            "Epoch 28/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4206 - accuracy: 0.8000 - val_loss: 0.6442 - val_accuracy: 0.6714\n",
            "Epoch 29/30\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.3890 - accuracy: 0.8202 - val_loss: 0.6662 - val_accuracy: 0.6449\n",
            "Epoch 30/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.3776 - accuracy: 0.8259 - val_loss: 0.6339 - val_accuracy: 0.6796\n",
            "0.6857143044471741\n",
            "Epoch 1/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.6856 - accuracy: 0.5487 - val_loss: 0.6088 - val_accuracy: 0.7143\n",
            "Epoch 2/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.6965 - accuracy: 0.5456 - val_loss: 0.7008 - val_accuracy: 0.3959\n",
            "Epoch 3/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.6786 - accuracy: 0.5987 - val_loss: 0.6199 - val_accuracy: 0.6653\n",
            "Epoch 4/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5937 - accuracy: 0.6886 - val_loss: 0.6025 - val_accuracy: 0.6776\n",
            "Epoch 5/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5461 - accuracy: 0.7421 - val_loss: 0.5529 - val_accuracy: 0.7306\n",
            "Epoch 6/30\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.5354 - accuracy: 0.7338 - val_loss: 0.5956 - val_accuracy: 0.6918\n",
            "Epoch 7/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5285 - accuracy: 0.7390 - val_loss: 0.5863 - val_accuracy: 0.6837\n",
            "Epoch 8/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5373 - accuracy: 0.7307 - val_loss: 0.5502 - val_accuracy: 0.7306\n",
            "Epoch 9/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5080 - accuracy: 0.7452 - val_loss: 0.5554 - val_accuracy: 0.7224\n",
            "Epoch 10/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4975 - accuracy: 0.7518 - val_loss: 0.5967 - val_accuracy: 0.6673\n",
            "Epoch 11/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4824 - accuracy: 0.7539 - val_loss: 0.5984 - val_accuracy: 0.6755\n",
            "Epoch 12/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4756 - accuracy: 0.7693 - val_loss: 0.5915 - val_accuracy: 0.6755\n",
            "Epoch 13/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4764 - accuracy: 0.7684 - val_loss: 0.5813 - val_accuracy: 0.6878\n",
            "Epoch 14/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4696 - accuracy: 0.7724 - val_loss: 0.6383 - val_accuracy: 0.6449\n",
            "Epoch 15/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4591 - accuracy: 0.7811 - val_loss: 0.6208 - val_accuracy: 0.6796\n",
            "Epoch 16/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4648 - accuracy: 0.7741 - val_loss: 0.6341 - val_accuracy: 0.6408\n",
            "Epoch 17/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4687 - accuracy: 0.7750 - val_loss: 0.5640 - val_accuracy: 0.7245\n",
            "Epoch 18/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4670 - accuracy: 0.7746 - val_loss: 0.6504 - val_accuracy: 0.6367\n",
            "Epoch 19/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4513 - accuracy: 0.7877 - val_loss: 0.5703 - val_accuracy: 0.7306\n",
            "Epoch 20/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4412 - accuracy: 0.8035 - val_loss: 0.6604 - val_accuracy: 0.6837\n",
            "Epoch 21/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4409 - accuracy: 0.7789 - val_loss: 0.6048 - val_accuracy: 0.6816\n",
            "Epoch 22/30\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.4388 - accuracy: 0.7917 - val_loss: 0.6115 - val_accuracy: 0.6918\n",
            "Epoch 23/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4220 - accuracy: 0.7987 - val_loss: 0.6849 - val_accuracy: 0.6551\n",
            "Epoch 24/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4192 - accuracy: 0.8000 - val_loss: 0.6284 - val_accuracy: 0.6735\n",
            "Epoch 25/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4155 - accuracy: 0.8026 - val_loss: 0.6384 - val_accuracy: 0.6551\n",
            "Epoch 26/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4332 - accuracy: 0.7947 - val_loss: 0.6570 - val_accuracy: 0.6898\n",
            "Epoch 27/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4085 - accuracy: 0.8083 - val_loss: 0.7269 - val_accuracy: 0.6735\n",
            "Epoch 28/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.3987 - accuracy: 0.8175 - val_loss: 0.6767 - val_accuracy: 0.6837\n",
            "Epoch 29/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.3889 - accuracy: 0.8202 - val_loss: 0.7005 - val_accuracy: 0.6980\n",
            "Epoch 30/30\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.4163 - accuracy: 0.8088 - val_loss: 0.7134 - val_accuracy: 0.6347\n",
            "0.6673469543457031\n",
            "Epoch 1/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.6954 - accuracy: 0.5083 - val_loss: 0.6830 - val_accuracy: 0.5857\n",
            "Epoch 2/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.6865 - accuracy: 0.5531 - val_loss: 0.6711 - val_accuracy: 0.5857\n",
            "Epoch 3/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.6915 - accuracy: 0.5364 - val_loss: 0.7480 - val_accuracy: 0.4143\n",
            "Epoch 4/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.6952 - accuracy: 0.4996 - val_loss: 0.7077 - val_accuracy: 0.4184\n",
            "Epoch 5/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.6672 - accuracy: 0.5904 - val_loss: 0.5826 - val_accuracy: 0.7102\n",
            "Epoch 6/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.6049 - accuracy: 0.6825 - val_loss: 0.5793 - val_accuracy: 0.7306\n",
            "Epoch 7/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5647 - accuracy: 0.7026 - val_loss: 0.5664 - val_accuracy: 0.7122\n",
            "Epoch 8/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5610 - accuracy: 0.7140 - val_loss: 0.6313 - val_accuracy: 0.6653\n",
            "Epoch 9/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5517 - accuracy: 0.7197 - val_loss: 0.5677 - val_accuracy: 0.7184\n",
            "Epoch 10/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5597 - accuracy: 0.7202 - val_loss: 0.5525 - val_accuracy: 0.7327\n",
            "Epoch 11/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5538 - accuracy: 0.7289 - val_loss: 0.5657 - val_accuracy: 0.7204\n",
            "Epoch 12/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5464 - accuracy: 0.7228 - val_loss: 0.5458 - val_accuracy: 0.7184\n",
            "Epoch 13/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5379 - accuracy: 0.7285 - val_loss: 0.5479 - val_accuracy: 0.7327\n",
            "Epoch 14/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5364 - accuracy: 0.7364 - val_loss: 0.5591 - val_accuracy: 0.6959\n",
            "Epoch 15/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5318 - accuracy: 0.7254 - val_loss: 0.7134 - val_accuracy: 0.6020\n",
            "Epoch 16/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5307 - accuracy: 0.7382 - val_loss: 0.5732 - val_accuracy: 0.7000\n",
            "Epoch 17/30\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.5171 - accuracy: 0.7487 - val_loss: 0.6318 - val_accuracy: 0.6796\n",
            "Epoch 18/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5144 - accuracy: 0.7434 - val_loss: 0.6462 - val_accuracy: 0.6061\n",
            "Epoch 19/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5095 - accuracy: 0.7478 - val_loss: 0.6162 - val_accuracy: 0.6694\n",
            "Epoch 20/30\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.5039 - accuracy: 0.7496 - val_loss: 0.6590 - val_accuracy: 0.6429\n",
            "Epoch 21/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4963 - accuracy: 0.7592 - val_loss: 0.5864 - val_accuracy: 0.7000\n",
            "Epoch 22/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5049 - accuracy: 0.7408 - val_loss: 0.6451 - val_accuracy: 0.6694\n",
            "Epoch 23/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5040 - accuracy: 0.7518 - val_loss: 0.5910 - val_accuracy: 0.6837\n",
            "Epoch 24/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4885 - accuracy: 0.7579 - val_loss: 0.6723 - val_accuracy: 0.6429\n",
            "Epoch 25/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4738 - accuracy: 0.7640 - val_loss: 0.7601 - val_accuracy: 0.6122\n",
            "Epoch 26/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4915 - accuracy: 0.7623 - val_loss: 0.6320 - val_accuracy: 0.6612\n",
            "Epoch 27/30\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.4934 - accuracy: 0.7667 - val_loss: 0.5834 - val_accuracy: 0.7102\n",
            "Epoch 28/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4967 - accuracy: 0.7553 - val_loss: 0.5876 - val_accuracy: 0.7122\n",
            "Epoch 29/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5035 - accuracy: 0.7518 - val_loss: 0.6604 - val_accuracy: 0.6673\n",
            "Epoch 30/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4799 - accuracy: 0.7706 - val_loss: 0.7240 - val_accuracy: 0.6388\n",
            "0.7326530814170837\n",
            "Epoch 1/30\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.6729 - accuracy: 0.6154 - val_loss: 0.7881 - val_accuracy: 0.2857\n",
            "Epoch 2/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.6665 - accuracy: 0.6140 - val_loss: 0.8097 - val_accuracy: 0.2857\n",
            "Epoch 3/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.6861 - accuracy: 0.5917 - val_loss: 0.8308 - val_accuracy: 0.2857\n",
            "Epoch 4/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.6367 - accuracy: 0.6338 - val_loss: 1.1145 - val_accuracy: 0.3510\n",
            "Epoch 5/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.6645 - accuracy: 0.6075 - val_loss: 0.7395 - val_accuracy: 0.4122\n",
            "Epoch 6/30\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.6090 - accuracy: 0.6776 - val_loss: 0.6523 - val_accuracy: 0.7020\n",
            "Epoch 7/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5782 - accuracy: 0.6961 - val_loss: 0.6017 - val_accuracy: 0.7082\n",
            "Epoch 8/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5700 - accuracy: 0.7110 - val_loss: 0.5905 - val_accuracy: 0.7041\n",
            "Epoch 9/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5620 - accuracy: 0.7215 - val_loss: 0.5178 - val_accuracy: 0.7694\n",
            "Epoch 10/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5638 - accuracy: 0.7145 - val_loss: 0.5222 - val_accuracy: 0.7469\n",
            "Epoch 11/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5514 - accuracy: 0.7298 - val_loss: 0.4812 - val_accuracy: 0.7980\n",
            "Epoch 12/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5497 - accuracy: 0.7250 - val_loss: 0.6001 - val_accuracy: 0.7204\n",
            "Epoch 13/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5508 - accuracy: 0.7193 - val_loss: 0.6245 - val_accuracy: 0.7082\n",
            "Epoch 14/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5459 - accuracy: 0.7250 - val_loss: 0.5747 - val_accuracy: 0.7122\n",
            "Epoch 15/30\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.5369 - accuracy: 0.7303 - val_loss: 0.5016 - val_accuracy: 0.7592\n",
            "Epoch 16/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5318 - accuracy: 0.7390 - val_loss: 0.5977 - val_accuracy: 0.6776\n",
            "Epoch 17/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5334 - accuracy: 0.7311 - val_loss: 0.6218 - val_accuracy: 0.6837\n",
            "Epoch 18/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5222 - accuracy: 0.7289 - val_loss: 0.4905 - val_accuracy: 0.7857\n",
            "Epoch 19/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5293 - accuracy: 0.7311 - val_loss: 0.5145 - val_accuracy: 0.7837\n",
            "Epoch 20/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5278 - accuracy: 0.7325 - val_loss: 0.5540 - val_accuracy: 0.7041\n",
            "Epoch 21/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5324 - accuracy: 0.7250 - val_loss: 0.5407 - val_accuracy: 0.7510\n",
            "Epoch 22/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5266 - accuracy: 0.7342 - val_loss: 0.6130 - val_accuracy: 0.6980\n",
            "Epoch 23/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5147 - accuracy: 0.7412 - val_loss: 0.4951 - val_accuracy: 0.7735\n",
            "Epoch 24/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5201 - accuracy: 0.7311 - val_loss: 0.5599 - val_accuracy: 0.7510\n",
            "Epoch 25/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5176 - accuracy: 0.7386 - val_loss: 0.5203 - val_accuracy: 0.7469\n",
            "Epoch 26/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5131 - accuracy: 0.7386 - val_loss: 0.6182 - val_accuracy: 0.6939\n",
            "Epoch 27/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5091 - accuracy: 0.7417 - val_loss: 0.5782 - val_accuracy: 0.7224\n",
            "Epoch 28/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5030 - accuracy: 0.7491 - val_loss: 0.6158 - val_accuracy: 0.6857\n",
            "Epoch 29/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4968 - accuracy: 0.7531 - val_loss: 0.5361 - val_accuracy: 0.7408\n",
            "Epoch 30/30\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4968 - accuracy: 0.7553 - val_loss: 0.5908 - val_accuracy: 0.7184\n",
            "0.7469387650489807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_78qrcPXoVVy",
        "outputId": "908d4f56-acdb-458d-c096-26193b613009"
      },
      "source": [
        "np.mean(acc)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7126530647277832"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbMuV7YfkVyA",
        "outputId": "8736d796-0010-4e51-b11c-510ee8768bd3"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  model=Sequential()\n",
        "  model.add(tf.compat.v1.keras.layers.CuDNNLSTM(50,return_sequences=True,input_shape=(11,1)))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(tf.compat.v1.keras.layers.CuDNNLSTM(50))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  opt = tf.keras.optimizers.Adam(beta_1=0.9,beta_2=0.999,learning_rate=0.02)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  model.fit(x_train_full ,y_train_full,epochs=30,batch_size=64,verbose=1 )"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "48/48 [==============================] - 2s 5ms/step - loss: 0.6893 - accuracy: 0.5742\n",
            "Epoch 2/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6456 - accuracy: 0.6224\n",
            "Epoch 3/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6137 - accuracy: 0.6782\n",
            "Epoch 4/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5707 - accuracy: 0.7200\n",
            "Epoch 5/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5512 - accuracy: 0.7310\n",
            "Epoch 6/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5489 - accuracy: 0.7200\n",
            "Epoch 7/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5458 - accuracy: 0.7267\n",
            "Epoch 8/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5329 - accuracy: 0.7307\n",
            "Epoch 9/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5324 - accuracy: 0.7366\n",
            "Epoch 10/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5226 - accuracy: 0.7326\n",
            "Epoch 11/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5287 - accuracy: 0.7340\n",
            "Epoch 12/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5192 - accuracy: 0.7406\n",
            "Epoch 13/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5193 - accuracy: 0.7416\n",
            "Epoch 14/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5081 - accuracy: 0.7459\n",
            "Epoch 15/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5072 - accuracy: 0.7466\n",
            "Epoch 16/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5090 - accuracy: 0.7416\n",
            "Epoch 17/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5089 - accuracy: 0.7409\n",
            "Epoch 18/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5036 - accuracy: 0.7549\n",
            "Epoch 19/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4984 - accuracy: 0.7586\n",
            "Epoch 20/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4884 - accuracy: 0.7629\n",
            "Epoch 21/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5536 - accuracy: 0.7217\n",
            "Epoch 22/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5111 - accuracy: 0.7489\n",
            "Epoch 23/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4902 - accuracy: 0.7502\n",
            "Epoch 24/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4962 - accuracy: 0.7546\n",
            "Epoch 25/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4928 - accuracy: 0.7595\n",
            "Epoch 26/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4873 - accuracy: 0.7612\n",
            "Epoch 27/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4881 - accuracy: 0.7632\n",
            "Epoch 28/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4916 - accuracy: 0.7576\n",
            "Epoch 29/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4735 - accuracy: 0.7778\n",
            "Epoch 30/30\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4716 - accuracy: 0.7715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8l2hfr1rq52"
      },
      "source": [
        "x_test=np.array(closing_price[3011:])\n",
        "y_test=np.array(label[3011:])\n",
        "index = np.arange(len(closing_price) - 3011)\n",
        "x_test,y_test = window(x_test,index , y_test,1)\n",
        "\n",
        "x_test = x_test / np.mean(x_test,axis = 1).reshape((len(x_test) , 1))\n",
        "x_test = x_test - np.ones((len(x_test) ,1))\n",
        "x_test =x_test.reshape(x_test.shape[0],x_test.shape[1] , 1)\n",
        "y_test = (y_test + 1)//2"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgtCVqpjmDFm",
        "outputId": "3359f0ec-0ad8-48b8-febb-440593742fee"
      },
      "source": [
        "model.evaluate(x_test,y_test)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120/120 [==============================] - 0s 3ms/step - loss: 0.5955 - accuracy: 0.6947\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5955130457878113, 0.6946545243263245]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8jL326Gpp7g",
        "outputId": "1665dd87-b30b-45b7-b04d-d35d32f7dc42"
      },
      "source": [
        "predict = model.predict_classes(x_test)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9BB0letpvEY",
        "outputId": "60b6397a-3703-456b-ec53-dd2d22f9e711"
      },
      "source": [
        "buy_and_hold(np.array(closing_price[3011:]) ,predict[ : , 0],0 )"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.767116834170854"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24ct8VT8y9oV"
      },
      "source": [
        "x_train , y_train , x_val ,y_val = prepare_data(data, y_t,walk_forward_splits)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSXkVUC1zFjO",
        "outputId": "c8ab024a-de22-4593-b5bd-7cf80056a41a"
      },
      "source": [
        "acc = train_data(x_train , y_train , x_val ,y_val,False)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6632652878761292\n",
            "0.6714285612106323\n",
            "0.6510204076766968\n",
            "0.6571428775787354\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgcBhB_7UccF"
      },
      "source": [
        "Training data with 2 layer GRU with hidden size 50 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAaqteGXRZ9i",
        "outputId": "dfb901cd-cb22-4b0c-fdbb-d55bdc5d7180"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  model=Sequential()\n",
        "  model.add(tf.compat.v1.keras.layers.CuDNNGRU(50,return_sequences=True,input_shape=(11,1)))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(tf.compat.v1.keras.layers.CuDNNGRU(50))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  opt = tf.keras.optimizers.Adam(beta_1=0.9,beta_2=0.999,learning_rate=0.02)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  model.save_weights('model.h5')\n",
        "  acc = []\n",
        "  for i,x in enumerate(x_train):\n",
        "    model.load_weights('model.h5')\n",
        "    hist = model.fit(x,y_train[i],validation_data=(x_val[i],y_val[i]),epochs=10,batch_size=64,verbose=0)\n",
        "    print(hist.history['val_accuracy'][9])\n",
        "    acc.append(hist.history['val_accuracy'][9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6877551078796387\n",
            "0.6836734414100647\n",
            "0.6836734414100647\n",
            "0.6877551078796387\n",
            "0.7122448682785034\n",
            "0.8204081654548645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAypX918oo_l",
        "outputId": "fe991128-d972-4682-cdd3-5e961414ce9a"
      },
      "source": [
        "np.mean(acc\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7125850220521291"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh43s6vGUkDF"
      },
      "source": [
        "#My approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW4qElevUx-J"
      },
      "source": [
        "sampling form 44 previous data with sample size 11 and equal distance between samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBFM2y1Q3eDr"
      },
      "source": [
        "expand_data = np.array([closing_price[i:i+44] for i in range(len(closing_price) - 43) ])\n",
        "\n",
        "\n",
        "#x_df = np.array([np.convolve(x,np.ones(4)/4,mode='valid') for x in x_df])\n",
        "final_data_x = np.array([np.array([x[i] for i in range(3,44,4)]) for x in expand_data])\n",
        "final_data_x = final_data_x / np.mean(final_data_x,axis = 1).reshape((len(final_data_x) , 1))\n",
        "final_data_x = final_data_x - np.ones((len(final_data_x) ,1))\n",
        "\n",
        "final_data_y = label[43:]\n",
        "\n",
        "\n",
        "x_train = final_data_x[0:3011]\n",
        "y_train = final_data_y[0:3011]\n",
        "x_test= final_data_x[3011:]\n",
        "y_test = final_data_y[3011:]\n",
        "\n",
        "\n",
        "y_train = (y_train + 1)//2\n",
        "y_test = (y_test + 1)//2\n",
        "\n",
        "\n",
        "x_train =x_train.reshape(x_train.shape[0],x_train.shape[1] , 1)\n",
        "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1] , 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G49PiUxncGeZ",
        "outputId": "5d04b06b-ab7a-41f9-d944-46dd1cefc526"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  model=Sequential()\n",
        "  model.add(tf.compat.v1.keras.layers.CuDNNGRU(50,return_sequences=True,input_shape=(11,1)))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(tf.compat.v1.keras.layers.CuDNNGRU(50))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  opt = tf.keras.optimizers.Adam(beta_1=0.9,beta_2=0.999,learning_rate=0.02)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=20,batch_size=64,verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "48/48 - 2s - loss: 0.6129 - accuracy: 0.6533 - val_loss: 0.5397 - val_accuracy: 0.7354\n",
            "Epoch 2/20\n",
            "48/48 - 0s - loss: 0.5206 - accuracy: 0.7386 - val_loss: 0.5497 - val_accuracy: 0.7038\n",
            "Epoch 3/20\n",
            "48/48 - 0s - loss: 0.4904 - accuracy: 0.7669 - val_loss: 0.5291 - val_accuracy: 0.7275\n",
            "Epoch 4/20\n",
            "48/48 - 0s - loss: 0.4736 - accuracy: 0.7795 - val_loss: 0.5497 - val_accuracy: 0.7007\n",
            "Epoch 5/20\n",
            "48/48 - 0s - loss: 0.4541 - accuracy: 0.7924 - val_loss: 0.5881 - val_accuracy: 0.6781\n",
            "Epoch 6/20\n",
            "48/48 - 0s - loss: 0.4669 - accuracy: 0.7778 - val_loss: 0.5355 - val_accuracy: 0.7275\n",
            "Epoch 7/20\n",
            "48/48 - 0s - loss: 0.4432 - accuracy: 0.8067 - val_loss: 0.5858 - val_accuracy: 0.6996\n",
            "Epoch 8/20\n",
            "48/48 - 0s - loss: 0.4420 - accuracy: 0.7921 - val_loss: 0.5690 - val_accuracy: 0.7151\n",
            "Epoch 9/20\n",
            "48/48 - 0s - loss: 0.4252 - accuracy: 0.8084 - val_loss: 0.5494 - val_accuracy: 0.7207\n",
            "Epoch 10/20\n",
            "48/48 - 0s - loss: 0.4279 - accuracy: 0.8044 - val_loss: 0.5702 - val_accuracy: 0.6912\n",
            "Epoch 11/20\n",
            "48/48 - 0s - loss: 0.4031 - accuracy: 0.8134 - val_loss: 0.6114 - val_accuracy: 0.6931\n",
            "Epoch 12/20\n",
            "48/48 - 0s - loss: 0.4159 - accuracy: 0.8074 - val_loss: 0.5737 - val_accuracy: 0.6938\n",
            "Epoch 13/20\n",
            "48/48 - 0s - loss: 0.4522 - accuracy: 0.7881 - val_loss: 0.5973 - val_accuracy: 0.6886\n",
            "Epoch 14/20\n",
            "48/48 - 0s - loss: 0.4310 - accuracy: 0.7961 - val_loss: 0.6076 - val_accuracy: 0.6975\n",
            "Epoch 15/20\n",
            "48/48 - 0s - loss: 0.4541 - accuracy: 0.7851 - val_loss: 0.6051 - val_accuracy: 0.6920\n",
            "Epoch 16/20\n",
            "48/48 - 0s - loss: 0.4386 - accuracy: 0.7934 - val_loss: 0.5723 - val_accuracy: 0.6902\n",
            "Epoch 17/20\n",
            "48/48 - 0s - loss: 0.4652 - accuracy: 0.7841 - val_loss: 0.5464 - val_accuracy: 0.7186\n",
            "Epoch 18/20\n",
            "48/48 - 0s - loss: 0.4182 - accuracy: 0.8153 - val_loss: 0.5654 - val_accuracy: 0.7241\n",
            "Epoch 19/20\n",
            "48/48 - 0s - loss: 0.4438 - accuracy: 0.7928 - val_loss: 0.5718 - val_accuracy: 0.6923\n",
            "Epoch 20/20\n",
            "48/48 - 0s - loss: 0.4636 - accuracy: 0.7781 - val_loss: 0.5375 - val_accuracy: 0.7307\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfp915AjGUmh"
      },
      "source": [
        "x_train = final_data_x[0:3011]\n",
        "y_train = final_data_y[0:3011]\n",
        "x_test= final_data_x[3011:]\n",
        "y_test = final_data_y[3011:]\n",
        "\n",
        "fea = x_train.shape[1]\n",
        "rn = np.random.rand(fea,50)\n",
        "x_train = np.dot(x_train,rn)\n",
        "x_test = np.dot(x_test,rn)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrOxi8gXHdLI",
        "outputId": "e5041b73-8d9b-431b-9979-5c741b812a3b"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3011, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n4vuSf8Hbk_"
      },
      "source": [
        "y_train = (y_train + 1)//2\n",
        "y_test = (y_test + 1)//2\n",
        "\n",
        "\n",
        "x_train =x_train.reshape(x_train.shape[0],x_train.shape[1] , 1)\n",
        "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1] , 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekxvhC_IGprq",
        "outputId": "2ac6b980-e6fb-4519-827d-9bc7a0a20fc1"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  model=Sequential()\n",
        "  model.add(tf.compat.v1.keras.layers.CuDNNGRU(50,return_sequences=True,input_shape=(11,1)))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(tf.compat.v1.keras.layers.CuDNNGRU(50))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  opt = tf.keras.optimizers.Adam(beta_1=0.9,beta_2=0.999,learning_rate=0.02)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=20,batch_size=64,verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 11, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 11, 1), dtype=tf.float32, name='cu_dnngru_8_input'), name='cu_dnngru_8_input', description=\"created by layer 'cu_dnngru_8_input'\"), but it was called on an input with incompatible shape (None, 50, 1).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 11, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 11, 1), dtype=tf.float32, name='cu_dnngru_8_input'), name='cu_dnngru_8_input', description=\"created by layer 'cu_dnngru_8_input'\"), but it was called on an input with incompatible shape (None, 50, 1).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 11, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 11, 1), dtype=tf.float32, name='cu_dnngru_8_input'), name='cu_dnngru_8_input', description=\"created by layer 'cu_dnngru_8_input'\"), but it was called on an input with incompatible shape (None, 50, 1).\n",
            "48/48 - 2s - loss: 0.6680 - accuracy: 0.5991 - val_loss: 0.6534 - val_accuracy: 0.6273\n",
            "Epoch 2/20\n",
            "48/48 - 0s - loss: 0.5865 - accuracy: 0.7001 - val_loss: 0.5865 - val_accuracy: 0.6986\n",
            "Epoch 3/20\n",
            "48/48 - 0s - loss: 0.5490 - accuracy: 0.7240 - val_loss: 0.5739 - val_accuracy: 0.6928\n",
            "Epoch 4/20\n",
            "48/48 - 0s - loss: 0.5238 - accuracy: 0.7476 - val_loss: 0.5567 - val_accuracy: 0.7154\n",
            "Epoch 5/20\n",
            "48/48 - 0s - loss: 0.5287 - accuracy: 0.7522 - val_loss: 0.5540 - val_accuracy: 0.7307\n",
            "Epoch 6/20\n",
            "48/48 - 0s - loss: 0.5235 - accuracy: 0.7400 - val_loss: 0.6425 - val_accuracy: 0.7186\n",
            "Epoch 7/20\n",
            "48/48 - 0s - loss: 0.5110 - accuracy: 0.7559 - val_loss: 0.5431 - val_accuracy: 0.7299\n",
            "Epoch 8/20\n",
            "48/48 - 0s - loss: 0.4976 - accuracy: 0.7635 - val_loss: 0.5598 - val_accuracy: 0.7059\n",
            "Epoch 9/20\n",
            "48/48 - 0s - loss: 0.5048 - accuracy: 0.7599 - val_loss: 0.5390 - val_accuracy: 0.7249\n",
            "Epoch 10/20\n",
            "48/48 - 0s - loss: 0.5036 - accuracy: 0.7635 - val_loss: 0.5947 - val_accuracy: 0.6733\n",
            "Epoch 11/20\n",
            "48/48 - 0s - loss: 0.4841 - accuracy: 0.7742 - val_loss: 0.5541 - val_accuracy: 0.7233\n",
            "Epoch 12/20\n",
            "48/48 - 0s - loss: 0.5065 - accuracy: 0.7562 - val_loss: 0.5631 - val_accuracy: 0.7012\n",
            "Epoch 13/20\n",
            "48/48 - 0s - loss: 0.4994 - accuracy: 0.7652 - val_loss: 0.5394 - val_accuracy: 0.7370\n",
            "Epoch 14/20\n",
            "48/48 - 0s - loss: 0.4905 - accuracy: 0.7688 - val_loss: 0.5577 - val_accuracy: 0.7238\n",
            "Epoch 15/20\n",
            "48/48 - 0s - loss: 0.6571 - accuracy: 0.6244 - val_loss: 0.6350 - val_accuracy: 0.5957\n",
            "Epoch 16/20\n",
            "48/48 - 0s - loss: 0.6500 - accuracy: 0.6270 - val_loss: 0.6571 - val_accuracy: 0.6186\n",
            "Epoch 17/20\n",
            "48/48 - 0s - loss: 0.6795 - accuracy: 0.5945 - val_loss: 0.6900 - val_accuracy: 0.5839\n",
            "Epoch 18/20\n",
            "48/48 - 0s - loss: 0.6841 - accuracy: 0.5942 - val_loss: 0.5994 - val_accuracy: 0.6923\n",
            "Epoch 19/20\n",
            "48/48 - 0s - loss: 0.6341 - accuracy: 0.6486 - val_loss: 0.6250 - val_accuracy: 0.6552\n",
            "Epoch 20/20\n",
            "48/48 - 0s - loss: 0.6385 - accuracy: 0.6496 - val_loss: 0.5905 - val_accuracy: 0.7123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "h-dHU1M3KI-s",
        "outputId": "2aa15372-ff1f-48d9-dc48-0121b1db54dd"
      },
      "source": [
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred = np.array([1 if y >=0.5 else 0 for y in y_pred])\n",
        "print(\"Accuracy:{:.6f}\".format(metrics.accuracy_score(y_test, y_pred)))\n",
        "print(\"Precision:{:.6f}\".format(metrics.precision_score(y_test, y_pred)))\n",
        "print(\"Recall:{:.6f}\".format(metrics.recall_score(y_test, y_pred)))\n",
        "print(\"F1 score:{:.6f}\".format(metrics.f1_score(y_test, y_pred)))\n",
        "    \n",
        "# Print Confusion Matrix\n",
        "confusion_matrix = pd.DataFrame(metrics.confusion_matrix(y_test, y_pred)) \n",
        "sns.heatmap(confusion_matrix, annot=True, cmap=\"Blues\");"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:0.712257\n",
            "Precision:0.752739\n",
            "Recall:0.699467\n",
            "F1 score:0.725126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJDMe69vKZyg",
        "outputId": "738f99c6-885f-40c2-9b97-991c0af41f3c"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.67370343],\n",
              "       [0.6712601 ],\n",
              "       [0.6663634 ],\n",
              "       ...,\n",
              "       [0.43360442],\n",
              "       [0.4261445 ],\n",
              "       [0.41190097]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaHVaZsVdFLi"
      },
      "source": [
        "def  buy_and_hold(pric , pred , index):\n",
        "  i = 0\n",
        "  for i,p in enumerate(pred):\n",
        "    if p == 1 :\n",
        "      break\n",
        "  value = pric[index + i]\n",
        "  a = 0 \n",
        "  a=value\n",
        "  i +=1\n",
        "  for j in range(len(pred) - i):\n",
        "    if pred[i + j] == 1:\n",
        "      value = pric[i+j + index]\n",
        "  return value / a  \n",
        "\n",
        "\n"
      ],
      "execution_count": 56,
      "outputs": []
    }
  ]
}