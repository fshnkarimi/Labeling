{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Time_Series_Labelling_Predictions.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jR3SjOL7m0gJ"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fshnkarimi/Labeling/blob/main/Time_Series_Labelling_Predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMURxUHNk5sf",
        "outputId": "eaeeea89-93ed-4db5-a3e6-65ad76f6e70e"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR3SjOL7m0gJ"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wURP0Jxui8-d"
      },
      "source": [
        "# Import Libraries \n",
        "import itertools as itt\n",
        "import numbers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from abc import abstractmethod\n",
        "from typing import Iterable, Tuple, List\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import plotly.graph_objects as go\n",
        "import xgboost as xgb\n",
        "from prettytable import PrettyTable\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.signal import butter, lfilter, freqz\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.neighbors import KNeighborsRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__9u9qrOj01l"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqwGOVvDRqvH"
      },
      "source": [
        "# Labelling algorithm X is price and th is our threshould\n",
        "\n",
        "def labeling(X, th):\n",
        "    n = len(X)\n",
        "    y = np.array([0 for i in range(n)])\n",
        "    FP = X[0]\n",
        "    xh = X[0]\n",
        "    xl = X[0]\n",
        "    HT = 0\n",
        "    LT = 0\n",
        "    cid = 0\n",
        "    FP_N = 0\n",
        "    for i in range(n):\n",
        "        if(X[i] > FP + X[0]*th):\n",
        "            xh, HT, FP_N, cid  = X[i], i, i, 1\n",
        "            break\n",
        "        if(X[i] < FP - X[0]*th):\n",
        "            xh,HT,FP_N,cid  = X[i],i,i,-1\n",
        "            break\n",
        "    for i in range(FP_N+1,n):\n",
        "        if(cid > 0):\n",
        "            if(X[i]>xh):\n",
        "                xh, HT = X[i], i\n",
        "            if(X[i] < xh - xh * th and LT<= HT):\n",
        "                for j in range(n):\n",
        "                    if(j > LT and j <= HT):\n",
        "                        y[j] = 1\n",
        "                xl, LT, cid = X[i], i, -1\n",
        "        if(cid < 0):\n",
        "            if(X[i] < xl):\n",
        "                xl, LT = X[i], i\n",
        "            if(X[i] > xl + xl * th and HT <= LT):\n",
        "                for j in range(n):\n",
        "                    if(j > HT and j <= LT):\n",
        "                        y[j] = 0\n",
        "                xh, HT, cid = X[i], i, 1\n",
        "    return y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDOBqAAqdWXr"
      },
      "source": [
        "# Implement Cross-validation methods\n",
        "\n",
        "class BaseTimeSeriesCrossValidator:\n",
        "    \"\"\"\n",
        "    Abstract class for time series cross-validation.\n",
        "    Time series cross-validation requires each sample has a prediction time pred_time, at which the features are used to\n",
        "    predict the response, and an evaluation time eval_time, at which the response is known and the error can be\n",
        "    computed. Importantly, it means that unlike in standard sklearn cross-validation, the samples X, response y,\n",
        "    pred_times and eval_times must all be pandas dataframe/series having the same index. It is also assumed that the\n",
        "    samples are time-ordered with respect to the prediction time (i.e. pred_times is non-decreasing).\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_splits : int, default=10\n",
        "        Number of folds. Must be at least 2.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_splits=10):\n",
        "        if not isinstance(n_splits, numbers.Integral):\n",
        "            raise ValueError(f\"The number of folds must be of Integral type. {n_splits} of type {type(n_splits)}\"\n",
        "                             f\" was passed.\")\n",
        "        n_splits = int(n_splits)\n",
        "        if n_splits <= 1:\n",
        "            raise ValueError(f\"K-fold cross-validation requires at least one train/test split by setting n_splits = 2 \"\n",
        "                             f\"or more, got n_splits = {n_splits}.\")\n",
        "        self.n_splits = n_splits\n",
        "        self.pred_times = None\n",
        "        self.eval_times = None\n",
        "        self.indices = None\n",
        "\n",
        "    @abstractmethod\n",
        "    def split(self, X: pd.DataFrame, y: pd.Series = None,\n",
        "              pred_times: pd.Series = None, eval_times: pd.Series = None):\n",
        "        if not isinstance(X, pd.DataFrame) and not isinstance(X, pd.Series):\n",
        "            raise ValueError('X should be a pandas DataFrame/Series.')\n",
        "        if not isinstance(y, pd.Series) and y is not None:\n",
        "            raise ValueError('y should be a pandas Series.')\n",
        "        if not isinstance(pred_times, pd.Series):\n",
        "            raise ValueError('pred_times should be a pandas Series.')\n",
        "        if not isinstance(eval_times, pd.Series):\n",
        "            raise ValueError('eval_times should be a pandas Series.')\n",
        "        if y is not None and (X.index == y.index).sum() != len(y):\n",
        "            raise ValueError('X and y must have the same index')\n",
        "        if (X.index == pred_times.index).sum() != len(pred_times):\n",
        "            raise ValueError('X and pred_times must have the same index')\n",
        "        if (X.index == eval_times.index).sum() != len(eval_times):\n",
        "            raise ValueError('X and eval_times must have the same index')\n",
        "\n",
        "        self.pred_times = pred_times\n",
        "        self.eval_times = eval_times\n",
        "        self.indices = np.arange(X.shape[0])\n",
        "        \n",
        "class CombPurgedKFoldCV(BaseTimeSeriesCrossValidator):\n",
        "    \"\"\"\n",
        "    Purged and embargoed combinatorial cross-validation\n",
        "    As described in Advances in financial machine learning, Marcos Lopez de Prado, 2018.\n",
        "    The samples are decomposed into n_splits folds containing equal numbers of samples, without shuffling. In each cross\n",
        "    validation round, n_test_splits folds are used as the test set, while the other folds are used as the train set.\n",
        "    There are as many rounds as n_test_splits folds among the n_splits folds.\n",
        "    Each sample should be tagged with a prediction time pred_time and an evaluation time eval_time. The split is such\n",
        "    that the intervals [pred_times, eval_times] associated to samples in the train and test set do not overlap. (The\n",
        "    overlapping samples are dropped.) In addition, an \"embargo\" period is defined, giving the minimal time between an\n",
        "    evaluation time in the test set and a prediction time in the training set. This is to avoid, in the presence of\n",
        "    temporal correlation, a contamination of the test set by the train set.\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_splits : int, default=10\n",
        "        Number of folds. Must be at least 2.\n",
        "    n_test_splits : int, default=2\n",
        "        Number of folds used in the test set. Must be at least 1.\n",
        "    embargo_td : pd.Timedelta, default=0\n",
        "        Embargo period (see explanations above).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_splits=10, n_test_splits=2, embargo_td=0):\n",
        "        super().__init__(n_splits)\n",
        "        if not isinstance(n_test_splits, numbers.Integral):\n",
        "            raise ValueError(f\"The number of test folds must be of Integral type. {n_test_splits} of type \"\n",
        "                             f\"{type(n_test_splits)} was passed.\")\n",
        "        n_test_splits = int(n_test_splits)\n",
        "        if n_test_splits <= 0 or n_test_splits > self.n_splits - 1:\n",
        "            raise ValueError(f\"K-fold cross-validation requires at least one train/test split by setting \"\n",
        "                             f\"n_test_splits between 1 and n_splits - 1, got n_test_splits = {n_test_splits}.\")\n",
        "        self.n_test_splits = n_test_splits\n",
        "\n",
        "        if embargo_td < 0:\n",
        "            raise ValueError(f\"The embargo time should be positive, got embargo = {embargo_td}.\")\n",
        "        self.embargo_td = embargo_td\n",
        "\n",
        "    def split(self, X: pd.DataFrame, y: pd.Series = None,\n",
        "              pred_times: pd.Series = None, eval_times: pd.Series = None) -> Iterable[Tuple[np.ndarray, np.ndarray]]:\n",
        "        \"\"\"\n",
        "        Yield the indices of the train and test sets.\n",
        "        Although the samples are passed in the form of a pandas dataframe, the indices returned are position indices,\n",
        "        not labels.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pd.DataFrame, shape (n_samples, n_features), required\n",
        "            Samples. Only used to extract n_samples.\n",
        "        y : pd.Series, not used, inherited from _BaseKFold\n",
        "        pred_times : pd.Series, shape (n_samples,), required\n",
        "            Times at which predictions are made. pred_times.index has to coincide with X.index.\n",
        "        eval_times : pd.Series, shape (n_samples,), required\n",
        "            Times at which the response becomes available and the error can be computed. eval_times.index has to\n",
        "            coincide with X.index.\n",
        "        Returns\n",
        "        -------\n",
        "        train_indices: np.ndarray\n",
        "            A numpy array containing all the indices in the train set.\n",
        "        test_indices : np.ndarray\n",
        "            A numpy array containing all the indices in the test set.\n",
        "        \"\"\"\n",
        "        super().split(X, y, pred_times, eval_times)\n",
        "\n",
        "        # Fold boundaries\n",
        "        fold_bounds = [(fold[0], fold[-1] + 1) for fold in np.array_split(self.indices, self.n_splits)]\n",
        "        # List of all combinations of n_test_splits folds selected to become test sets\n",
        "        selected_fold_bounds = list(itt.combinations(fold_bounds, self.n_test_splits))\n",
        "        \n",
        "        # In order for the first round to have its whole test set at the end of the dataset\n",
        "        selected_fold_bounds.reverse()\n",
        "\n",
        "        for fold_bound_list in selected_fold_bounds:\n",
        "            # Computes the bounds of the test set, and the corresponding indices\n",
        "            test_fold_bounds, test_indices = self.compute_test_set(fold_bound_list)\n",
        "            # Computes the train set indices\n",
        "            train_indices = self.compute_train_set(test_fold_bounds, test_indices)\n",
        "\n",
        "            yield train_indices, test_indices\n",
        "\n",
        "    def compute_train_set(self, test_fold_bounds: List[Tuple[int, int]], test_indices: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the position indices of samples in the train set.\n",
        "        Parameters\n",
        "        ----------\n",
        "        test_fold_bounds : List of tuples of position indices\n",
        "            Each tuple records the bounds of a block of indices in the test set.\n",
        "        test_indices : np.ndarray\n",
        "            A numpy array containing all the indices in the test set.\n",
        "        Returns\n",
        "        -------\n",
        "        train_indices: np.ndarray\n",
        "            A numpy array containing all the indices in the train set.\n",
        "        \"\"\"\n",
        "        # As a first approximation, the train set is the complement of the test set\n",
        "        train_indices = np.setdiff1d(self.indices, test_indices)\n",
        "        # But we now have to purge and embargo\n",
        "        for test_fold_start, test_fold_end in test_fold_bounds:\n",
        "            # Purge\n",
        "            train_indices = purge(self, train_indices, test_fold_start, test_fold_end)\n",
        "            # Embargo\n",
        "            train_indices = embargo(self, train_indices, test_indices, test_fold_end)\n",
        "        return train_indices\n",
        "\n",
        "    def compute_test_set(self, fold_bound_list: List[Tuple[int, int]]) -> Tuple[List[Tuple[int, int]], np.ndarray]:\n",
        "        \"\"\"\n",
        "        Compute the indices of the samples in the test set.\n",
        "        Parameters\n",
        "        ----------\n",
        "        fold_bound_list: List of tuples of position indices\n",
        "            Each tuple records the bounds of the folds belonging to the test set.\n",
        "        Returns\n",
        "        -------\n",
        "        test_fold_bounds: List of tuples of position indices\n",
        "            Like fold_bound_list, but with the neighboring folds in the test set merged.\n",
        "        test_indices: np.ndarray\n",
        "            A numpy array containing the test indices.\n",
        "        \"\"\"\n",
        "        test_indices = np.empty(0)\n",
        "        test_fold_bounds = []\n",
        "        for fold_start, fold_end in fold_bound_list:\n",
        "            # Records the boundaries of the current test split\n",
        "            if not test_fold_bounds or fold_start != test_fold_bounds[-1][-1]:\n",
        "                test_fold_bounds.append((fold_start, fold_end))\n",
        "            # If the current test split is contiguous to the previous one, simply updates the endpoint\n",
        "            elif fold_start == test_fold_bounds[-1][-1]:\n",
        "                test_fold_bounds[-1] = (test_fold_bounds[-1][0], fold_end)\n",
        "            test_indices = np.union1d(test_indices, self.indices[fold_start:fold_end]).astype(int)\n",
        "        return test_fold_bounds, test_indices\n",
        "\n",
        "\n",
        "def embargo(cv: BaseTimeSeriesCrossValidator, train_indices: np.ndarray,\n",
        "            test_indices: np.ndarray, test_fold_end: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Apply the embargo procedure to part of the train set.\n",
        "    This amounts to dropping the train set samples whose prediction time occurs within self.embargo_dt of the test\n",
        "    set sample evaluation times. This method applies the embargo only to the part of the training set immediately\n",
        "    following the end of the test set determined by test_fold_end.\n",
        "    Parameters\n",
        "    ----------\n",
        "    cv: Cross-validation class\n",
        "        Needs to have the attributes cv.pred_times, cv.eval_times, cv.embargo_dt and cv.indices.\n",
        "    train_indices: np.ndarray\n",
        "        A numpy array containing all the indices of the samples currently included in the train set.\n",
        "    test_indices : np.ndarray\n",
        "        A numpy array containing all the indices of the samples in the test set.\n",
        "    test_fold_end : int\n",
        "        Index corresponding to the end of a test set block.\n",
        "    Returns\n",
        "    -------\n",
        "    train_indices: np.ndarray\n",
        "        The same array, with the indices subject to embargo removed.\n",
        "    \"\"\"\n",
        "    if not hasattr(cv, 'embargo_td'):\n",
        "        raise ValueError(\"The passed cross-validation object should have a member cv.embargo_td defining the embargo\"\n",
        "                         \"time.\")\n",
        "    last_test_eval_time = cv.eval_times.iloc[cv.indices[:test_fold_end]].max()\n",
        "    min_train_index = len(cv.pred_times[cv.pred_times <= last_test_eval_time + cv.embargo_td])\n",
        "    if min_train_index < cv.indices.shape[0]:\n",
        "        allowed_indices = np.concatenate((cv.indices[:test_fold_end], cv.indices[min_train_index:]))\n",
        "        train_indices = np.intersect1d(train_indices, allowed_indices)\n",
        "    return train_indices\n",
        "\n",
        "\n",
        "def purge(cv: BaseTimeSeriesCrossValidator, train_indices: np.ndarray,\n",
        "          test_fold_start: int, test_fold_end: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Purge part of the train set.\n",
        "    Given a left boundary index test_fold_start of the test set, this method removes from the train set all the\n",
        "    samples whose evaluation time is posterior to the prediction time of the first test sample after the boundary.\n",
        "    Parameters\n",
        "    ----------\n",
        "    cv: Cross-validation class\n",
        "        Needs to have the attributes cv.pred_times, cv.eval_times and cv.indices.\n",
        "    train_indices: np.ndarray\n",
        "        A numpy array containing all the indices of the samples currently included in the train set.\n",
        "    test_fold_start : int\n",
        "        Index corresponding to the start of a test set block.\n",
        "    test_fold_end : int\n",
        "        Index corresponding to the end of the same test set block.\n",
        "    Returns\n",
        "    -------\n",
        "    train_indices: np.ndarray\n",
        "        A numpy array containing the train indices purged at test_fold_start.\n",
        "    \"\"\"\n",
        "    time_test_fold_start = cv.pred_times.iloc[test_fold_start]\n",
        "    # The train indices before the start of the test fold, purged.\n",
        "    train_indices_1 = np.intersect1d(train_indices, cv.indices[cv.eval_times < time_test_fold_start])\n",
        "    # The train indices after the end of the test fold.\n",
        "    train_indices_2 = np.intersect1d(train_indices, cv.indices[test_fold_end:])\n",
        "\n",
        "    return np.concatenate((train_indices_1, train_indices_2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf7Vd77V19PT"
      },
      "source": [
        "def get_metrics(label, y_pred, show_results=False):\n",
        "  \n",
        "  results = []\n",
        "  results.append(metrics.accuracy_score(label, y_pred))\n",
        "  results.append(metrics.precision_score(label, y_pred))\n",
        "  results.append(metrics.recall_score(label, y_pred))\n",
        "  results.append(metrics.f1_score(label, y_pred))\n",
        "\n",
        "  if show_results:\n",
        "    print(\"Accuracy:{:.6f}\".format(metrics.accuracy_score(label, y_pred)))\n",
        "    print(\"Precision:{:.6f}\".format(metrics.precision_score(label, y_pred)))\n",
        "    print(\"Recall:{:.6f}\".format(metrics.recall_score(label, y_pred)))\n",
        "    print(\"F1 score:{:.6f}\".format(metrics.f1_score(label, y_pred)))\n",
        "    \n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzChyAFBka7-"
      },
      "source": [
        "def data_pre1d(df, index, label, step_size):\n",
        "  x = np.array([df[i + step_size - 1:i+step_size*11:step_size] for i in index if (i-1 + 11*step_size) in index])\n",
        "  y = np.array([label[i-1 + 11*step_size] for i in index if (i-1 + 11*step_size) in index])\n",
        "  x = x / np.mean(x,axis = 1).reshape((len(x), 1))\n",
        "  x = x - np.ones((len(x), 1))\n",
        "  x = x.reshape(x.shape[0], x.shape[1], 1)\n",
        "  y = (y + 1) // 2\n",
        "  return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfyQgHA1jwWM"
      },
      "source": [
        "def denoise_data(df, col_name, k=100):\n",
        "  clf = KNeighborsRegressor(n_neighbors=k, weights='uniform')\n",
        "  clf.fit(df.index.values[:, np.newaxis], df[\"<CLOSE>\"])\n",
        "  y_pred = clf.predict(df.index.values[:, np.newaxis])  \n",
        "  reverse_pred = y_pred[::-1]\n",
        "  ax = pd.Series(df[\"<CLOSE>\"]).plot(color='lightgray')\n",
        "  pd.Series(y_pred).plot(color='black', ax=ax, figsize=(12, 8))\n",
        "  plt.title(\"Denoising data\")\n",
        "  df['<CLOSE>'] = y_pred\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rQBNo1O7f2c"
      },
      "source": [
        "def visualize(dfs, col_name, date, th, start, end):\n",
        "  label = labeling(dfs[col_name], th)\n",
        "  # date_col = dfs[date].astype(str).str[:4]\n",
        "  date_col = dfs[date]\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(go.Scatter(x=date_col[start:end], y=dfs[col_name][start:end]))\n",
        "  fig.show()\n",
        "  fig = go.Figure([go.Scatter(x=date_col[start:end], y=label[start:end])])\n",
        "  fig.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEk0ml6W2quS"
      },
      "source": [
        "# Define a class for training models\n",
        "\n",
        "class Train_data():\n",
        "  def __init__(self, data,  train_start, train_end, test_end):\n",
        "    self.train_start = train_start\n",
        "    self.train_end = train_end\n",
        "    self.test_end = test_end \n",
        "    self.data = data\n",
        "    self.train_data = pd.Series(np.array(data[train_start:train_end]))\n",
        "    self.train_label = None\n",
        "    self.test_data = pd.Series(np.array(data[train_end:test_end]))\n",
        "    self.test_label = None\n",
        "    self.splits = []\n",
        "    self.X_train  = []\n",
        "    self.y_train = []\n",
        "    self.X_test  = []\n",
        "    self.y_test = []\n",
        "    self.GRU_beta_result = None\n",
        "    self.LSTM_beta_result = None\n",
        "    self.XGboost_beta_result = None\n",
        "    self.SVM_beta_result = None\n",
        "    self.Logreg_beta_result = None\n",
        "    self.GRU_result = None\n",
        "    self.LSTM_result = None\n",
        "    self.XGboost_result = None\n",
        "    self.SVM_result = None\n",
        "    self.Logreg_result = None\n",
        "    self.GRU_beta_predict = None\n",
        "    self.LSTM_beta_predict = None\n",
        "    self.XGboost_beta_predict = None\n",
        "    self.SVM_beta_predict = None\n",
        "    self.Logreg_beta_predict = None\n",
        "    self.GRU_predict = None\n",
        "    self.LSTM_predict = None\n",
        "    self.XGboost_predict = None\n",
        "    self.SVM_predict = None\n",
        "    self.Logreg_predict = None\n",
        "\n",
        "  def set_threshold(self, th):\n",
        "    label  = labeling(self.data, th)\n",
        "    self.train_label = pd.Series(np.array(label[self.train_start:self.train_end]))\n",
        "    self.test_label = pd.Series(np.array(label[self.train_end:self.test_end]))\n",
        "\n",
        "  def K_fold_purged(self, num_split, num_test, time_gaps, emb=0, purging=True):\n",
        "    n_splits = num_split\n",
        "    n_test_splits = num_test\n",
        "    time_gap = time_gaps\n",
        "    embargo_td = emb\n",
        "    t1_ = self.train_data.index\n",
        "    t1 = pd.Series(t1_).shift(time_gap).fillna(0).astype(int)\n",
        "    t2 = pd.Series(t1_).shift(-time_gap).fillna(1e12).astype(int)\n",
        "    \n",
        "    if purging:\n",
        "      cpkf = CombPurgedKFoldCV(n_splits=n_splits, n_test_splits=n_test_splits, embargo_td=embargo_td)\n",
        "      comb_purged_splits = list(cpkf.split(self.train_data, pred_times=t1, eval_times=t2))\n",
        "      self.splits = comb_purged_splits\n",
        "    \n",
        "    else:\n",
        "      cvts = TimeSeriesSplit(n_splits=n_splits)\n",
        "      cvts_splits = list(cvts.split(self.train_data))\n",
        "      self.splits = cvts_splits\n",
        "\n",
        "\n",
        "  def SVM(self, beta):\n",
        "\n",
        "    Xtrain = self.X_train.reshape(self.X_train.shape[0], self.X_train.shape[1])\n",
        "    Xtest = self.X_test.reshape(self.X_test.shape[0], self.X_test.shape[1])\n",
        "    clf = SVC(C=1)\n",
        "    clf.fit( Xtrain,self.y_train)\n",
        "    y_pred = clf.predict(Xtest)\n",
        "    if beta : \n",
        "      self.SVM_beta_result  =  get_metrics(self.y_test, y_pred,0)\n",
        "      self.SVM_beta_predict = y_pred    \n",
        "    else:\n",
        "      self.SVM_result  =  get_metrics(self.y_test,y_pred,0)\n",
        "      self.SVM_predict = y_pred\n",
        "    \n",
        "\n",
        "  def log_reg(self, beta):\n",
        "    \n",
        "    Xtrain = self.X_train.reshape(self.X_train.shape[0], self.X_train.shape[1])\n",
        "    Xtest = self.X_test.reshape(self.X_test.shape[0], self.X_test.shape[1])\n",
        "    clf = LogisticRegression(C=10, penalty=\"l2\")\n",
        "    clf.fit( Xtrain,self.y_train)\n",
        "    y_pred = clf.predict(Xtest)\n",
        "    if beta: \n",
        "      self.Logreg_beta_result  =  get_metrics(self.y_test,y_pred,0)\n",
        "      self.Logreg_beta_predict = y_pred\n",
        "    else:\n",
        "      self.Logreg_result  =  get_metrics(self.y_test,y_pred,0)\n",
        "      self.Logreg_predict = y_pred\n",
        "    \n",
        "    \n",
        "\n",
        "  def data_preprocess(self, step_size):  \n",
        "    self.X_train, self.y_train = data_pre1d(self.train_data, self.train_data.index, self.train_label,step_size)\n",
        "    self.X_test, self.y_test = data_pre1d(self.test_data, self.test_data.index, self.test_label,step_size)\n",
        "    \n",
        "\n",
        "  def LSTM(self, epoch, layer_size, window_size, lr, beta):\n",
        "    with tf.device('/device:GPU:0'):\n",
        "      model=Sequential()\n",
        "      model.add(tf.compat.v1.keras.layers.CuDNNLSTM(layer_size,return_sequences=True,input_shape=(window_size,1)))\n",
        "      model.add(Dropout(0.4))\n",
        "      model.add(tf.compat.v1.keras.layers.CuDNNLSTM(layer_size))\n",
        "      model.add(Dropout(0.4))\n",
        "      model.add(Dense(1, activation='sigmoid'))\n",
        "      opt = tf.keras.optimizers.Adam(beta_1=0.9,beta_2=0.999,learning_rate=lr)\n",
        "      model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy']) \n",
        "      \n",
        "      if beta:\n",
        "        print(f'start lstm beta training...\\n')\n",
        "        model.fit(self.X_train,self.y_train,validation_data=(self.X_test, self.y_test),epochs=epoch,batch_size=16,verbose=1)\n",
        "        y_pred = model.predict(self.X_test)\n",
        "        y_pred = np.array([1 if y >=0.5 else 0 for y in y_pred])\n",
        "        self.LSTM_beta_result = get_metrics(self.y_test, y_pred, False)\n",
        "        self.LSTM_beta_predict = y_pred\n",
        "      else:\n",
        "        print(f'start lstm training...\\n')\n",
        "        model.fit(self.X_train,self.y_train,validation_data=(self.X_test, self.y_test),epochs=epoch,batch_size=16,verbose=1)\n",
        "        y_pred = model.predict(self.X_test)\n",
        "        y_pred = np.array([1 if y >=0.5 else 0 for y in y_pred])\n",
        "        self.LSTM_result = get_metrics(self.y_test, y_pred, False)\n",
        "        self.LSTM_predict = y_pred\n",
        "      \n",
        "      print('end training. \\n')\n",
        "\n",
        "  def GRU(self, epoch, layer_size, window_size, lr, beta):\n",
        "    with tf.device('/device:GPU:0'):\n",
        "      model=Sequential()\n",
        "      model.add(tf.compat.v1.keras.layers.CuDNNGRU(layer_size,return_sequences=True,input_shape=(window_size,1)))\n",
        "      model.add(Dropout(0.4))\n",
        "      model.add(tf.compat.v1.keras.layers.CuDNNGRU(layer_size))\n",
        "      model.add(Dropout(0.4))\n",
        "      model.add(Dense(1, activation='sigmoid'))\n",
        "      opt = tf.keras.optimizers.Adam(beta_1=0.9,beta_2=0.999,learning_rate=lr)\n",
        "      model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "      if beta:\n",
        "        print('start gru beta training...\\n')\n",
        "        model.fit(self.X_train,self.y_train,validation_data=(self.X_test,self.y_test),epochs=epoch,batch_size=16,verbose=1)\n",
        "        y_pred = model.predict(self.X_test)\n",
        "        y_pred = np.array([1 if y >=0.5 else 0 for y in y_pred])\n",
        "        self.GRU_beta_result = get_metrics(self.y_test,y_pred,False)\n",
        "        self.GRU_beta_predict = y_pred\n",
        "      else:\n",
        "        print('start gru training...\\n')\n",
        "        model.fit(self.X_train,self.y_train,validation_data=(self.X_test,self.y_test),epochs=epoch,batch_size=16,verbose=1)\n",
        "        y_pred = model.predict(self.X_test)\n",
        "        y_pred = np.array([1 if y >=0.5 else 0 for y in y_pred])\n",
        "        self.GRU_result = get_metrics(self.y_test,y_pred,False)\n",
        "        self.GRU_predict = y_pred\n",
        "\n",
        "      print('end training. \\n')\n",
        "\n",
        "  def XGboost(self,es,beta):\n",
        "    Xtrain = self.X_train.reshape(self.X_train.shape[0],self.X_train.shape[1])\n",
        "    Xtest = self.X_test.reshape(self.X_test.shape[0],self.X_test.shape[1])\n",
        "    # XGBoost classifier with Early-stopping\n",
        "    clf = xgb.XGBClassifier(n_jobs=1)\n",
        "    if beta:\n",
        "      print('start xgboost beta training...\\n')\n",
        "      clf.fit(Xtrain, self.y_train, early_stopping_rounds=es, eval_metric=\"auc\",eval_set=[(Xtest, self.y_test)])\n",
        "      y_pred = clf.predict(Xtest)\n",
        "      self.XGboost_beta_result = get_metrics(self.y_test,y_pred,False)\n",
        "      self.XGboost_beta_predict = y_pred\n",
        "    else:\n",
        "      print('start xgboost training...\\n')\n",
        "      clf.fit(Xtrain, self.y_train, early_stopping_rounds=es, eval_metric=\"auc\",eval_set=[(Xtest, self.y_test)])\n",
        "      y_pred = clf.predict(Xtest)\n",
        "      self.XGboost_result = get_metrics(self.y_test,y_pred,False)\n",
        "      self.XGboost_predict = y_pred\n",
        "\n",
        "    print('end training. \\n')\n",
        "\n",
        "\n",
        "  def table(self, th = 0.1):\n",
        "    x = PrettyTable()\n",
        "    x.field_names = [\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 score\"]\n",
        "    self.LSTM_result.insert(0,\"LSTM \"+str(th))\n",
        "    self.GRU_result.insert(0,\"GRU \"+str(th))\n",
        "    self.XGboost_result.insert(0,\"XGBoost \"+str(th))\n",
        "    self.Logreg_result.insert(0,\"Logreg \"+str(th))\n",
        "    self.SVM_result.insert(0,\"SVM \"+str(th))\n",
        "    self.LSTM_beta_result.insert(0,\"LSTM beta \"+str(th))\n",
        "    self.GRU_beta_result.insert(0,\"GRU beta \"+str(th))\n",
        "    self.XGboost_beta_result.insert(0,\"XGBoost beta \"+str(th))\n",
        "    self.Logreg_beta_result.insert(0,\"logreg beta \"+str(th))\n",
        "    self.SVM_beta_result.insert(0,\"svm beta \"+str(th))\n",
        "    x.add_row(self.LSTM_result)\n",
        "    x.add_row(self.GRU_result)\n",
        "    x.add_row(self.XGboost_result)\n",
        "    x.add_row(self.Logreg_result)\n",
        "    x.add_row(self.SVM_result)\n",
        "    x.add_row(self.LSTM_beta_result)\n",
        "    x.add_row(self.GRU_beta_result)\n",
        "    x.add_row(self.XGboost_beta_result)\n",
        "    x.add_row(self.Logreg_beta_result)\n",
        "    x.add_row(self.SVM_beta_result)\n",
        "    return x\n",
        "\n",
        "\n",
        "  \n",
        "  def train_models(self, best_parameters, step_sizes=4, th = 0.1):\n",
        "\n",
        "    lstm_epoch = best_parameters[0]\n",
        "    lstm_layer = best_parameters[1]\n",
        "    lstm_lr = best_parameters[2]\n",
        "    gru_epoch = best_parameters[3]\n",
        "    gru_layer = best_parameters[4]\n",
        "    gru_lr = best_parameters[5]\n",
        "\n",
        "    self.data_preprocess(step_size=1)\n",
        "    self.LSTM(lstm_epoch, lstm_layer, 11, lstm_lr, False)\n",
        "    self.GRU(gru_epoch, gru_layer, 11, gru_lr, False)\n",
        "    self.XGboost(50, False)\n",
        "    self.SVM(False)\n",
        "    self.log_reg(False)\n",
        "\n",
        "    self.data_preprocess(step_size=step_sizes)\n",
        "    self.LSTM(lstm_epoch,lstm_layer,11,lstm_lr,True)\n",
        "    self.GRU(gru_epoch,gru_layer,11,gru_lr,True)\n",
        "    self.XGboost(50,True)\n",
        "    self.SVM(True)\n",
        "    self.log_reg(True)\n",
        "\n",
        "    print(self.table(th = th))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur-dIbFYaJXc"
      },
      "source": [
        "# Default best parameters\n",
        "\n",
        "default_best_param = [5, 10, 0.01, 5, 10, 0.01]\n",
        "\n",
        "def final_result(historical, name, best_param=default_best_param, step_sizes=4, th=0.1):\n",
        "  historical.train_models(best_param, step_sizes, th=th)\n",
        "  results = []\n",
        "  results.append(historical.LSTM_result)\n",
        "  results.append(historical.GRU_result)\n",
        "  results.append(historical.XGboost_result)\n",
        "  results.append(historical.Logreg_result)\n",
        "  results.append(historical.SVM_result)\n",
        "  results.append(historical.LSTM_beta_result)\n",
        "  results.append(historical.GRU_beta_result)\n",
        "  results.append(historical.XGboost_beta_result)\n",
        "  results.append(historical.Logreg_beta_result)\n",
        "  results.append(historical.SVM_beta_result)\n",
        "  res = pd.DataFrame()\n",
        "  \n",
        "  for x in results:\n",
        "    dic = {\n",
        "      \"Name\": name,\n",
        "      \"Model\": x[0],\n",
        "      \"acc\": x[1],\n",
        "      \"Perc\": x[2], \n",
        "      \"recal\": x[3], \n",
        "      \"f1\" :x[4]\n",
        "\n",
        "    }\n",
        "    res = res.append(dic, ignore_index=True)\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EE2k11tpBIo"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ02uVQzLwrK"
      },
      "source": [
        "## AMD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "id": "HLMLm9U-LwrM",
        "outputId": "1d1f736f-513e-4445-a19d-0b21ff355364"
      },
      "source": [
        "dfs = pd.read_csv(\"AMD.csv\")\n",
        "# dfs = dfs[::-1].reset_index()\n",
        "# Denoise prices\n",
        "dfs = denoise_data(dfs, '<CLOSE>')\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>US1.AMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>7.03</td>\n",
              "      <td>7.08</td>\n",
              "      <td>6.840</td>\n",
              "      <td>7.868584</td>\n",
              "      <td>22229263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>US1.AMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>7.04</td>\n",
              "      <td>7.12</td>\n",
              "      <td>6.890</td>\n",
              "      <td>7.868584</td>\n",
              "      <td>34185258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>US1.AMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>7.01</td>\n",
              "      <td>7.05</td>\n",
              "      <td>6.780</td>\n",
              "      <td>7.868584</td>\n",
              "      <td>23076122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>US1.AMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>6.89</td>\n",
              "      <td>6.93</td>\n",
              "      <td>6.770</td>\n",
              "      <td>7.868584</td>\n",
              "      <td>20010148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US1.AMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>6.87</td>\n",
              "      <td>7.10</td>\n",
              "      <td>6.790</td>\n",
              "      <td>7.868584</td>\n",
              "      <td>24895450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>US1.AMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>104.99</td>\n",
              "      <td>108.43</td>\n",
              "      <td>103.440</td>\n",
              "      <td>94.725300</td>\n",
              "      <td>1337149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>US1.AMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>106.90</td>\n",
              "      <td>107.63</td>\n",
              "      <td>101.425</td>\n",
              "      <td>94.725300</td>\n",
              "      <td>1661005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>US1.AMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>102.16</td>\n",
              "      <td>102.64</td>\n",
              "      <td>99.820</td>\n",
              "      <td>94.725300</td>\n",
              "      <td>1241696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>US1.AMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>102.08</td>\n",
              "      <td>104.43</td>\n",
              "      <td>102.070</td>\n",
              "      <td>94.725300</td>\n",
              "      <td>1313306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>US1.AMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>102.58</td>\n",
              "      <td>102.97</td>\n",
              "      <td>100.670</td>\n",
              "      <td>94.725300</td>\n",
              "      <td>1095599</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2769 rows Ã— 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     <TICKER> <PER>    <DATE>  <TIME>  ...  <HIGH>    <LOW>    <CLOSE>     <VOL>\n",
              "0     US1.AMD     D  20101004       0  ...    7.08    6.840   7.868584  22229263\n",
              "1     US1.AMD     D  20101005       0  ...    7.12    6.890   7.868584  34185258\n",
              "2     US1.AMD     D  20101006       0  ...    7.05    6.780   7.868584  23076122\n",
              "3     US1.AMD     D  20101007       0  ...    6.93    6.770   7.868584  20010148\n",
              "4     US1.AMD     D  20101008       0  ...    7.10    6.790   7.868584  24895450\n",
              "...       ...   ...       ...     ...  ...     ...      ...        ...       ...\n",
              "2764  US1.AMD     D  20210927       0  ...  108.43  103.440  94.725300   1337149\n",
              "2765  US1.AMD     D  20210928       0  ...  107.63  101.425  94.725300   1661005\n",
              "2766  US1.AMD     D  20210929       0  ...  102.64   99.820  94.725300   1241696\n",
              "2767  US1.AMD     D  20210930       0  ...  104.43  102.070  94.725300   1313306\n",
              "2768  US1.AMD     D  20211001       0  ...  102.97  100.670  94.725300   1095599\n",
              "\n",
              "[2769 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 238
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10kEvSjaOKw9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d76a6096-f7f9-40ab-ffaa-898c0ab1f344"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.05, 100, 2300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"7ac96a99-7425-4b7b-9cfb-7631b1da6452\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"7ac96a99-7425-4b7b-9cfb-7631b1da6452\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '7ac96a99-7425-4b7b-9cfb-7631b1da6452',\n",
              "                        [{\"type\": \"scatter\", \"x\": [20140924, 20140925, 20140926, 20140929, 20140930, 20141001, 20141002, 20141003, 20141006, 20141007, 20141008, 20141009, 20141010, 20141013, 20141014, 20141015, 20141016, 20141017, 20141020, 20141021, 20141022, 20141023, 20141024, 20141027, 20141028, 20141029, 20141030, 20141031, 20141103, 20141104, 20141105, 20141106, 20141107, 20141110, 20141111, 20141112, 20141113, 20141114, 20141117, 20141118, 20141119, 20141120, 20141121, 20141124, 20141125, 20141126, 20141128, 20141201, 20141202, 20141203, 20141204, 20141205, 20141208, 20141209, 20141210, 20141211, 20141212, 20141215, 20141216, 20141217, 20141218, 20141219, 20141222, 20141223, 20141224, 20141226, 20141229, 20141230, 20141231, 20150102, 20150105, 20150106, 20150107, 20150108, 20150109, 20150112, 20150113, 20150114, 20150115, 20150116, 20150120, 20150121, 20150122, 20150123, 20150126, 20150127, 20150128, 20150129, 20150130, 20150202, 20150203, 20150204, 20150205, 20150206, 20150209, 20150210, 20150211, 20150212, 20150213, 20150217, 20150218, 20150219, 20150220, 20150223, 20150224, 20150225, 20150226, 20150227, 20150302, 20150303, 20150304, 20150305, 20150306, 20150309, 20150310, 20150311, 20150312, 20150313, 20150316, 20150317, 20150318, 20150319, 20150320, 20150323, 20150324, 20150325, 20150326, 20150327, 20150330, 20150331, 20150401, 20150402, 20150406, 20150407, 20150408, 20150409, 20150410, 20150413, 20150414, 20150415, 20150416, 20150417, 20150420, 20150421, 20150422, 20150423, 20150424, 20150427, 20150428, 20150429, 20150430, 20150501, 20150504, 20150505, 20150506, 20150507, 20150508, 20150511, 20150512, 20150513, 20150514, 20150515, 20150518, 20150519, 20150520, 20150521, 20150522, 20150526, 20150527, 20150528, 20150529, 20150601, 20150602, 20150603, 20150604, 20150605, 20150608, 20150609, 20150610, 20150611, 20150612, 20150615, 20150616, 20150617, 20150618, 20150619, 20150622, 20150623, 20150624, 20150625, 20150626, 20150629, 20150630, 20150701, 20150702, 20150706, 20150707, 20150708, 20150709, 20150710, 20150713, 20150714, 20150715, 20150716, 20150717, 20150720, 20150721, 20150722, 20150723, 20150724, 20150727, 20150728, 20150729, 20150730, 20150731, 20150803, 20150804, 20150805, 20150806, 20150807, 20150810, 20150811, 20150812, 20150813, 20150814, 20150817, 20150818, 20150819, 20150820, 20150821, 20150824, 20150825, 20150826, 20150827, 20150828, 20150831, 20150901, 20150902, 20150903, 20150904, 20150908, 20150909, 20150910, 20150911, 20150914, 20150915, 20150916, 20150917, 20150918, 20150921, 20150922, 20150923, 20150924, 20150925, 20150928, 20150929, 20150930, 20151001, 20151002, 20151005, 20151006, 20151007, 20151008, 20151009, 20151012, 20151013, 20151014, 20151015, 20151016, 20151019, 20151020, 20151021, 20151022, 20151023, 20151026, 20151027, 20151028, 20151029, 20151030, 20151102, 20151103, 20151104, 20151105, 20151106, 20151109, 20151110, 20151111, 20151112, 20151113, 20151116, 20151117, 20151118, 20151119, 20151120, 20151123, 20151124, 20151125, 20151127, 20151130, 20151201, 20151202, 20151203, 20151204, 20151207, 20151208, 20151209, 20151210, 20151211, 20151214, 20151215, 20151216, 20151217, 20151218, 20151221, 20151222, 20151223, 20151224, 20151228, 20151229, 20151230, 20151231, 20160104, 20160105, 20160106, 20160107, 20160108, 20160111, 20160112, 20160113, 20160114, 20160115, 20160119, 20160120, 20160121, 20160122, 20160125, 20160126, 20160127, 20160128, 20160129, 20160201, 20160202, 20160203, 20160204, 20160205, 20160208, 20160209, 20160210, 20160211, 20160212, 20160216, 20160217, 20160218, 20160219, 20160222, 20160223, 20160224, 20160225, 20160226, 20160229, 20160301, 20160302, 20160303, 20160304, 20160307, 20160308, 20160309, 20160310, 20160311, 20160314, 20160315, 20160316, 20160317, 20160318, 20160321, 20160322, 20160323, 20160324, 20160328, 20160329, 20160330, 20160331, 20160401, 20160404, 20160405, 20160406, 20160407, 20160408, 20160411, 20160412, 20160413, 20160414, 20160415, 20160418, 20160419, 20160420, 20160421, 20160422, 20160425, 20160426, 20160427, 20160428, 20160429, 20160502, 20160503, 20160504, 20160505, 20160506, 20160509, 20160510, 20160511, 20160512, 20160513, 20160516, 20160517, 20160518, 20160519, 20160520, 20160523, 20160524, 20160525, 20160526, 20160527, 20160531, 20160601, 20160602, 20160603, 20160606, 20160607, 20160608, 20160609, 20160610, 20160613, 20160614, 20160615, 20160616, 20160617, 20160620, 20160621, 20160622, 20160623, 20160624, 20160627, 20160628, 20160629, 20160630, 20160701, 20160705, 20160706, 20160707, 20160708, 20160711, 20160712, 20160713, 20160714, 20160715, 20160718, 20160719, 20160720, 20160721, 20160722, 20160725, 20160726, 20160727, 20160728, 20160729, 20160801, 20160802, 20160803, 20160804, 20160805, 20160808, 20160809, 20160810, 20160811, 20160812, 20160815, 20160816, 20160817, 20160818, 20160819, 20160822, 20160823, 20160824, 20160825, 20160826, 20160829, 20160830, 20160831, 20160901, 20160902, 20160906, 20160907, 20160908, 20160909, 20160912, 20160913, 20160914, 20160915, 20160916, 20160919, 20160920, 20160921, 20160922, 20160923, 20160926, 20160927, 20160928, 20160929, 20160930, 20161003, 20161004, 20161005, 20161006, 20161007, 20161010, 20161011, 20161012, 20161013, 20161014, 20161017, 20161018, 20161019, 20161020, 20161021, 20161024, 20161025, 20161026, 20161027, 20161028, 20161031, 20161101, 20161102, 20161103, 20161104, 20161107, 20161108, 20161109, 20161110, 20161111, 20161114, 20161115, 20161116, 20161117, 20161118, 20161121, 20161122, 20161123, 20161125, 20161128, 20161129, 20161130, 20161201, 20161202, 20161205, 20161206, 20161207, 20161208, 20161209, 20161212, 20161213, 20161214, 20161215, 20161216, 20161219, 20161220, 20161221, 20161222, 20161223, 20161227, 20161228, 20161229, 20161230, 20170103, 20170104, 20170105, 20170106, 20170109, 20170110, 20170111, 20170112, 20170113, 20170117, 20170118, 20170119, 20170120, 20170123, 20170124, 20170125, 20170126, 20170127, 20170130, 20170131, 20170201, 20170202, 20170203, 20170206, 20170207, 20170208, 20170209, 20170210, 20170213, 20170214, 20170215, 20170216, 20170217, 20170221, 20170222, 20170223, 20170224, 20170227, 20170228, 20170301, 20170302, 20170303, 20170306, 20170307, 20170308, 20170309, 20170310, 20170313, 20170314, 20170315, 20170316, 20170317, 20170320, 20170321, 20170322, 20170323, 20170324, 20170327, 20170328, 20170329, 20170330, 20170331, 20170403, 20170404, 20170405, 20170406, 20170407, 20170410, 20170411, 20170412, 20170413, 20170417, 20170418, 20170419, 20170420, 20170421, 20170424, 20170425, 20170426, 20170427, 20170428, 20170501, 20170502, 20170503, 20170504, 20170505, 20170508, 20170509, 20170510, 20170511, 20170512, 20170515, 20170516, 20170517, 20170518, 20170519, 20170522, 20170523, 20170524, 20170525, 20170526, 20170530, 20170531, 20170601, 20170602, 20170605, 20170606, 20170607, 20170608, 20170609, 20170612, 20170613, 20170614, 20170615, 20170616, 20170619, 20170620, 20170621, 20170622, 20170623, 20170626, 20170627, 20170628, 20170629, 20170630, 20170703, 20170705, 20170706, 20170707, 20170710, 20170711, 20170712, 20170713, 20170714, 20170717, 20170718, 20170719, 20170720, 20170721, 20170724, 20170725, 20170726, 20170727, 20170728, 20170731, 20170801, 20170802, 20170803, 20170804, 20170807, 20170808, 20170809, 20170810, 20170811, 20170814, 20170815, 20170816, 20170817, 20170818, 20170821, 20170822, 20170823, 20170824, 20170825, 20170828, 20170829, 20170830, 20170831, 20170901, 20170905, 20170906, 20170907, 20170908, 20170911, 20170912, 20170913, 20170914, 20170915, 20170918, 20170919, 20170920, 20170921, 20170922, 20170925, 20170926, 20170927, 20170928, 20170929, 20171002, 20171003, 20171004, 20171005, 20171006, 20171009, 20171010, 20171011, 20171012, 20171013, 20171016, 20171017, 20171018, 20171019, 20171020, 20171023, 20171024, 20171025, 20171026, 20171027, 20171030, 20171031, 20171101, 20171102, 20171103, 20171106, 20171107, 20171108, 20171109, 20171110, 20171113, 20171114, 20171115, 20171116, 20171117, 20171120, 20171121, 20171122, 20171124, 20171127, 20171128, 20171129, 20171130, 20171201, 20171204, 20171205, 20171206, 20171207, 20171208, 20171211, 20171212, 20171213, 20171214, 20171215, 20171218, 20171219, 20171220, 20171221, 20171222, 20171226, 20171227, 20171228, 20171229, 20180102, 20180103, 20180104, 20180105, 20180108, 20180109, 20180110, 20180111, 20180112, 20180116, 20180117, 20180118, 20180119, 20180122, 20180123, 20180124, 20180125, 20180126, 20180129, 20180130, 20180131, 20180201, 20180202, 20180205, 20180206, 20180207, 20180208, 20180209, 20180212, 20180213, 20180214, 20180215, 20180216, 20180220, 20180221, 20180222, 20180223, 20180226, 20180227, 20180228, 20180301, 20180302, 20180305, 20180306, 20180307, 20180308, 20180309, 20180312, 20180313, 20180314, 20180315, 20180316, 20180319, 20180320, 20180321, 20180322, 20180323, 20180326, 20180327, 20180328, 20180329, 20180402, 20180403, 20180404, 20180405, 20180406, 20180409, 20180410, 20180411, 20180412, 20180413, 20180416, 20180417, 20180418, 20180419, 20180420, 20180423, 20180424, 20180425, 20180426, 20180427, 20180430, 20180501, 20180502, 20180503, 20180504, 20180507, 20180508, 20180509, 20180510, 20180511, 20180514, 20180515, 20180516, 20180517, 20180518, 20180521, 20180522, 20180523, 20180524, 20180525, 20180529, 20180530, 20180531, 20180601, 20180604, 20180605, 20180606, 20180607, 20180608, 20180611, 20180612, 20180613, 20180614, 20180615, 20180618, 20180619, 20180620, 20180621, 20180622, 20180625, 20180626, 20180627, 20180628, 20180629, 20180702, 20180703, 20180705, 20180706, 20180709, 20180710, 20180711, 20180712, 20180713, 20180716, 20180717, 20180718, 20180719, 20180720, 20180723, 20180724, 20180725, 20180726, 20180727, 20180730, 20180731, 20180801, 20180802, 20180803, 20180806, 20180807, 20180808, 20180809, 20180810, 20180813, 20180814, 20180815, 20180816, 20180817, 20180820, 20180821, 20180822, 20180823, 20180824, 20180827, 20180828, 20180829, 20180830, 20180831, 20180904, 20180905, 20180906, 20180907, 20180910, 20180911, 20180912, 20180913, 20180914, 20180917, 20180918, 20180919, 20180920, 20180921, 20180924, 20180925, 20180926, 20180927, 20180928, 20181001, 20181002, 20181003, 20181004, 20181005, 20181008, 20181009, 20181010, 20181011, 20181012, 20181015, 20181016, 20181017, 20181018, 20181019, 20181022, 20181023, 20181024, 20181025, 20181026, 20181029, 20181030, 20181031, 20181101, 20181102, 20181105, 20181106, 20181107, 20181108, 20181109, 20181112, 20181113, 20181114, 20181115, 20181116, 20181119, 20181120, 20181121, 20181123, 20181126, 20181127, 20181128, 20181129, 20181130, 20181203, 20181204, 20181206, 20181207, 20181210, 20181211, 20181212, 20181213, 20181214, 20181217, 20181218, 20181219, 20181220, 20181221, 20181224, 20181226, 20181227, 20181228, 20181231, 20190102, 20190103, 20190104, 20190107, 20190108, 20190109, 20190110, 20190111, 20190114, 20190115, 20190116, 20190117, 20190118, 20190122, 20190123, 20190124, 20190125, 20190128, 20190129, 20190130, 20190131, 20190201, 20190204, 20190205, 20190206, 20190207, 20190208, 20190211, 20190212, 20190213, 20190214, 20190215, 20190219, 20190220, 20190221, 20190222, 20190225, 20190226, 20190227, 20190228, 20190301, 20190304, 20190305, 20190306, 20190307, 20190308, 20190311, 20190312, 20190313, 20190314, 20190315, 20190318, 20190319, 20190320, 20190321, 20190322, 20190325, 20190326, 20190327, 20190328, 20190329, 20190401, 20190402, 20190403, 20190404, 20190405, 20190408, 20190409, 20190410, 20190411, 20190412, 20190415, 20190416, 20190417, 20190418, 20190422, 20190423, 20190424, 20190425, 20190426, 20190429, 20190430, 20190501, 20190502, 20190503, 20190506, 20190507, 20190508, 20190509, 20190510, 20190513, 20190514, 20190515, 20190516, 20190517, 20190520, 20190521, 20190522, 20190523, 20190524, 20190528, 20190529, 20190530, 20190531, 20190603, 20190604, 20190605, 20190606, 20190607, 20190610, 20190611, 20190612, 20190613, 20190614, 20190617, 20190618, 20190619, 20190620, 20190621, 20190624, 20190625, 20190626, 20190627, 20190628, 20190701, 20190702, 20190703, 20190705, 20190708, 20190709, 20190710, 20190711, 20190712, 20190715, 20190716, 20190717, 20190718, 20190719, 20190722, 20190723, 20190724, 20190725, 20190726, 20190729, 20190730, 20190731, 20190801, 20190802, 20190805, 20190806, 20190807, 20190808, 20190809, 20190812, 20190813, 20190814, 20190815, 20190816, 20190819, 20190820, 20190821, 20190822, 20190823, 20190826, 20190827, 20190828, 20190829, 20190830, 20190903, 20190904, 20190905, 20190906, 20190909, 20190910, 20190911, 20190912, 20190913, 20190916, 20190917, 20190918, 20190919, 20190920, 20190923, 20190924, 20190925, 20190926, 20190927, 20190930, 20191001, 20191002, 20191003, 20191004, 20191007, 20191008, 20191009, 20191010, 20191011, 20191014, 20191015, 20191016, 20191017, 20191018, 20191021, 20191022, 20191023, 20191024, 20191025, 20191028, 20191029, 20191030, 20191031, 20191101, 20191104, 20191105, 20191106, 20191107, 20191108, 20191111, 20191112, 20191113, 20191114, 20191115, 20191118, 20191119, 20191120, 20191121, 20191122, 20191125, 20191126, 20191127, 20191129, 20191202, 20191203, 20191204, 20191205, 20191206, 20191209, 20191210, 20191211, 20191212, 20191213, 20191216, 20191217, 20191218, 20191219, 20191220, 20191223, 20191224, 20191226, 20191227, 20191230, 20191231, 20200102, 20200103, 20200106, 20200107, 20200108, 20200109, 20200110, 20200113, 20200114, 20200115, 20200116, 20200117, 20200121, 20200122, 20200123, 20200124, 20200127, 20200128, 20200129, 20200130, 20200131, 20200203, 20200204, 20200205, 20200206, 20200207, 20200210, 20200211, 20200212, 20200213, 20200214, 20200218, 20200219, 20200220, 20200221, 20200224, 20200225, 20200226, 20200227, 20200228, 20200302, 20200303, 20200304, 20200305, 20200306, 20200309, 20200310, 20200311, 20200312, 20200313, 20200316, 20200317, 20200318, 20200319, 20200320, 20200323, 20200324, 20200325, 20200326, 20200327, 20200330, 20200331, 20200401, 20200402, 20200403, 20200406, 20200407, 20200408, 20200409, 20200413, 20200414, 20200415, 20200416, 20200417, 20200420, 20200421, 20200422, 20200423, 20200424, 20200427, 20200428, 20200429, 20200430, 20200501, 20200504, 20200505, 20200506, 20200507, 20200508, 20200511, 20200512, 20200513, 20200514, 20200515, 20200518, 20200519, 20200520, 20200521, 20200522, 20200526, 20200527, 20200528, 20200529, 20200601, 20200602, 20200603, 20200604, 20200605, 20200608, 20200609, 20200610, 20200611, 20200612, 20200615, 20200616, 20200617, 20200618, 20200619, 20200622, 20200623, 20200624, 20200625, 20200626, 20200629, 20200630, 20200701, 20200702, 20200706, 20200707, 20200708, 20200709, 20200710, 20200713, 20200714, 20200715, 20200716, 20200717, 20200720, 20200721, 20200722, 20200723, 20200724, 20200727, 20200728, 20200729, 20200730, 20200731, 20200803, 20200804, 20200805, 20200806, 20200807, 20200810, 20200811, 20200812, 20200813, 20200814, 20200817, 20200818, 20200819, 20200820, 20200821, 20200824, 20200825, 20200826, 20200827, 20200828, 20200831, 20200901, 20200902, 20200903, 20200904, 20200908, 20200909, 20200910, 20200911, 20200914, 20200915, 20200916, 20200917, 20200918, 20200921, 20200922, 20200923, 20200924, 20200925, 20200928, 20200929, 20200930, 20201001, 20201002, 20201005, 20201006, 20201007, 20201008, 20201009, 20201012, 20201013, 20201014, 20201015, 20201016, 20201019, 20201020, 20201021, 20201022, 20201023, 20201026, 20201027, 20201028, 20201029, 20201030, 20201102, 20201103, 20201104, 20201105, 20201106, 20201109, 20201110, 20201111, 20201112, 20201113, 20201116, 20201117, 20201118, 20201119, 20201120, 20201123, 20201124, 20201125, 20201127, 20201130, 20201201, 20201202, 20201203, 20201204, 20201207, 20201208, 20201209, 20201210, 20201211, 20201214, 20201215, 20201216, 20201217, 20201218, 20201221, 20201222, 20201223, 20201224, 20201228, 20201229, 20201230, 20201231, 20210104, 20210105, 20210106, 20210107, 20210108, 20210111, 20210112, 20210113, 20210114, 20210115, 20210119, 20210120, 20210121, 20210122, 20210125, 20210126, 20210127, 20210128, 20210129, 20210201, 20210202, 20210203, 20210204, 20210205, 20210208, 20210209, 20210210, 20210211, 20210212, 20210216, 20210217, 20210218, 20210219, 20210222, 20210223, 20210224, 20210225, 20210226, 20210301, 20210302, 20210303, 20210304, 20210305, 20210308, 20210309, 20210310, 20210311, 20210312, 20210315, 20210316, 20210317, 20210318, 20210319, 20210322, 20210323, 20210324, 20210325, 20210326, 20210329, 20210330, 20210331, 20210401, 20210405, 20210406, 20210407, 20210408, 20210409, 20210412, 20210413, 20210414, 20210415, 20210416, 20210419, 20210420, 20210421, 20210422, 20210423, 20210426, 20210427, 20210428, 20210429, 20210430, 20210503, 20210504, 20210505, 20210506, 20210507, 20210510, 20210511, 20210512, 20210513, 20210514, 20210517, 20210518, 20210519, 20210520, 20210521, 20210524, 20210525, 20210526, 20210527, 20210528, 20210601, 20210602, 20210603, 20210604, 20210607, 20210608, 20210609, 20210610, 20210611, 20210614, 20210615, 20210616, 20210617, 20210618, 20210621, 20210622, 20210623, 20210624], \"y\": [3.4679120000000005, 3.4494620000000005, 3.429362, 3.4094619999999995, 3.3971119999999995, 3.3841419999999998, 3.371142, 3.3587919999999998, 3.3461420000000004, 3.333392000000001, 3.321692, 3.3091920000000012, 3.2965920000000013, 3.283992, 3.270992, 3.2575920000000003, 3.243342, 3.2287419999999996, 3.214142, 3.199692, 3.185292, 3.171092, 3.156142, 3.1412419999999996, 3.1261419999999993, 3.1103419999999993, 3.0939919999999996, 3.077892, 3.0616920000000003, 3.044342, 3.0264419999999994, 3.007092, 2.989842, 2.9730920000000003, 2.955992, 2.9404920000000003, 2.925592, 2.9102919999999997, 2.879142, 2.864892, 2.8517919999999997, 2.8394920000000003, 2.833092, 2.824242, 2.824242, 2.8164419999999994, 2.8087419999999996, 2.8005420000000005, 2.793542, 2.787442, 2.781542, 2.775242, 2.769442, 2.764142000000001, 2.7590420000000004, 2.755942, 2.753492, 2.7503919999999997, 2.747442, 2.745892, 2.744542, 2.7420919999999995, 2.7425420000000003, 2.7444430000000004, 2.7463430000000004, 2.749343, 2.751643, 2.7527430000000006, 2.752993, 2.753942999999999, 2.753943, 2.754893, 2.755493000000001, 2.756743, 2.7585429999999995, 2.759293, 2.7579480000000003, 2.7570479999999997, 2.7562980000000006, 2.7544980000000003, 2.752998, 2.7520480000000003, 2.750348, 2.7496479999999996, 2.749449, 2.749649, 2.7504989999999996, 2.7520489999999995, 2.7520489999999995, 2.751999, 2.7524990000000003, 2.7549989999999998, 2.7537989999999994, 2.7508989999999995, 2.746098999999999, 2.7406489999999994, 2.7356489999999996, 2.7309490000000007, 2.727699999999999, 2.7240999999999995, 2.7195000000000005, 2.7152, 2.7116, 2.7091, 2.7060500000000003, 2.704, 2.7021, 2.6999, 2.6984500000000002, 2.6970500000000004, 2.6951, 2.6929, 2.6906, 2.6875, 2.6837000000000004, 2.6803, 2.6766500000000004, 2.6727999999999996, 2.6687, 2.6645000000000003, 2.6602499999999996, 2.65645, 2.6528499999999995, 2.64995, 2.64365, 2.64055, 2.63695, 2.6336500000000003, 2.6316999999999995, 2.631, 2.6315, 2.6315000000000004, 2.6304000000000003, 2.6291999999999995, 2.6294, 2.6285999999999996, 2.6273, 2.6271, 2.6271000000000004, 2.6277500000000003, 2.6270499999999997, 2.6240500000000004, 2.61895, 2.6098500000000002, 2.6036999999999995, 2.5987, 2.5928000000000004, 2.583, 2.5720500000000004, 2.56035, 2.5488500000000003, 2.5376000000000003, 2.5275000000000003, 2.516379, 2.504579, 2.491479, 2.478479, 2.465679, 2.452529, 2.438029, 2.4232789999999995, 2.409079, 2.396879, 2.387379, 2.377379, 2.3675790000000005, 2.3612789999999997, 2.355079, 2.3491289999999996, 2.342679, 2.336029, 2.327829, 2.3193289999999998, 2.3103290000000003, 2.299829, 2.290229, 2.281929, 2.2734289999999997, 2.2641289999999996, 2.254229, 2.2453289999999995, 2.235829, 2.225729, 2.215929, 2.206829, 2.1982289999999995, 2.1891789999999998, 2.1787289999999997, 2.168679, 2.160029, 2.1512290000000003, 2.141329, 2.134029, 2.127529, 2.124129, 2.1193790000000003, 2.1145790000000004, 2.1103289999999997, 2.105879, 2.101279, 2.0965789999999997, 2.091329, 2.085229, 2.0792289999999998, 2.073629, 2.067479, 2.0609789999999997, 2.055179, 2.049379, 2.0440790000000004, 2.0384790000000006, 2.028779, 2.024779, 2.021479, 2.016979, 2.013329, 2.010179, 2.0101789999999995, 2.0075790000000002, 2.0043790000000006, 2.001679, 1.9990290000000002, 1.9974290000000001, 1.9959289999999998, 1.9950790000000003, 1.9933290000000006, 1.9915790000000004, 1.9903790000000001, 1.988729, 1.9867790000000003, 1.9854790000000002, 1.9850790000000003, 1.9837789999999997, 1.9822789999999997, 1.9789789999999998, 1.974679, 1.9691790000000002, 1.9637790000000002, 1.957829, 1.9514790000000002, 1.9454790000000002, 1.940679, 1.9383789999999999, 1.9356790000000001, 1.933779, 1.9305790000000003, 1.9293790000000002, 1.932379, 1.935729, 1.9395290000000003, 1.9432790000000002, 1.9464290000000002, 1.9482790000000003, 1.9515, 1.9563000000000001, 1.9676999999999998, 1.9741999999999995, 1.9798999999999998, 1.9857, 1.9926000000000001, 2.00185, 2.0018500000000006, 2.0097500000000004, 2.0146499999999996, 2.02065, 2.02895, 2.03525, 2.04335, 2.05175, 2.0606500000000003, 2.0695, 2.0789500000000003, 2.08765, 2.0961000000000003, 2.1032, 2.1076, 2.1107500000000003, 2.11605, 2.122, 2.1275, 2.1317000000000004, 2.1344, 2.1370999999999998, 2.1378, 2.1408009999999997, 2.142501, 2.145601, 2.1493010000000004, 2.152901, 2.155801, 2.1596010000000003, 2.1622510000000004, 2.163751, 2.1661409999999997, 2.1670409999999998, 2.168741, 2.169441, 2.169141, 2.169041, 2.1691909999999996, 2.170141, 2.1720409999999997, 2.1738910000000002, 2.1757409999999995, 2.175741, 2.1791650000000002, 2.1822649999999997, 2.185365, 2.188165, 2.1906650000000005, 2.194015, 2.197565, 2.2022150000000003, 2.207465, 2.211565, 2.217565, 2.2214650000000002, 2.224365, 2.2273649999999994, 2.2331149999999997, 2.240414999999999, 2.2450649999999994, 2.2504150000000003, 2.2569650000000006, 2.2642150000000005, 2.270615, 2.2769649999999997, 2.281965, 2.2886149999999996, 2.295165, 2.3017649999999996, 2.3076149999999997, 2.3140149999999995, 2.3203150000000003, 2.327065, 2.333615, 2.341315, 2.347015, 2.354365, 2.3619649999999996, 2.370065, 2.3782650000000003, 2.3841650000000003, 2.389615, 2.395115, 2.399165, 2.4027149999999997, 2.405015, 2.4214150000000005, 2.432415, 2.445665, 2.460315, 2.4739649999999997, 2.4866650000000003, 2.500565, 2.512765, 2.525315, 2.5375150000000004, 2.550815, 2.563815, 2.576515, 2.5875150000000002, 2.5978149999999998, 2.610065, 2.6225650000000003, 2.6327649999999996, 2.642865, 2.651065, 2.659865, 2.6702650000000006, 2.6824149999999998, 2.695515, 2.7113649999999994, 2.7299149999999996, 2.750615, 2.772215, 2.7934149999999995, 2.8116149999999993, 2.832264999999999, 2.854765, 2.8772649999999995, 2.902265, 2.926115, 2.9523150000000005, 2.9995139999999996, 3.025714, 3.057613999999999, 3.0872640000000002, 3.1208639999999996, 3.1491139999999995, 3.1491139999999995, 3.179763, 3.2084630000000005, 3.234973000000001, 3.2650729999999997, 3.2965729999999995, 3.328673, 3.360373, 3.391523, 3.4234229999999997, 3.4551729999999994, 3.487873, 3.519073, 3.551423, 3.583323000000001, 3.6148990000000003, 3.6465989999999993, 3.6814989999999996, 3.7153989999999997, 3.7487989999999995, 3.779599, 3.816149, 3.859999, 3.9051990000000005, 3.949999000000001, 3.9935489999999993, 4.0389990000000004, 4.0828489999999995, 4.122799, 4.1605490000000005, 4.197949, 4.239249, 4.279749000000001, 4.317649, 4.353299000000001, 4.391199000000001, 4.430649, 4.473299, 4.552199, 4.594099, 4.641749, 4.6891490000000005, 4.7375490000000005, 4.783699, 4.783699, 4.830849, 4.879649, 4.929099, 4.976649, 5.023099000000001, 5.068599000000001, 5.1157189999999995, 5.162118999999999, 5.203619, 5.238119, 5.270869, 5.303319000000001, 5.334519000000001, 5.355018999999999, 5.3805689999999995, 5.404469000000001, 5.428668999999998, 5.454418999999998, 5.481718999999999, 5.507969, 5.537469, 5.564719000000001, 5.593369, 5.622269, 5.652519, 5.685419, 5.718419, 5.7521689999999985, 5.783218999999999, 5.814918999999999, 5.84436, 5.87426, 5.90166, 5.928909999999999, 5.95341, 6.00381, 6.027559999999999, 6.049259999999999, 6.073159999999999, 6.094060000000001, 6.121659999999999, 6.15501, 6.15501, 6.1833100000000005, 6.20931, 6.23676, 6.2637599999999996, 6.29151, 6.31501, 6.337909999999999, 6.359259999999999, 6.381310000000001, 6.3986100000000015, 6.41716, 6.425609999999999, 6.442209999999999, 6.458011000000002, 6.479111, 6.508711, 6.542311, 6.5781610000000015, 6.6162610000000015, 6.652411, 6.690810999999999, 6.7284109999999995, 6.766611000000001, 6.805011, 6.844111000000001, 6.876510999999999, 6.910810999999999, 6.945910999999999, 6.9890110000000005, 7.029511, 7.078761000000001, 7.128011000000002, 7.182711000000003, 7.229811000000001, 7.268211, 7.306911000000001, 7.345011, 7.386211, 7.432760999999999, 7.481210999999999, 7.5347610000000005, 7.5876610000000015, 7.643661000000001, 7.6929609999999995, 7.742111000000001, 7.789461000000001, 7.838711000000001, 7.887261, 7.932411000000001, 7.976161000000001, 8.023361000000001, 8.070861, 8.112461, 8.143861, 8.173861000000002, 8.195311, 8.219711, 8.242811, 8.263511000000001, 8.286760999999998, 8.316211, 8.345861000000001, 8.377461, 8.409041, 8.441741, 8.477191000000001, 8.535491, 8.599391, 8.662291, 8.741390999999998, 8.813790999999998, 8.889140999999997, 8.962841, 9.036990999999999, 9.110141, 9.179941, 9.249241, 9.313441000000001, 9.381441, 9.533040999999999, 9.609490999999998, 9.681491, 9.763991, 9.838941, 9.920691000000001, 9.920691000000001, 9.990091, 10.053, 10.1151, 10.1806, 10.246749999999999, 10.315150000000001, 10.38675, 10.462700000000002, 10.5363, 10.60845, 10.675350000000002, 10.74505, 10.819049999999997, 10.8823, 10.950399999999995, 11.0172, 11.082149999999999, 11.146949999999999, 11.212800000000001, 11.2824, 11.356, 11.43605, 11.51305, 11.584650000000002, 11.656799999999999, 11.726649999999998, 11.795, 11.858100000000002, 11.91935, 11.970149999999999, 12.008549999999998, 12.0493, 12.0892, 12.130650000000001, 12.173749999999995, 12.215749999999998, 12.258750000000003, 12.349150000000002, 12.40155, 12.44925, 12.498649999999998, 12.507200000000001, 12.515349999999998, 12.515350000000002, 12.51285, 12.5113, 12.5048, 12.501, 12.503400000000001, 12.505550000000001, 12.511549999999998, 12.516249999999998, 12.528799999999997, 12.525949999999996, 12.522599999999997, 12.5209, 12.5106, 12.504049999999998, 12.4969, 12.493300000000001, 12.489199999999999, 12.485999999999997, 12.485449999999998, 12.48155, 12.47555, 12.473550000000003, 12.481849999999998, 12.498050000000003, 12.521250000000002, 12.5458, 12.56785, 12.58965, 12.609950000000001, 12.625849999999998, 12.63565, 12.65135, 12.67265, 12.70585, 12.74355, 12.80125, 12.812350000000002, 12.822250000000002, 12.811899999999998, 12.8038, 12.790199000000003, 12.787799, 12.787799, 12.782399, 12.781099, 12.786699000000004, 12.792649000000004, 12.805749000000002, 12.809749, 12.808949, 12.804098999999997, 12.795798999999999, 12.790099, 12.776098999999999, 12.770299000000001, 12.762448000000001, 12.764798000000003, 12.782148, 12.792949, 12.801849000000002, 12.805799, 12.809599, 12.804298999999999, 12.793999, 12.784298999999999, 12.778699000000001, 12.773149, 12.766449, 12.743648999999996, 12.727649000000001, 12.714349000000002, 12.706699, 12.696099000000002, 12.682399, 12.669349000000002, 12.652749000000002, 12.633899000000001, 12.613049, 12.591549, 12.574149, 12.554749, 12.543449, 12.534849000000001, 12.533749, 12.534699, 12.536299000000001, 12.541699, 12.540198999999998, 12.533198999999998, 12.530498999999997, 12.522399, 12.514699000000002, 12.505949000000003, 12.496349000000002, 12.492949, 12.487848999999999, 12.492348999999999, 12.490248999999999, 12.520198999999998, 12.542448999999998, 12.566048999999998, 12.591749000000002, 12.618649, 12.644448999999998, 12.663699, 12.687199, 12.707698999999998, 12.726898999999998, 12.731549, 12.754299000000001, 12.778549, 12.803249000000001, 12.834848999999998, 12.868198999999999, 12.901949, 12.933749, 12.964248999999997, 12.992649000000002, 13.018849000000003, 13.050548999999998, 13.084349000000001, 13.095249, 13.089299, 13.069199000000001, 13.056349, 13.043498999999997, 13.032398999999998, 13.025898999999997, 13.025898999999999, 13.030399, 13.036599, 13.034300000000004, 13.019100000000002, 12.991800000000003, 12.958800000000002, 12.928699999999997, 12.898699999999996, 12.8773, 12.85895, 12.846250000000003, 12.835450000000003, 12.827351, 12.809051, 12.794350999999997, 12.772451, 12.742501000000003, 12.712601999999999, 12.677002, 12.642002, 12.601951999999999, 12.563951999999999, 12.529651999999999, 12.493352000000002, 12.456902000000003, 12.417102, 12.376453000000001, 12.336403, 12.291803, 12.260436000000002, 12.230535999999999, 12.204286, 12.176086, 12.147786000000002, 12.120185999999997, 12.094236, 12.065486, 12.019135999999998, 12.013435999999999, 12.012386, 12.003385999999999, 11.995935999999999, 11.987636, 11.987636, 11.983935999999998, 11.981636000000002, 11.981335999999999, 11.978786, 11.975836000000001, 11.975536000000002, 11.977036, 11.981235999999999, 11.989036000000004, 11.989436000000001, 11.983636000000004, 11.981037, 11.985135000000003, 11.985285000000001, 11.996285, 12.006385, 12.005285, 11.997985000000002, 11.992185000000001, 11.985734999999998, 11.972635999999998, 11.955036, 11.940836000000001, 11.921235999999999, 11.909135999999998, 11.898035999999998, 11.890235999999998, 11.885986000000003, 11.875786, 11.866836, 11.860085999999999, 11.857136, 11.848186, 11.836186, 11.821736000000001, 11.791886, 11.772436000000003, 11.756036000000002, 11.733736, 11.708536000000002, 11.681036, 11.655936000000002, 11.655936000000004, 11.628936000000001, 11.604035999999999, 11.580636, 11.553936, 11.522335999999997, 11.511636000000003, 11.500786, 11.488986, 11.484385999999997, 11.474535999999995, 11.464585999999999, 11.456685999999998, 11.440886, 11.416786, 11.394135999999998, 11.377335, 11.362134999999999, 11.344734999999996, 11.333834999999999, 11.320834999999999, 11.311034999999999, 11.297935000000003, 11.285085000000002, 11.276935000000002, 11.266734999999999, 11.254134999999996, 11.240635, 11.225335, 11.214535000000001, 11.203485, 11.204784, 11.208583999999998, 11.217084000000002, 11.229233999999998, 11.238983999999999, 11.247784000000001, 11.261384, 11.275734, 11.292833999999997, 11.311284, 11.331484000000003, 11.347984000000004, 11.36035, 11.375350000000003, 11.39375, 11.413049999999997, 11.437700000000001, 11.4628, 11.4873, 11.5128, 11.544, 11.5695, 11.587549999999998, 11.604599999999998, 11.623099999999999, 11.644300000000001, 11.674600000000003, 11.703399999999998, 11.738599999999998, 11.767101000000002, 11.800401, 11.836001, 11.869800999999997, 11.907301, 11.943251, 11.977351, 12.021450999999999, 12.064151000000003, 12.09995, 12.123152000000003, 12.152301999999999, 12.166052, 12.188652, 12.213852, 12.251252, 12.284551999999998, 12.319952, 12.35815, 12.400049999999998, 12.49495, 12.538449999999997, 12.57925, 12.626500000000004, 12.66895, 12.717450000000001, 12.717450000000001, 12.767750000000001, 12.815498999999999, 12.858398999999999, 12.898148999999998, 12.943549, 12.986549000000002, 13.028949, 13.093399, 13.165249000000001, 13.237098999999999, 13.300799000000001, 13.368499000000002, 13.441298999999999, 13.509798999999997, 13.590565000000002, 13.671665, 13.752764999999997, 13.829464999999997, 13.908915000000002, 13.993515000000002, 14.084615000000001, 14.175315000000003, 14.264315, 14.362015, 14.463764999999999, 14.567065, 14.680665000000001, 14.808264999999999, 14.950314999999998, 15.102714999999998, 15.257214999999999, 15.413915, 15.562815, 15.716215000000002, 15.895972000000002, 16.081671999999998, 16.258971999999996, 16.622799999999998, 16.8226, 17.0448, 17.2492, 17.4755, 17.702585, 17.702584999999996, 17.911585, 18.112634999999997, 18.315634999999997, 18.514535, 18.730884999999997, 18.947335, 19.156335, 19.366335, 19.559135, 19.753735000000002, 19.922435, 20.087135000000004, 20.242735, 20.391835, 20.528185000000004, 20.672485, 20.792534999999997, 20.915635, 21.049285, 21.180784999999997, 21.328386000000002, 21.466085999999997, 21.598636000000003, 21.697026, 21.810176000000002, 21.917076000000005, 21.996576, 22.040825999999996, 22.06037599999999, 22.080174999999993, 22.09992, 22.124619999999997, 22.168319999999994, 22.207335999999994, 22.244135999999997, 22.334836000000006, 22.380036, 22.425235999999998, 22.459022000000004, 22.497322000000004, 22.554372000000008, 22.614271999999996, 22.614272, 22.671172000000002, 22.709272000000002, 22.751572000000007, 22.787334, 22.831435000000003, 22.877235, 22.924235000000003, 22.971535000000003, 23.020235, 23.070631999999996, 23.142081999999995, 23.190582, 23.237782000000006, 23.263732000000005, 23.295082999999998, 23.327883, 23.367783, 23.399883000000003, 23.436883, 23.464683, 23.47628300000001, 23.468583000000002, 23.453833, 23.439532999999997, 23.421632999999996, 23.412632999999996, 23.402633, 23.386566999999996, 23.375467, 23.367966999999993, 23.347463999999995, 23.346914, 23.355514, 23.362713999999997, 23.367513999999996, 23.371613999999994, 23.376614000000004, 23.379164, 23.378964000000007, 23.367164000000002, 23.346713000000005, 23.314513000000005, 23.25961300000001, 23.207013000000003, 23.163612000000004, 23.134112000000002, 23.084359, 22.996102, 22.942201999999998, 22.908302, 22.879574, 22.821875, 22.75377499999999, 22.664175, 22.586174999999994, 22.489574999999995, 22.394789999999993, 22.303618999999994, 22.220069, 22.139569, 22.066269, 21.979668999999998, 21.893518999999998, 21.810919, 21.728619, 21.666819000000004, 21.594819, 21.539519000000006, 21.490719000000002, 21.449718000000004, 21.409818, 21.380168000000005, 21.331768, 21.302368000000005, 21.269471, 21.235570999999997, 21.207871, 21.15977, 21.114970000000003, 21.077880000000004, 21.088030000000003, 21.09413, 21.145129999999995, 21.216179999999998, 21.299429999999997, 21.299429999999997, 21.387829999999994, 21.464399999999998, 21.53295, 21.585949999999997, 21.647334, 21.715633999999994, 21.798833999999996, 21.871235000000002, 21.949135000000002, 22.024235000000004, 22.106349, 22.188599, 22.258298999999997, 22.321773999999994, 22.388473999999995, 22.476612999999997, 22.559513000000003, 22.649018, 22.736318000000004, 22.815218, 22.889218, 22.952318, 23.016918, 23.080631999999994, 23.119932, 23.176732, 23.246632000000005, 23.334231999999997, 23.408682, 23.475382, 23.541382000000002, 23.614782, 23.695382, 23.769481999999996, 23.847481999999996, 23.941581999999993, 24.042282, 24.248181999999996, 24.342811999999995, 24.442111999999998, 24.527412000000005, 24.607212, 24.709262, 24.709262, 24.819564999999994, 24.909765000000004, 24.977964999999994, 25.046214999999993, 25.140064999999996, 25.237665000000003, 25.353365000000004, 25.475115000000002, 25.603714999999998, 25.730614999999997, 25.849866, 25.956066, 26.061866000000006, 26.155866, 26.251966999999993, 26.337666999999996, 26.43582, 26.53422, 26.59552, 26.63962, 26.69362, 26.759719000000004, 26.830619000000002, 26.910019, 26.995919, 27.076518999999994, 27.161918999999994, 27.254289999999997, 27.357189999999996, 27.463739999999994, 27.55754, 27.65009, 27.754389999999997, 27.85339, 27.945590000000003, 28.111389999999997, 28.205190000000005, 28.304790000000004, 28.409091000000004, 28.511990999999995, 28.617090999999995, 28.727790999999996, 28.727791000000003, 28.845790999999995, 28.929987999999994, 28.999088, 29.058888000000003, 29.105088000000002, 29.165288000000004, 29.224188000000005, 29.331088, 29.412387999999993, 29.479788, 29.521987999999997, 29.560788, 29.597887999999998, 29.652688, 29.718573000000003, 29.775023, 29.836823, 29.892322999999998, 29.920323000000003, 29.933122999999995, 29.944421999999996, 29.962322, 29.991388, 30.033288999999996, 30.063939, 30.095439000000002, 30.131963999999996, 30.164362999999998, 30.190124, 30.217623999999997, 30.238599, 30.259098999999996, 30.286499, 30.310199, 30.343698999999997, 30.369199000000002, 30.395084999999998, 30.419185, 30.457660999999995, 30.47006099999999, 30.483261, 30.503676999999996, 30.524276999999998, 30.543276999999993, 30.558877, 30.562577, 30.587077, 30.603977, 30.617576999999997, 30.619676999999996, 30.629312, 30.646511999999998, 30.670382, 30.701582, 30.745183, 30.788981999999997, 30.810132, 30.839031999999996, 30.879282, 30.920382000000004, 30.958232000000002, 30.979882000000003, 31.011982000000003, 31.030581999999995, 31.03704, 31.036139999999996, 31.05164, 31.078739999999996, 31.12774, 31.18564, 31.253039999999995, 31.311239999999998, 31.369089999999996, 31.432189999999995, 31.50838999999999, 31.590989999999998, 31.68599, 31.77239, 31.86404, 32.070739999999994, 32.153147, 32.233346999999995, 32.316447, 32.385647, 32.44834699999999, 32.44834699999999, 32.501896, 32.558595999999994, 32.615379000000004, 32.668379, 32.726379, 32.786779, 32.846029, 32.915329, 32.981329, 33.072479, 33.142879, 33.229579, 33.317285999999996, 33.405486, 33.495186000000004, 33.632486, 33.788585999999995, 33.960086000000004, 34.146685999999995, 34.31998599999999, 34.48328599999999, 34.602486, 34.75168599999999, 34.913286, 35.076186, 35.256085999999996, 35.43778599999999, 35.615586, 35.78268599999999, 35.963086000000004, 36.128035999999994, 36.294336, 36.496435999999996, 36.702736, 36.911336000000006, 37.11773600000001, 37.32027, 37.693118999999996, 37.889269, 38.049369, 38.23137, 38.395970000000005, 38.573769999999996, 38.573769999999996, 38.77062800000001, 38.966828, 39.152928, 39.341928, 39.554628, 39.788328, 40.02463100000001, 40.269631, 40.516355000000004, 40.78995499999999, 41.083655, 41.36173900000001, 41.607239, 41.808339, 41.99638899999999, 42.18798899999999, 42.34108899999999, 42.50588899999998, 42.69098899999998, 42.875789, 43.092153999999994, 43.28955399999999, 43.47795399999999, 43.60555399999999, 43.751753, 43.900354, 43.978854, 44.107853999999996, 44.17390399999999, 44.277303999999994, 44.35310399999999, 44.43350400000001, 44.502504, 44.582204000000004, 44.71374600000001, 44.963846, 45.080645999999994, 45.196246, 45.28954600000001, 45.366946000000006, 45.449045999999996, 45.512196, 45.512196, 45.62409600000001, 45.73249600000001, 45.845196000000016, 45.945296, 46.069246, 46.21969599999999, 46.35699600000001, 46.51639599999999, 46.68738900000001, 46.86518900000001, 46.996389, 47.165889, 47.330389, 47.500589999999995, 47.67839, 47.844857000000005, 47.984657, 48.11215699999999, 48.214957, 48.351707, 48.479006999999996, 48.60590699999999, 48.69915699999999, 48.819557, 48.95355699999999, 49.06335, 49.16165, 49.27855, 49.37915000000001, 49.47025000000001, 49.55865000000001, 49.656050000000015, 49.74065000000002, 49.837050000000005, 49.910250000000005, 49.94665, 49.97794999999999, 50.02924999999998, 50.08315, 50.14014999999999, 50.17784999999999, 50.22245, 50.26585, 50.31360000000001, 50.39200000000001, 50.468900000000005, 50.4881, 50.5126, 50.5453, 50.57290000000001, 50.614850000000004, 50.66245000000001, 50.699100000000016, 50.771600000000014, 50.8239, 50.8782, 50.9171, 50.92359999999999, 50.9279, 50.96030000000002, 50.98870000000001, 50.989399999999996, 50.98519999999999, 50.975500000000004, 50.964400000000005, 50.9841, 50.97410000000001, 50.92080000000001, 50.8951, 50.9159, 50.97419999999999, 51.049049999999994, 51.14904999999999, 51.278949999999995, 51.44205, 51.56345000000001, 51.79055000000001, 51.97935, 52.17555, 52.799650000000014, 53.12045, 53.44010000000001, 53.9005, 54.316100000000006, 54.7967, 54.7967, 55.2268, 55.659299999999995, 56.030599999999986, 56.46019999999999, 56.861799999999995, 57.21240000000001, 57.59009999999999, 57.931999999999995, 58.2772, 58.62650000000001, 59.01, 59.404399999999995, 59.8225, 60.25679999999999, 60.620099999999994, 61.000099999999996, 61.42119999999999, 61.85969999999998, 62.25235, 62.52845, 62.798550000000006, 63.016949999999994, 63.27004999999999, 63.49045, 63.72464999999999, 63.94434999999999, 64.17535000000001, 64.38055, 64.58105, 64.77485, 65.01785, 65.27114999999999, 65.51934999999999, 65.75155000000001, 66.01035, 66.28395, 66.58205, 67.16135, 67.44235, 67.78225, 68.08184999999999, 68.40655, 68.72575, 68.72575, 69.00245000000001, 69.28175, 69.58794999999999, 69.87835, 70.17795, 70.48244999999999, 70.78525, 71.06545000000001, 71.32135, 71.58035000000001, 71.87275000000001, 72.16895000000001, 72.42605, 72.66025, 72.87665000000001, 73.05515000000001, 73.27355, 73.50415, 73.77074999999999, 74.05595, 74.36915, 74.65965, 74.89755, 75.16274999999999, 75.44094999999999, 75.73125, 76.04955, 76.38225, 76.70485000000001, 77.03515, 77.35555000000001, 77.68525000000001, 78.00205000000001, 78.34005, 78.67795000000001, 79.39945, 79.80095, 80.17705, 80.56375000000001, 80.95505000000001, 81.33375, 81.65735, 81.65735, 82.00385, 82.30234999999999, 82.65405000000001, 82.93105, 83.20945, 83.50045000000001, 83.69755, 83.84815, 84.00545000000002, 84.14460000000001, 84.2124, 84.2743, 84.31340000000002, 84.3876, 84.4825, 84.63649999999998, 84.7386, 84.82359999999998, 84.9618, 85.0835, 85.2391, 85.38180000000003, 85.4719, 85.54160000000002, 85.59190000000001, 85.6234, 85.6506, 85.7275, 85.79989999999998, 85.83249999999998, 85.85749999999996, 85.8444, 85.89410000000002, 85.93, 86.0192, 86.08879999999999, 86.17819999999998, 86.29329999999999, 86.3928, 86.51769999999999, 86.65989999999998, 86.8177, 86.9954, 87.15360000000003, 87.29129999999998, 87.44349999999999, 87.57179999999998, 87.6873, 87.74609999999997, 87.77589999999998, 87.8255, 87.80070000000002, 87.82710000000002, 87.83, 87.82620000000003, 87.76790000000001, 87.68090000000001, 87.63510000000001, 87.53150000000001, 87.46420000000002, 87.39699999999999, 87.3778, 87.3565, 87.36139999999997, 87.37360000000001, 87.40780000000001, 87.39479999999998, 87.3655, 87.3459, 87.34179999999998, 87.34219999999999, 87.32430000000001, 87.34539999999998, 87.37009999999998, 87.36449999999999, 87.336, 87.31710000000001, 87.2726, 87.2561, 87.319, 87.3283, 87.3002, 87.26460000000002, 87.21640000000001, 87.22130000000001, 87.22129999999999, 87.18720000000002, 87.15239999999999, 87.09189999999997, 87.05739999999997, 86.98069999999998, 86.93610000000001, 86.86355000000002, 86.78984999999999, 86.69274999999999, 86.60905000000001, 86.48464999999999, 86.32954999999998, 86.1866, 86.0667, 85.929, 85.80060000000002, 85.61290000000001, 85.41, 85.1882, 84.95039999999999, 84.73809999999999, 84.55189999999999, 84.3648, 84.2118, 84.0747, 83.9303, 83.79840000000002, 83.6543, 83.5208, 83.38159999999999, 83.25460000000001, 83.1593, 83.0282, 82.88529999999999, 82.7285, 82.5882, 82.47920000000002, 82.305, 82.2236, 82.15150000000001, 82.0412, 81.91449999999999, 81.8185, 81.8185, 81.7179, 81.6547, 81.6152, 81.59719999999999, 81.5814, 81.5488, 81.5406, 81.55720000000001, 81.618, 81.6367, 81.6746, 81.69590000000001, 81.67439999999999, 81.634, 81.628, 81.6366, 81.65290000000002, 81.64750000000001, 81.66279999999999, 81.6745, 81.67139999999999, 81.71849999999999, 81.76759999999999, 81.81559999999999, 81.896, 82.00619999999999, 82.13949999999998, 82.333, 82.62289999999999, 82.90009999999998, 83.21170000000001, 83.52510000000001, 83.902, 84.2004, 84.47379999999997, 84.7233, 85.29279999999999, 85.55479999999997, 85.87639999999999, 86.18669999999999, 86.49989999999998, 86.76010000000001, 86.76010000000001, 87.0254, 87.3118, 87.6147, 87.8803, 88.14880000000001, 88.40710000000001, 88.6991, 88.97830000000002, 89.2578, 89.5714, 89.86129999999999]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('7ac96a99-7425-4b7b-9cfb-7631b1da6452');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"648a2d0a-bd3e-4253-b1a0-ce7d1aaef282\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"648a2d0a-bd3e-4253-b1a0-ce7d1aaef282\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '648a2d0a-bd3e-4253-b1a0-ce7d1aaef282',\n",
              "                        [{\"type\": \"scatter\", \"x\": [20140924, 20140925, 20140926, 20140929, 20140930, 20141001, 20141002, 20141003, 20141006, 20141007, 20141008, 20141009, 20141010, 20141013, 20141014, 20141015, 20141016, 20141017, 20141020, 20141021, 20141022, 20141023, 20141024, 20141027, 20141028, 20141029, 20141030, 20141031, 20141103, 20141104, 20141105, 20141106, 20141107, 20141110, 20141111, 20141112, 20141113, 20141114, 20141117, 20141118, 20141119, 20141120, 20141121, 20141124, 20141125, 20141126, 20141128, 20141201, 20141202, 20141203, 20141204, 20141205, 20141208, 20141209, 20141210, 20141211, 20141212, 20141215, 20141216, 20141217, 20141218, 20141219, 20141222, 20141223, 20141224, 20141226, 20141229, 20141230, 20141231, 20150102, 20150105, 20150106, 20150107, 20150108, 20150109, 20150112, 20150113, 20150114, 20150115, 20150116, 20150120, 20150121, 20150122, 20150123, 20150126, 20150127, 20150128, 20150129, 20150130, 20150202, 20150203, 20150204, 20150205, 20150206, 20150209, 20150210, 20150211, 20150212, 20150213, 20150217, 20150218, 20150219, 20150220, 20150223, 20150224, 20150225, 20150226, 20150227, 20150302, 20150303, 20150304, 20150305, 20150306, 20150309, 20150310, 20150311, 20150312, 20150313, 20150316, 20150317, 20150318, 20150319, 20150320, 20150323, 20150324, 20150325, 20150326, 20150327, 20150330, 20150331, 20150401, 20150402, 20150406, 20150407, 20150408, 20150409, 20150410, 20150413, 20150414, 20150415, 20150416, 20150417, 20150420, 20150421, 20150422, 20150423, 20150424, 20150427, 20150428, 20150429, 20150430, 20150501, 20150504, 20150505, 20150506, 20150507, 20150508, 20150511, 20150512, 20150513, 20150514, 20150515, 20150518, 20150519, 20150520, 20150521, 20150522, 20150526, 20150527, 20150528, 20150529, 20150601, 20150602, 20150603, 20150604, 20150605, 20150608, 20150609, 20150610, 20150611, 20150612, 20150615, 20150616, 20150617, 20150618, 20150619, 20150622, 20150623, 20150624, 20150625, 20150626, 20150629, 20150630, 20150701, 20150702, 20150706, 20150707, 20150708, 20150709, 20150710, 20150713, 20150714, 20150715, 20150716, 20150717, 20150720, 20150721, 20150722, 20150723, 20150724, 20150727, 20150728, 20150729, 20150730, 20150731, 20150803, 20150804, 20150805, 20150806, 20150807, 20150810, 20150811, 20150812, 20150813, 20150814, 20150817, 20150818, 20150819, 20150820, 20150821, 20150824, 20150825, 20150826, 20150827, 20150828, 20150831, 20150901, 20150902, 20150903, 20150904, 20150908, 20150909, 20150910, 20150911, 20150914, 20150915, 20150916, 20150917, 20150918, 20150921, 20150922, 20150923, 20150924, 20150925, 20150928, 20150929, 20150930, 20151001, 20151002, 20151005, 20151006, 20151007, 20151008, 20151009, 20151012, 20151013, 20151014, 20151015, 20151016, 20151019, 20151020, 20151021, 20151022, 20151023, 20151026, 20151027, 20151028, 20151029, 20151030, 20151102, 20151103, 20151104, 20151105, 20151106, 20151109, 20151110, 20151111, 20151112, 20151113, 20151116, 20151117, 20151118, 20151119, 20151120, 20151123, 20151124, 20151125, 20151127, 20151130, 20151201, 20151202, 20151203, 20151204, 20151207, 20151208, 20151209, 20151210, 20151211, 20151214, 20151215, 20151216, 20151217, 20151218, 20151221, 20151222, 20151223, 20151224, 20151228, 20151229, 20151230, 20151231, 20160104, 20160105, 20160106, 20160107, 20160108, 20160111, 20160112, 20160113, 20160114, 20160115, 20160119, 20160120, 20160121, 20160122, 20160125, 20160126, 20160127, 20160128, 20160129, 20160201, 20160202, 20160203, 20160204, 20160205, 20160208, 20160209, 20160210, 20160211, 20160212, 20160216, 20160217, 20160218, 20160219, 20160222, 20160223, 20160224, 20160225, 20160226, 20160229, 20160301, 20160302, 20160303, 20160304, 20160307, 20160308, 20160309, 20160310, 20160311, 20160314, 20160315, 20160316, 20160317, 20160318, 20160321, 20160322, 20160323, 20160324, 20160328, 20160329, 20160330, 20160331, 20160401, 20160404, 20160405, 20160406, 20160407, 20160408, 20160411, 20160412, 20160413, 20160414, 20160415, 20160418, 20160419, 20160420, 20160421, 20160422, 20160425, 20160426, 20160427, 20160428, 20160429, 20160502, 20160503, 20160504, 20160505, 20160506, 20160509, 20160510, 20160511, 20160512, 20160513, 20160516, 20160517, 20160518, 20160519, 20160520, 20160523, 20160524, 20160525, 20160526, 20160527, 20160531, 20160601, 20160602, 20160603, 20160606, 20160607, 20160608, 20160609, 20160610, 20160613, 20160614, 20160615, 20160616, 20160617, 20160620, 20160621, 20160622, 20160623, 20160624, 20160627, 20160628, 20160629, 20160630, 20160701, 20160705, 20160706, 20160707, 20160708, 20160711, 20160712, 20160713, 20160714, 20160715, 20160718, 20160719, 20160720, 20160721, 20160722, 20160725, 20160726, 20160727, 20160728, 20160729, 20160801, 20160802, 20160803, 20160804, 20160805, 20160808, 20160809, 20160810, 20160811, 20160812, 20160815, 20160816, 20160817, 20160818, 20160819, 20160822, 20160823, 20160824, 20160825, 20160826, 20160829, 20160830, 20160831, 20160901, 20160902, 20160906, 20160907, 20160908, 20160909, 20160912, 20160913, 20160914, 20160915, 20160916, 20160919, 20160920, 20160921, 20160922, 20160923, 20160926, 20160927, 20160928, 20160929, 20160930, 20161003, 20161004, 20161005, 20161006, 20161007, 20161010, 20161011, 20161012, 20161013, 20161014, 20161017, 20161018, 20161019, 20161020, 20161021, 20161024, 20161025, 20161026, 20161027, 20161028, 20161031, 20161101, 20161102, 20161103, 20161104, 20161107, 20161108, 20161109, 20161110, 20161111, 20161114, 20161115, 20161116, 20161117, 20161118, 20161121, 20161122, 20161123, 20161125, 20161128, 20161129, 20161130, 20161201, 20161202, 20161205, 20161206, 20161207, 20161208, 20161209, 20161212, 20161213, 20161214, 20161215, 20161216, 20161219, 20161220, 20161221, 20161222, 20161223, 20161227, 20161228, 20161229, 20161230, 20170103, 20170104, 20170105, 20170106, 20170109, 20170110, 20170111, 20170112, 20170113, 20170117, 20170118, 20170119, 20170120, 20170123, 20170124, 20170125, 20170126, 20170127, 20170130, 20170131, 20170201, 20170202, 20170203, 20170206, 20170207, 20170208, 20170209, 20170210, 20170213, 20170214, 20170215, 20170216, 20170217, 20170221, 20170222, 20170223, 20170224, 20170227, 20170228, 20170301, 20170302, 20170303, 20170306, 20170307, 20170308, 20170309, 20170310, 20170313, 20170314, 20170315, 20170316, 20170317, 20170320, 20170321, 20170322, 20170323, 20170324, 20170327, 20170328, 20170329, 20170330, 20170331, 20170403, 20170404, 20170405, 20170406, 20170407, 20170410, 20170411, 20170412, 20170413, 20170417, 20170418, 20170419, 20170420, 20170421, 20170424, 20170425, 20170426, 20170427, 20170428, 20170501, 20170502, 20170503, 20170504, 20170505, 20170508, 20170509, 20170510, 20170511, 20170512, 20170515, 20170516, 20170517, 20170518, 20170519, 20170522, 20170523, 20170524, 20170525, 20170526, 20170530, 20170531, 20170601, 20170602, 20170605, 20170606, 20170607, 20170608, 20170609, 20170612, 20170613, 20170614, 20170615, 20170616, 20170619, 20170620, 20170621, 20170622, 20170623, 20170626, 20170627, 20170628, 20170629, 20170630, 20170703, 20170705, 20170706, 20170707, 20170710, 20170711, 20170712, 20170713, 20170714, 20170717, 20170718, 20170719, 20170720, 20170721, 20170724, 20170725, 20170726, 20170727, 20170728, 20170731, 20170801, 20170802, 20170803, 20170804, 20170807, 20170808, 20170809, 20170810, 20170811, 20170814, 20170815, 20170816, 20170817, 20170818, 20170821, 20170822, 20170823, 20170824, 20170825, 20170828, 20170829, 20170830, 20170831, 20170901, 20170905, 20170906, 20170907, 20170908, 20170911, 20170912, 20170913, 20170914, 20170915, 20170918, 20170919, 20170920, 20170921, 20170922, 20170925, 20170926, 20170927, 20170928, 20170929, 20171002, 20171003, 20171004, 20171005, 20171006, 20171009, 20171010, 20171011, 20171012, 20171013, 20171016, 20171017, 20171018, 20171019, 20171020, 20171023, 20171024, 20171025, 20171026, 20171027, 20171030, 20171031, 20171101, 20171102, 20171103, 20171106, 20171107, 20171108, 20171109, 20171110, 20171113, 20171114, 20171115, 20171116, 20171117, 20171120, 20171121, 20171122, 20171124, 20171127, 20171128, 20171129, 20171130, 20171201, 20171204, 20171205, 20171206, 20171207, 20171208, 20171211, 20171212, 20171213, 20171214, 20171215, 20171218, 20171219, 20171220, 20171221, 20171222, 20171226, 20171227, 20171228, 20171229, 20180102, 20180103, 20180104, 20180105, 20180108, 20180109, 20180110, 20180111, 20180112, 20180116, 20180117, 20180118, 20180119, 20180122, 20180123, 20180124, 20180125, 20180126, 20180129, 20180130, 20180131, 20180201, 20180202, 20180205, 20180206, 20180207, 20180208, 20180209, 20180212, 20180213, 20180214, 20180215, 20180216, 20180220, 20180221, 20180222, 20180223, 20180226, 20180227, 20180228, 20180301, 20180302, 20180305, 20180306, 20180307, 20180308, 20180309, 20180312, 20180313, 20180314, 20180315, 20180316, 20180319, 20180320, 20180321, 20180322, 20180323, 20180326, 20180327, 20180328, 20180329, 20180402, 20180403, 20180404, 20180405, 20180406, 20180409, 20180410, 20180411, 20180412, 20180413, 20180416, 20180417, 20180418, 20180419, 20180420, 20180423, 20180424, 20180425, 20180426, 20180427, 20180430, 20180501, 20180502, 20180503, 20180504, 20180507, 20180508, 20180509, 20180510, 20180511, 20180514, 20180515, 20180516, 20180517, 20180518, 20180521, 20180522, 20180523, 20180524, 20180525, 20180529, 20180530, 20180531, 20180601, 20180604, 20180605, 20180606, 20180607, 20180608, 20180611, 20180612, 20180613, 20180614, 20180615, 20180618, 20180619, 20180620, 20180621, 20180622, 20180625, 20180626, 20180627, 20180628, 20180629, 20180702, 20180703, 20180705, 20180706, 20180709, 20180710, 20180711, 20180712, 20180713, 20180716, 20180717, 20180718, 20180719, 20180720, 20180723, 20180724, 20180725, 20180726, 20180727, 20180730, 20180731, 20180801, 20180802, 20180803, 20180806, 20180807, 20180808, 20180809, 20180810, 20180813, 20180814, 20180815, 20180816, 20180817, 20180820, 20180821, 20180822, 20180823, 20180824, 20180827, 20180828, 20180829, 20180830, 20180831, 20180904, 20180905, 20180906, 20180907, 20180910, 20180911, 20180912, 20180913, 20180914, 20180917, 20180918, 20180919, 20180920, 20180921, 20180924, 20180925, 20180926, 20180927, 20180928, 20181001, 20181002, 20181003, 20181004, 20181005, 20181008, 20181009, 20181010, 20181011, 20181012, 20181015, 20181016, 20181017, 20181018, 20181019, 20181022, 20181023, 20181024, 20181025, 20181026, 20181029, 20181030, 20181031, 20181101, 20181102, 20181105, 20181106, 20181107, 20181108, 20181109, 20181112, 20181113, 20181114, 20181115, 20181116, 20181119, 20181120, 20181121, 20181123, 20181126, 20181127, 20181128, 20181129, 20181130, 20181203, 20181204, 20181206, 20181207, 20181210, 20181211, 20181212, 20181213, 20181214, 20181217, 20181218, 20181219, 20181220, 20181221, 20181224, 20181226, 20181227, 20181228, 20181231, 20190102, 20190103, 20190104, 20190107, 20190108, 20190109, 20190110, 20190111, 20190114, 20190115, 20190116, 20190117, 20190118, 20190122, 20190123, 20190124, 20190125, 20190128, 20190129, 20190130, 20190131, 20190201, 20190204, 20190205, 20190206, 20190207, 20190208, 20190211, 20190212, 20190213, 20190214, 20190215, 20190219, 20190220, 20190221, 20190222, 20190225, 20190226, 20190227, 20190228, 20190301, 20190304, 20190305, 20190306, 20190307, 20190308, 20190311, 20190312, 20190313, 20190314, 20190315, 20190318, 20190319, 20190320, 20190321, 20190322, 20190325, 20190326, 20190327, 20190328, 20190329, 20190401, 20190402, 20190403, 20190404, 20190405, 20190408, 20190409, 20190410, 20190411, 20190412, 20190415, 20190416, 20190417, 20190418, 20190422, 20190423, 20190424, 20190425, 20190426, 20190429, 20190430, 20190501, 20190502, 20190503, 20190506, 20190507, 20190508, 20190509, 20190510, 20190513, 20190514, 20190515, 20190516, 20190517, 20190520, 20190521, 20190522, 20190523, 20190524, 20190528, 20190529, 20190530, 20190531, 20190603, 20190604, 20190605, 20190606, 20190607, 20190610, 20190611, 20190612, 20190613, 20190614, 20190617, 20190618, 20190619, 20190620, 20190621, 20190624, 20190625, 20190626, 20190627, 20190628, 20190701, 20190702, 20190703, 20190705, 20190708, 20190709, 20190710, 20190711, 20190712, 20190715, 20190716, 20190717, 20190718, 20190719, 20190722, 20190723, 20190724, 20190725, 20190726, 20190729, 20190730, 20190731, 20190801, 20190802, 20190805, 20190806, 20190807, 20190808, 20190809, 20190812, 20190813, 20190814, 20190815, 20190816, 20190819, 20190820, 20190821, 20190822, 20190823, 20190826, 20190827, 20190828, 20190829, 20190830, 20190903, 20190904, 20190905, 20190906, 20190909, 20190910, 20190911, 20190912, 20190913, 20190916, 20190917, 20190918, 20190919, 20190920, 20190923, 20190924, 20190925, 20190926, 20190927, 20190930, 20191001, 20191002, 20191003, 20191004, 20191007, 20191008, 20191009, 20191010, 20191011, 20191014, 20191015, 20191016, 20191017, 20191018, 20191021, 20191022, 20191023, 20191024, 20191025, 20191028, 20191029, 20191030, 20191031, 20191101, 20191104, 20191105, 20191106, 20191107, 20191108, 20191111, 20191112, 20191113, 20191114, 20191115, 20191118, 20191119, 20191120, 20191121, 20191122, 20191125, 20191126, 20191127, 20191129, 20191202, 20191203, 20191204, 20191205, 20191206, 20191209, 20191210, 20191211, 20191212, 20191213, 20191216, 20191217, 20191218, 20191219, 20191220, 20191223, 20191224, 20191226, 20191227, 20191230, 20191231, 20200102, 20200103, 20200106, 20200107, 20200108, 20200109, 20200110, 20200113, 20200114, 20200115, 20200116, 20200117, 20200121, 20200122, 20200123, 20200124, 20200127, 20200128, 20200129, 20200130, 20200131, 20200203, 20200204, 20200205, 20200206, 20200207, 20200210, 20200211, 20200212, 20200213, 20200214, 20200218, 20200219, 20200220, 20200221, 20200224, 20200225, 20200226, 20200227, 20200228, 20200302, 20200303, 20200304, 20200305, 20200306, 20200309, 20200310, 20200311, 20200312, 20200313, 20200316, 20200317, 20200318, 20200319, 20200320, 20200323, 20200324, 20200325, 20200326, 20200327, 20200330, 20200331, 20200401, 20200402, 20200403, 20200406, 20200407, 20200408, 20200409, 20200413, 20200414, 20200415, 20200416, 20200417, 20200420, 20200421, 20200422, 20200423, 20200424, 20200427, 20200428, 20200429, 20200430, 20200501, 20200504, 20200505, 20200506, 20200507, 20200508, 20200511, 20200512, 20200513, 20200514, 20200515, 20200518, 20200519, 20200520, 20200521, 20200522, 20200526, 20200527, 20200528, 20200529, 20200601, 20200602, 20200603, 20200604, 20200605, 20200608, 20200609, 20200610, 20200611, 20200612, 20200615, 20200616, 20200617, 20200618, 20200619, 20200622, 20200623, 20200624, 20200625, 20200626, 20200629, 20200630, 20200701, 20200702, 20200706, 20200707, 20200708, 20200709, 20200710, 20200713, 20200714, 20200715, 20200716, 20200717, 20200720, 20200721, 20200722, 20200723, 20200724, 20200727, 20200728, 20200729, 20200730, 20200731, 20200803, 20200804, 20200805, 20200806, 20200807, 20200810, 20200811, 20200812, 20200813, 20200814, 20200817, 20200818, 20200819, 20200820, 20200821, 20200824, 20200825, 20200826, 20200827, 20200828, 20200831, 20200901, 20200902, 20200903, 20200904, 20200908, 20200909, 20200910, 20200911, 20200914, 20200915, 20200916, 20200917, 20200918, 20200921, 20200922, 20200923, 20200924, 20200925, 20200928, 20200929, 20200930, 20201001, 20201002, 20201005, 20201006, 20201007, 20201008, 20201009, 20201012, 20201013, 20201014, 20201015, 20201016, 20201019, 20201020, 20201021, 20201022, 20201023, 20201026, 20201027, 20201028, 20201029, 20201030, 20201102, 20201103, 20201104, 20201105, 20201106, 20201109, 20201110, 20201111, 20201112, 20201113, 20201116, 20201117, 20201118, 20201119, 20201120, 20201123, 20201124, 20201125, 20201127, 20201130, 20201201, 20201202, 20201203, 20201204, 20201207, 20201208, 20201209, 20201210, 20201211, 20201214, 20201215, 20201216, 20201217, 20201218, 20201221, 20201222, 20201223, 20201224, 20201228, 20201229, 20201230, 20201231, 20210104, 20210105, 20210106, 20210107, 20210108, 20210111, 20210112, 20210113, 20210114, 20210115, 20210119, 20210120, 20210121, 20210122, 20210125, 20210126, 20210127, 20210128, 20210129, 20210201, 20210202, 20210203, 20210204, 20210205, 20210208, 20210209, 20210210, 20210211, 20210212, 20210216, 20210217, 20210218, 20210219, 20210222, 20210223, 20210224, 20210225, 20210226, 20210301, 20210302, 20210303, 20210304, 20210305, 20210308, 20210309, 20210310, 20210311, 20210312, 20210315, 20210316, 20210317, 20210318, 20210319, 20210322, 20210323, 20210324, 20210325, 20210326, 20210329, 20210330, 20210331, 20210401, 20210405, 20210406, 20210407, 20210408, 20210409, 20210412, 20210413, 20210414, 20210415, 20210416, 20210419, 20210420, 20210421, 20210422, 20210423, 20210426, 20210427, 20210428, 20210429, 20210430, 20210503, 20210504, 20210505, 20210506, 20210507, 20210510, 20210511, 20210512, 20210513, 20210514, 20210517, 20210518, 20210519, 20210520, 20210521, 20210524, 20210525, 20210526, 20210527, 20210528, 20210601, 20210602, 20210603, 20210604, 20210607, 20210608, 20210609, 20210610, 20210611, 20210614, 20210615, 20210616, 20210617, 20210618, 20210621, 20210622, 20210623, 20210624], \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('648a2d0a-bd3e-4253-b1a0-ce7d1aaef282');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n0OAbKd7r39"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3r8_YciLwrN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "812c08ba-4db1-4ea3-b674-4bdb32037888"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .05]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  historical = Train_data(dfs[col_name], train_start=100, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"AMD\", step_sizes=4, th= th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 15ms/step - loss: 0.6871 - accuracy: 0.5574 - val_loss: 0.7528 - val_accuracy: 0.3347\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.4816 - accuracy: 0.7769 - val_loss: 1.2270 - val_accuracy: 0.4122\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.3635 - accuracy: 0.8367 - val_loss: 1.2250 - val_accuracy: 0.5367\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.3258 - accuracy: 0.8503 - val_loss: 1.7383 - val_accuracy: 0.5102\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.3027 - accuracy: 0.8669 - val_loss: 1.4181 - val_accuracy: 0.5551\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 14ms/step - loss: 0.5632 - accuracy: 0.6781 - val_loss: 1.4436 - val_accuracy: 0.5143\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.2737 - accuracy: 0.8769 - val_loss: 1.3269 - val_accuracy: 0.5388\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.2791 - accuracy: 0.8716 - val_loss: 1.9652 - val_accuracy: 0.4939\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.2628 - accuracy: 0.8817 - val_loss: 1.4348 - val_accuracy: 0.5245\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.2655 - accuracy: 0.8905 - val_loss: 1.8375 - val_accuracy: 0.5122\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.665822\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.666028\n",
            "[2]\tvalidation_0-auc:0.666028\n",
            "[3]\tvalidation_0-auc:0.665953\n",
            "[4]\tvalidation_0-auc:0.665831\n",
            "[5]\tvalidation_0-auc:0.665794\n",
            "[6]\tvalidation_0-auc:0.66585\n",
            "[7]\tvalidation_0-auc:0.665775\n",
            "[8]\tvalidation_0-auc:0.665775\n",
            "[9]\tvalidation_0-auc:0.66628\n",
            "[10]\tvalidation_0-auc:0.666252\n",
            "[11]\tvalidation_0-auc:0.666402\n",
            "[12]\tvalidation_0-auc:0.666476\n",
            "[13]\tvalidation_0-auc:0.666514\n",
            "[14]\tvalidation_0-auc:0.66657\n",
            "[15]\tvalidation_0-auc:0.666626\n",
            "[16]\tvalidation_0-auc:0.696833\n",
            "[17]\tvalidation_0-auc:0.696871\n",
            "[18]\tvalidation_0-auc:0.696918\n",
            "[19]\tvalidation_0-auc:0.696899\n",
            "[20]\tvalidation_0-auc:0.697048\n",
            "[21]\tvalidation_0-auc:0.696992\n",
            "[22]\tvalidation_0-auc:0.711058\n",
            "[23]\tvalidation_0-auc:0.711058\n",
            "[24]\tvalidation_0-auc:0.711002\n",
            "[25]\tvalidation_0-auc:0.710946\n",
            "[26]\tvalidation_0-auc:0.706644\n",
            "[27]\tvalidation_0-auc:0.707897\n",
            "[28]\tvalidation_0-auc:0.707869\n",
            "[29]\tvalidation_0-auc:0.70684\n",
            "[30]\tvalidation_0-auc:0.701958\n",
            "[31]\tvalidation_0-auc:0.700181\n",
            "[32]\tvalidation_0-auc:0.700481\n",
            "[33]\tvalidation_0-auc:0.700069\n",
            "[34]\tvalidation_0-auc:0.698133\n",
            "[35]\tvalidation_0-auc:0.698208\n",
            "[36]\tvalidation_0-auc:0.698021\n",
            "[37]\tvalidation_0-auc:0.692849\n",
            "[38]\tvalidation_0-auc:0.691924\n",
            "[39]\tvalidation_0-auc:0.691381\n",
            "[40]\tvalidation_0-auc:0.689885\n",
            "[41]\tvalidation_0-auc:0.689885\n",
            "[42]\tvalidation_0-auc:0.68923\n",
            "[43]\tvalidation_0-auc:0.680318\n",
            "[44]\tvalidation_0-auc:0.680224\n",
            "[45]\tvalidation_0-auc:0.680037\n",
            "[46]\tvalidation_0-auc:0.678887\n",
            "[47]\tvalidation_0-auc:0.678756\n",
            "[48]\tvalidation_0-auc:0.678718\n",
            "[49]\tvalidation_0-auc:0.67827\n",
            "[50]\tvalidation_0-auc:0.678288\n",
            "[51]\tvalidation_0-auc:0.689006\n",
            "[52]\tvalidation_0-auc:0.692064\n",
            "[53]\tvalidation_0-auc:0.692457\n",
            "[54]\tvalidation_0-auc:0.692466\n",
            "[55]\tvalidation_0-auc:0.692354\n",
            "[56]\tvalidation_0-auc:0.692344\n",
            "[57]\tvalidation_0-auc:0.691465\n",
            "[58]\tvalidation_0-auc:0.691428\n",
            "[59]\tvalidation_0-auc:0.691372\n",
            "[60]\tvalidation_0-auc:0.691372\n",
            "[61]\tvalidation_0-auc:0.692335\n",
            "[62]\tvalidation_0-auc:0.692503\n",
            "[63]\tvalidation_0-auc:0.692316\n",
            "[64]\tvalidation_0-auc:0.692588\n",
            "[65]\tvalidation_0-auc:0.698891\n",
            "[66]\tvalidation_0-auc:0.698872\n",
            "[67]\tvalidation_0-auc:0.699115\n",
            "[68]\tvalidation_0-auc:0.69154\n",
            "[69]\tvalidation_0-auc:0.692101\n",
            "[70]\tvalidation_0-auc:0.694869\n",
            "[71]\tvalidation_0-auc:0.694907\n",
            "[72]\tvalidation_0-auc:0.695019\n",
            "Stopping. Best iteration:\n",
            "[22]\tvalidation_0-auc:0.711058\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 4s 14ms/step - loss: 0.4587 - accuracy: 0.7767 - val_loss: 0.9805 - val_accuracy: 0.4530\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3348 - accuracy: 0.8582 - val_loss: 2.2807 - val_accuracy: 0.4201\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3363 - accuracy: 0.8509 - val_loss: 2.0971 - val_accuracy: 0.4464\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3178 - accuracy: 0.8654 - val_loss: 2.0164 - val_accuracy: 0.4267\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3047 - accuracy: 0.8684 - val_loss: 1.6545 - val_accuracy: 0.4573\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 13ms/step - loss: 0.4354 - accuracy: 0.7876 - val_loss: 1.5145 - val_accuracy: 0.4551\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3061 - accuracy: 0.8666 - val_loss: 2.1514 - val_accuracy: 0.4442\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2875 - accuracy: 0.8781 - val_loss: 2.0450 - val_accuracy: 0.4530\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2726 - accuracy: 0.8763 - val_loss: 2.0576 - val_accuracy: 0.4595\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2690 - accuracy: 0.8811 - val_loss: 2.2853 - val_accuracy: 0.4595\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.644895\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.651086\n",
            "[2]\tvalidation_0-auc:0.651648\n",
            "[3]\tvalidation_0-auc:0.645832\n",
            "[4]\tvalidation_0-auc:0.645863\n",
            "[5]\tvalidation_0-auc:0.646477\n",
            "[6]\tvalidation_0-auc:0.646477\n",
            "[7]\tvalidation_0-auc:0.646664\n",
            "[8]\tvalidation_0-auc:0.648412\n",
            "[9]\tvalidation_0-auc:0.648329\n",
            "[10]\tvalidation_0-auc:0.650649\n",
            "[11]\tvalidation_0-auc:0.6635\n",
            "[12]\tvalidation_0-auc:0.664624\n",
            "[13]\tvalidation_0-auc:0.66504\n",
            "[14]\tvalidation_0-auc:0.665061\n",
            "[15]\tvalidation_0-auc:0.664645\n",
            "[16]\tvalidation_0-auc:0.664978\n",
            "[17]\tvalidation_0-auc:0.665082\n",
            "[18]\tvalidation_0-auc:0.665623\n",
            "[19]\tvalidation_0-auc:0.66581\n",
            "[20]\tvalidation_0-auc:0.668713\n",
            "[21]\tvalidation_0-auc:0.669171\n",
            "[22]\tvalidation_0-auc:0.664135\n",
            "[23]\tvalidation_0-auc:0.66428\n",
            "[24]\tvalidation_0-auc:0.664114\n",
            "[25]\tvalidation_0-auc:0.664031\n",
            "[26]\tvalidation_0-auc:0.662491\n",
            "[27]\tvalidation_0-auc:0.66222\n",
            "[28]\tvalidation_0-auc:0.662147\n",
            "[29]\tvalidation_0-auc:0.664478\n",
            "[30]\tvalidation_0-auc:0.630473\n",
            "[31]\tvalidation_0-auc:0.631472\n",
            "[32]\tvalidation_0-auc:0.632096\n",
            "[33]\tvalidation_0-auc:0.63064\n",
            "[34]\tvalidation_0-auc:0.627841\n",
            "[35]\tvalidation_0-auc:0.627695\n",
            "[36]\tvalidation_0-auc:0.62911\n",
            "[37]\tvalidation_0-auc:0.629173\n",
            "[38]\tvalidation_0-auc:0.632034\n",
            "[39]\tvalidation_0-auc:0.632034\n",
            "[40]\tvalidation_0-auc:0.633054\n",
            "[41]\tvalidation_0-auc:0.632034\n",
            "[42]\tvalidation_0-auc:0.631389\n",
            "[43]\tvalidation_0-auc:0.632242\n",
            "[44]\tvalidation_0-auc:0.632242\n",
            "[45]\tvalidation_0-auc:0.632367\n",
            "[46]\tvalidation_0-auc:0.632617\n",
            "[47]\tvalidation_0-auc:0.632284\n",
            "[48]\tvalidation_0-auc:0.632409\n",
            "[49]\tvalidation_0-auc:0.632533\n",
            "[50]\tvalidation_0-auc:0.630661\n",
            "[51]\tvalidation_0-auc:0.630161\n",
            "[52]\tvalidation_0-auc:0.630244\n",
            "[53]\tvalidation_0-auc:0.630078\n",
            "[54]\tvalidation_0-auc:0.644635\n",
            "[55]\tvalidation_0-auc:0.645322\n",
            "[56]\tvalidation_0-auc:0.666975\n",
            "[57]\tvalidation_0-auc:0.667038\n",
            "[58]\tvalidation_0-auc:0.6671\n",
            "[59]\tvalidation_0-auc:0.666705\n",
            "[60]\tvalidation_0-auc:0.6304\n",
            "[61]\tvalidation_0-auc:0.624865\n",
            "[62]\tvalidation_0-auc:0.62501\n",
            "[63]\tvalidation_0-auc:0.62499\n",
            "[64]\tvalidation_0-auc:0.625177\n",
            "[65]\tvalidation_0-auc:0.624178\n",
            "[66]\tvalidation_0-auc:0.624261\n",
            "[67]\tvalidation_0-auc:0.623658\n",
            "[68]\tvalidation_0-auc:0.623075\n",
            "[69]\tvalidation_0-auc:0.623117\n",
            "[70]\tvalidation_0-auc:0.623096\n",
            "[71]\tvalidation_0-auc:0.628621\n",
            "Stopping. Best iteration:\n",
            "[21]\tvalidation_0-auc:0.669171\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+---------------------+---------------------+--------------------+--------------------+\n",
            "|      Model       |       Accuracy      |      Precision      |       Recall       |      F1 score      |\n",
            "+------------------+---------------------+---------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     |  0.5551020408163265 |  0.4258241758241758 | 0.9451219512195121 | 0.587121212121212  |\n",
            "|     GRU 0.1      |  0.5122448979591837 |  0.4050632911392405 | 0.975609756097561  | 0.5724508050089445 |\n",
            "|   XGBoost 0.1    | 0.49183673469387756 |  0.3960880195599022 | 0.9878048780487805 | 0.5654450261780105 |\n",
            "|    Logreg 0.1    |  0.4959183673469388 | 0.39753086419753086 | 0.9817073170731707 | 0.5659050966608085 |\n",
            "|     SVM 0.1      |  0.5061224489795918 | 0.40298507462686567 | 0.9878048780487805 | 0.5724381625441696 |\n",
            "|  LSTM beta 0.1   |  0.4573304157549234 | 0.39603960396039606 | 0.975609756097561  | 0.5633802816901409 |\n",
            "|   GRU beta 0.1   | 0.45951859956236324 |  0.3949367088607595 | 0.9512195121951219 | 0.5581395348837209 |\n",
            "| XGBoost beta 0.1 |  0.4442013129102845 |  0.3891625615763547 | 0.9634146341463414 | 0.5543859649122806 |\n",
            "| logreg beta 0.1  | 0.45295404814004375 | 0.39141414141414144 | 0.9451219512195121 | 0.5535714285714286 |\n",
            "|   svm beta 0.1   |  0.4573304157549234 |        0.395        | 0.9634146341463414 | 0.5602836879432624 |\n",
            "+------------------+---------------------+---------------------+--------------------+--------------------+\n",
            "Threshhold =  0.05\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 15ms/step - loss: 0.6927 - accuracy: 0.5183 - val_loss: 0.6738 - val_accuracy: 0.7898\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.5009 - accuracy: 0.7290 - val_loss: 0.1373 - val_accuracy: 0.9490\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.2832 - accuracy: 0.8834 - val_loss: 0.1980 - val_accuracy: 0.9000\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.2533 - accuracy: 0.8882 - val_loss: 0.1573 - val_accuracy: 0.9327\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.2423 - accuracy: 0.8911 - val_loss: 0.1529 - val_accuracy: 0.9347\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 13ms/step - loss: 0.5689 - accuracy: 0.6775 - val_loss: 0.1601 - val_accuracy: 0.9551\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.2542 - accuracy: 0.8876 - val_loss: 0.1237 - val_accuracy: 0.9510\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.2070 - accuracy: 0.9112 - val_loss: 0.1192 - val_accuracy: 0.9551\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.2104 - accuracy: 0.8964 - val_loss: 0.1170 - val_accuracy: 0.9571\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.2007 - accuracy: 0.9077 - val_loss: 0.1154 - val_accuracy: 0.9694\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.985663\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.98427\n",
            "[2]\tvalidation_0-auc:0.985851\n",
            "[3]\tvalidation_0-auc:0.985387\n",
            "[4]\tvalidation_0-auc:0.985261\n",
            "[5]\tvalidation_0-auc:0.985261\n",
            "[6]\tvalidation_0-auc:0.985537\n",
            "[7]\tvalidation_0-auc:0.985713\n",
            "[8]\tvalidation_0-auc:0.986026\n",
            "[9]\tvalidation_0-auc:0.994682\n",
            "[10]\tvalidation_0-auc:0.994958\n",
            "[11]\tvalidation_0-auc:0.994895\n",
            "[12]\tvalidation_0-auc:0.995384\n",
            "[13]\tvalidation_0-auc:0.99571\n",
            "[14]\tvalidation_0-auc:0.995773\n",
            "[15]\tvalidation_0-auc:0.995672\n",
            "[16]\tvalidation_0-auc:0.995698\n",
            "[17]\tvalidation_0-auc:0.995836\n",
            "[18]\tvalidation_0-auc:0.995911\n",
            "[19]\tvalidation_0-auc:0.995886\n",
            "[20]\tvalidation_0-auc:0.996011\n",
            "[21]\tvalidation_0-auc:0.996011\n",
            "[22]\tvalidation_0-auc:0.996011\n",
            "[23]\tvalidation_0-auc:0.996061\n",
            "[24]\tvalidation_0-auc:0.996011\n",
            "[25]\tvalidation_0-auc:0.995936\n",
            "[26]\tvalidation_0-auc:0.995861\n",
            "[27]\tvalidation_0-auc:0.995886\n",
            "[28]\tvalidation_0-auc:0.996086\n",
            "[29]\tvalidation_0-auc:0.995911\n",
            "[30]\tvalidation_0-auc:0.99576\n",
            "[31]\tvalidation_0-auc:0.99576\n",
            "[32]\tvalidation_0-auc:0.995785\n",
            "[33]\tvalidation_0-auc:0.995735\n",
            "[34]\tvalidation_0-auc:0.995685\n",
            "[35]\tvalidation_0-auc:0.995936\n",
            "[36]\tvalidation_0-auc:0.995961\n",
            "[37]\tvalidation_0-auc:0.995735\n",
            "[38]\tvalidation_0-auc:0.995961\n",
            "[39]\tvalidation_0-auc:0.995735\n",
            "[40]\tvalidation_0-auc:0.99576\n",
            "[41]\tvalidation_0-auc:0.99581\n",
            "[42]\tvalidation_0-auc:0.995685\n",
            "[43]\tvalidation_0-auc:0.99576\n",
            "[44]\tvalidation_0-auc:0.99561\n",
            "[45]\tvalidation_0-auc:0.995585\n",
            "[46]\tvalidation_0-auc:0.99561\n",
            "[47]\tvalidation_0-auc:0.99561\n",
            "[48]\tvalidation_0-auc:0.99571\n",
            "[49]\tvalidation_0-auc:0.99571\n",
            "[50]\tvalidation_0-auc:0.99561\n",
            "[51]\tvalidation_0-auc:0.99561\n",
            "[52]\tvalidation_0-auc:0.99561\n",
            "[53]\tvalidation_0-auc:0.99561\n",
            "[54]\tvalidation_0-auc:0.99561\n",
            "[55]\tvalidation_0-auc:0.995635\n",
            "[56]\tvalidation_0-auc:0.99556\n",
            "[57]\tvalidation_0-auc:0.995359\n",
            "[58]\tvalidation_0-auc:0.995359\n",
            "[59]\tvalidation_0-auc:0.995509\n",
            "[60]\tvalidation_0-auc:0.995484\n",
            "[61]\tvalidation_0-auc:0.995434\n",
            "[62]\tvalidation_0-auc:0.99561\n",
            "[63]\tvalidation_0-auc:0.99561\n",
            "[64]\tvalidation_0-auc:0.99561\n",
            "[65]\tvalidation_0-auc:0.99566\n",
            "[66]\tvalidation_0-auc:0.995685\n",
            "[67]\tvalidation_0-auc:0.99556\n",
            "[68]\tvalidation_0-auc:0.99556\n",
            "[69]\tvalidation_0-auc:0.99556\n",
            "[70]\tvalidation_0-auc:0.995509\n",
            "[71]\tvalidation_0-auc:0.995484\n",
            "[72]\tvalidation_0-auc:0.995509\n",
            "[73]\tvalidation_0-auc:0.995509\n",
            "[74]\tvalidation_0-auc:0.995459\n",
            "[75]\tvalidation_0-auc:0.995459\n",
            "[76]\tvalidation_0-auc:0.995459\n",
            "[77]\tvalidation_0-auc:0.99556\n",
            "[78]\tvalidation_0-auc:0.995534\n",
            "Stopping. Best iteration:\n",
            "[28]\tvalidation_0-auc:0.996086\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 4s 19ms/step - loss: 0.5391 - accuracy: 0.7151 - val_loss: 0.3623 - val_accuracy: 0.8621\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3421 - accuracy: 0.8612 - val_loss: 0.2822 - val_accuracy: 0.8818\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3228 - accuracy: 0.8660 - val_loss: 0.3386 - val_accuracy: 0.8709\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.2982 - accuracy: 0.8769 - val_loss: 0.2453 - val_accuracy: 0.9125\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.2879 - accuracy: 0.8847 - val_loss: 0.2140 - val_accuracy: 0.9212\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 14ms/step - loss: 0.4850 - accuracy: 0.7616 - val_loss: 0.3230 - val_accuracy: 0.8796\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2980 - accuracy: 0.8865 - val_loss: 0.2982 - val_accuracy: 0.8928\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2864 - accuracy: 0.8799 - val_loss: 0.2101 - val_accuracy: 0.9037\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2684 - accuracy: 0.8926 - val_loss: 0.1856 - val_accuracy: 0.9147\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2515 - accuracy: 0.8986 - val_loss: 0.2010 - val_accuracy: 0.9278\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.921207\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.939793\n",
            "[2]\tvalidation_0-auc:0.941159\n",
            "[3]\tvalidation_0-auc:0.953156\n",
            "[4]\tvalidation_0-auc:0.954042\n",
            "[5]\tvalidation_0-auc:0.958675\n",
            "[6]\tvalidation_0-auc:0.954577\n",
            "[7]\tvalidation_0-auc:0.959764\n",
            "[8]\tvalidation_0-auc:0.959745\n",
            "[9]\tvalidation_0-auc:0.971465\n",
            "[10]\tvalidation_0-auc:0.972536\n",
            "[11]\tvalidation_0-auc:0.972794\n",
            "[12]\tvalidation_0-auc:0.972757\n",
            "[13]\tvalidation_0-auc:0.972831\n",
            "[14]\tvalidation_0-auc:0.973367\n",
            "[15]\tvalidation_0-auc:0.97333\n",
            "[16]\tvalidation_0-auc:0.9744\n",
            "[17]\tvalidation_0-auc:0.976375\n",
            "[18]\tvalidation_0-auc:0.976412\n",
            "[19]\tvalidation_0-auc:0.976338\n",
            "[20]\tvalidation_0-auc:0.976984\n",
            "[21]\tvalidation_0-auc:0.976135\n",
            "[22]\tvalidation_0-auc:0.976836\n",
            "[23]\tvalidation_0-auc:0.976873\n",
            "[24]\tvalidation_0-auc:0.976689\n",
            "[25]\tvalidation_0-auc:0.975766\n",
            "[26]\tvalidation_0-auc:0.976357\n",
            "[27]\tvalidation_0-auc:0.977021\n",
            "[28]\tvalidation_0-auc:0.977316\n",
            "[29]\tvalidation_0-auc:0.97691\n",
            "[30]\tvalidation_0-auc:0.977261\n",
            "[31]\tvalidation_0-auc:0.977667\n",
            "[32]\tvalidation_0-auc:0.97859\n",
            "[33]\tvalidation_0-auc:0.978516\n",
            "[34]\tvalidation_0-auc:0.978516\n",
            "[35]\tvalidation_0-auc:0.978332\n",
            "[36]\tvalidation_0-auc:0.977889\n",
            "[37]\tvalidation_0-auc:0.977889\n",
            "[38]\tvalidation_0-auc:0.977889\n",
            "[39]\tvalidation_0-auc:0.97811\n",
            "[40]\tvalidation_0-auc:0.978295\n",
            "[41]\tvalidation_0-auc:0.978258\n",
            "[42]\tvalidation_0-auc:0.978516\n",
            "[43]\tvalidation_0-auc:0.978516\n",
            "[44]\tvalidation_0-auc:0.978479\n",
            "[45]\tvalidation_0-auc:0.978295\n",
            "[46]\tvalidation_0-auc:0.978479\n",
            "[47]\tvalidation_0-auc:0.978073\n",
            "[48]\tvalidation_0-auc:0.977999\n",
            "[49]\tvalidation_0-auc:0.97763\n",
            "[50]\tvalidation_0-auc:0.977446\n",
            "[51]\tvalidation_0-auc:0.977335\n",
            "[52]\tvalidation_0-auc:0.97704\n",
            "[53]\tvalidation_0-auc:0.97763\n",
            "[54]\tvalidation_0-auc:0.97715\n",
            "[55]\tvalidation_0-auc:0.976744\n",
            "[56]\tvalidation_0-auc:0.976929\n",
            "[57]\tvalidation_0-auc:0.977003\n",
            "[58]\tvalidation_0-auc:0.976707\n",
            "[59]\tvalidation_0-auc:0.976892\n",
            "[60]\tvalidation_0-auc:0.976615\n",
            "[61]\tvalidation_0-auc:0.976615\n",
            "[62]\tvalidation_0-auc:0.976578\n",
            "[63]\tvalidation_0-auc:0.976947\n",
            "[64]\tvalidation_0-auc:0.97691\n",
            "[65]\tvalidation_0-auc:0.977169\n",
            "[66]\tvalidation_0-auc:0.977243\n",
            "[67]\tvalidation_0-auc:0.977132\n",
            "[68]\tvalidation_0-auc:0.977021\n",
            "[69]\tvalidation_0-auc:0.977704\n",
            "[70]\tvalidation_0-auc:0.978184\n",
            "[71]\tvalidation_0-auc:0.978036\n",
            "[72]\tvalidation_0-auc:0.977778\n",
            "[73]\tvalidation_0-auc:0.977962\n",
            "[74]\tvalidation_0-auc:0.977962\n",
            "[75]\tvalidation_0-auc:0.977889\n",
            "[76]\tvalidation_0-auc:0.978553\n",
            "[77]\tvalidation_0-auc:0.978258\n",
            "[78]\tvalidation_0-auc:0.978405\n",
            "[79]\tvalidation_0-auc:0.978811\n",
            "[80]\tvalidation_0-auc:0.978332\n",
            "[81]\tvalidation_0-auc:0.978073\n",
            "[82]\tvalidation_0-auc:0.978184\n",
            "[83]\tvalidation_0-auc:0.978332\n",
            "[84]\tvalidation_0-auc:0.978405\n",
            "[85]\tvalidation_0-auc:0.978442\n",
            "[86]\tvalidation_0-auc:0.978553\n",
            "[87]\tvalidation_0-auc:0.978442\n",
            "[88]\tvalidation_0-auc:0.978332\n",
            "[89]\tvalidation_0-auc:0.977999\n",
            "[90]\tvalidation_0-auc:0.978184\n",
            "[91]\tvalidation_0-auc:0.978184\n",
            "[92]\tvalidation_0-auc:0.977667\n",
            "[93]\tvalidation_0-auc:0.977962\n",
            "[94]\tvalidation_0-auc:0.978405\n",
            "[95]\tvalidation_0-auc:0.978959\n",
            "[96]\tvalidation_0-auc:0.978848\n",
            "[97]\tvalidation_0-auc:0.979033\n",
            "[98]\tvalidation_0-auc:0.979144\n",
            "[99]\tvalidation_0-auc:0.979107\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.05     | 0.9346938775510204 | 0.942643391521197  | 0.9767441860465116 | 0.9593908629441624 |\n",
            "|      GRU 0.05     | 0.9693877551020408 |      0.984375      | 0.9767441860465116 | 0.980544747081712  |\n",
            "|    XGBoost 0.05   | 0.9591836734693877 | 0.9622166246851386 | 0.9870801033591732 | 0.9744897959183674 |\n",
            "|    Logreg 0.05    | 0.9530612244897959 | 0.9642857142857143 | 0.9767441860465116 | 0.9704749679075738 |\n",
            "|      SVM 0.05     | 0.9571428571428572 | 0.9621212121212122 | 0.9844961240310077 | 0.9731800766283525 |\n",
            "|   LSTM beta 0.05  | 0.9212253829321663 | 0.9443037974683545 | 0.9638242894056848 | 0.9539641943734015 |\n",
            "|   GRU beta 0.05   | 0.9277899343544858 | 0.9657894736842105 | 0.9483204134366925 | 0.9569752281616688 |\n",
            "| XGBoost beta 0.05 | 0.9321663019693655 | 0.9540816326530612 | 0.9664082687338501 | 0.9602053915275995 |\n",
            "|  logreg beta 0.05 | 0.8927789934354485 | 0.9333333333333333 | 0.9405684754521964 | 0.9369369369369369 |\n",
            "|   svm beta 0.05   | 0.9146608315098468 | 0.9393939393939394 | 0.9612403100775194 | 0.9501915708812261 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "lztUIjcddMmo",
        "outputId": "b65c9059-814a-407e-d41f-d047bd4ca3f0"
      },
      "source": [
        "Result_cross.to_csv('AMD_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.425824</td>\n",
              "      <td>0.555102</td>\n",
              "      <td>0.587121</td>\n",
              "      <td>0.945122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.405063</td>\n",
              "      <td>0.512245</td>\n",
              "      <td>0.572451</td>\n",
              "      <td>0.975610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.396088</td>\n",
              "      <td>0.491837</td>\n",
              "      <td>0.565445</td>\n",
              "      <td>0.987805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.397531</td>\n",
              "      <td>0.495918</td>\n",
              "      <td>0.565905</td>\n",
              "      <td>0.981707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.402985</td>\n",
              "      <td>0.506122</td>\n",
              "      <td>0.572438</td>\n",
              "      <td>0.987805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.396040</td>\n",
              "      <td>0.457330</td>\n",
              "      <td>0.563380</td>\n",
              "      <td>0.975610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.394937</td>\n",
              "      <td>0.459519</td>\n",
              "      <td>0.558140</td>\n",
              "      <td>0.951220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.389163</td>\n",
              "      <td>0.444201</td>\n",
              "      <td>0.554386</td>\n",
              "      <td>0.963415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.391414</td>\n",
              "      <td>0.452954</td>\n",
              "      <td>0.553571</td>\n",
              "      <td>0.945122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.395000</td>\n",
              "      <td>0.457330</td>\n",
              "      <td>0.560284</td>\n",
              "      <td>0.963415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.05</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.942643</td>\n",
              "      <td>0.934694</td>\n",
              "      <td>0.959391</td>\n",
              "      <td>0.976744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.05</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.969388</td>\n",
              "      <td>0.980545</td>\n",
              "      <td>0.976744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.05</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.962217</td>\n",
              "      <td>0.959184</td>\n",
              "      <td>0.974490</td>\n",
              "      <td>0.987080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.05</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.964286</td>\n",
              "      <td>0.953061</td>\n",
              "      <td>0.970475</td>\n",
              "      <td>0.976744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.05</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.962121</td>\n",
              "      <td>0.957143</td>\n",
              "      <td>0.973180</td>\n",
              "      <td>0.984496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.05</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.944304</td>\n",
              "      <td>0.921225</td>\n",
              "      <td>0.953964</td>\n",
              "      <td>0.963824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.05</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.965789</td>\n",
              "      <td>0.927790</td>\n",
              "      <td>0.956975</td>\n",
              "      <td>0.948320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.05</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.954082</td>\n",
              "      <td>0.932166</td>\n",
              "      <td>0.960205</td>\n",
              "      <td>0.966408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.05</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.892779</td>\n",
              "      <td>0.936937</td>\n",
              "      <td>0.940568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.05</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.914661</td>\n",
              "      <td>0.950192</td>\n",
              "      <td>0.961240</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  AMD  0.425824  0.555102  0.587121  0.945122\n",
              "1            GRU 0.1  AMD  0.405063  0.512245  0.572451  0.975610\n",
              "2        XGBoost 0.1  AMD  0.396088  0.491837  0.565445  0.987805\n",
              "3         Logreg 0.1  AMD  0.397531  0.495918  0.565905  0.981707\n",
              "4            SVM 0.1  AMD  0.402985  0.506122  0.572438  0.987805\n",
              "5      LSTM beta 0.1  AMD  0.396040  0.457330  0.563380  0.975610\n",
              "6       GRU beta 0.1  AMD  0.394937  0.459519  0.558140  0.951220\n",
              "7   XGBoost beta 0.1  AMD  0.389163  0.444201  0.554386  0.963415\n",
              "8    logreg beta 0.1  AMD  0.391414  0.452954  0.553571  0.945122\n",
              "9       svm beta 0.1  AMD  0.395000  0.457330  0.560284  0.963415\n",
              "0          LSTM 0.05  AMD  0.942643  0.934694  0.959391  0.976744\n",
              "1           GRU 0.05  AMD  0.984375  0.969388  0.980545  0.976744\n",
              "2       XGBoost 0.05  AMD  0.962217  0.959184  0.974490  0.987080\n",
              "3        Logreg 0.05  AMD  0.964286  0.953061  0.970475  0.976744\n",
              "4           SVM 0.05  AMD  0.962121  0.957143  0.973180  0.984496\n",
              "5     LSTM beta 0.05  AMD  0.944304  0.921225  0.953964  0.963824\n",
              "6      GRU beta 0.05  AMD  0.965789  0.927790  0.956975  0.948320\n",
              "7  XGBoost beta 0.05  AMD  0.954082  0.932166  0.960205  0.966408\n",
              "8   logreg beta 0.05  AMD  0.933333  0.892779  0.936937  0.940568\n",
              "9      svm beta 0.05  AMD  0.939394  0.914661  0.950192  0.961240"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moW7IK21LwrN"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJFEttzELwrP"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGHuEhB-LwrP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26cfc093-ab3f-4be0-a38f-fd808a3713a3"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  historical = Train_data(dfs[col_name], train_start=100, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"AMD\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 15ms/step - loss: 0.6866 - accuracy: 0.5568 - val_loss: 0.7148 - val_accuracy: 0.3347\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.4074 - accuracy: 0.8254 - val_loss: 1.6396 - val_accuracy: 0.4980\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.3047 - accuracy: 0.8716 - val_loss: 1.4614 - val_accuracy: 0.4980\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.2763 - accuracy: 0.8763 - val_loss: 2.2512 - val_accuracy: 0.4857\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.2629 - accuracy: 0.8787 - val_loss: 2.4357 - val_accuracy: 0.5122\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 13ms/step - loss: 0.6232 - accuracy: 0.6320 - val_loss: 1.4866 - val_accuracy: 0.5143\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.2979 - accuracy: 0.8669 - val_loss: 1.9019 - val_accuracy: 0.5102\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.2693 - accuracy: 0.8793 - val_loss: 2.1052 - val_accuracy: 0.5143\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.2622 - accuracy: 0.8828 - val_loss: 2.0419 - val_accuracy: 0.5020\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.2604 - accuracy: 0.8864 - val_loss: 2.0135 - val_accuracy: 0.4857\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.665822\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.666028\n",
            "[2]\tvalidation_0-auc:0.666028\n",
            "[3]\tvalidation_0-auc:0.665953\n",
            "[4]\tvalidation_0-auc:0.665831\n",
            "[5]\tvalidation_0-auc:0.665794\n",
            "[6]\tvalidation_0-auc:0.66585\n",
            "[7]\tvalidation_0-auc:0.665775\n",
            "[8]\tvalidation_0-auc:0.665775\n",
            "[9]\tvalidation_0-auc:0.66628\n",
            "[10]\tvalidation_0-auc:0.666252\n",
            "[11]\tvalidation_0-auc:0.666402\n",
            "[12]\tvalidation_0-auc:0.666476\n",
            "[13]\tvalidation_0-auc:0.666514\n",
            "[14]\tvalidation_0-auc:0.66657\n",
            "[15]\tvalidation_0-auc:0.666626\n",
            "[16]\tvalidation_0-auc:0.696833\n",
            "[17]\tvalidation_0-auc:0.696871\n",
            "[18]\tvalidation_0-auc:0.696918\n",
            "[19]\tvalidation_0-auc:0.696899\n",
            "[20]\tvalidation_0-auc:0.697048\n",
            "[21]\tvalidation_0-auc:0.696992\n",
            "[22]\tvalidation_0-auc:0.711058\n",
            "[23]\tvalidation_0-auc:0.711058\n",
            "[24]\tvalidation_0-auc:0.711002\n",
            "[25]\tvalidation_0-auc:0.710946\n",
            "[26]\tvalidation_0-auc:0.706644\n",
            "[27]\tvalidation_0-auc:0.707897\n",
            "[28]\tvalidation_0-auc:0.707869\n",
            "[29]\tvalidation_0-auc:0.70684\n",
            "[30]\tvalidation_0-auc:0.701958\n",
            "[31]\tvalidation_0-auc:0.700181\n",
            "[32]\tvalidation_0-auc:0.700481\n",
            "[33]\tvalidation_0-auc:0.700069\n",
            "[34]\tvalidation_0-auc:0.698133\n",
            "[35]\tvalidation_0-auc:0.698208\n",
            "[36]\tvalidation_0-auc:0.698021\n",
            "[37]\tvalidation_0-auc:0.692849\n",
            "[38]\tvalidation_0-auc:0.691924\n",
            "[39]\tvalidation_0-auc:0.691381\n",
            "[40]\tvalidation_0-auc:0.689885\n",
            "[41]\tvalidation_0-auc:0.689885\n",
            "[42]\tvalidation_0-auc:0.68923\n",
            "[43]\tvalidation_0-auc:0.680318\n",
            "[44]\tvalidation_0-auc:0.680224\n",
            "[45]\tvalidation_0-auc:0.680037\n",
            "[46]\tvalidation_0-auc:0.678887\n",
            "[47]\tvalidation_0-auc:0.678756\n",
            "[48]\tvalidation_0-auc:0.678718\n",
            "[49]\tvalidation_0-auc:0.67827\n",
            "[50]\tvalidation_0-auc:0.678288\n",
            "[51]\tvalidation_0-auc:0.689006\n",
            "[52]\tvalidation_0-auc:0.692064\n",
            "[53]\tvalidation_0-auc:0.692457\n",
            "[54]\tvalidation_0-auc:0.692466\n",
            "[55]\tvalidation_0-auc:0.692354\n",
            "[56]\tvalidation_0-auc:0.692344\n",
            "[57]\tvalidation_0-auc:0.691465\n",
            "[58]\tvalidation_0-auc:0.691428\n",
            "[59]\tvalidation_0-auc:0.691372\n",
            "[60]\tvalidation_0-auc:0.691372\n",
            "[61]\tvalidation_0-auc:0.692335\n",
            "[62]\tvalidation_0-auc:0.692503\n",
            "[63]\tvalidation_0-auc:0.692316\n",
            "[64]\tvalidation_0-auc:0.692588\n",
            "[65]\tvalidation_0-auc:0.698891\n",
            "[66]\tvalidation_0-auc:0.698872\n",
            "[67]\tvalidation_0-auc:0.699115\n",
            "[68]\tvalidation_0-auc:0.69154\n",
            "[69]\tvalidation_0-auc:0.692101\n",
            "[70]\tvalidation_0-auc:0.694869\n",
            "[71]\tvalidation_0-auc:0.694907\n",
            "[72]\tvalidation_0-auc:0.695019\n",
            "Stopping. Best iteration:\n",
            "[22]\tvalidation_0-auc:0.711058\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 15ms/step - loss: 0.5296 - accuracy: 0.7345 - val_loss: 1.2591 - val_accuracy: 0.4508\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3571 - accuracy: 0.8491 - val_loss: 1.5393 - val_accuracy: 0.4442\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3395 - accuracy: 0.8564 - val_loss: 1.2278 - val_accuracy: 0.4508\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3354 - accuracy: 0.8515 - val_loss: 1.8503 - val_accuracy: 0.4442\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3209 - accuracy: 0.8533 - val_loss: 2.1925 - val_accuracy: 0.4201\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 14ms/step - loss: 0.4205 - accuracy: 0.8008 - val_loss: 1.5622 - val_accuracy: 0.4486\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2883 - accuracy: 0.8733 - val_loss: 2.1249 - val_accuracy: 0.4486\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2720 - accuracy: 0.8793 - val_loss: 1.9426 - val_accuracy: 0.4595\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2560 - accuracy: 0.8835 - val_loss: 2.0459 - val_accuracy: 0.4595\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.2584 - accuracy: 0.8793 - val_loss: 2.2447 - val_accuracy: 0.4573\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.644895\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.651086\n",
            "[2]\tvalidation_0-auc:0.651648\n",
            "[3]\tvalidation_0-auc:0.645832\n",
            "[4]\tvalidation_0-auc:0.645863\n",
            "[5]\tvalidation_0-auc:0.646477\n",
            "[6]\tvalidation_0-auc:0.646477\n",
            "[7]\tvalidation_0-auc:0.646664\n",
            "[8]\tvalidation_0-auc:0.648412\n",
            "[9]\tvalidation_0-auc:0.648329\n",
            "[10]\tvalidation_0-auc:0.650649\n",
            "[11]\tvalidation_0-auc:0.6635\n",
            "[12]\tvalidation_0-auc:0.664624\n",
            "[13]\tvalidation_0-auc:0.66504\n",
            "[14]\tvalidation_0-auc:0.665061\n",
            "[15]\tvalidation_0-auc:0.664645\n",
            "[16]\tvalidation_0-auc:0.664978\n",
            "[17]\tvalidation_0-auc:0.665082\n",
            "[18]\tvalidation_0-auc:0.665623\n",
            "[19]\tvalidation_0-auc:0.66581\n",
            "[20]\tvalidation_0-auc:0.668713\n",
            "[21]\tvalidation_0-auc:0.669171\n",
            "[22]\tvalidation_0-auc:0.664135\n",
            "[23]\tvalidation_0-auc:0.66428\n",
            "[24]\tvalidation_0-auc:0.664114\n",
            "[25]\tvalidation_0-auc:0.664031\n",
            "[26]\tvalidation_0-auc:0.662491\n",
            "[27]\tvalidation_0-auc:0.66222\n",
            "[28]\tvalidation_0-auc:0.662147\n",
            "[29]\tvalidation_0-auc:0.664478\n",
            "[30]\tvalidation_0-auc:0.630473\n",
            "[31]\tvalidation_0-auc:0.631472\n",
            "[32]\tvalidation_0-auc:0.632096\n",
            "[33]\tvalidation_0-auc:0.63064\n",
            "[34]\tvalidation_0-auc:0.627841\n",
            "[35]\tvalidation_0-auc:0.627695\n",
            "[36]\tvalidation_0-auc:0.62911\n",
            "[37]\tvalidation_0-auc:0.629173\n",
            "[38]\tvalidation_0-auc:0.632034\n",
            "[39]\tvalidation_0-auc:0.632034\n",
            "[40]\tvalidation_0-auc:0.633054\n",
            "[41]\tvalidation_0-auc:0.632034\n",
            "[42]\tvalidation_0-auc:0.631389\n",
            "[43]\tvalidation_0-auc:0.632242\n",
            "[44]\tvalidation_0-auc:0.632242\n",
            "[45]\tvalidation_0-auc:0.632367\n",
            "[46]\tvalidation_0-auc:0.632617\n",
            "[47]\tvalidation_0-auc:0.632284\n",
            "[48]\tvalidation_0-auc:0.632409\n",
            "[49]\tvalidation_0-auc:0.632533\n",
            "[50]\tvalidation_0-auc:0.630661\n",
            "[51]\tvalidation_0-auc:0.630161\n",
            "[52]\tvalidation_0-auc:0.630244\n",
            "[53]\tvalidation_0-auc:0.630078\n",
            "[54]\tvalidation_0-auc:0.644635\n",
            "[55]\tvalidation_0-auc:0.645322\n",
            "[56]\tvalidation_0-auc:0.666975\n",
            "[57]\tvalidation_0-auc:0.667038\n",
            "[58]\tvalidation_0-auc:0.6671\n",
            "[59]\tvalidation_0-auc:0.666705\n",
            "[60]\tvalidation_0-auc:0.6304\n",
            "[61]\tvalidation_0-auc:0.624865\n",
            "[62]\tvalidation_0-auc:0.62501\n",
            "[63]\tvalidation_0-auc:0.62499\n",
            "[64]\tvalidation_0-auc:0.625177\n",
            "[65]\tvalidation_0-auc:0.624178\n",
            "[66]\tvalidation_0-auc:0.624261\n",
            "[67]\tvalidation_0-auc:0.623658\n",
            "[68]\tvalidation_0-auc:0.623075\n",
            "[69]\tvalidation_0-auc:0.623117\n",
            "[70]\tvalidation_0-auc:0.623096\n",
            "[71]\tvalidation_0-auc:0.628621\n",
            "Stopping. Best iteration:\n",
            "[21]\tvalidation_0-auc:0.669171\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+---------------------+---------------------+--------------------+--------------------+\n",
            "|      Model       |       Accuracy      |      Precision      |       Recall       |      F1 score      |\n",
            "+------------------+---------------------+---------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     |  0.5122448979591837 |  0.4050632911392405 | 0.975609756097561  | 0.5724508050089445 |\n",
            "|     GRU 0.1      |  0.4857142857142857 |  0.3932038834951456 | 0.9878048780487805 |       0.5625       |\n",
            "|   XGBoost 0.1    | 0.49183673469387756 |  0.3960880195599022 | 0.9878048780487805 | 0.5654450261780105 |\n",
            "|    Logreg 0.1    |  0.4959183673469388 | 0.39753086419753086 | 0.9817073170731707 | 0.5659050966608085 |\n",
            "|     SVM 0.1      |  0.5061224489795918 | 0.40298507462686567 | 0.9878048780487805 | 0.5724381625441696 |\n",
            "|  LSTM beta 0.1   |  0.4201312910284464 |  0.3822843822843823 |        1.0         | 0.5531197301854974 |\n",
            "|   GRU beta 0.1   |  0.4573304157549234 |        0.395        | 0.9634146341463414 | 0.5602836879432624 |\n",
            "| XGBoost beta 0.1 |  0.4442013129102845 |  0.3891625615763547 | 0.9634146341463414 | 0.5543859649122806 |\n",
            "| logreg beta 0.1  | 0.45295404814004375 | 0.39141414141414144 | 0.9451219512195121 | 0.5535714285714286 |\n",
            "|   svm beta 0.1   |  0.4573304157549234 |        0.395        | 0.9634146341463414 | 0.5602836879432624 |\n",
            "+------------------+---------------------+---------------------+--------------------+--------------------+\n",
            "Threshhold =  0.05\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 15ms/step - loss: 0.6923 - accuracy: 0.5207 - val_loss: 0.6588 - val_accuracy: 0.8551\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.4198 - accuracy: 0.8314 - val_loss: 0.2053 - val_accuracy: 0.9000\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.2610 - accuracy: 0.8929 - val_loss: 0.1496 - val_accuracy: 0.9469\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.2392 - accuracy: 0.8988 - val_loss: 0.1389 - val_accuracy: 0.9429\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.2383 - accuracy: 0.8982 - val_loss: 0.1400 - val_accuracy: 0.9551\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 14ms/step - loss: 0.6311 - accuracy: 0.6237 - val_loss: 0.2111 - val_accuracy: 0.9122\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.2248 - accuracy: 0.8953 - val_loss: 0.1174 - val_accuracy: 0.9551\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.2114 - accuracy: 0.9077 - val_loss: 0.1193 - val_accuracy: 0.9531\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.2100 - accuracy: 0.9036 - val_loss: 0.1157 - val_accuracy: 0.9592\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.2061 - accuracy: 0.9065 - val_loss: 0.1143 - val_accuracy: 0.9571\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.985663\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.98427\n",
            "[2]\tvalidation_0-auc:0.985851\n",
            "[3]\tvalidation_0-auc:0.985387\n",
            "[4]\tvalidation_0-auc:0.985261\n",
            "[5]\tvalidation_0-auc:0.985261\n",
            "[6]\tvalidation_0-auc:0.985537\n",
            "[7]\tvalidation_0-auc:0.985713\n",
            "[8]\tvalidation_0-auc:0.986026\n",
            "[9]\tvalidation_0-auc:0.994682\n",
            "[10]\tvalidation_0-auc:0.994958\n",
            "[11]\tvalidation_0-auc:0.994895\n",
            "[12]\tvalidation_0-auc:0.995384\n",
            "[13]\tvalidation_0-auc:0.99571\n",
            "[14]\tvalidation_0-auc:0.995773\n",
            "[15]\tvalidation_0-auc:0.995672\n",
            "[16]\tvalidation_0-auc:0.995698\n",
            "[17]\tvalidation_0-auc:0.995836\n",
            "[18]\tvalidation_0-auc:0.995911\n",
            "[19]\tvalidation_0-auc:0.995886\n",
            "[20]\tvalidation_0-auc:0.996011\n",
            "[21]\tvalidation_0-auc:0.996011\n",
            "[22]\tvalidation_0-auc:0.996011\n",
            "[23]\tvalidation_0-auc:0.996061\n",
            "[24]\tvalidation_0-auc:0.996011\n",
            "[25]\tvalidation_0-auc:0.995936\n",
            "[26]\tvalidation_0-auc:0.995861\n",
            "[27]\tvalidation_0-auc:0.995886\n",
            "[28]\tvalidation_0-auc:0.996086\n",
            "[29]\tvalidation_0-auc:0.995911\n",
            "[30]\tvalidation_0-auc:0.99576\n",
            "[31]\tvalidation_0-auc:0.99576\n",
            "[32]\tvalidation_0-auc:0.995785\n",
            "[33]\tvalidation_0-auc:0.995735\n",
            "[34]\tvalidation_0-auc:0.995685\n",
            "[35]\tvalidation_0-auc:0.995936\n",
            "[36]\tvalidation_0-auc:0.995961\n",
            "[37]\tvalidation_0-auc:0.995735\n",
            "[38]\tvalidation_0-auc:0.995961\n",
            "[39]\tvalidation_0-auc:0.995735\n",
            "[40]\tvalidation_0-auc:0.99576\n",
            "[41]\tvalidation_0-auc:0.99581\n",
            "[42]\tvalidation_0-auc:0.995685\n",
            "[43]\tvalidation_0-auc:0.99576\n",
            "[44]\tvalidation_0-auc:0.99561\n",
            "[45]\tvalidation_0-auc:0.995585\n",
            "[46]\tvalidation_0-auc:0.99561\n",
            "[47]\tvalidation_0-auc:0.99561\n",
            "[48]\tvalidation_0-auc:0.99571\n",
            "[49]\tvalidation_0-auc:0.99571\n",
            "[50]\tvalidation_0-auc:0.99561\n",
            "[51]\tvalidation_0-auc:0.99561\n",
            "[52]\tvalidation_0-auc:0.99561\n",
            "[53]\tvalidation_0-auc:0.99561\n",
            "[54]\tvalidation_0-auc:0.99561\n",
            "[55]\tvalidation_0-auc:0.995635\n",
            "[56]\tvalidation_0-auc:0.99556\n",
            "[57]\tvalidation_0-auc:0.995359\n",
            "[58]\tvalidation_0-auc:0.995359\n",
            "[59]\tvalidation_0-auc:0.995509\n",
            "[60]\tvalidation_0-auc:0.995484\n",
            "[61]\tvalidation_0-auc:0.995434\n",
            "[62]\tvalidation_0-auc:0.99561\n",
            "[63]\tvalidation_0-auc:0.99561\n",
            "[64]\tvalidation_0-auc:0.99561\n",
            "[65]\tvalidation_0-auc:0.99566\n",
            "[66]\tvalidation_0-auc:0.995685\n",
            "[67]\tvalidation_0-auc:0.99556\n",
            "[68]\tvalidation_0-auc:0.99556\n",
            "[69]\tvalidation_0-auc:0.99556\n",
            "[70]\tvalidation_0-auc:0.995509\n",
            "[71]\tvalidation_0-auc:0.995484\n",
            "[72]\tvalidation_0-auc:0.995509\n",
            "[73]\tvalidation_0-auc:0.995509\n",
            "[74]\tvalidation_0-auc:0.995459\n",
            "[75]\tvalidation_0-auc:0.995459\n",
            "[76]\tvalidation_0-auc:0.995459\n",
            "[77]\tvalidation_0-auc:0.99556\n",
            "[78]\tvalidation_0-auc:0.995534\n",
            "Stopping. Best iteration:\n",
            "[28]\tvalidation_0-auc:0.996086\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 15ms/step - loss: 0.6067 - accuracy: 0.6331 - val_loss: 0.3103 - val_accuracy: 0.8643\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3362 - accuracy: 0.8594 - val_loss: 0.2528 - val_accuracy: 0.8884\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3120 - accuracy: 0.8817 - val_loss: 0.2497 - val_accuracy: 0.9212\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.2849 - accuracy: 0.8793 - val_loss: 0.2612 - val_accuracy: 0.8906\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.2863 - accuracy: 0.8877 - val_loss: 0.2229 - val_accuracy: 0.9037\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 13ms/step - loss: 0.4227 - accuracy: 0.8057 - val_loss: 0.3046 - val_accuracy: 0.8731\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2834 - accuracy: 0.8877 - val_loss: 0.1994 - val_accuracy: 0.9300\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.2795 - accuracy: 0.8775 - val_loss: 0.2002 - val_accuracy: 0.9190\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2525 - accuracy: 0.8950 - val_loss: 0.2039 - val_accuracy: 0.9103\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2542 - accuracy: 0.8926 - val_loss: 0.1881 - val_accuracy: 0.9125\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.921207\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.939793\n",
            "[2]\tvalidation_0-auc:0.941159\n",
            "[3]\tvalidation_0-auc:0.953156\n",
            "[4]\tvalidation_0-auc:0.954042\n",
            "[5]\tvalidation_0-auc:0.958675\n",
            "[6]\tvalidation_0-auc:0.954577\n",
            "[7]\tvalidation_0-auc:0.959764\n",
            "[8]\tvalidation_0-auc:0.959745\n",
            "[9]\tvalidation_0-auc:0.971465\n",
            "[10]\tvalidation_0-auc:0.972536\n",
            "[11]\tvalidation_0-auc:0.972794\n",
            "[12]\tvalidation_0-auc:0.972757\n",
            "[13]\tvalidation_0-auc:0.972831\n",
            "[14]\tvalidation_0-auc:0.973367\n",
            "[15]\tvalidation_0-auc:0.97333\n",
            "[16]\tvalidation_0-auc:0.9744\n",
            "[17]\tvalidation_0-auc:0.976375\n",
            "[18]\tvalidation_0-auc:0.976412\n",
            "[19]\tvalidation_0-auc:0.976338\n",
            "[20]\tvalidation_0-auc:0.976984\n",
            "[21]\tvalidation_0-auc:0.976135\n",
            "[22]\tvalidation_0-auc:0.976836\n",
            "[23]\tvalidation_0-auc:0.976873\n",
            "[24]\tvalidation_0-auc:0.976689\n",
            "[25]\tvalidation_0-auc:0.975766\n",
            "[26]\tvalidation_0-auc:0.976357\n",
            "[27]\tvalidation_0-auc:0.977021\n",
            "[28]\tvalidation_0-auc:0.977316\n",
            "[29]\tvalidation_0-auc:0.97691\n",
            "[30]\tvalidation_0-auc:0.977261\n",
            "[31]\tvalidation_0-auc:0.977667\n",
            "[32]\tvalidation_0-auc:0.97859\n",
            "[33]\tvalidation_0-auc:0.978516\n",
            "[34]\tvalidation_0-auc:0.978516\n",
            "[35]\tvalidation_0-auc:0.978332\n",
            "[36]\tvalidation_0-auc:0.977889\n",
            "[37]\tvalidation_0-auc:0.977889\n",
            "[38]\tvalidation_0-auc:0.977889\n",
            "[39]\tvalidation_0-auc:0.97811\n",
            "[40]\tvalidation_0-auc:0.978295\n",
            "[41]\tvalidation_0-auc:0.978258\n",
            "[42]\tvalidation_0-auc:0.978516\n",
            "[43]\tvalidation_0-auc:0.978516\n",
            "[44]\tvalidation_0-auc:0.978479\n",
            "[45]\tvalidation_0-auc:0.978295\n",
            "[46]\tvalidation_0-auc:0.978479\n",
            "[47]\tvalidation_0-auc:0.978073\n",
            "[48]\tvalidation_0-auc:0.977999\n",
            "[49]\tvalidation_0-auc:0.97763\n",
            "[50]\tvalidation_0-auc:0.977446\n",
            "[51]\tvalidation_0-auc:0.977335\n",
            "[52]\tvalidation_0-auc:0.97704\n",
            "[53]\tvalidation_0-auc:0.97763\n",
            "[54]\tvalidation_0-auc:0.97715\n",
            "[55]\tvalidation_0-auc:0.976744\n",
            "[56]\tvalidation_0-auc:0.976929\n",
            "[57]\tvalidation_0-auc:0.977003\n",
            "[58]\tvalidation_0-auc:0.976707\n",
            "[59]\tvalidation_0-auc:0.976892\n",
            "[60]\tvalidation_0-auc:0.976615\n",
            "[61]\tvalidation_0-auc:0.976615\n",
            "[62]\tvalidation_0-auc:0.976578\n",
            "[63]\tvalidation_0-auc:0.976947\n",
            "[64]\tvalidation_0-auc:0.97691\n",
            "[65]\tvalidation_0-auc:0.977169\n",
            "[66]\tvalidation_0-auc:0.977243\n",
            "[67]\tvalidation_0-auc:0.977132\n",
            "[68]\tvalidation_0-auc:0.977021\n",
            "[69]\tvalidation_0-auc:0.977704\n",
            "[70]\tvalidation_0-auc:0.978184\n",
            "[71]\tvalidation_0-auc:0.978036\n",
            "[72]\tvalidation_0-auc:0.977778\n",
            "[73]\tvalidation_0-auc:0.977962\n",
            "[74]\tvalidation_0-auc:0.977962\n",
            "[75]\tvalidation_0-auc:0.977889\n",
            "[76]\tvalidation_0-auc:0.978553\n",
            "[77]\tvalidation_0-auc:0.978258\n",
            "[78]\tvalidation_0-auc:0.978405\n",
            "[79]\tvalidation_0-auc:0.978811\n",
            "[80]\tvalidation_0-auc:0.978332\n",
            "[81]\tvalidation_0-auc:0.978073\n",
            "[82]\tvalidation_0-auc:0.978184\n",
            "[83]\tvalidation_0-auc:0.978332\n",
            "[84]\tvalidation_0-auc:0.978405\n",
            "[85]\tvalidation_0-auc:0.978442\n",
            "[86]\tvalidation_0-auc:0.978553\n",
            "[87]\tvalidation_0-auc:0.978442\n",
            "[88]\tvalidation_0-auc:0.978332\n",
            "[89]\tvalidation_0-auc:0.977999\n",
            "[90]\tvalidation_0-auc:0.978184\n",
            "[91]\tvalidation_0-auc:0.978184\n",
            "[92]\tvalidation_0-auc:0.977667\n",
            "[93]\tvalidation_0-auc:0.977962\n",
            "[94]\tvalidation_0-auc:0.978405\n",
            "[95]\tvalidation_0-auc:0.978959\n",
            "[96]\tvalidation_0-auc:0.978848\n",
            "[97]\tvalidation_0-auc:0.979033\n",
            "[98]\tvalidation_0-auc:0.979144\n",
            "[99]\tvalidation_0-auc:0.979107\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.05     | 0.9551020408163265 | 0.9667519181585678 | 0.9767441860465116 | 0.9717223650385604 |\n",
            "|      GRU 0.05     | 0.9571428571428572 | 0.9644670050761421 | 0.9819121447028424 | 0.9731113956466069 |\n",
            "|    XGBoost 0.05   | 0.9591836734693877 | 0.9622166246851386 | 0.9870801033591732 | 0.9744897959183674 |\n",
            "|    Logreg 0.05    | 0.9530612244897959 | 0.9642857142857143 | 0.9767441860465116 | 0.9704749679075738 |\n",
            "|      SVM 0.05     | 0.9571428571428572 | 0.9621212121212122 | 0.9844961240310077 | 0.9731800766283525 |\n",
            "|   LSTM beta 0.05  | 0.9037199124726477 | 0.9431524547803618 | 0.9431524547803618 | 0.9431524547803618 |\n",
            "|   GRU beta 0.05   | 0.912472647702407  | 0.9414758269720102 | 0.9560723514211886 | 0.9487179487179486 |\n",
            "| XGBoost beta 0.05 | 0.9321663019693655 | 0.9540816326530612 | 0.9664082687338501 | 0.9602053915275995 |\n",
            "|  logreg beta 0.05 | 0.8927789934354485 | 0.9333333333333333 | 0.9405684754521964 | 0.9369369369369369 |\n",
            "|   svm beta 0.05   | 0.9146608315098468 | 0.9393939393939394 | 0.9612403100775194 | 0.9501915708812261 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979
        },
        "id": "QU7y-UXa7BuB",
        "outputId": "d535cfe0-7305-40eb-8033-c3b38a126621"
      },
      "source": [
        "Result_purging.to_csv('AMD_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.05</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.646388</td>\n",
              "      <td>0.795918</td>\n",
              "      <td>0.772727</td>\n",
              "      <td>0.960452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.05</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.939024</td>\n",
              "      <td>0.932653</td>\n",
              "      <td>0.903226</td>\n",
              "      <td>0.870056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.05</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.825243</td>\n",
              "      <td>0.912245</td>\n",
              "      <td>0.887728</td>\n",
              "      <td>0.960452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.05</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.924490</td>\n",
              "      <td>0.887538</td>\n",
              "      <td>0.824859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.05</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.793296</td>\n",
              "      <td>0.853061</td>\n",
              "      <td>0.797753</td>\n",
              "      <td>0.802260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.05</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.710227</td>\n",
              "      <td>0.816193</td>\n",
              "      <td>0.748503</td>\n",
              "      <td>0.791139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.05</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.770950</td>\n",
              "      <td>0.866521</td>\n",
              "      <td>0.818991</td>\n",
              "      <td>0.873418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.05</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.839506</td>\n",
              "      <td>0.894967</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>0.860759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.05</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.834437</td>\n",
              "      <td>0.875274</td>\n",
              "      <td>0.815534</td>\n",
              "      <td>0.797468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.05</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.862069</td>\n",
              "      <td>0.884026</td>\n",
              "      <td>0.825083</td>\n",
              "      <td>0.791139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.516746</td>\n",
              "      <td>0.779592</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.939130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.604396</td>\n",
              "      <td>0.842857</td>\n",
              "      <td>0.740741</td>\n",
              "      <td>0.956522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.541063</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.695652</td>\n",
              "      <td>0.973913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.690789</td>\n",
              "      <td>0.883673</td>\n",
              "      <td>0.786517</td>\n",
              "      <td>0.913043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.480447</td>\n",
              "      <td>0.751020</td>\n",
              "      <td>0.585034</td>\n",
              "      <td>0.747826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.629032</td>\n",
              "      <td>0.859956</td>\n",
              "      <td>0.709091</td>\n",
              "      <td>0.812500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.661290</td>\n",
              "      <td>0.877462</td>\n",
              "      <td>0.745455</td>\n",
              "      <td>0.854167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.611111</td>\n",
              "      <td>0.859956</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.916667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.543046</td>\n",
              "      <td>0.818381</td>\n",
              "      <td>0.663968</td>\n",
              "      <td>0.854167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.531034</td>\n",
              "      <td>0.809628</td>\n",
              "      <td>0.639004</td>\n",
              "      <td>0.802083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.511737</td>\n",
              "      <td>0.775510</td>\n",
              "      <td>0.664634</td>\n",
              "      <td>0.947826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.611111</td>\n",
              "      <td>0.846939</td>\n",
              "      <td>0.745763</td>\n",
              "      <td>0.956522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.543689</td>\n",
              "      <td>0.802041</td>\n",
              "      <td>0.697819</td>\n",
              "      <td>0.973913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.748092</td>\n",
              "      <td>0.897959</td>\n",
              "      <td>0.796748</td>\n",
              "      <td>0.852174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.547739</td>\n",
              "      <td>0.804082</td>\n",
              "      <td>0.694268</td>\n",
              "      <td>0.947826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.634783</td>\n",
              "      <td>0.857768</td>\n",
              "      <td>0.691943</td>\n",
              "      <td>0.760417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.551282</td>\n",
              "      <td>0.824945</td>\n",
              "      <td>0.682540</td>\n",
              "      <td>0.895833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.513333</td>\n",
              "      <td>0.798687</td>\n",
              "      <td>0.626016</td>\n",
              "      <td>0.802083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.642276</td>\n",
              "      <td>0.866521</td>\n",
              "      <td>0.721461</td>\n",
              "      <td>0.822917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.542254</td>\n",
              "      <td>0.816193</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.802083</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0          LSTM 0.05  AMD  0.646388  0.795918  0.772727  0.960452\n",
              "1           GRU 0.05  AMD  0.939024  0.932653  0.903226  0.870056\n",
              "2       XGBoost 0.05  AMD  0.825243  0.912245  0.887728  0.960452\n",
              "3        Logreg 0.05  AMD  0.960526  0.924490  0.887538  0.824859\n",
              "4           SVM 0.05  AMD  0.793296  0.853061  0.797753  0.802260\n",
              "5     LSTM beta 0.05  AMD  0.710227  0.816193  0.748503  0.791139\n",
              "6      GRU beta 0.05  AMD  0.770950  0.866521  0.818991  0.873418\n",
              "7  XGBoost beta 0.05  AMD  0.839506  0.894967  0.850000  0.860759\n",
              "8   logreg beta 0.05  AMD  0.834437  0.875274  0.815534  0.797468\n",
              "9      svm beta 0.05  AMD  0.862069  0.884026  0.825083  0.791139\n",
              "0           LSTM 0.1  AMD  0.516746  0.779592  0.666667  0.939130\n",
              "1            GRU 0.1  AMD  0.604396  0.842857  0.740741  0.956522\n",
              "2        XGBoost 0.1  AMD  0.541063  0.800000  0.695652  0.973913\n",
              "3         Logreg 0.1  AMD  0.690789  0.883673  0.786517  0.913043\n",
              "4            SVM 0.1  AMD  0.480447  0.751020  0.585034  0.747826\n",
              "5      LSTM beta 0.1  AMD  0.629032  0.859956  0.709091  0.812500\n",
              "6       GRU beta 0.1  AMD  0.661290  0.877462  0.745455  0.854167\n",
              "7   XGBoost beta 0.1  AMD  0.611111  0.859956  0.733333  0.916667\n",
              "8    logreg beta 0.1  AMD  0.543046  0.818381  0.663968  0.854167\n",
              "9       svm beta 0.1  AMD  0.531034  0.809628  0.639004  0.802083\n",
              "0          LSTM 0.15  AMD  0.511737  0.775510  0.664634  0.947826\n",
              "1           GRU 0.15  AMD  0.611111  0.846939  0.745763  0.956522\n",
              "2       XGBoost 0.15  AMD  0.543689  0.802041  0.697819  0.973913\n",
              "3        Logreg 0.15  AMD  0.748092  0.897959  0.796748  0.852174\n",
              "4           SVM 0.15  AMD  0.547739  0.804082  0.694268  0.947826\n",
              "5     LSTM beta 0.15  AMD  0.634783  0.857768  0.691943  0.760417\n",
              "6      GRU beta 0.15  AMD  0.551282  0.824945  0.682540  0.895833\n",
              "7  XGBoost beta 0.15  AMD  0.513333  0.798687  0.626016  0.802083\n",
              "8   logreg beta 0.15  AMD  0.642276  0.866521  0.721461  0.822917\n",
              "9      svm beta 0.15  AMD  0.542254  0.816193  0.647059  0.802083"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9283uXG7ldX"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2a8-3zLCs9f"
      },
      "source": [
        "## AAPL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "id": "twW1qvrnCs99",
        "outputId": "3832e97f-5d15-4189-a3e9-ad4212d6fae9"
      },
      "source": [
        "dfs = pd.read_csv(\"AAPL.csv\")\n",
        "# dfs = dfs[::-1].reset_index()\n",
        "# Denoise prices\n",
        "dfs = denoise_data(dfs, '<CLOSE>')\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>US1.AAPL</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>281.60</td>\n",
              "      <td>282.90</td>\n",
              "      <td>277.7700</td>\n",
              "      <td>323.93085</td>\n",
              "      <td>14964464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>US1.AAPL</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>281.96</td>\n",
              "      <td>289.44</td>\n",
              "      <td>281.8205</td>\n",
              "      <td>323.93085</td>\n",
              "      <td>16379352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>US1.AAPL</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>289.64</td>\n",
              "      <td>291.99</td>\n",
              "      <td>285.2600</td>\n",
              "      <td>323.93085</td>\n",
              "      <td>22791119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>US1.AAPL</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>290.26</td>\n",
              "      <td>290.48</td>\n",
              "      <td>286.9100</td>\n",
              "      <td>323.93085</td>\n",
              "      <td>13965617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US1.AAPL</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>291.83</td>\n",
              "      <td>294.50</td>\n",
              "      <td>290.0000</td>\n",
              "      <td>323.93085</td>\n",
              "      <td>21492761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2762</th>\n",
              "      <td>US1.AAPL</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>145.40</td>\n",
              "      <td>145.96</td>\n",
              "      <td>143.8300</td>\n",
              "      <td>140.59160</td>\n",
              "      <td>3504880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2763</th>\n",
              "      <td>US1.AAPL</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>143.19</td>\n",
              "      <td>144.75</td>\n",
              "      <td>141.7000</td>\n",
              "      <td>140.59160</td>\n",
              "      <td>5602142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>US1.AAPL</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>142.47</td>\n",
              "      <td>144.45</td>\n",
              "      <td>142.0300</td>\n",
              "      <td>140.59160</td>\n",
              "      <td>3205269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>US1.AAPL</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>143.67</td>\n",
              "      <td>144.37</td>\n",
              "      <td>141.2900</td>\n",
              "      <td>140.59160</td>\n",
              "      <td>4102531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>US1.AAPL</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>141.89</td>\n",
              "      <td>142.91</td>\n",
              "      <td>139.1300</td>\n",
              "      <td>140.59160</td>\n",
              "      <td>4233615</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2767 rows Ã— 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      <TICKER> <PER>    <DATE>  <TIME>  ...  <HIGH>     <LOW>    <CLOSE>     <VOL>\n",
              "0     US1.AAPL     D  20101004       0  ...  282.90  277.7700  323.93085  14964464\n",
              "1     US1.AAPL     D  20101005       0  ...  289.44  281.8205  323.93085  16379352\n",
              "2     US1.AAPL     D  20101006       0  ...  291.99  285.2600  323.93085  22791119\n",
              "3     US1.AAPL     D  20101007       0  ...  290.48  286.9100  323.93085  13965617\n",
              "4     US1.AAPL     D  20101008       0  ...  294.50  290.0000  323.93085  21492761\n",
              "...        ...   ...       ...     ...  ...     ...       ...        ...       ...\n",
              "2762  US1.AAPL     D  20210927       0  ...  145.96  143.8300  140.59160   3504880\n",
              "2763  US1.AAPL     D  20210928       0  ...  144.75  141.7000  140.59160   5602142\n",
              "2764  US1.AAPL     D  20210929       0  ...  144.45  142.0300  140.59160   3205269\n",
              "2765  US1.AAPL     D  20210930       0  ...  144.37  141.2900  140.59160   4102531\n",
              "2766  US1.AAPL     D  20211001       0  ...  142.91  139.1300  140.59160   4233615\n",
              "\n",
              "[2767 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 239
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hyemRmnHCs9-",
        "outputId": "455ba7ae-2987-4a88-cfb0-997881ed20c0"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 100, 2300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"583a2ab5-cecc-4882-bae8-4f0609b17d25\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"583a2ab5-cecc-4882-bae8-4f0609b17d25\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '583a2ab5-cecc-4882-bae8-4f0609b17d25',\n",
              "                        [{\"type\": \"scatter\", \"x\": [20110225, 20110228, 20110301, 20110302, 20110303, 20110304, 20110307, 20110308, 20110309, 20110310, 20110311, 20110314, 20110315, 20110316, 20110317, 20110318, 20110321, 20110322, 20110323, 20110324, 20110325, 20110328, 20110329, 20110330, 20110331, 20110401, 20110404, 20110405, 20110406, 20110407, 20110408, 20110411, 20110412, 20110413, 20110414, 20110415, 20110418, 20110419, 20110420, 20110421, 20110425, 20110426, 20110427, 20110428, 20110429, 20110502, 20110503, 20110504, 20110505, 20110506, 20110509, 20110510, 20110511, 20110512, 20110513, 20110516, 20110517, 20110518, 20110519, 20110520, 20110523, 20110524, 20110525, 20110526, 20110527, 20110531, 20110601, 20110602, 20110603, 20110606, 20110607, 20110608, 20110609, 20110610, 20110613, 20110614, 20110615, 20110616, 20110617, 20110620, 20110621, 20110622, 20110623, 20110624, 20110627, 20110628, 20110629, 20110630, 20110701, 20110705, 20110706, 20110707, 20110708, 20110711, 20110712, 20110713, 20110714, 20110715, 20110718, 20110719, 20110720, 20110721, 20110722, 20110725, 20110726, 20110727, 20110728, 20110729, 20110801, 20110802, 20110803, 20110804, 20110805, 20110808, 20110809, 20110810, 20110811, 20110812, 20110815, 20110816, 20110817, 20110818, 20110819, 20110822, 20110823, 20110824, 20110825, 20110826, 20110829, 20110830, 20110831, 20110901, 20110902, 20110906, 20110907, 20110908, 20110909, 20110912, 20110913, 20110914, 20110915, 20110916, 20110919, 20110920, 20110921, 20110922, 20110923, 20110926, 20110927, 20110928, 20110929, 20110930, 20111003, 20111004, 20111005, 20111006, 20111007, 20111010, 20111011, 20111012, 20111013, 20111014, 20111017, 20111018, 20111019, 20111020, 20111021, 20111024, 20111025, 20111026, 20111027, 20111028, 20111031, 20111101, 20111102, 20111103, 20111104, 20111107, 20111108, 20111109, 20111110, 20111111, 20111114, 20111115, 20111116, 20111117, 20111118, 20111121, 20111122, 20111123, 20111125, 20111128, 20111129, 20111130, 20111201, 20111202, 20111205, 20111206, 20111207, 20111208, 20111209, 20111212, 20111213, 20111214, 20111215, 20111216, 20111219, 20111220, 20111221, 20111222, 20111223, 20111227, 20111228, 20111229, 20111230, 20120103, 20120104, 20120105, 20120106, 20120109, 20120110, 20120111, 20120112, 20120113, 20120117, 20120118, 20120119, 20120120, 20120123, 20120124, 20120125, 20120126, 20120127, 20120130, 20120131, 20120201, 20120202, 20120203, 20120206, 20120207, 20120208, 20120209, 20120210, 20120213, 20120214, 20120215, 20120216, 20120217, 20120221, 20120222, 20120223, 20120224, 20120227, 20120228, 20120229, 20120301, 20120302, 20120305, 20120306, 20120307, 20120308, 20120309, 20120312, 20120313, 20120314, 20120315, 20120316, 20120319, 20120320, 20120321, 20120322, 20120323, 20120326, 20120327, 20120328, 20120329, 20120330, 20120402, 20120403, 20120404, 20120405, 20120409, 20120410, 20120411, 20120412, 20120413, 20120416, 20120417, 20120418, 20120419, 20120420, 20120423, 20120424, 20120425, 20120426, 20120427, 20120430, 20120501, 20120502, 20120503, 20120504, 20120507, 20120508, 20120509, 20120510, 20120511, 20120514, 20120515, 20120516, 20120517, 20120518, 20120521, 20120522, 20120523, 20120524, 20120525, 20120529, 20120530, 20120531, 20120601, 20120604, 20120605, 20120606, 20120607, 20120608, 20120611, 20120612, 20120613, 20120614, 20120615, 20120618, 20120619, 20120620, 20120621, 20120622, 20120625, 20120626, 20120627, 20120628, 20120629, 20120702, 20120703, 20120705, 20120706, 20120709, 20120710, 20120711, 20120712, 20120713, 20120716, 20120717, 20120718, 20120719, 20120720, 20120723, 20120724, 20120725, 20120726, 20120727, 20120730, 20120731, 20120801, 20120802, 20120803, 20120806, 20120807, 20120808, 20120809, 20120810, 20120813, 20120814, 20120815, 20120816, 20120817, 20120820, 20120821, 20120822, 20120823, 20120824, 20120827, 20120828, 20120829, 20120830, 20120831, 20120904, 20120905, 20120906, 20120907, 20120910, 20120911, 20120912, 20120913, 20120914, 20120917, 20120918, 20120919, 20120920, 20120921, 20120924, 20120925, 20120926, 20120927, 20120928, 20121001, 20121002, 20121003, 20121004, 20121005, 20121008, 20121009, 20121010, 20121011, 20121012, 20121015, 20121016, 20121017, 20121018, 20121019, 20121022, 20121023, 20121024, 20121025, 20121026, 20121031, 20121101, 20121102, 20121105, 20121106, 20121107, 20121108, 20121109, 20121112, 20121113, 20121114, 20121115, 20121116, 20121119, 20121120, 20121121, 20121123, 20121126, 20121127, 20121128, 20121129, 20121130, 20121203, 20121204, 20121205, 20121206, 20121207, 20121210, 20121211, 20121212, 20121213, 20121214, 20121217, 20121218, 20121219, 20121220, 20121221, 20121224, 20121226, 20121227, 20121228, 20121231, 20130102, 20130103, 20130104, 20130107, 20130108, 20130109, 20130110, 20130111, 20130114, 20130115, 20130116, 20130117, 20130118, 20130122, 20130123, 20130124, 20130125, 20130128, 20130129, 20130130, 20130131, 20130201, 20130204, 20130205, 20130206, 20130207, 20130208, 20130211, 20130212, 20130213, 20130214, 20130215, 20130219, 20130220, 20130221, 20130222, 20130225, 20130226, 20130227, 20130228, 20130301, 20130304, 20130305, 20130306, 20130307, 20130308, 20130311, 20130312, 20130313, 20130314, 20130315, 20130318, 20130319, 20130320, 20130321, 20130322, 20130325, 20130326, 20130327, 20130328, 20130401, 20130402, 20130403, 20130404, 20130405, 20130408, 20130409, 20130410, 20130411, 20130412, 20130415, 20130416, 20130417, 20130418, 20130419, 20130422, 20130423, 20130424, 20130425, 20130426, 20130429, 20130430, 20130501, 20130502, 20130503, 20130506, 20130507, 20130508, 20130509, 20130510, 20130513, 20130514, 20130515, 20130516, 20130517, 20130520, 20130521, 20130522, 20130523, 20130524, 20130528, 20130529, 20130530, 20130531, 20130603, 20130604, 20130605, 20130606, 20130607, 20130610, 20130611, 20130612, 20130613, 20130614, 20130617, 20130618, 20130619, 20130620, 20130621, 20130624, 20130625, 20130626, 20130627, 20130628, 20130701, 20130702, 20130703, 20130705, 20130708, 20130709, 20130710, 20130711, 20130712, 20130715, 20130716, 20130717, 20130718, 20130719, 20130722, 20130723, 20130724, 20130725, 20130726, 20130729, 20130730, 20130731, 20130801, 20130802, 20130805, 20130806, 20130807, 20130808, 20130809, 20130812, 20130813, 20130814, 20130815, 20130816, 20130819, 20130820, 20130821, 20130822, 20130823, 20130826, 20130827, 20130828, 20130829, 20130830, 20130903, 20130904, 20130905, 20130906, 20130909, 20130910, 20130911, 20130912, 20130913, 20130916, 20130917, 20130918, 20130919, 20130920, 20130923, 20130924, 20130925, 20130926, 20130927, 20130930, 20131001, 20131002, 20131003, 20131004, 20131007, 20131008, 20131009, 20131010, 20131011, 20131014, 20131015, 20131016, 20131017, 20131018, 20131021, 20131022, 20131023, 20131024, 20131025, 20131028, 20131029, 20131030, 20131031, 20131101, 20131104, 20131105, 20131106, 20131107, 20131108, 20131111, 20131112, 20131113, 20131114, 20131115, 20131118, 20131119, 20131120, 20131121, 20131122, 20131125, 20131126, 20131127, 20131129, 20131202, 20131203, 20131204, 20131205, 20131206, 20131209, 20131210, 20131211, 20131212, 20131213, 20131216, 20131217, 20131218, 20131219, 20131220, 20131223, 20131224, 20131226, 20131227, 20131230, 20131231, 20140102, 20140103, 20140106, 20140107, 20140108, 20140109, 20140110, 20140113, 20140114, 20140115, 20140116, 20140117, 20140121, 20140122, 20140123, 20140124, 20140127, 20140128, 20140129, 20140130, 20140131, 20140203, 20140204, 20140205, 20140206, 20140207, 20140210, 20140211, 20140212, 20140213, 20140214, 20140218, 20140219, 20140220, 20140221, 20140224, 20140225, 20140226, 20140227, 20140228, 20140303, 20140304, 20140305, 20140306, 20140307, 20140310, 20140311, 20140312, 20140313, 20140314, 20140317, 20140318, 20140319, 20140320, 20140321, 20140324, 20140325, 20140326, 20140327, 20140328, 20140331, 20140401, 20140402, 20140403, 20140404, 20140407, 20140408, 20140409, 20140410, 20140411, 20140414, 20140415, 20140416, 20140417, 20140421, 20140422, 20140423, 20140424, 20140425, 20140428, 20140429, 20140430, 20140501, 20140502, 20140505, 20140506, 20140507, 20140508, 20140509, 20140512, 20140513, 20140514, 20140515, 20140516, 20140519, 20140520, 20140521, 20140522, 20140523, 20140527, 20140528, 20140529, 20140530, 20140602, 20140603, 20140604, 20140605, 20140606, 20140609, 20140610, 20140611, 20140612, 20140613, 20140616, 20140617, 20140618, 20140619, 20140620, 20140623, 20140624, 20140625, 20140626, 20140627, 20140630, 20140701, 20140702, 20140703, 20140707, 20140708, 20140709, 20140710, 20140711, 20140714, 20140715, 20140716, 20140717, 20140718, 20140721, 20140722, 20140723, 20140724, 20140725, 20140728, 20140729, 20140730, 20140731, 20140801, 20140804, 20140805, 20140806, 20140807, 20140808, 20140811, 20140812, 20140813, 20140814, 20140815, 20140818, 20140819, 20140820, 20140821, 20140822, 20140825, 20140826, 20140827, 20140828, 20140829, 20140902, 20140903, 20140904, 20140905, 20140908, 20140909, 20140910, 20140911, 20140912, 20140915, 20140916, 20140917, 20140918, 20140919, 20140922, 20140923, 20140924, 20140925, 20140926, 20140929, 20140930, 20141001, 20141002, 20141003, 20141006, 20141007, 20141008, 20141009, 20141010, 20141013, 20141014, 20141015, 20141016, 20141017, 20141020, 20141021, 20141022, 20141023, 20141024, 20141027, 20141028, 20141029, 20141030, 20141031, 20141103, 20141104, 20141105, 20141106, 20141107, 20141110, 20141111, 20141112, 20141113, 20141114, 20141117, 20141118, 20141119, 20141120, 20141121, 20141124, 20141125, 20141126, 20141128, 20141201, 20141202, 20141203, 20141204, 20141205, 20141208, 20141209, 20141210, 20141211, 20141212, 20141215, 20141216, 20141217, 20141218, 20141219, 20141222, 20141223, 20141224, 20141226, 20141229, 20141230, 20141231, 20150102, 20150105, 20150106, 20150107, 20150108, 20150109, 20150112, 20150113, 20150114, 20150115, 20150116, 20150120, 20150121, 20150122, 20150123, 20150126, 20150127, 20150128, 20150129, 20150130, 20150202, 20150203, 20150204, 20150205, 20150206, 20150209, 20150210, 20150211, 20150212, 20150213, 20150217, 20150218, 20150219, 20150220, 20150223, 20150224, 20150225, 20150226, 20150227, 20150302, 20150303, 20150304, 20150305, 20150306, 20150309, 20150310, 20150311, 20150312, 20150313, 20150316, 20150317, 20150318, 20150319, 20150320, 20150323, 20150324, 20150325, 20150326, 20150327, 20150330, 20150331, 20150401, 20150402, 20150406, 20150407, 20150408, 20150409, 20150410, 20150413, 20150414, 20150415, 20150416, 20150417, 20150420, 20150421, 20150422, 20150423, 20150424, 20150427, 20150428, 20150429, 20150430, 20150501, 20150504, 20150505, 20150506, 20150507, 20150508, 20150511, 20150512, 20150513, 20150514, 20150515, 20150518, 20150519, 20150520, 20150521, 20150522, 20150526, 20150527, 20150528, 20150529, 20150601, 20150602, 20150603, 20150604, 20150605, 20150608, 20150609, 20150610, 20150611, 20150612, 20150615, 20150616, 20150617, 20150618, 20150619, 20150622, 20150623, 20150624, 20150625, 20150626, 20150629, 20150630, 20150701, 20150702, 20150706, 20150707, 20150708, 20150709, 20150710, 20150713, 20150714, 20150715, 20150716, 20150717, 20150720, 20150721, 20150722, 20150723, 20150724, 20150727, 20150728, 20150729, 20150730, 20150731, 20150803, 20150804, 20150805, 20150806, 20150807, 20150810, 20150811, 20150812, 20150813, 20150814, 20150817, 20150818, 20150819, 20150820, 20150821, 20150824, 20150825, 20150826, 20150827, 20150828, 20150831, 20150901, 20150902, 20150903, 20150904, 20150908, 20150909, 20150910, 20150911, 20150914, 20150915, 20150916, 20150917, 20150918, 20150921, 20150922, 20150923, 20150924, 20150925, 20150928, 20150929, 20150930, 20151001, 20151002, 20151005, 20151006, 20151007, 20151008, 20151009, 20151012, 20151013, 20151014, 20151015, 20151016, 20151019, 20151020, 20151021, 20151022, 20151023, 20151026, 20151027, 20151028, 20151029, 20151030, 20151102, 20151103, 20151104, 20151105, 20151106, 20151109, 20151110, 20151111, 20151112, 20151113, 20151116, 20151117, 20151118, 20151119, 20151120, 20151123, 20151124, 20151125, 20151127, 20151130, 20151201, 20151202, 20151203, 20151204, 20151207, 20151208, 20151209, 20151210, 20151211, 20151214, 20151215, 20151216, 20151217, 20151218, 20151221, 20151222, 20151223, 20151224, 20151228, 20151229, 20151230, 20151231, 20160104, 20160105, 20160106, 20160107, 20160108, 20160111, 20160112, 20160113, 20160114, 20160115, 20160119, 20160120, 20160121, 20160122, 20160125, 20160126, 20160127, 20160128, 20160129, 20160201, 20160202, 20160203, 20160204, 20160205, 20160208, 20160209, 20160210, 20160211, 20160212, 20160216, 20160217, 20160218, 20160219, 20160222, 20160223, 20160224, 20160225, 20160226, 20160229, 20160301, 20160302, 20160303, 20160304, 20160307, 20160308, 20160309, 20160310, 20160311, 20160314, 20160315, 20160316, 20160317, 20160318, 20160321, 20160322, 20160323, 20160324, 20160328, 20160329, 20160330, 20160331, 20160401, 20160404, 20160405, 20160406, 20160407, 20160408, 20160411, 20160412, 20160413, 20160414, 20160415, 20160418, 20160419, 20160420, 20160421, 20160422, 20160425, 20160426, 20160427, 20160428, 20160429, 20160502, 20160503, 20160504, 20160505, 20160506, 20160509, 20160510, 20160511, 20160512, 20160513, 20160516, 20160517, 20160518, 20160519, 20160520, 20160523, 20160524, 20160525, 20160526, 20160527, 20160531, 20160601, 20160602, 20160603, 20160606, 20160607, 20160608, 20160609, 20160610, 20160613, 20160614, 20160615, 20160616, 20160617, 20160620, 20160621, 20160622, 20160623, 20160624, 20160627, 20160628, 20160629, 20160630, 20160701, 20160705, 20160706, 20160707, 20160708, 20160711, 20160712, 20160713, 20160714, 20160715, 20160718, 20160719, 20160720, 20160721, 20160722, 20160725, 20160726, 20160727, 20160728, 20160729, 20160801, 20160802, 20160803, 20160804, 20160805, 20160808, 20160809, 20160810, 20160811, 20160812, 20160815, 20160816, 20160817, 20160818, 20160819, 20160822, 20160823, 20160824, 20160825, 20160826, 20160829, 20160830, 20160831, 20160901, 20160902, 20160906, 20160907, 20160908, 20160909, 20160912, 20160913, 20160914, 20160915, 20160916, 20160919, 20160920, 20160921, 20160922, 20160923, 20160926, 20160927, 20160928, 20160929, 20160930, 20161003, 20161004, 20161005, 20161006, 20161007, 20161010, 20161011, 20161012, 20161013, 20161014, 20161017, 20161018, 20161019, 20161020, 20161021, 20161024, 20161025, 20161026, 20161027, 20161028, 20161031, 20161101, 20161102, 20161103, 20161104, 20161107, 20161108, 20161109, 20161110, 20161111, 20161114, 20161115, 20161116, 20161117, 20161118, 20161121, 20161122, 20161123, 20161125, 20161128, 20161129, 20161130, 20161201, 20161202, 20161205, 20161206, 20161207, 20161208, 20161209, 20161212, 20161213, 20161214, 20161215, 20161216, 20161219, 20161220, 20161221, 20161222, 20161223, 20161227, 20161228, 20161229, 20161230, 20170103, 20170104, 20170105, 20170106, 20170109, 20170110, 20170111, 20170112, 20170113, 20170117, 20170118, 20170119, 20170120, 20170123, 20170124, 20170125, 20170126, 20170127, 20170130, 20170131, 20170201, 20170202, 20170203, 20170206, 20170207, 20170208, 20170209, 20170210, 20170213, 20170214, 20170215, 20170216, 20170217, 20170221, 20170222, 20170223, 20170224, 20170227, 20170228, 20170301, 20170302, 20170303, 20170306, 20170307, 20170308, 20170309, 20170310, 20170313, 20170314, 20170315, 20170316, 20170317, 20170320, 20170321, 20170322, 20170323, 20170324, 20170327, 20170328, 20170329, 20170330, 20170331, 20170403, 20170404, 20170405, 20170406, 20170407, 20170410, 20170411, 20170412, 20170413, 20170417, 20170418, 20170419, 20170420, 20170421, 20170424, 20170425, 20170426, 20170427, 20170428, 20170501, 20170502, 20170503, 20170504, 20170505, 20170508, 20170509, 20170510, 20170511, 20170512, 20170515, 20170516, 20170517, 20170518, 20170519, 20170522, 20170523, 20170524, 20170525, 20170526, 20170530, 20170531, 20170601, 20170602, 20170605, 20170606, 20170607, 20170608, 20170609, 20170612, 20170613, 20170614, 20170615, 20170616, 20170619, 20170620, 20170621, 20170622, 20170623, 20170626, 20170627, 20170628, 20170629, 20170630, 20170703, 20170705, 20170706, 20170707, 20170710, 20170711, 20170712, 20170713, 20170714, 20170717, 20170718, 20170719, 20170720, 20170721, 20170724, 20170725, 20170726, 20170727, 20170728, 20170731, 20170801, 20170802, 20170803, 20170804, 20170807, 20170808, 20170809, 20170810, 20170811, 20170814, 20170815, 20170816, 20170817, 20170818, 20170821, 20170822, 20170823, 20170824, 20170825, 20170828, 20170829, 20170830, 20170831, 20170901, 20170905, 20170906, 20170907, 20170908, 20170911, 20170912, 20170913, 20170914, 20170915, 20170918, 20170919, 20170920, 20170921, 20170922, 20170925, 20170926, 20170927, 20170928, 20170929, 20171002, 20171003, 20171004, 20171005, 20171006, 20171009, 20171010, 20171011, 20171012, 20171013, 20171016, 20171017, 20171018, 20171019, 20171020, 20171023, 20171024, 20171025, 20171026, 20171027, 20171030, 20171031, 20171101, 20171102, 20171103, 20171106, 20171107, 20171108, 20171109, 20171110, 20171113, 20171114, 20171115, 20171116, 20171117, 20171120, 20171121, 20171122, 20171124, 20171127, 20171128, 20171129, 20171130, 20171201, 20171204, 20171205, 20171206, 20171207, 20171208, 20171211, 20171212, 20171213, 20171214, 20171215, 20171218, 20171219, 20171220, 20171221, 20171222, 20171226, 20171227, 20171228, 20171229, 20180102, 20180103, 20180104, 20180105, 20180108, 20180109, 20180110, 20180111, 20180112, 20180116, 20180117, 20180118, 20180119, 20180122, 20180123, 20180124, 20180125, 20180126, 20180129, 20180130, 20180131, 20180201, 20180202, 20180205, 20180206, 20180207, 20180208, 20180209, 20180212, 20180213, 20180214, 20180215, 20180216, 20180220, 20180221, 20180222, 20180223, 20180226, 20180227, 20180228, 20180301, 20180302, 20180305, 20180306, 20180307, 20180308, 20180309, 20180312, 20180313, 20180314, 20180315, 20180316, 20180319, 20180320, 20180321, 20180322, 20180323, 20180326, 20180327, 20180328, 20180329, 20180402, 20180403, 20180404, 20180405, 20180406, 20180409, 20180410, 20180411, 20180412, 20180413, 20180416, 20180417, 20180418, 20180419, 20180420, 20180423, 20180424, 20180425, 20180426, 20180427, 20180430, 20180501, 20180502, 20180503, 20180504, 20180507, 20180508, 20180509, 20180510, 20180511, 20180514, 20180515, 20180516, 20180517, 20180518, 20180521, 20180522, 20180523, 20180524, 20180525, 20180529, 20180530, 20180531, 20180601, 20180604, 20180605, 20180606, 20180607, 20180608, 20180611, 20180612, 20180613, 20180614, 20180615, 20180618, 20180619, 20180620, 20180621, 20180622, 20180625, 20180626, 20180627, 20180628, 20180629, 20180702, 20180703, 20180705, 20180706, 20180709, 20180710, 20180711, 20180712, 20180713, 20180716, 20180717, 20180718, 20180719, 20180720, 20180723, 20180724, 20180725, 20180726, 20180727, 20180730, 20180731, 20180801, 20180802, 20180803, 20180806, 20180807, 20180808, 20180809, 20180810, 20180813, 20180814, 20180815, 20180816, 20180817, 20180820, 20180821, 20180822, 20180823, 20180824, 20180827, 20180828, 20180829, 20180830, 20180831, 20180904, 20180905, 20180906, 20180907, 20180910, 20180911, 20180912, 20180913, 20180914, 20180917, 20180918, 20180919, 20180920, 20180921, 20180924, 20180925, 20180926, 20180927, 20180928, 20181001, 20181002, 20181003, 20181004, 20181005, 20181008, 20181009, 20181010, 20181011, 20181012, 20181015, 20181016, 20181017, 20181018, 20181019, 20181022, 20181023, 20181024, 20181025, 20181026, 20181029, 20181030, 20181031, 20181101, 20181102, 20181105, 20181106, 20181107, 20181108, 20181109, 20181112, 20181113, 20181114, 20181115, 20181116, 20181119, 20181120, 20181121, 20181123, 20181126, 20181127, 20181128, 20181129, 20181130, 20181203, 20181204, 20181206, 20181207, 20181210, 20181211, 20181212, 20181213, 20181214, 20181217, 20181218, 20181219, 20181220, 20181221, 20181224, 20181226, 20181227, 20181228, 20181231, 20190102, 20190103, 20190104, 20190107, 20190108, 20190109, 20190110, 20190111, 20190114, 20190115, 20190116, 20190117, 20190118, 20190122, 20190123, 20190124, 20190125, 20190128, 20190129, 20190130, 20190131, 20190201, 20190204, 20190205, 20190206, 20190207, 20190208, 20190211, 20190212, 20190213, 20190214, 20190215, 20190219, 20190220, 20190221, 20190222, 20190225, 20190226, 20190227, 20190228, 20190301, 20190304, 20190305, 20190306, 20190307, 20190308, 20190311, 20190312, 20190313, 20190314, 20190315, 20190318, 20190319, 20190320, 20190321, 20190322, 20190325, 20190326, 20190327, 20190328, 20190329, 20190401, 20190402, 20190403, 20190404, 20190405, 20190408, 20190409, 20190410, 20190411, 20190412, 20190415, 20190416, 20190417, 20190418, 20190422, 20190423, 20190424, 20190425, 20190426, 20190429, 20190430, 20190501, 20190502, 20190503, 20190506, 20190507, 20190508, 20190509, 20190510, 20190513, 20190514, 20190515, 20190516, 20190517, 20190520, 20190521, 20190522, 20190523, 20190524, 20190528, 20190529, 20190530, 20190531, 20190603, 20190604, 20190605, 20190606, 20190607, 20190610, 20190611, 20190612, 20190613, 20190614, 20190617, 20190618, 20190619, 20190620, 20190621, 20190624, 20190625, 20190626, 20190627, 20190628, 20190701, 20190702, 20190703, 20190705, 20190708, 20190709, 20190710, 20190711, 20190712, 20190715, 20190716, 20190717, 20190718, 20190719, 20190722, 20190723, 20190724, 20190725, 20190726, 20190729, 20190730, 20190731, 20190801, 20190802, 20190805, 20190806, 20190807, 20190808, 20190809, 20190812, 20190813, 20190814, 20190815, 20190816, 20190819, 20190820, 20190821, 20190822, 20190823, 20190826, 20190827, 20190828, 20190829, 20190830, 20190903, 20190904, 20190905, 20190906, 20190909, 20190910, 20190911, 20190912, 20190913, 20190916, 20190917, 20190918, 20190919, 20190920, 20190923, 20190924, 20190925, 20190926, 20190927, 20190930, 20191001, 20191002, 20191003, 20191004, 20191007, 20191008, 20191009, 20191010, 20191011, 20191014, 20191015, 20191016, 20191017, 20191018, 20191021, 20191022, 20191023, 20191024, 20191025, 20191028, 20191029, 20191030, 20191031, 20191101, 20191104, 20191105, 20191106, 20191107, 20191108, 20191111, 20191112, 20191113, 20191114, 20191115, 20191118, 20191119, 20191120], \"y\": [341.772301, 342.044801, 342.335301, 342.5977009999999, 342.85700099999997, 343.03980100000007, 343.130901, 343.23950099999996, 343.40130099999993, 343.5598010000001, 343.65660100000014, 343.74760100000015, 343.8345010000001, 343.9756010000001, 344.02830100000006, 344.089001, 344.22580100000005, 344.344301, 344.442401, 344.452501, 344.417051, 344.29195100000004, 344.1584509999999, 343.9888509999999, 343.834551, 343.7120509999999, 343.707151, 343.707851, 343.5856510000001, 343.37475100000006, 343.089251, 342.90945100000005, 342.774951, 342.69435100000004, 342.50675100000007, 342.3843510000001, 342.30285100000003, 342.17825100000005, 342.01555099999996, 341.896351, 341.8082509999999, 341.781251, 341.78475099999997, 341.7901509999999, 341.731651, 341.6395509999999, 341.63805099999996, 341.71155100000004, 341.974351, 342.28635099999997, 342.6291509999999, 343.01625099999995, 343.354151, 343.792151, 344.255061, 344.692061, 345.0178609999999, 345.3829609999999, 345.73318100000006, 346.17608100000007, 346.59818099999995, 347.00380099999995, 347.24170099999986, 347.524901, 347.758101, 348.15740099999994, 348.48080099999993, 348.82560099999995, 349.1829009999999, 349.624601, 349.978301, 350.2645009999999, 350.46980099999996, 350.54620100000005, 350.796341, 351.11234099999996, 351.43744100000004, 351.887741, 352.40628000000004, 352.40628000000004, 352.9250799999999, 353.42258000000004, 353.92418, 354.3406800000001, 354.774799, 355.293299, 355.86039900000003, 356.312499, 356.73319899999996, 357.15439900000007, 357.540099, 357.93969899999996, 358.43749899999995, 359.051499, 359.71499900000003, 360.34609900000004, 360.90201899999994, 361.46331899999996, 361.996119, 362.51961900000003, 363.0233189999999, 363.4501189999999, 363.7676189999999, 364.038619, 364.298519, 364.679919, 365.119819, 365.45681900000005, 365.9471190000001, 366.544319, 367.213919, 367.95411899999993, 368.850619, 369.68281899999994, 370.551719, 371.16611900000004, 372.119218, 372.71681799999993, 373.262318, 373.884968, 374.61066800000003, 375.33796800000005, 375.33796800000005, 376.070968, 376.78166799999997, 377.48876800000005, 378.19596800000016, 378.930468, 379.6765679999999, 380.53356799999995, 381.3334679999999, 381.9328679999999, 382.55356800000004, 383.033768, 383.6584680000001, 384.186568, 384.60806800000006, 385.017168, 385.351268, 385.6824679999999, 385.85936799999996, 385.9899680000001, 386.17806799999994, 386.31286800000004, 386.591468, 386.931168, 387.24676800000003, 387.59773799999994, 387.85813799999994, 388.01133799999997, 388.150138, 388.2166380000001, 388.26243800000003, 388.2173380000001, 388.0335280000001, 387.78972799999997, 387.67082800000003, 387.57042800000005, 387.621508, 387.71700799999996, 387.82510800000006, 388.117308, 388.40750799999995, 388.92500799999993, 389.23070799999994, 389.23070799999994, 389.711708, 390.1093080000001, 390.520308, 390.91180799999995, 391.32510800000006, 391.75530800000007, 392.322308, 392.974728, 393.610178, 394.121278, 394.650678, 395.18987799999996, 395.55527800000004, 395.929939, 396.235369, 396.853069, 397.48716900000005, 398.22136900000004, 398.95646899999997, 399.68206900000007, 400.402069, 401.18196900000004, 401.978069, 402.7718689999999, 403.5671689999999, 404.4032689999999, 405.332469, 406.1529690000001, 407.04416899999995, 408.016319, 408.97381899999993, 409.95751900000005, 410.94651899999997, 412.099519, 413.26071900000005, 414.52192899999994, 415.9335290000001, 417.4481290000001, 419.07542899999993, 420.7130289999999, 422.3826289999999, 424.1350289999999, 425.58052899999996, 426.88032899999996, 428.1655289999999, 429.49882899999994, 430.7290289999999, 432.05132899999995, 433.513329, 435.42102900000003, 437.32053, 439.24583, 441.20103, 443.28203, 445.30383, 447.2530300000001, 449.16463, 451.18543, 453.36473, 455.56783000000013, 457.6359299999999, 459.62823, 461.81733, 464.05034999999987, 466.33814999999987, 468.8227499999999, 471.33844999999997, 473.83335000000005, 476.20625, 478.5828500000001, 480.86064999999996, 482.91765, 485.3205499999999, 487.63775000000004, 489.8472499999999, 491.9222999999999, 493.8820999999999, 495.75399999999996, 498.0327999999999, 502.36479999999995, 504.27523, 506.18683000000004, 508.15543, 510.06555000000003, 511.78184999999996, 511.7818500000001, 513.55965, 515.35245, 517.24465, 519.1606500000001, 521.0208500000001, 522.78305, 524.35295, 525.84925, 527.16575, 528.43445, 529.9817499999999, 531.52315, 533.1752499999999, 534.7778500000001, 536.28975, 537.87795, 539.4884500000001, 541.0394500000001, 542.43275, 543.8433500000001, 545.24465, 546.74873, 548.26658, 549.82228, 551.2463799999999, 552.7315800000001, 554.25048, 555.69198, 557.22825, 558.62055, 560.04835, 561.4302100000001, 562.67611, 563.93131, 565.0769100000001, 566.24701, 567.3959199999999, 569.59682, 570.75722, 571.82012, 572.9826519999999, 574.019051, 575.0638009999999, 575.063801, 576.169601, 577.188901, 578.1563729999999, 579.058773, 579.997873, 580.903863, 581.741863, 582.628063, 583.312963, 583.927563, 584.497663, 584.796863, 585.209863, 585.7601629999999, 586.4034629999999, 587.0935629999999, 587.711763, 588.269263, 588.744963, 589.073993, 589.4296929999999, 589.7724929999999, 589.967593, 590.1243929999999, 590.399393, 590.721693, 591.0659929999999, 591.3601930000001, 591.696092, 592.169893, 592.633293, 593.3273929999999, 593.764193, 594.1028729999999, 594.616673, 595.399673, 595.7512730000001, 596.1423730000001, 596.663273, 597.3128730000001, 598.2697730000001, 598.9806729999999, 598.9806729999999, 599.523473, 600.249973, 601.224523, 602.334223, 603.6427229999999, 604.542323, 605.484633, 606.4763630000001, 607.6228090000001, 608.808209, 609.8466089999998, 610.7661889999999, 611.763689, 612.8814890000001, 613.8715890000001, 614.776489, 615.6843889999999, 616.735289, 617.8232889999999, 618.8181890000001, 619.739189, 620.7996890000001, 621.9059890000001, 622.573989, 623.3004889999999, 623.944889, 624.788489, 625.610889, 626.214289, 626.5220290000001, 627.088129, 627.6117290000001, 628.133729, 628.603429, 628.9258289999999, 629.1608289999999, 629.3233290000001, 629.377729, 629.462429, 629.5694289999999, 629.4350289999999, 629.0720289999999, 628.6869290000001, 628.2435290000001, 627.813169, 627.408569, 626.8470689999999, 626.4162689999999, 626.354969, 626.221859, 626.151359, 626.030859, 625.9964590000001, 625.8490589999999, 625.577628, 625.408929, 625.1198290000001, 624.899229, 624.612729, 624.012257, 623.4366570000001, 622.697357, 621.9257570000001, 621.274257, 620.519257, 619.777256, 618.8288560000001, 618.0076560000001, 617.596456, 617.110956, 616.477206, 615.720906, 614.8133059999999, 613.873906, 612.945946, 611.883746, 610.988716, 610.266116, 609.4914160000001, 607.5775170000001, 606.5294170000001, 605.384117, 604.312617, 603.1508170000001, 601.6882180000001, 601.688218, 599.8964169999999, 598.395217, 596.7334169999999, 595.109417, 593.524717, 591.910017, 589.667517, 587.331417, 585.1896170000001, 583.121717, 580.944417, 578.796717, 576.570917, 574.189217, 572.142217, 570.1090670000001, 568.092967, 566.0129670000001, 563.9000570000001, 561.580477, 559.230967, 556.875237, 554.490791, 552.0833909999999, 549.6741909999998, 547.3967909999999, 545.252791, 542.867891, 540.686491, 538.5347909999999, 536.3362910000001, 533.9225610000001, 531.453761, 529.2405610000001, 527.115361, 525.0576609999999, 522.9643609999999, 519.050311, 516.9857109999999, 514.81341, 512.80661, 511.03781, 509.48487, 509.48487, 507.6635699999999, 506.05717000000004, 504.51176999999996, 503.05056999999994, 501.62216999999987, 500.19047, 498.65067000000005, 497.17037000000005, 495.62307, 494.11307, 492.80857000000003, 491.66227000000003, 490.45137, 489.2911700000001, 488.22267, 487.19496999999996, 486.23417000000006, 485.15607000000006, 483.75767, 482.17427, 480.47616999999997, 478.66336999999993, 476.75747, 474.97042, 473.198119, 471.391619, 469.71311899999995, 468.15331900000007, 466.8235189999999, 465.82981899999993, 464.813069, 463.982149, 463.29230900000005, 462.469109, 461.717009, 460.42731, 459.78310999999997, 458.88140999999996, 457.91001000000006, 457.0371600000001, 456.17636000000005, 455.40436, 455.4043600000001, 454.67196, 453.93622000000005, 453.26381999999995, 452.38552, 451.31287, 450.3388700000001, 449.58466999999996, 448.839969, 448.09546900000004, 447.417569, 446.6342690000001, 445.817669, 445.21816899999993, 444.74726899999996, 444.063269, 443.3572689999999, 442.717669, 441.969899, 441.15079900000006, 440.9632490000001, 440.79479900000007, 440.46199900000005, 440.014499, 439.46679900000004, 438.938799, 438.38119900000015, 437.896949, 437.28034900000006, 436.79969900000003, 436.302299, 435.76489900000007, 435.13960900000006, 434.61218899999994, 434.16338899999994, 433.70467900000006, 433.3756789999999, 433.04107899999985, 432.82627899999994, 432.669579, 432.46627900000004, 432.353879, 432.114079, 431.930579, 431.70617900000013, 431.8084090000001, 431.99330900000007, 432.090609, 432.31200900000005, 432.54120900000004, 432.75090899999987, 432.940859, 433.281959, 433.693059, 434.02146, 434.23235999999997, 434.28526, 434.28456, 434.43865999999997, 434.80816, 435.17445999999995, 435.5181099999999, 435.92960999999997, 436.4876100000001, 437.07310999999993, 437.8082099999999, 438.53891000000004, 439.23121, 439.98431, 440.63401, 441.27890999999994, 441.92531, 442.44041000000004, 442.98240999999996, 443.6712100000001, 444.42521, 445.14671000000004, 446.18061, 447.2073599999999, 447.97896000000003, 449.30800999999997, 449.75341000000003, 450.21820999999994, 450.69341000000003, 451.11531, 451.35711000000003, 451.3571100000001, 451.87131, 452.3086600000001, 452.6247800000001, 452.87932000000006, 453.11962, 453.25002, 453.56246999999996, 453.92927000000003, 454.21577, 454.60902000000004, 455.19682000000006, 455.6607200000001, 456.1913200000001, 456.66212, 457.19491999999997, 457.74192000000016, 458.30882, 458.86812, 459.5002700000001, 460.1387699999999, 460.83672000000007, 461.5404199999999, 462.28252, 463.10817, 463.91759499999995, 464.827795, 465.57844500000004, 466.44004500000005, 467.286945, 468.16534499999995, 469.072345, 470.02882, 470.91662, 471.72417, 472.70002, 473.72776999999996, 474.79207, 477.23397, 478.5033700000001, 479.7519200000001, 480.98532, 482.0432200000001, 483.06832, 483.06832, 484.05292000000003, 485.11622000000006, 486.30122000000006, 487.53980999999993, 488.89932, 490.13882000000007, 491.53762, 492.91182000000003, 494.28992, 495.58622, 496.93532, 498.34212, 499.6913200000001, 501.10822, 502.24661999999995, 503.43607, 504.57592, 505.60551999999996, 506.51681999999994, 507.47882000000004, 508.6136200000001, 509.6725200000001, 510.6168200000001, 511.56552000000005, 512.4611199999999, 513.4626200000001, 514.4507200000002, 515.18672, 515.7289099999999, 516.1434099999999, 516.59816, 516.94036, 517.1908599999999, 517.5359599999999, 517.97566, 519.05106, 519.42586, 520.0341599999999, 520.6422600000001, 521.2878599999999, 521.87706, 522.49686, 522.49686, 522.57866, 522.63146, 522.6418100000001, 522.58581, 522.6563600000001, 523.06596, 523.4651600000001, 523.94076, 524.6371600000001, 525.37521, 526.0883100000001, 526.72391, 527.4983100000001, 528.0286100000001, 528.59421, 529.1527100000001, 529.60151, 530.02671, 530.53461, 530.87536, 531.15311, 531.59571, 532.0286600000001, 532.42876, 532.9310599999999, 533.39156, 533.80096, 534.17586, 534.52426, 534.896559, 535.251759, 535.512059, 535.6703590000001, 535.725809, 535.8403089999999, 535.9034090000001, 535.872159, 535.927534, 536.024234, 536.3040840000001, 536.453084, 536.603634, 536.772834, 536.8731340000002, 537.034729, 537.252129, 537.5153789999999, 537.627779, 537.6710090000001, 537.704209, 537.800059, 537.752959, 537.6997590000001, 537.7301990000001, 537.7141990000001, 537.7538490000001, 537.7935489999999, 537.908249, 537.987649, 537.899049, 538.116959, 538.269959, 538.6976639999999, 538.958264, 539.2100640000001, 539.442664, 539.7691639999999, 540.1124639999999, 540.400364, 540.712364, 540.986864, 541.297364, 541.6506139999999, 542.037864, 542.468964, 542.9126640000001, 543.400114, 543.741814, 544.102914, 544.999814, 545.596214, 546.241714, 546.949414, 547.891814, 548.7823239999999, 548.782324, 549.667024, 550.608724, 551.691924, 552.8362239999999, 553.9359239999999, 549.409024, 544.777924, 540.174424, 535.6930239999999, 531.115624, 526.5222239999999, 521.881224, 517.3408240000001, 512.753724, 508.594424, 504.497164, 500.4073139999999, 496.305014, 492.197914, 488.029614, 483.8331139999999, 479.643714, 475.38091399999996, 471.03136399999994, 466.63146400000005, 462.226164, 457.735864, 453.247364, 448.74096399999996, 444.331564, 439.973664, 435.669064, 431.32446400000003, 427.04806400000007, 422.81411400000013, 418.4846140000001, 414.193515, 405.55051499999996, 401.2169149999999, 396.892615, 392.569615, 388.21621500000003, 383.81648100000007, 383.81648100000007, 379.406381, 375.050881, 370.75388100000004, 366.4293810000001, 362.0634810000001, 357.710681, 353.38288100000005, 349.0388810000001, 344.62238099999996, 340.152181, 335.74578099999997, 331.37703099999993, 327.014131, 322.6527310000001, 318.248531, 313.837831, 309.458481, 305.16138099999995, 300.948401, 296.739101, 292.469051, 288.223651, 284.008551, 279.781111, 275.58551100000005, 271.37576, 267.135661, 262.838261, 258.538061, 254.306762, 249.63746200000003, 244.93346200000005, 240.01060100000004, 235.09590100000003, 230.20620100000005, 220.41070100000002, 215.380101, 210.44310099999998, 205.520801, 200.64710100000005, 195.783601, 190.854801, 190.854801, 185.913901, 180.97155100000003, 176.070851, 171.10394100000002, 166.070141, 161.031341, 155.966841, 150.88144100000002, 145.715641, 140.421141, 135.15774100000002, 129.802641, 124.49734099999998, 119.24234099999998, 113.91524100000001, 108.51924099999998, 103.09674100000001, 97.70814100000001, 97.844541, 97.971841, 98.112841, 98.283941, 98.45734100000001, 98.624241, 98.79034100000001, 98.958741, 99.128541, 99.31654, 99.52090000000003, 99.7462, 99.98450000000001, 100.21529999999997, 100.45049999999999, 100.6675, 100.8953, 101.12519999999999, 101.37114999999999, 101.58745, 101.82405, 102.05825000000003, 102.25865, 102.45265, 102.64745, 102.84925000000001, 103.05135, 103.24465000000001, 103.44165000000001, 103.62185000000001, 103.79115, 103.91644900000001, 104.02854899999998, 104.11914899999998, 104.22314899999998, 104.365749, 104.502049, 104.67564899999999, 104.83968400000002, 105.003985, 105.192385, 105.38198499999999, 105.56298499999998, 105.719585, 105.85278499999998, 105.95558499999999, 106.04558499999999, 106.148085, 106.286985, 106.415585, 106.502685, 106.59923500000002, 106.69123499999999, 106.74633500000002, 106.790735, 106.86893500000001, 106.94323499999999, 107.04463499999999, 107.149535, 107.24743500000002, 107.34953500000002, 107.721535, 107.90973500000001, 108.115936, 108.29213500000002, 108.473435, 108.656135, 108.65613499999999, 108.82913400000001, 109.01793399999998, 109.22233399999999, 109.45318999999999, 109.70778999999999, 109.96789000000001, 110.21969000000001, 110.48939, 110.79529000000001, 111.08299000000001, 111.41164, 111.72594000000001, 112.02184000000001, 112.32704, 112.61564, 112.91018999999999, 113.21649, 113.49434999999998, 113.74815000000001, 114.00675000000001, 114.27955, 114.53685, 114.78375, 115.06554999999999, 115.32484999999998, 115.57654999999998, 115.82235, 116.07704999999999, 116.30364999999998, 116.51095, 116.73204999999999, 116.93185, 117.09234999999998, 117.26504999999997, 117.41775, 117.58725, 117.74575, 118.06565, 118.24905000000001, 118.42089999999999, 118.57990099999999, 118.73300099999999, 118.87580100000001, 118.875801, 119.00240099999999, 119.12540099999997, 119.23840099999998, 119.353601, 119.43820099999999, 119.54950100000002, 119.632151, 119.74245099999999, 119.84935100000001, 119.96405100000001, 120.14025100000005, 120.299651, 120.42675099999998, 120.524051, 120.66365099999999, 120.826551, 120.943351, 121.07460099999997, 121.21080099999998, 121.38960099999998, 121.57050099999998, 121.76170100000002, 121.92770100000001, 122.09070100000001, 122.26070100000001, 122.43300099999999, 122.608301, 122.7888, 122.963, 123.14949999999999, 123.31999999999998, 123.53620000000001, 123.7609, 124.0016, 124.2446, 124.64959999999999, 124.82419999999998, 125.01810000000002, 125.19345000000001, 125.36965000000001, 125.59005, 125.81625, 125.81625, 126.00074999999998, 126.17455, 126.32655000000001, 126.46955, 126.61705, 126.78975, 126.91265000000003, 126.99345000000001, 127.10345000000001, 127.19234999999999, 127.27414999999999, 127.32445, 127.37925, 127.45594999999999, 127.52275, 127.56245000000001, 127.57065, 127.53224999999999, 127.46175, 127.41615, 127.38545, 127.35695000000001, 127.33025000000002, 127.28550000000001, 127.25990000000002, 127.29279999999999, 127.29539999999997, 127.26289999999999, 127.2237, 127.1749, 127.11829999999998, 127.08819999999997, 127.05210000000001, 127.00470000000001, 126.97409999999999, 126.93610000000001, 126.838, 126.75599999999999, 126.65799999999999, 126.5429, 126.45539999999997, 126.31599999999999, 126.20909999999999, 126.0887, 125.98139999999998, 125.91919999999999, 125.8417, 125.75950000000003, 125.62270000000001, 125.43590000000002, 125.22489900000001, 125.00929900000001, 124.832899, 124.702049, 124.578849, 124.44034899999998, 124.24664899999998, 124.10144899999999, 123.94184900000002, 123.765749, 123.62704899999999, 123.480905, 123.33090499999999, 123.204005, 123.07108400000001, 122.93698400000001, 122.79838399999998, 122.61078399999998, 122.440083, 122.305883, 122.18738300000003, 122.041283, 121.90428299999998, 121.793683, 121.667433, 121.50553299999997, 121.332433, 121.16493300000002, 121.00913300000003, 120.679333, 120.49943300000001, 120.29273300000001, 120.113233, 119.92833399999999, 119.732234, 119.73223399999998, 119.50863399999999, 119.330834, 119.12093399999999, 118.92053399999999, 118.75533399999999, 118.58763399999998, 118.44293399999998, 118.33223399999999, 118.19033400000001, 118.04963400000001, 117.965034, 117.896234, 117.80258399999998, 117.728484, 117.68278400000001, 117.63358400000003, 117.56678400000001, 117.50448399999998, 117.43188400000001, 117.33548400000001, 117.22048400000001, 117.107684, 116.949384, 116.816084, 116.684984, 116.611998, 116.54559800000001, 116.47279799999998, 116.38629800000001, 116.315498, 116.23879800000003, 116.190398, 116.173798, 116.11409800000003, 116.020598, 115.916498, 115.838198, 115.62209800000001, 115.457698, 115.31279799999999, 115.19229800000001, 115.06539799999999, 114.92529799999998, 114.92529799999998, 114.80969799999998, 114.66579700000003, 114.494897, 114.34439699999999, 114.20189699999999, 114.10349699999999, 114.03749700000002, 113.95179700000003, 113.88779699999999, 113.80559699999998, 113.66069599999999, 113.57849600000002, 113.45299599999998, 113.30829599999998, 113.114096, 112.91219600000001, 112.73249600000001, 112.58189599999997, 112.429496, 112.366896, 112.306497, 112.23579700000002, 112.106897, 111.94029699999999, 111.82169699999999, 111.68925700000001, 111.611757, 111.422757, 111.26015699999999, 111.13985699999998, 110.98055699999999, 110.82390100000002, 110.66140100000001, 110.48540100000002, 110.27212200000002, 109.84552200000002, 109.648922, 109.450623, 109.238323, 109.07082300000002, 108.908523, 108.72112299999999, 108.72112299999999, 108.53392300000002, 108.378023, 108.23432300000002, 108.092423, 107.96432300000001, 107.83062299999997, 107.690724, 107.58272400000001, 107.48232399999999, 107.402324, 107.31122399999998, 107.214123, 107.10662300000001, 107.01562300000002, 106.908823, 106.82112299999999, 106.729023, 106.636823, 106.558923, 106.46202299999999, 106.33022299999999, 106.236623, 106.15822299999998, 106.026623, 105.877923, 105.734773, 105.59967300000002, 105.46967299999999, 105.33917299999997, 105.22967299999999, 105.130372, 105.02260200000002, 104.96431199999999, 104.888612, 104.81781199999999, 104.78481200000002, 104.74741200000003, 104.73111200000002, 104.679298, 104.589638, 104.47063800000002, 104.362638, 104.244838, 104.12433800000001, 104.00363800000001, 103.870738, 103.740838, 103.55593800000001, 103.352138, 103.09953800000001, 102.85323799999998, 102.622638, 102.408238, 102.178838, 101.97433799999999, 101.77753800000002, 101.60663800000002, 101.41833799999999, 101.231739, 101.07803899999999, 100.943539, 100.806239, 100.66583899999999, 100.52753899999999, 100.41153899999999, 100.288339, 100.19413899999999, 100.13784, 100.08874, 100.06504000000001, 100.05664000000003, 100.07574000000002, 100.08294, 100.07673999999999, 100.06343999999997, 100.07974, 100.07404000000002, 100.09974, 100.12134, 100.13864, 100.09584, 100.07687999999999, 100.03028, 100.04697999999999, 100.06528, 100.06527999999999, 100.04947999999996, 100.04657999999998, 100.03548, 99.99248000000001, 99.96238000000001, 99.96628000000001, 99.97228000000001, 99.98108, 99.98858, 100.00728000000001, 100.02678000000002, 100.02738000000001, 100.01608000000002, 100.02768000000002, 100.03608000000003, 100.05528000000004, 100.09598000000001, 100.13328000000001, 100.16438000000002, 100.19458000000003, 100.222279, 100.20357899999999, 100.169479, 100.121202, 100.12080200000003, 100.14540199999999, 100.17695199999999, 100.226452, 100.25945199999998, 100.294952, 100.32865199999998, 100.35755199999996, 100.381552, 100.41175199999999, 100.432652, 100.45285199999998, 100.46705199999998, 100.53795199999999, 100.57805199999999, 100.592063, 100.589763, 100.585163, 100.573863, 100.573863, 100.542964, 100.52083400000001, 100.48072400000001, 100.46352399999999, 100.436824, 100.40762400000001, 100.370524, 100.327424, 100.28342400000001, 100.268984, 100.250084, 100.21198399999997, 100.195084, 100.21558400000002, 100.27648400000002, 100.381384, 100.48748399999998, 100.64468400000001, 100.83218400000001, 101.03008400000003, 101.239784, 101.41503300000001, 101.601833, 101.80053300000002, 102.01283300000001, 102.206733, 102.40303300000004, 102.60303300000004, 102.82983300000002, 103.05503300000002, 103.25543300000001, 103.46093300000001, 103.675733, 103.897133, 104.11843300000001, 104.52143299999997, 104.70093299999999, 104.87123299999999, 105.03903300000003, 105.21053300000001, 105.39133300000003, 105.59043299999999, 105.59043300000002, 105.793933, 105.96353300000001, 106.118133, 106.26593299999999, 106.40493299999999, 106.53143299999998, 106.67403299999997, 106.79763299999999, 106.91463300000001, 107.043033, 107.20043299999999, 107.35823299999998, 107.477033, 107.60563300000001, 107.70173300000002, 107.83913300000003, 108.01883300000001, 108.18233300000001, 108.338933, 108.50013299999999, 108.65883300000002, 108.82088300000001, 108.98348300000002, 109.139883, 109.287683, 109.423083, 109.54358300000001, 109.673783, 109.77688300000001, 109.888683, 110.00048300000002, 110.122883, 110.26318300000001, 110.401883, 110.56718300000003, 110.745983, 110.93745999999999, 111.06748499999998, 111.19048499999997, 111.31803499999998, 111.42803499999998, 111.546235, 111.65333500000001, 111.76693499999999, 111.85973500000001, 111.943335, 112.01353499999999, 112.094935, 112.17583499999999, 112.260335, 112.34433500000002, 112.440535, 112.53963500000002, 112.646124, 112.74502400000002, 112.85032400000001, 112.96192400000001, 113.08122399999998, 113.203024, 113.333524, 113.466224, 113.60602399999998, 113.76392399999999, 113.91602400000002, 114.05592400000002, 114.194524, 114.32382399999999, 114.556224, 114.80992400000001, 115.046224, 115.26892400000001, 115.466324, 115.631024, 115.806024, 115.991924, 116.18912400000004, 116.40392400000002, 116.612924, 116.839325, 117.30352500000001, 117.53502499999999, 117.778525, 118.01462500000001, 118.258925, 118.498625, 118.49862499999999, 118.76612500000003, 119.01632500000001, 119.273725, 119.506425, 119.73792499999999, 119.95452499999998, 120.17142499999999, 120.38662500000001, 120.60302500000002, 120.818325, 121.05162500000002, 121.28842500000002, 121.52222500000002, 121.761025, 121.97692500000001, 122.23532500000002, 122.49952499999998, 122.76862499999999, 123.041825, 123.365025, 123.690625, 124.031625, 124.37992499999999, 124.712825, 125.049825, 125.38122500000001, 125.74002500000002, 126.08922500000003, 126.46382500000001, 126.80892500000002, 127.12672500000001, 127.438025, 127.75572500000003, 128.050625, 128.33952499999998, 128.651675, 128.956475, 129.60767500000003, 129.939075, 130.282275, 130.619775, 130.994375, 131.369675, 131.369675, 131.73017499999997, 132.074475, 132.424475, 132.821675, 133.20957499999997, 133.590175, 133.971375, 134.37265000000002, 134.76315, 135.14835000000002, 135.47975, 135.84195000000003, 136.20735, 136.57444999999996, 136.94494999999998, 137.31064999999998, 137.69134999999997, 138.06585, 138.44234999999998, 138.80395000000001, 139.15665, 139.52115, 139.86935, 140.21655, 140.57775, 140.93665000000001, 141.22645, 141.48075, 141.74895, 142.00065, 142.24275, 142.46554999999998, 142.70995000000002, 142.94065, 143.18215, 143.67275, 143.84325, 143.99595000000002, 144.16355000000001, 144.29735000000002, 144.42255, 144.53780000000003, 144.5378, 144.65460000000002, 144.76059999999998, 144.86939999999998, 144.97019999999998, 145.07059999999998, 145.17485, 145.29555000000005, 145.41905000000003, 145.54405, 145.67955, 145.82335000000003, 145.95745, 146.09055, 146.21375, 146.35195000000002, 146.48925, 146.60154999999997, 146.70155, 146.79915, 146.91285, 147.09285, 147.25605, 147.42965, 147.61305000000002, 147.80715, 148.01905, 148.15735, 148.33374999999998, 148.51725, 148.72445, 148.92735, 149.09715, 149.23415, 149.36415, 149.52275, 149.68595000000002, 149.84205, 149.99305, 150.16755, 150.36005, 150.55995000000001, 150.76805000000002, 150.99225, 151.19515, 151.40365, 151.59805, 151.77215, 151.98045, 152.16455, 152.33805, 152.48445, 152.63795, 152.78775, 152.93704999999997, 153.06114999999997, 153.12965, 153.17255, 153.20755, 153.27434999999997, 153.32675, 153.32925, 153.32945, 153.33495000000002, 153.34025, 153.31394999999998, 153.31115, 153.30935000000002, 153.36554999999998, 153.39944999999997, 153.43435, 153.45485, 153.48695, 153.55265, 153.61825000000002, 153.68005000000002, 153.70315, 153.73795, 153.76815000000002, 153.78455000000002, 153.83905000000001, 153.91565, 154.03364999999997, 154.23445, 154.44915, 154.66395, 154.66395, 154.93714999999997, 155.23735, 155.56275, 155.86175, 156.17035, 156.45825, 156.74175, 156.99225, 157.22515, 157.49835, 157.74115, 158.00435000000002, 158.29534999999998, 158.60909999999998, 158.9167, 159.2303, 159.5186, 159.7626, 160.02460000000002, 160.2769, 160.49670000000003, 160.7028, 160.8973, 161.0898, 161.2732, 161.49630000000002, 161.7106, 161.9126, 162.10729999999998, 162.31109999999998, 162.5699, 162.8203, 163.0762, 163.3258, 163.5048, 163.65540000000001, 163.7978, 164.0126, 164.12419999999997, 164.29280000000003, 164.4481, 164.5999, 164.7269, 164.7269, 164.861, 165.02484999999996, 165.20245, 165.40085, 165.56505000000004, 165.75615000000002, 165.95575, 166.14165, 166.29704999999998, 166.43745, 166.54635, 166.61765000000003, 166.69175000000004, 166.75054999999998, 166.80094999999997, 166.86294999999998, 166.95365, 166.94285, 166.89924999999997, 166.93364999999997, 166.94615000000002, 166.89895, 166.87474999999998, 166.91505, 166.99800100000002, 167.137501, 167.34990100000002, 167.568801, 167.755101, 167.923201, 168.116701, 168.33230100000003, 168.584001, 168.82270100000002, 169.06930100000005, 169.47470099999998, 169.68480100000005, 169.89260100000004, 170.07740099999998, 170.28680099999997, 170.516301, 170.73490100000004, 170.73490099999998, 170.929701, 171.11685100000003, 171.34355100000002, 171.55995100000004, 171.75115099999996, 171.93275099999994, 172.08135099999998, 172.19465100000002, 172.21395099999998, 172.274351, 172.26735099999993, 172.26345099999998, 172.26185100000004, 172.20385100000004, 172.14455100000004, 172.112451, 172.077851, 172.00265100000001, 171.956651, 171.94935100000004, 171.960351, 172.011051, 172.04735100000005, 172.104451, 172.18695100000002, 172.234051, 172.212651, 172.12135100000003, 172.033051, 171.93245100000001, 171.87526699999998, 171.799967, 171.713267, 171.668267, 171.66176699999997, 171.73746700000004, 171.81306700000002, 171.95776700000002, 172.082467, 172.225567, 172.37596700000003, 172.554067, 172.70116699999994, 172.818567, 172.93766699999998, 173.07596699999993, 173.19576700000002, 173.30816700000003, 173.478567, 173.64586699999995, 173.81966699999998, 174.008767, 174.171967, 174.328767, 174.473567, 174.59276699999998, 174.75166700000003, 174.92646699999997, 175.11711700000004, 175.30491700000002, 175.46971699999997, 175.62251699999996, 175.74411699999996, 175.873917, 175.99651699999998, 176.13451700000002, 176.254117, 176.39951699999997, 176.54621699999998, 176.69671699999998, 176.87181700000002, 177.05141700000001, 177.19851700000004, 177.365917, 177.60346700000002, 177.89366700000002, 178.09216699999996, 178.368467, 178.65686699999998, 179.20076699999998, 179.463416, 179.693316, 179.842616, 180.02851599999997, 180.223316, 180.223316, 180.42241600000006, 180.611016, 180.75941600000002, 180.88931600000004, 181.02261599999997, 181.15731599999998, 181.33771600000003, 181.52311600000004, 181.69711600000002, 181.84011600000002, 181.989316, 182.125616, 182.342216, 182.59871600000002, 182.87901600000004, 183.18476600000002, 183.46936600000004, 183.762966, 184.09866599999998, 184.42166599999996, 184.79756599999996, 185.20766599999996, 185.66106599999995, 186.06826599999997, 186.56086600000003, 187.05056600000003, 187.52206600000002, 188.005866, 188.477766, 188.923366, 189.37546600000002, 189.889066, 190.41826599999996, 190.93646599999997, 191.48716599999997, 192.02916599999998, 192.550566, 193.45536599999997, 193.86036799999994, 194.370968, 194.924368, 195.53586800000005, 196.14286800000002, 196.14286800000002, 196.684052, 197.224252, 197.78435199999996, 198.33215199999998, 198.818702, 199.260402, 199.71340200000003, 200.08020199999996, 200.479802, 200.87700199999995, 201.27620199999998, 201.668802, 202.10390199999998, 202.50210199999998, 202.880302, 203.236002, 203.63480199999998, 203.93580199999997, 204.20420200000004, 204.55220200000002, 204.84050200000001, 205.17970200000008, 205.50590200000005, 205.788502, 206.106502, 206.44360200000003, 206.768952, 207.001652, 207.26705199999998, 207.48685199999997, 207.67755199999996, 207.89605200000003, 208.171352, 208.46945200000002, 208.637052, 208.893452, 209.10545199999996, 209.33265200000002, 209.51235200000002, 209.59965200000002, 209.672752, 209.718952, 209.718952, 209.78865199999996, 209.88230199999998, 209.88570199999998, 209.82600200000002, 209.72210199999998, 209.601807, 209.494207, 209.35690699999998, 209.260507, 209.152507, 209.05920699999996, 208.998107, 208.85190699999998, 208.690307, 208.45950699999997, 208.251607, 208.01820699999996, 207.79240700000003, 207.584807, 207.308907, 207.002807, 206.719407, 206.41720700000002, 206.08680700000005, 205.68860700000002, 205.138907, 204.636807, 204.12230699999998, 203.59400700000003, 203.102307, 202.609007, 201.94095699999997, 201.34805699999998, 200.73835699999995, 200.148457, 199.57855700000005, 198.981757, 198.32845700000001, 197.67385700000005, 197.05375700000005, 196.452357, 195.85515700000005, 195.261957, 194.61895699999997, 193.961557, 193.25885699999998, 192.58535699999996, 191.87325699999997, 191.13603699999996, 190.519737, 189.95123699999996, 189.40233699999996, 188.93163500000003, 188.434035, 187.96623499999998, 187.412135, 186.88073500000002, 186.39633500000002, 185.92283500000002, 185.44113499999995, 184.948635, 184.47588499999998, 183.97738499999997, 183.47638499999994, 182.98268499999998, 182.46148499999998, 181.94648500000002, 181.41728500000002, 180.87318499999998, 180.284185, 179.753985, 179.26998500000002, 178.78748499999998, 178.26388499999996, 177.82468500000002, 177.40938499999996, 176.97808499999996, 176.61468499999995, 176.21058499999998, 175.53678499999998, 175.22408499999997, 174.883585, 174.53763500000002, 174.33733499999994, 174.04943500000002, 174.049435, 173.77683500000003, 173.51923500000004, 173.27063500000008, 172.96983500000002, 172.64863499999998, 172.486335, 172.41083500000002, 172.326535, 172.18413500000003, 172.068935, 172.025035, 172.07803499999997, 172.16233499999998, 172.28373499999998, 172.35843499999996, 172.41543499999997, 172.549435, 172.81123499999998, 173.082235, 173.40893, 173.73783, 174.06713, 174.30983, 174.55723, 174.81732999999997, 174.97372999999996, 175.31183, 175.65593, 176.08983, 176.47973000000002, 176.82203, 177.16013000000004, 177.45893, 177.77793000000003, 177.99302999999998, 178.22063000000003, 178.52223, 179.23682999999997, 179.60123, 179.89542999999995, 180.15803000000002, 180.39242999999996, 180.60362999999998, 180.60362999999998, 180.80612999999997, 181.15877999999992, 181.45868000000002, 181.72978000000003, 181.95487999999997, 182.21847999999997, 182.50589, 182.83539000000005, 183.23689, 183.63158999999996, 184.03049000000001, 184.41378999999998, 184.79299, 185.18349, 185.58329000000006, 186.04119, 186.44259000000002, 186.87338400000002, 187.31500400000004, 187.648104, 187.94110399999997, 188.27380399999996, 188.55840399999994, 188.79660399999997, 189.07060399999997, 189.387804, 189.72610400000002, 190.074104, 190.365304, 190.67580400000003, 191.00000400000002, 191.31360400000003, 191.63660400000003, 191.96770400000003, 192.30120399999998, 192.91830399999998, 193.20050400000002, 193.52390400000004, 193.88015400000003, 194.21715399999997, 194.529054, 194.851454, 194.851454, 195.20305399999998, 195.56719399999994, 195.965794, 196.260589, 196.49208899999996, 196.607089, 196.73958900000002, 196.86888899999997, 197.02308899999997, 197.166989, 197.290289, 197.42958900000002, 197.54648900000004, 197.67618900000005, 197.87268900000004, 198.091389, 198.30778899999999, 198.534889, 198.74698899999999, 198.832889, 198.94538900000003, 199.02958900000002, 199.115089, 199.20528899999996, 199.297689, 199.34803900000003, 199.451139, 199.59503899999999, 199.735439, 199.88443900000001, 200.020939, 200.218209, 200.40270899999996, 200.515609, 200.64280900000006, 200.79690900000003, 200.98170900000005, 201.14510900000002, 201.31630899999996, 201.39790899999997, 201.48300899999995, 201.575109, 201.68800900000002, 201.847909, 202.05870900000002, 202.297309, 202.51390900000007, 202.86480899999998, 203.24840899999995, 203.60980899999998, 203.95230899999999, 204.332509, 204.802509, 205.298809, 205.82980899999998, 206.38630899999995, 206.939609, 207.510809, 208.100109, 208.722909, 209.37260899999993, 210.07120899999998, 210.71050899999997, 211.350899, 211.988999, 212.52049900000003, 213.02819900000003, 213.568499, 214.18499900000006, 214.81239900000003, 215.456499, 216.08939900000004, 216.699399, 217.322199, 217.950405, 218.582205, 219.24140500000001, 219.91260499999993, 220.57210499999997, 221.92970499999998, 222.545105, 223.138105, 223.713205, 224.334905, 224.97580499999995, 224.97580499999995, 225.642604, 226.28450400000006, 226.90800400000003, 227.470804, 228.03620400000003, 228.647104, 229.32130400000003, 229.933904, 230.594404, 231.229604, 231.85595399999994, 232.52185399999996, 233.24995399999992, 233.97665399999997, 234.6770539999999, 235.38851399999996, 236.053514, 236.808619, 237.612019, 238.57891899999998, 239.507419, 240.43248900000003, 241.331689, 242.32678899999996, 243.298989, 244.207489, 245.163689, 246.17658899999995, 247.208489, 248.20818900000003, 249.273989, 250.27378900000002, 251.26383900000002, 252.39073900000002, 253.511739, 254.635239, 255.75723900000006, 257.95393900000005, 258.98538900000005, 260.069789, 261.180089, 262.28648899999996, 263.236289, 263.236289, 264.15418900000003]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('583a2ab5-cecc-4882-bae8-4f0609b17d25');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"c61a39d4-b291-4476-bdac-51a37f28a474\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"c61a39d4-b291-4476-bdac-51a37f28a474\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'c61a39d4-b291-4476-bdac-51a37f28a474',\n",
              "                        [{\"type\": \"scatter\", \"x\": [20110225, 20110228, 20110301, 20110302, 20110303, 20110304, 20110307, 20110308, 20110309, 20110310, 20110311, 20110314, 20110315, 20110316, 20110317, 20110318, 20110321, 20110322, 20110323, 20110324, 20110325, 20110328, 20110329, 20110330, 20110331, 20110401, 20110404, 20110405, 20110406, 20110407, 20110408, 20110411, 20110412, 20110413, 20110414, 20110415, 20110418, 20110419, 20110420, 20110421, 20110425, 20110426, 20110427, 20110428, 20110429, 20110502, 20110503, 20110504, 20110505, 20110506, 20110509, 20110510, 20110511, 20110512, 20110513, 20110516, 20110517, 20110518, 20110519, 20110520, 20110523, 20110524, 20110525, 20110526, 20110527, 20110531, 20110601, 20110602, 20110603, 20110606, 20110607, 20110608, 20110609, 20110610, 20110613, 20110614, 20110615, 20110616, 20110617, 20110620, 20110621, 20110622, 20110623, 20110624, 20110627, 20110628, 20110629, 20110630, 20110701, 20110705, 20110706, 20110707, 20110708, 20110711, 20110712, 20110713, 20110714, 20110715, 20110718, 20110719, 20110720, 20110721, 20110722, 20110725, 20110726, 20110727, 20110728, 20110729, 20110801, 20110802, 20110803, 20110804, 20110805, 20110808, 20110809, 20110810, 20110811, 20110812, 20110815, 20110816, 20110817, 20110818, 20110819, 20110822, 20110823, 20110824, 20110825, 20110826, 20110829, 20110830, 20110831, 20110901, 20110902, 20110906, 20110907, 20110908, 20110909, 20110912, 20110913, 20110914, 20110915, 20110916, 20110919, 20110920, 20110921, 20110922, 20110923, 20110926, 20110927, 20110928, 20110929, 20110930, 20111003, 20111004, 20111005, 20111006, 20111007, 20111010, 20111011, 20111012, 20111013, 20111014, 20111017, 20111018, 20111019, 20111020, 20111021, 20111024, 20111025, 20111026, 20111027, 20111028, 20111031, 20111101, 20111102, 20111103, 20111104, 20111107, 20111108, 20111109, 20111110, 20111111, 20111114, 20111115, 20111116, 20111117, 20111118, 20111121, 20111122, 20111123, 20111125, 20111128, 20111129, 20111130, 20111201, 20111202, 20111205, 20111206, 20111207, 20111208, 20111209, 20111212, 20111213, 20111214, 20111215, 20111216, 20111219, 20111220, 20111221, 20111222, 20111223, 20111227, 20111228, 20111229, 20111230, 20120103, 20120104, 20120105, 20120106, 20120109, 20120110, 20120111, 20120112, 20120113, 20120117, 20120118, 20120119, 20120120, 20120123, 20120124, 20120125, 20120126, 20120127, 20120130, 20120131, 20120201, 20120202, 20120203, 20120206, 20120207, 20120208, 20120209, 20120210, 20120213, 20120214, 20120215, 20120216, 20120217, 20120221, 20120222, 20120223, 20120224, 20120227, 20120228, 20120229, 20120301, 20120302, 20120305, 20120306, 20120307, 20120308, 20120309, 20120312, 20120313, 20120314, 20120315, 20120316, 20120319, 20120320, 20120321, 20120322, 20120323, 20120326, 20120327, 20120328, 20120329, 20120330, 20120402, 20120403, 20120404, 20120405, 20120409, 20120410, 20120411, 20120412, 20120413, 20120416, 20120417, 20120418, 20120419, 20120420, 20120423, 20120424, 20120425, 20120426, 20120427, 20120430, 20120501, 20120502, 20120503, 20120504, 20120507, 20120508, 20120509, 20120510, 20120511, 20120514, 20120515, 20120516, 20120517, 20120518, 20120521, 20120522, 20120523, 20120524, 20120525, 20120529, 20120530, 20120531, 20120601, 20120604, 20120605, 20120606, 20120607, 20120608, 20120611, 20120612, 20120613, 20120614, 20120615, 20120618, 20120619, 20120620, 20120621, 20120622, 20120625, 20120626, 20120627, 20120628, 20120629, 20120702, 20120703, 20120705, 20120706, 20120709, 20120710, 20120711, 20120712, 20120713, 20120716, 20120717, 20120718, 20120719, 20120720, 20120723, 20120724, 20120725, 20120726, 20120727, 20120730, 20120731, 20120801, 20120802, 20120803, 20120806, 20120807, 20120808, 20120809, 20120810, 20120813, 20120814, 20120815, 20120816, 20120817, 20120820, 20120821, 20120822, 20120823, 20120824, 20120827, 20120828, 20120829, 20120830, 20120831, 20120904, 20120905, 20120906, 20120907, 20120910, 20120911, 20120912, 20120913, 20120914, 20120917, 20120918, 20120919, 20120920, 20120921, 20120924, 20120925, 20120926, 20120927, 20120928, 20121001, 20121002, 20121003, 20121004, 20121005, 20121008, 20121009, 20121010, 20121011, 20121012, 20121015, 20121016, 20121017, 20121018, 20121019, 20121022, 20121023, 20121024, 20121025, 20121026, 20121031, 20121101, 20121102, 20121105, 20121106, 20121107, 20121108, 20121109, 20121112, 20121113, 20121114, 20121115, 20121116, 20121119, 20121120, 20121121, 20121123, 20121126, 20121127, 20121128, 20121129, 20121130, 20121203, 20121204, 20121205, 20121206, 20121207, 20121210, 20121211, 20121212, 20121213, 20121214, 20121217, 20121218, 20121219, 20121220, 20121221, 20121224, 20121226, 20121227, 20121228, 20121231, 20130102, 20130103, 20130104, 20130107, 20130108, 20130109, 20130110, 20130111, 20130114, 20130115, 20130116, 20130117, 20130118, 20130122, 20130123, 20130124, 20130125, 20130128, 20130129, 20130130, 20130131, 20130201, 20130204, 20130205, 20130206, 20130207, 20130208, 20130211, 20130212, 20130213, 20130214, 20130215, 20130219, 20130220, 20130221, 20130222, 20130225, 20130226, 20130227, 20130228, 20130301, 20130304, 20130305, 20130306, 20130307, 20130308, 20130311, 20130312, 20130313, 20130314, 20130315, 20130318, 20130319, 20130320, 20130321, 20130322, 20130325, 20130326, 20130327, 20130328, 20130401, 20130402, 20130403, 20130404, 20130405, 20130408, 20130409, 20130410, 20130411, 20130412, 20130415, 20130416, 20130417, 20130418, 20130419, 20130422, 20130423, 20130424, 20130425, 20130426, 20130429, 20130430, 20130501, 20130502, 20130503, 20130506, 20130507, 20130508, 20130509, 20130510, 20130513, 20130514, 20130515, 20130516, 20130517, 20130520, 20130521, 20130522, 20130523, 20130524, 20130528, 20130529, 20130530, 20130531, 20130603, 20130604, 20130605, 20130606, 20130607, 20130610, 20130611, 20130612, 20130613, 20130614, 20130617, 20130618, 20130619, 20130620, 20130621, 20130624, 20130625, 20130626, 20130627, 20130628, 20130701, 20130702, 20130703, 20130705, 20130708, 20130709, 20130710, 20130711, 20130712, 20130715, 20130716, 20130717, 20130718, 20130719, 20130722, 20130723, 20130724, 20130725, 20130726, 20130729, 20130730, 20130731, 20130801, 20130802, 20130805, 20130806, 20130807, 20130808, 20130809, 20130812, 20130813, 20130814, 20130815, 20130816, 20130819, 20130820, 20130821, 20130822, 20130823, 20130826, 20130827, 20130828, 20130829, 20130830, 20130903, 20130904, 20130905, 20130906, 20130909, 20130910, 20130911, 20130912, 20130913, 20130916, 20130917, 20130918, 20130919, 20130920, 20130923, 20130924, 20130925, 20130926, 20130927, 20130930, 20131001, 20131002, 20131003, 20131004, 20131007, 20131008, 20131009, 20131010, 20131011, 20131014, 20131015, 20131016, 20131017, 20131018, 20131021, 20131022, 20131023, 20131024, 20131025, 20131028, 20131029, 20131030, 20131031, 20131101, 20131104, 20131105, 20131106, 20131107, 20131108, 20131111, 20131112, 20131113, 20131114, 20131115, 20131118, 20131119, 20131120, 20131121, 20131122, 20131125, 20131126, 20131127, 20131129, 20131202, 20131203, 20131204, 20131205, 20131206, 20131209, 20131210, 20131211, 20131212, 20131213, 20131216, 20131217, 20131218, 20131219, 20131220, 20131223, 20131224, 20131226, 20131227, 20131230, 20131231, 20140102, 20140103, 20140106, 20140107, 20140108, 20140109, 20140110, 20140113, 20140114, 20140115, 20140116, 20140117, 20140121, 20140122, 20140123, 20140124, 20140127, 20140128, 20140129, 20140130, 20140131, 20140203, 20140204, 20140205, 20140206, 20140207, 20140210, 20140211, 20140212, 20140213, 20140214, 20140218, 20140219, 20140220, 20140221, 20140224, 20140225, 20140226, 20140227, 20140228, 20140303, 20140304, 20140305, 20140306, 20140307, 20140310, 20140311, 20140312, 20140313, 20140314, 20140317, 20140318, 20140319, 20140320, 20140321, 20140324, 20140325, 20140326, 20140327, 20140328, 20140331, 20140401, 20140402, 20140403, 20140404, 20140407, 20140408, 20140409, 20140410, 20140411, 20140414, 20140415, 20140416, 20140417, 20140421, 20140422, 20140423, 20140424, 20140425, 20140428, 20140429, 20140430, 20140501, 20140502, 20140505, 20140506, 20140507, 20140508, 20140509, 20140512, 20140513, 20140514, 20140515, 20140516, 20140519, 20140520, 20140521, 20140522, 20140523, 20140527, 20140528, 20140529, 20140530, 20140602, 20140603, 20140604, 20140605, 20140606, 20140609, 20140610, 20140611, 20140612, 20140613, 20140616, 20140617, 20140618, 20140619, 20140620, 20140623, 20140624, 20140625, 20140626, 20140627, 20140630, 20140701, 20140702, 20140703, 20140707, 20140708, 20140709, 20140710, 20140711, 20140714, 20140715, 20140716, 20140717, 20140718, 20140721, 20140722, 20140723, 20140724, 20140725, 20140728, 20140729, 20140730, 20140731, 20140801, 20140804, 20140805, 20140806, 20140807, 20140808, 20140811, 20140812, 20140813, 20140814, 20140815, 20140818, 20140819, 20140820, 20140821, 20140822, 20140825, 20140826, 20140827, 20140828, 20140829, 20140902, 20140903, 20140904, 20140905, 20140908, 20140909, 20140910, 20140911, 20140912, 20140915, 20140916, 20140917, 20140918, 20140919, 20140922, 20140923, 20140924, 20140925, 20140926, 20140929, 20140930, 20141001, 20141002, 20141003, 20141006, 20141007, 20141008, 20141009, 20141010, 20141013, 20141014, 20141015, 20141016, 20141017, 20141020, 20141021, 20141022, 20141023, 20141024, 20141027, 20141028, 20141029, 20141030, 20141031, 20141103, 20141104, 20141105, 20141106, 20141107, 20141110, 20141111, 20141112, 20141113, 20141114, 20141117, 20141118, 20141119, 20141120, 20141121, 20141124, 20141125, 20141126, 20141128, 20141201, 20141202, 20141203, 20141204, 20141205, 20141208, 20141209, 20141210, 20141211, 20141212, 20141215, 20141216, 20141217, 20141218, 20141219, 20141222, 20141223, 20141224, 20141226, 20141229, 20141230, 20141231, 20150102, 20150105, 20150106, 20150107, 20150108, 20150109, 20150112, 20150113, 20150114, 20150115, 20150116, 20150120, 20150121, 20150122, 20150123, 20150126, 20150127, 20150128, 20150129, 20150130, 20150202, 20150203, 20150204, 20150205, 20150206, 20150209, 20150210, 20150211, 20150212, 20150213, 20150217, 20150218, 20150219, 20150220, 20150223, 20150224, 20150225, 20150226, 20150227, 20150302, 20150303, 20150304, 20150305, 20150306, 20150309, 20150310, 20150311, 20150312, 20150313, 20150316, 20150317, 20150318, 20150319, 20150320, 20150323, 20150324, 20150325, 20150326, 20150327, 20150330, 20150331, 20150401, 20150402, 20150406, 20150407, 20150408, 20150409, 20150410, 20150413, 20150414, 20150415, 20150416, 20150417, 20150420, 20150421, 20150422, 20150423, 20150424, 20150427, 20150428, 20150429, 20150430, 20150501, 20150504, 20150505, 20150506, 20150507, 20150508, 20150511, 20150512, 20150513, 20150514, 20150515, 20150518, 20150519, 20150520, 20150521, 20150522, 20150526, 20150527, 20150528, 20150529, 20150601, 20150602, 20150603, 20150604, 20150605, 20150608, 20150609, 20150610, 20150611, 20150612, 20150615, 20150616, 20150617, 20150618, 20150619, 20150622, 20150623, 20150624, 20150625, 20150626, 20150629, 20150630, 20150701, 20150702, 20150706, 20150707, 20150708, 20150709, 20150710, 20150713, 20150714, 20150715, 20150716, 20150717, 20150720, 20150721, 20150722, 20150723, 20150724, 20150727, 20150728, 20150729, 20150730, 20150731, 20150803, 20150804, 20150805, 20150806, 20150807, 20150810, 20150811, 20150812, 20150813, 20150814, 20150817, 20150818, 20150819, 20150820, 20150821, 20150824, 20150825, 20150826, 20150827, 20150828, 20150831, 20150901, 20150902, 20150903, 20150904, 20150908, 20150909, 20150910, 20150911, 20150914, 20150915, 20150916, 20150917, 20150918, 20150921, 20150922, 20150923, 20150924, 20150925, 20150928, 20150929, 20150930, 20151001, 20151002, 20151005, 20151006, 20151007, 20151008, 20151009, 20151012, 20151013, 20151014, 20151015, 20151016, 20151019, 20151020, 20151021, 20151022, 20151023, 20151026, 20151027, 20151028, 20151029, 20151030, 20151102, 20151103, 20151104, 20151105, 20151106, 20151109, 20151110, 20151111, 20151112, 20151113, 20151116, 20151117, 20151118, 20151119, 20151120, 20151123, 20151124, 20151125, 20151127, 20151130, 20151201, 20151202, 20151203, 20151204, 20151207, 20151208, 20151209, 20151210, 20151211, 20151214, 20151215, 20151216, 20151217, 20151218, 20151221, 20151222, 20151223, 20151224, 20151228, 20151229, 20151230, 20151231, 20160104, 20160105, 20160106, 20160107, 20160108, 20160111, 20160112, 20160113, 20160114, 20160115, 20160119, 20160120, 20160121, 20160122, 20160125, 20160126, 20160127, 20160128, 20160129, 20160201, 20160202, 20160203, 20160204, 20160205, 20160208, 20160209, 20160210, 20160211, 20160212, 20160216, 20160217, 20160218, 20160219, 20160222, 20160223, 20160224, 20160225, 20160226, 20160229, 20160301, 20160302, 20160303, 20160304, 20160307, 20160308, 20160309, 20160310, 20160311, 20160314, 20160315, 20160316, 20160317, 20160318, 20160321, 20160322, 20160323, 20160324, 20160328, 20160329, 20160330, 20160331, 20160401, 20160404, 20160405, 20160406, 20160407, 20160408, 20160411, 20160412, 20160413, 20160414, 20160415, 20160418, 20160419, 20160420, 20160421, 20160422, 20160425, 20160426, 20160427, 20160428, 20160429, 20160502, 20160503, 20160504, 20160505, 20160506, 20160509, 20160510, 20160511, 20160512, 20160513, 20160516, 20160517, 20160518, 20160519, 20160520, 20160523, 20160524, 20160525, 20160526, 20160527, 20160531, 20160601, 20160602, 20160603, 20160606, 20160607, 20160608, 20160609, 20160610, 20160613, 20160614, 20160615, 20160616, 20160617, 20160620, 20160621, 20160622, 20160623, 20160624, 20160627, 20160628, 20160629, 20160630, 20160701, 20160705, 20160706, 20160707, 20160708, 20160711, 20160712, 20160713, 20160714, 20160715, 20160718, 20160719, 20160720, 20160721, 20160722, 20160725, 20160726, 20160727, 20160728, 20160729, 20160801, 20160802, 20160803, 20160804, 20160805, 20160808, 20160809, 20160810, 20160811, 20160812, 20160815, 20160816, 20160817, 20160818, 20160819, 20160822, 20160823, 20160824, 20160825, 20160826, 20160829, 20160830, 20160831, 20160901, 20160902, 20160906, 20160907, 20160908, 20160909, 20160912, 20160913, 20160914, 20160915, 20160916, 20160919, 20160920, 20160921, 20160922, 20160923, 20160926, 20160927, 20160928, 20160929, 20160930, 20161003, 20161004, 20161005, 20161006, 20161007, 20161010, 20161011, 20161012, 20161013, 20161014, 20161017, 20161018, 20161019, 20161020, 20161021, 20161024, 20161025, 20161026, 20161027, 20161028, 20161031, 20161101, 20161102, 20161103, 20161104, 20161107, 20161108, 20161109, 20161110, 20161111, 20161114, 20161115, 20161116, 20161117, 20161118, 20161121, 20161122, 20161123, 20161125, 20161128, 20161129, 20161130, 20161201, 20161202, 20161205, 20161206, 20161207, 20161208, 20161209, 20161212, 20161213, 20161214, 20161215, 20161216, 20161219, 20161220, 20161221, 20161222, 20161223, 20161227, 20161228, 20161229, 20161230, 20170103, 20170104, 20170105, 20170106, 20170109, 20170110, 20170111, 20170112, 20170113, 20170117, 20170118, 20170119, 20170120, 20170123, 20170124, 20170125, 20170126, 20170127, 20170130, 20170131, 20170201, 20170202, 20170203, 20170206, 20170207, 20170208, 20170209, 20170210, 20170213, 20170214, 20170215, 20170216, 20170217, 20170221, 20170222, 20170223, 20170224, 20170227, 20170228, 20170301, 20170302, 20170303, 20170306, 20170307, 20170308, 20170309, 20170310, 20170313, 20170314, 20170315, 20170316, 20170317, 20170320, 20170321, 20170322, 20170323, 20170324, 20170327, 20170328, 20170329, 20170330, 20170331, 20170403, 20170404, 20170405, 20170406, 20170407, 20170410, 20170411, 20170412, 20170413, 20170417, 20170418, 20170419, 20170420, 20170421, 20170424, 20170425, 20170426, 20170427, 20170428, 20170501, 20170502, 20170503, 20170504, 20170505, 20170508, 20170509, 20170510, 20170511, 20170512, 20170515, 20170516, 20170517, 20170518, 20170519, 20170522, 20170523, 20170524, 20170525, 20170526, 20170530, 20170531, 20170601, 20170602, 20170605, 20170606, 20170607, 20170608, 20170609, 20170612, 20170613, 20170614, 20170615, 20170616, 20170619, 20170620, 20170621, 20170622, 20170623, 20170626, 20170627, 20170628, 20170629, 20170630, 20170703, 20170705, 20170706, 20170707, 20170710, 20170711, 20170712, 20170713, 20170714, 20170717, 20170718, 20170719, 20170720, 20170721, 20170724, 20170725, 20170726, 20170727, 20170728, 20170731, 20170801, 20170802, 20170803, 20170804, 20170807, 20170808, 20170809, 20170810, 20170811, 20170814, 20170815, 20170816, 20170817, 20170818, 20170821, 20170822, 20170823, 20170824, 20170825, 20170828, 20170829, 20170830, 20170831, 20170901, 20170905, 20170906, 20170907, 20170908, 20170911, 20170912, 20170913, 20170914, 20170915, 20170918, 20170919, 20170920, 20170921, 20170922, 20170925, 20170926, 20170927, 20170928, 20170929, 20171002, 20171003, 20171004, 20171005, 20171006, 20171009, 20171010, 20171011, 20171012, 20171013, 20171016, 20171017, 20171018, 20171019, 20171020, 20171023, 20171024, 20171025, 20171026, 20171027, 20171030, 20171031, 20171101, 20171102, 20171103, 20171106, 20171107, 20171108, 20171109, 20171110, 20171113, 20171114, 20171115, 20171116, 20171117, 20171120, 20171121, 20171122, 20171124, 20171127, 20171128, 20171129, 20171130, 20171201, 20171204, 20171205, 20171206, 20171207, 20171208, 20171211, 20171212, 20171213, 20171214, 20171215, 20171218, 20171219, 20171220, 20171221, 20171222, 20171226, 20171227, 20171228, 20171229, 20180102, 20180103, 20180104, 20180105, 20180108, 20180109, 20180110, 20180111, 20180112, 20180116, 20180117, 20180118, 20180119, 20180122, 20180123, 20180124, 20180125, 20180126, 20180129, 20180130, 20180131, 20180201, 20180202, 20180205, 20180206, 20180207, 20180208, 20180209, 20180212, 20180213, 20180214, 20180215, 20180216, 20180220, 20180221, 20180222, 20180223, 20180226, 20180227, 20180228, 20180301, 20180302, 20180305, 20180306, 20180307, 20180308, 20180309, 20180312, 20180313, 20180314, 20180315, 20180316, 20180319, 20180320, 20180321, 20180322, 20180323, 20180326, 20180327, 20180328, 20180329, 20180402, 20180403, 20180404, 20180405, 20180406, 20180409, 20180410, 20180411, 20180412, 20180413, 20180416, 20180417, 20180418, 20180419, 20180420, 20180423, 20180424, 20180425, 20180426, 20180427, 20180430, 20180501, 20180502, 20180503, 20180504, 20180507, 20180508, 20180509, 20180510, 20180511, 20180514, 20180515, 20180516, 20180517, 20180518, 20180521, 20180522, 20180523, 20180524, 20180525, 20180529, 20180530, 20180531, 20180601, 20180604, 20180605, 20180606, 20180607, 20180608, 20180611, 20180612, 20180613, 20180614, 20180615, 20180618, 20180619, 20180620, 20180621, 20180622, 20180625, 20180626, 20180627, 20180628, 20180629, 20180702, 20180703, 20180705, 20180706, 20180709, 20180710, 20180711, 20180712, 20180713, 20180716, 20180717, 20180718, 20180719, 20180720, 20180723, 20180724, 20180725, 20180726, 20180727, 20180730, 20180731, 20180801, 20180802, 20180803, 20180806, 20180807, 20180808, 20180809, 20180810, 20180813, 20180814, 20180815, 20180816, 20180817, 20180820, 20180821, 20180822, 20180823, 20180824, 20180827, 20180828, 20180829, 20180830, 20180831, 20180904, 20180905, 20180906, 20180907, 20180910, 20180911, 20180912, 20180913, 20180914, 20180917, 20180918, 20180919, 20180920, 20180921, 20180924, 20180925, 20180926, 20180927, 20180928, 20181001, 20181002, 20181003, 20181004, 20181005, 20181008, 20181009, 20181010, 20181011, 20181012, 20181015, 20181016, 20181017, 20181018, 20181019, 20181022, 20181023, 20181024, 20181025, 20181026, 20181029, 20181030, 20181031, 20181101, 20181102, 20181105, 20181106, 20181107, 20181108, 20181109, 20181112, 20181113, 20181114, 20181115, 20181116, 20181119, 20181120, 20181121, 20181123, 20181126, 20181127, 20181128, 20181129, 20181130, 20181203, 20181204, 20181206, 20181207, 20181210, 20181211, 20181212, 20181213, 20181214, 20181217, 20181218, 20181219, 20181220, 20181221, 20181224, 20181226, 20181227, 20181228, 20181231, 20190102, 20190103, 20190104, 20190107, 20190108, 20190109, 20190110, 20190111, 20190114, 20190115, 20190116, 20190117, 20190118, 20190122, 20190123, 20190124, 20190125, 20190128, 20190129, 20190130, 20190131, 20190201, 20190204, 20190205, 20190206, 20190207, 20190208, 20190211, 20190212, 20190213, 20190214, 20190215, 20190219, 20190220, 20190221, 20190222, 20190225, 20190226, 20190227, 20190228, 20190301, 20190304, 20190305, 20190306, 20190307, 20190308, 20190311, 20190312, 20190313, 20190314, 20190315, 20190318, 20190319, 20190320, 20190321, 20190322, 20190325, 20190326, 20190327, 20190328, 20190329, 20190401, 20190402, 20190403, 20190404, 20190405, 20190408, 20190409, 20190410, 20190411, 20190412, 20190415, 20190416, 20190417, 20190418, 20190422, 20190423, 20190424, 20190425, 20190426, 20190429, 20190430, 20190501, 20190502, 20190503, 20190506, 20190507, 20190508, 20190509, 20190510, 20190513, 20190514, 20190515, 20190516, 20190517, 20190520, 20190521, 20190522, 20190523, 20190524, 20190528, 20190529, 20190530, 20190531, 20190603, 20190604, 20190605, 20190606, 20190607, 20190610, 20190611, 20190612, 20190613, 20190614, 20190617, 20190618, 20190619, 20190620, 20190621, 20190624, 20190625, 20190626, 20190627, 20190628, 20190701, 20190702, 20190703, 20190705, 20190708, 20190709, 20190710, 20190711, 20190712, 20190715, 20190716, 20190717, 20190718, 20190719, 20190722, 20190723, 20190724, 20190725, 20190726, 20190729, 20190730, 20190731, 20190801, 20190802, 20190805, 20190806, 20190807, 20190808, 20190809, 20190812, 20190813, 20190814, 20190815, 20190816, 20190819, 20190820, 20190821, 20190822, 20190823, 20190826, 20190827, 20190828, 20190829, 20190830, 20190903, 20190904, 20190905, 20190906, 20190909, 20190910, 20190911, 20190912, 20190913, 20190916, 20190917, 20190918, 20190919, 20190920, 20190923, 20190924, 20190925, 20190926, 20190927, 20190930, 20191001, 20191002, 20191003, 20191004, 20191007, 20191008, 20191009, 20191010, 20191011, 20191014, 20191015, 20191016, 20191017, 20191018, 20191021, 20191022, 20191023, 20191024, 20191025, 20191028, 20191029, 20191030, 20191031, 20191101, 20191104, 20191105, 20191106, 20191107, 20191108, 20191111, 20191112, 20191113, 20191114, 20191115, 20191118, 20191119, 20191120], \"y\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c61a39d4-b291-4476-bdac-51a37f28a474');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpfPksd5Cs-A"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G4Fo7yNCs-B",
        "outputId": "74e280ab-b0e0-449e-8fc0-73a1374a5ac9"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.05, .1, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "\n",
        "  historical = Train_data(dfs[col_name], train_start=100, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"AAPL\", step_sizes=4, th= th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.05\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 15ms/step - loss: 0.6278 - accuracy: 0.6882 - val_loss: 0.5262 - val_accuracy: 0.8082\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.6229 - accuracy: 0.6899 - val_loss: 0.5486 - val_accuracy: 0.8082\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.5623 - accuracy: 0.7580 - val_loss: 0.5116 - val_accuracy: 0.8082\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.5815 - accuracy: 0.7379 - val_loss: 0.5112 - val_accuracy: 0.8082\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.3578 - accuracy: 0.8669 - val_loss: 0.1145 - val_accuracy: 0.9490\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 13ms/step - loss: 0.6260 - accuracy: 0.6846 - val_loss: 0.5090 - val_accuracy: 0.8082\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.3480 - accuracy: 0.8533 - val_loss: 0.1762 - val_accuracy: 0.9327\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.1779 - accuracy: 0.9420 - val_loss: 0.0720 - val_accuracy: 0.9735\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.1428 - accuracy: 0.9491 - val_loss: 0.0723 - val_accuracy: 0.9653\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.1433 - accuracy: 0.9479 - val_loss: 0.0586 - val_accuracy: 0.9796\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.991403\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.991605\n",
            "[2]\tvalidation_0-auc:0.991605\n",
            "[3]\tvalidation_0-auc:0.991565\n",
            "[4]\tvalidation_0-auc:0.992048\n",
            "[5]\tvalidation_0-auc:0.991712\n",
            "[6]\tvalidation_0-auc:0.991645\n",
            "[7]\tvalidation_0-auc:0.993123\n",
            "[8]\tvalidation_0-auc:0.993002\n",
            "[9]\tvalidation_0-auc:0.993109\n",
            "[10]\tvalidation_0-auc:0.993029\n",
            "[11]\tvalidation_0-auc:0.997945\n",
            "[12]\tvalidation_0-auc:0.998106\n",
            "[13]\tvalidation_0-auc:0.99816\n",
            "[14]\tvalidation_0-auc:0.998214\n",
            "[15]\tvalidation_0-auc:0.998321\n",
            "[16]\tvalidation_0-auc:0.99824\n",
            "[17]\tvalidation_0-auc:0.998187\n",
            "[18]\tvalidation_0-auc:0.99824\n",
            "[19]\tvalidation_0-auc:0.99824\n",
            "[20]\tvalidation_0-auc:0.998214\n",
            "[21]\tvalidation_0-auc:0.998187\n",
            "[22]\tvalidation_0-auc:0.998133\n",
            "[23]\tvalidation_0-auc:0.998133\n",
            "[24]\tvalidation_0-auc:0.998187\n",
            "[25]\tvalidation_0-auc:0.998106\n",
            "[26]\tvalidation_0-auc:0.998106\n",
            "[27]\tvalidation_0-auc:0.99816\n",
            "[28]\tvalidation_0-auc:0.99824\n",
            "[29]\tvalidation_0-auc:0.998187\n",
            "[30]\tvalidation_0-auc:0.998133\n",
            "[31]\tvalidation_0-auc:0.998106\n",
            "[32]\tvalidation_0-auc:0.99816\n",
            "[33]\tvalidation_0-auc:0.998133\n",
            "[34]\tvalidation_0-auc:0.9982\n",
            "[35]\tvalidation_0-auc:0.9982\n",
            "[36]\tvalidation_0-auc:0.998254\n",
            "[37]\tvalidation_0-auc:0.998375\n",
            "[38]\tvalidation_0-auc:0.998402\n",
            "[39]\tvalidation_0-auc:0.998348\n",
            "[40]\tvalidation_0-auc:0.998321\n",
            "[41]\tvalidation_0-auc:0.998321\n",
            "[42]\tvalidation_0-auc:0.998348\n",
            "[43]\tvalidation_0-auc:0.998321\n",
            "[44]\tvalidation_0-auc:0.998402\n",
            "[45]\tvalidation_0-auc:0.998402\n",
            "[46]\tvalidation_0-auc:0.998509\n",
            "[47]\tvalidation_0-auc:0.998482\n",
            "[48]\tvalidation_0-auc:0.998455\n",
            "[49]\tvalidation_0-auc:0.998442\n",
            "[50]\tvalidation_0-auc:0.998442\n",
            "[51]\tvalidation_0-auc:0.998469\n",
            "[52]\tvalidation_0-auc:0.998388\n",
            "[53]\tvalidation_0-auc:0.998442\n",
            "[54]\tvalidation_0-auc:0.998388\n",
            "[55]\tvalidation_0-auc:0.998415\n",
            "[56]\tvalidation_0-auc:0.998415\n",
            "[57]\tvalidation_0-auc:0.998522\n",
            "[58]\tvalidation_0-auc:0.998522\n",
            "[59]\tvalidation_0-auc:0.998334\n",
            "[60]\tvalidation_0-auc:0.998361\n",
            "[61]\tvalidation_0-auc:0.998361\n",
            "[62]\tvalidation_0-auc:0.998254\n",
            "[63]\tvalidation_0-auc:0.998281\n",
            "[64]\tvalidation_0-auc:0.998281\n",
            "[65]\tvalidation_0-auc:0.998281\n",
            "[66]\tvalidation_0-auc:0.998281\n",
            "[67]\tvalidation_0-auc:0.998281\n",
            "[68]\tvalidation_0-auc:0.998281\n",
            "[69]\tvalidation_0-auc:0.998281\n",
            "[70]\tvalidation_0-auc:0.998308\n",
            "[71]\tvalidation_0-auc:0.998334\n",
            "[72]\tvalidation_0-auc:0.998334\n",
            "[73]\tvalidation_0-auc:0.998334\n",
            "[74]\tvalidation_0-auc:0.998227\n",
            "[75]\tvalidation_0-auc:0.9982\n",
            "[76]\tvalidation_0-auc:0.9982\n",
            "[77]\tvalidation_0-auc:0.998227\n",
            "[78]\tvalidation_0-auc:0.998227\n",
            "[79]\tvalidation_0-auc:0.998254\n",
            "[80]\tvalidation_0-auc:0.998227\n",
            "[81]\tvalidation_0-auc:0.998254\n",
            "[82]\tvalidation_0-auc:0.998254\n",
            "[83]\tvalidation_0-auc:0.998254\n",
            "[84]\tvalidation_0-auc:0.998227\n",
            "[85]\tvalidation_0-auc:0.9982\n",
            "[86]\tvalidation_0-auc:0.998173\n",
            "[87]\tvalidation_0-auc:0.998146\n",
            "[88]\tvalidation_0-auc:0.998119\n",
            "[89]\tvalidation_0-auc:0.998119\n",
            "[90]\tvalidation_0-auc:0.998119\n",
            "[91]\tvalidation_0-auc:0.998066\n",
            "[92]\tvalidation_0-auc:0.998093\n",
            "[93]\tvalidation_0-auc:0.998066\n",
            "[94]\tvalidation_0-auc:0.998066\n",
            "[95]\tvalidation_0-auc:0.998039\n",
            "[96]\tvalidation_0-auc:0.998039\n",
            "[97]\tvalidation_0-auc:0.998066\n",
            "[98]\tvalidation_0-auc:0.998119\n",
            "[99]\tvalidation_0-auc:0.998119\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 15ms/step - loss: 0.4875 - accuracy: 0.7815 - val_loss: 0.3016 - val_accuracy: 0.8972\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3443 - accuracy: 0.8847 - val_loss: 0.3145 - val_accuracy: 0.8950\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3064 - accuracy: 0.8974 - val_loss: 0.3147 - val_accuracy: 0.8950\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.2981 - accuracy: 0.9040 - val_loss: 0.3387 - val_accuracy: 0.8950\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.2890 - accuracy: 0.9004 - val_loss: 0.3241 - val_accuracy: 0.9015\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 14ms/step - loss: 0.5443 - accuracy: 0.7351 - val_loss: 0.3240 - val_accuracy: 0.8709\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2454 - accuracy: 0.9246 - val_loss: 0.2144 - val_accuracy: 0.9234\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.1651 - accuracy: 0.9475 - val_loss: 0.2529 - val_accuracy: 0.9300\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.1561 - accuracy: 0.9547 - val_loss: 0.2129 - val_accuracy: 0.9409\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.1634 - accuracy: 0.9547 - val_loss: 0.1745 - val_accuracy: 0.9278\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.948186\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.948186\n",
            "[2]\tvalidation_0-auc:0.967733\n",
            "[3]\tvalidation_0-auc:0.967733\n",
            "[4]\tvalidation_0-auc:0.983764\n",
            "[5]\tvalidation_0-auc:0.984775\n",
            "[6]\tvalidation_0-auc:0.984673\n",
            "[7]\tvalidation_0-auc:0.984643\n",
            "[8]\tvalidation_0-auc:0.985068\n",
            "[9]\tvalidation_0-auc:0.98797\n",
            "[10]\tvalidation_0-auc:0.986695\n",
            "[11]\tvalidation_0-auc:0.986431\n",
            "[12]\tvalidation_0-auc:0.986724\n",
            "[13]\tvalidation_0-auc:0.986871\n",
            "[14]\tvalidation_0-auc:0.987134\n",
            "[15]\tvalidation_0-auc:0.98794\n",
            "[16]\tvalidation_0-auc:0.987823\n",
            "[17]\tvalidation_0-auc:0.987559\n",
            "[18]\tvalidation_0-auc:0.987501\n",
            "[19]\tvalidation_0-auc:0.988951\n",
            "[20]\tvalidation_0-auc:0.988805\n",
            "[21]\tvalidation_0-auc:0.988805\n",
            "[22]\tvalidation_0-auc:0.989039\n",
            "[23]\tvalidation_0-auc:0.98901\n",
            "[24]\tvalidation_0-auc:0.989332\n",
            "[25]\tvalidation_0-auc:0.99005\n",
            "[26]\tvalidation_0-auc:0.989406\n",
            "[27]\tvalidation_0-auc:0.990607\n",
            "[28]\tvalidation_0-auc:0.990256\n",
            "[29]\tvalidation_0-auc:0.991428\n",
            "[30]\tvalidation_0-auc:0.991193\n",
            "[31]\tvalidation_0-auc:0.991428\n",
            "[32]\tvalidation_0-auc:0.991472\n",
            "[33]\tvalidation_0-auc:0.99156\n",
            "[34]\tvalidation_0-auc:0.991618\n",
            "[35]\tvalidation_0-auc:0.991853\n",
            "[36]\tvalidation_0-auc:0.991911\n",
            "[37]\tvalidation_0-auc:0.991794\n",
            "[38]\tvalidation_0-auc:0.991823\n",
            "[39]\tvalidation_0-auc:0.991823\n",
            "[40]\tvalidation_0-auc:0.991853\n",
            "[41]\tvalidation_0-auc:0.991941\n",
            "[42]\tvalidation_0-auc:0.991823\n",
            "[43]\tvalidation_0-auc:0.991941\n",
            "[44]\tvalidation_0-auc:0.991941\n",
            "[45]\tvalidation_0-auc:0.992029\n",
            "[46]\tvalidation_0-auc:0.992058\n",
            "[47]\tvalidation_0-auc:0.992117\n",
            "[48]\tvalidation_0-auc:0.992117\n",
            "[49]\tvalidation_0-auc:0.992263\n",
            "[50]\tvalidation_0-auc:0.992175\n",
            "[51]\tvalidation_0-auc:0.992292\n",
            "[52]\tvalidation_0-auc:0.992292\n",
            "[53]\tvalidation_0-auc:0.99238\n",
            "[54]\tvalidation_0-auc:0.992498\n",
            "[55]\tvalidation_0-auc:0.992351\n",
            "[56]\tvalidation_0-auc:0.992351\n",
            "[57]\tvalidation_0-auc:0.992292\n",
            "[58]\tvalidation_0-auc:0.992585\n",
            "[59]\tvalidation_0-auc:0.992351\n",
            "[60]\tvalidation_0-auc:0.992556\n",
            "[61]\tvalidation_0-auc:0.99241\n",
            "[62]\tvalidation_0-auc:0.992322\n",
            "[63]\tvalidation_0-auc:0.992322\n",
            "[64]\tvalidation_0-auc:0.992234\n",
            "[65]\tvalidation_0-auc:0.992204\n",
            "[66]\tvalidation_0-auc:0.992204\n",
            "[67]\tvalidation_0-auc:0.992204\n",
            "[68]\tvalidation_0-auc:0.992087\n",
            "[69]\tvalidation_0-auc:0.992117\n",
            "[70]\tvalidation_0-auc:0.992117\n",
            "[71]\tvalidation_0-auc:0.992087\n",
            "[72]\tvalidation_0-auc:0.992117\n",
            "[73]\tvalidation_0-auc:0.992087\n",
            "[74]\tvalidation_0-auc:0.992029\n",
            "[75]\tvalidation_0-auc:0.992058\n",
            "[76]\tvalidation_0-auc:0.992117\n",
            "[77]\tvalidation_0-auc:0.992146\n",
            "[78]\tvalidation_0-auc:0.992117\n",
            "[79]\tvalidation_0-auc:0.992146\n",
            "[80]\tvalidation_0-auc:0.992175\n",
            "[81]\tvalidation_0-auc:0.992146\n",
            "[82]\tvalidation_0-auc:0.992234\n",
            "[83]\tvalidation_0-auc:0.992234\n",
            "[84]\tvalidation_0-auc:0.992175\n",
            "[85]\tvalidation_0-auc:0.992146\n",
            "[86]\tvalidation_0-auc:0.992146\n",
            "[87]\tvalidation_0-auc:0.992263\n",
            "[88]\tvalidation_0-auc:0.992292\n",
            "[89]\tvalidation_0-auc:0.992263\n",
            "[90]\tvalidation_0-auc:0.992322\n",
            "[91]\tvalidation_0-auc:0.99241\n",
            "[92]\tvalidation_0-auc:0.992615\n",
            "[93]\tvalidation_0-auc:0.992585\n",
            "[94]\tvalidation_0-auc:0.992527\n",
            "[95]\tvalidation_0-auc:0.992498\n",
            "[96]\tvalidation_0-auc:0.992498\n",
            "[97]\tvalidation_0-auc:0.992615\n",
            "[98]\tvalidation_0-auc:0.992644\n",
            "[99]\tvalidation_0-auc:0.992615\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.05     | 0.9489795918367347 | 0.9843342036553525 | 0.952020202020202  | 0.9679075738125802 |\n",
            "|      GRU 0.05     | 0.9795918367346939 | 0.9873737373737373 | 0.9873737373737373 | 0.9873737373737373 |\n",
            "|    XGBoost 0.05   | 0.9795918367346939 | 0.9948717948717949 | 0.9797979797979798 | 0.9872773536895675 |\n",
            "|    Logreg 0.05    | 0.9224489795918367 | 0.9124423963133641 |        1.0         | 0.9542168674698795 |\n",
            "|      SVM 0.05     | 0.9755102040816327 | 0.9848484848484849 | 0.9848484848484849 | 0.9848484848484849 |\n",
            "|   LSTM beta 0.05  | 0.9015317286652079 | 0.9344262295081968 | 0.9421487603305785 | 0.9382716049382717 |\n",
            "|   GRU beta 0.05   | 0.9277899343544858 | 0.976878612716763  | 0.931129476584022  | 0.9534555712270804 |\n",
            "| XGBoost beta 0.05 | 0.949671772428884  | 0.9913294797687862 | 0.9449035812672176 | 0.9675599435825105 |\n",
            "|  logreg beta 0.05 | 0.9452954048140044 | 0.9355670103092784 |        1.0         | 0.966711051930759  |\n",
            "|   svm beta 0.05   | 0.9387308533916849 | 0.9539295392953929 | 0.9696969696969697 | 0.9617486338797815 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 15ms/step - loss: 0.6326 - accuracy: 0.6805 - val_loss: 0.5259 - val_accuracy: 0.8082\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.6236 - accuracy: 0.6899 - val_loss: 0.5510 - val_accuracy: 0.8082\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.4369 - accuracy: 0.8272 - val_loss: 0.1589 - val_accuracy: 0.9653\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.2774 - accuracy: 0.9101 - val_loss: 0.0952 - val_accuracy: 0.9735\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.1990 - accuracy: 0.9373 - val_loss: 0.0940 - val_accuracy: 0.9612\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 14ms/step - loss: 0.6255 - accuracy: 0.6846 - val_loss: 0.5493 - val_accuracy: 0.8082\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.4858 - accuracy: 0.7710 - val_loss: 0.0993 - val_accuracy: 0.9673\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.2480 - accuracy: 0.9154 - val_loss: 0.1033 - val_accuracy: 0.9612\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.1645 - accuracy: 0.9491 - val_loss: 0.0954 - val_accuracy: 0.9571\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.1524 - accuracy: 0.9473 - val_loss: 0.0728 - val_accuracy: 0.9735\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.991403\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.991605\n",
            "[2]\tvalidation_0-auc:0.991605\n",
            "[3]\tvalidation_0-auc:0.991565\n",
            "[4]\tvalidation_0-auc:0.992048\n",
            "[5]\tvalidation_0-auc:0.991712\n",
            "[6]\tvalidation_0-auc:0.991645\n",
            "[7]\tvalidation_0-auc:0.993123\n",
            "[8]\tvalidation_0-auc:0.993002\n",
            "[9]\tvalidation_0-auc:0.993109\n",
            "[10]\tvalidation_0-auc:0.993029\n",
            "[11]\tvalidation_0-auc:0.997945\n",
            "[12]\tvalidation_0-auc:0.998106\n",
            "[13]\tvalidation_0-auc:0.99816\n",
            "[14]\tvalidation_0-auc:0.998214\n",
            "[15]\tvalidation_0-auc:0.998321\n",
            "[16]\tvalidation_0-auc:0.99824\n",
            "[17]\tvalidation_0-auc:0.998187\n",
            "[18]\tvalidation_0-auc:0.99824\n",
            "[19]\tvalidation_0-auc:0.99824\n",
            "[20]\tvalidation_0-auc:0.998214\n",
            "[21]\tvalidation_0-auc:0.998187\n",
            "[22]\tvalidation_0-auc:0.998133\n",
            "[23]\tvalidation_0-auc:0.998133\n",
            "[24]\tvalidation_0-auc:0.998187\n",
            "[25]\tvalidation_0-auc:0.998106\n",
            "[26]\tvalidation_0-auc:0.998106\n",
            "[27]\tvalidation_0-auc:0.99816\n",
            "[28]\tvalidation_0-auc:0.99824\n",
            "[29]\tvalidation_0-auc:0.998187\n",
            "[30]\tvalidation_0-auc:0.998133\n",
            "[31]\tvalidation_0-auc:0.998106\n",
            "[32]\tvalidation_0-auc:0.99816\n",
            "[33]\tvalidation_0-auc:0.998133\n",
            "[34]\tvalidation_0-auc:0.9982\n",
            "[35]\tvalidation_0-auc:0.9982\n",
            "[36]\tvalidation_0-auc:0.998254\n",
            "[37]\tvalidation_0-auc:0.998375\n",
            "[38]\tvalidation_0-auc:0.998402\n",
            "[39]\tvalidation_0-auc:0.998348\n",
            "[40]\tvalidation_0-auc:0.998321\n",
            "[41]\tvalidation_0-auc:0.998321\n",
            "[42]\tvalidation_0-auc:0.998348\n",
            "[43]\tvalidation_0-auc:0.998321\n",
            "[44]\tvalidation_0-auc:0.998402\n",
            "[45]\tvalidation_0-auc:0.998402\n",
            "[46]\tvalidation_0-auc:0.998509\n",
            "[47]\tvalidation_0-auc:0.998482\n",
            "[48]\tvalidation_0-auc:0.998455\n",
            "[49]\tvalidation_0-auc:0.998442\n",
            "[50]\tvalidation_0-auc:0.998442\n",
            "[51]\tvalidation_0-auc:0.998469\n",
            "[52]\tvalidation_0-auc:0.998388\n",
            "[53]\tvalidation_0-auc:0.998442\n",
            "[54]\tvalidation_0-auc:0.998388\n",
            "[55]\tvalidation_0-auc:0.998415\n",
            "[56]\tvalidation_0-auc:0.998415\n",
            "[57]\tvalidation_0-auc:0.998522\n",
            "[58]\tvalidation_0-auc:0.998522\n",
            "[59]\tvalidation_0-auc:0.998334\n",
            "[60]\tvalidation_0-auc:0.998361\n",
            "[61]\tvalidation_0-auc:0.998361\n",
            "[62]\tvalidation_0-auc:0.998254\n",
            "[63]\tvalidation_0-auc:0.998281\n",
            "[64]\tvalidation_0-auc:0.998281\n",
            "[65]\tvalidation_0-auc:0.998281\n",
            "[66]\tvalidation_0-auc:0.998281\n",
            "[67]\tvalidation_0-auc:0.998281\n",
            "[68]\tvalidation_0-auc:0.998281\n",
            "[69]\tvalidation_0-auc:0.998281\n",
            "[70]\tvalidation_0-auc:0.998308\n",
            "[71]\tvalidation_0-auc:0.998334\n",
            "[72]\tvalidation_0-auc:0.998334\n",
            "[73]\tvalidation_0-auc:0.998334\n",
            "[74]\tvalidation_0-auc:0.998227\n",
            "[75]\tvalidation_0-auc:0.9982\n",
            "[76]\tvalidation_0-auc:0.9982\n",
            "[77]\tvalidation_0-auc:0.998227\n",
            "[78]\tvalidation_0-auc:0.998227\n",
            "[79]\tvalidation_0-auc:0.998254\n",
            "[80]\tvalidation_0-auc:0.998227\n",
            "[81]\tvalidation_0-auc:0.998254\n",
            "[82]\tvalidation_0-auc:0.998254\n",
            "[83]\tvalidation_0-auc:0.998254\n",
            "[84]\tvalidation_0-auc:0.998227\n",
            "[85]\tvalidation_0-auc:0.9982\n",
            "[86]\tvalidation_0-auc:0.998173\n",
            "[87]\tvalidation_0-auc:0.998146\n",
            "[88]\tvalidation_0-auc:0.998119\n",
            "[89]\tvalidation_0-auc:0.998119\n",
            "[90]\tvalidation_0-auc:0.998119\n",
            "[91]\tvalidation_0-auc:0.998066\n",
            "[92]\tvalidation_0-auc:0.998093\n",
            "[93]\tvalidation_0-auc:0.998066\n",
            "[94]\tvalidation_0-auc:0.998066\n",
            "[95]\tvalidation_0-auc:0.998039\n",
            "[96]\tvalidation_0-auc:0.998039\n",
            "[97]\tvalidation_0-auc:0.998066\n",
            "[98]\tvalidation_0-auc:0.998119\n",
            "[99]\tvalidation_0-auc:0.998119\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 15ms/step - loss: 0.6272 - accuracy: 0.6783 - val_loss: 0.5050 - val_accuracy: 0.7943\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 12ms/step - loss: 0.4048 - accuracy: 0.8503 - val_loss: 0.3498 - val_accuracy: 0.8687\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3310 - accuracy: 0.8865 - val_loss: 0.3192 - val_accuracy: 0.8972\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.2797 - accuracy: 0.9089 - val_loss: 0.3508 - val_accuracy: 0.8950\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.2882 - accuracy: 0.9046 - val_loss: 0.3009 - val_accuracy: 0.9015\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 14ms/step - loss: 0.5209 - accuracy: 0.7713 - val_loss: 0.2414 - val_accuracy: 0.9125\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2368 - accuracy: 0.9197 - val_loss: 0.2542 - val_accuracy: 0.9190\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.1986 - accuracy: 0.9354 - val_loss: 0.1942 - val_accuracy: 0.9278\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.1638 - accuracy: 0.9457 - val_loss: 0.1669 - val_accuracy: 0.9431\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.1552 - accuracy: 0.9493 - val_loss: 0.2026 - val_accuracy: 0.9431\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.948186\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.948186\n",
            "[2]\tvalidation_0-auc:0.967733\n",
            "[3]\tvalidation_0-auc:0.967733\n",
            "[4]\tvalidation_0-auc:0.983764\n",
            "[5]\tvalidation_0-auc:0.984775\n",
            "[6]\tvalidation_0-auc:0.984673\n",
            "[7]\tvalidation_0-auc:0.984643\n",
            "[8]\tvalidation_0-auc:0.985068\n",
            "[9]\tvalidation_0-auc:0.98797\n",
            "[10]\tvalidation_0-auc:0.986695\n",
            "[11]\tvalidation_0-auc:0.986431\n",
            "[12]\tvalidation_0-auc:0.986724\n",
            "[13]\tvalidation_0-auc:0.986871\n",
            "[14]\tvalidation_0-auc:0.987134\n",
            "[15]\tvalidation_0-auc:0.98794\n",
            "[16]\tvalidation_0-auc:0.987823\n",
            "[17]\tvalidation_0-auc:0.987559\n",
            "[18]\tvalidation_0-auc:0.987501\n",
            "[19]\tvalidation_0-auc:0.988951\n",
            "[20]\tvalidation_0-auc:0.988805\n",
            "[21]\tvalidation_0-auc:0.988805\n",
            "[22]\tvalidation_0-auc:0.989039\n",
            "[23]\tvalidation_0-auc:0.98901\n",
            "[24]\tvalidation_0-auc:0.989332\n",
            "[25]\tvalidation_0-auc:0.99005\n",
            "[26]\tvalidation_0-auc:0.989406\n",
            "[27]\tvalidation_0-auc:0.990607\n",
            "[28]\tvalidation_0-auc:0.990256\n",
            "[29]\tvalidation_0-auc:0.991428\n",
            "[30]\tvalidation_0-auc:0.991193\n",
            "[31]\tvalidation_0-auc:0.991428\n",
            "[32]\tvalidation_0-auc:0.991472\n",
            "[33]\tvalidation_0-auc:0.99156\n",
            "[34]\tvalidation_0-auc:0.991618\n",
            "[35]\tvalidation_0-auc:0.991853\n",
            "[36]\tvalidation_0-auc:0.991911\n",
            "[37]\tvalidation_0-auc:0.991794\n",
            "[38]\tvalidation_0-auc:0.991823\n",
            "[39]\tvalidation_0-auc:0.991823\n",
            "[40]\tvalidation_0-auc:0.991853\n",
            "[41]\tvalidation_0-auc:0.991941\n",
            "[42]\tvalidation_0-auc:0.991823\n",
            "[43]\tvalidation_0-auc:0.991941\n",
            "[44]\tvalidation_0-auc:0.991941\n",
            "[45]\tvalidation_0-auc:0.992029\n",
            "[46]\tvalidation_0-auc:0.992058\n",
            "[47]\tvalidation_0-auc:0.992117\n",
            "[48]\tvalidation_0-auc:0.992117\n",
            "[49]\tvalidation_0-auc:0.992263\n",
            "[50]\tvalidation_0-auc:0.992175\n",
            "[51]\tvalidation_0-auc:0.992292\n",
            "[52]\tvalidation_0-auc:0.992292\n",
            "[53]\tvalidation_0-auc:0.99238\n",
            "[54]\tvalidation_0-auc:0.992498\n",
            "[55]\tvalidation_0-auc:0.992351\n",
            "[56]\tvalidation_0-auc:0.992351\n",
            "[57]\tvalidation_0-auc:0.992292\n",
            "[58]\tvalidation_0-auc:0.992585\n",
            "[59]\tvalidation_0-auc:0.992351\n",
            "[60]\tvalidation_0-auc:0.992556\n",
            "[61]\tvalidation_0-auc:0.99241\n",
            "[62]\tvalidation_0-auc:0.992322\n",
            "[63]\tvalidation_0-auc:0.992322\n",
            "[64]\tvalidation_0-auc:0.992234\n",
            "[65]\tvalidation_0-auc:0.992204\n",
            "[66]\tvalidation_0-auc:0.992204\n",
            "[67]\tvalidation_0-auc:0.992204\n",
            "[68]\tvalidation_0-auc:0.992087\n",
            "[69]\tvalidation_0-auc:0.992117\n",
            "[70]\tvalidation_0-auc:0.992117\n",
            "[71]\tvalidation_0-auc:0.992087\n",
            "[72]\tvalidation_0-auc:0.992117\n",
            "[73]\tvalidation_0-auc:0.992087\n",
            "[74]\tvalidation_0-auc:0.992029\n",
            "[75]\tvalidation_0-auc:0.992058\n",
            "[76]\tvalidation_0-auc:0.992117\n",
            "[77]\tvalidation_0-auc:0.992146\n",
            "[78]\tvalidation_0-auc:0.992117\n",
            "[79]\tvalidation_0-auc:0.992146\n",
            "[80]\tvalidation_0-auc:0.992175\n",
            "[81]\tvalidation_0-auc:0.992146\n",
            "[82]\tvalidation_0-auc:0.992234\n",
            "[83]\tvalidation_0-auc:0.992234\n",
            "[84]\tvalidation_0-auc:0.992175\n",
            "[85]\tvalidation_0-auc:0.992146\n",
            "[86]\tvalidation_0-auc:0.992146\n",
            "[87]\tvalidation_0-auc:0.992263\n",
            "[88]\tvalidation_0-auc:0.992292\n",
            "[89]\tvalidation_0-auc:0.992263\n",
            "[90]\tvalidation_0-auc:0.992322\n",
            "[91]\tvalidation_0-auc:0.99241\n",
            "[92]\tvalidation_0-auc:0.992615\n",
            "[93]\tvalidation_0-auc:0.992585\n",
            "[94]\tvalidation_0-auc:0.992527\n",
            "[95]\tvalidation_0-auc:0.992498\n",
            "[96]\tvalidation_0-auc:0.992498\n",
            "[97]\tvalidation_0-auc:0.992615\n",
            "[98]\tvalidation_0-auc:0.992644\n",
            "[99]\tvalidation_0-auc:0.992615\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.9612244897959183 | 0.9564164648910412 | 0.9974747474747475 | 0.9765142150803462 |\n",
            "|     GRU 0.1      | 0.9734693877551021 | 0.9848101265822785 | 0.9823232323232324 | 0.9835651074589128 |\n",
            "|   XGBoost 0.1    | 0.9795918367346939 | 0.9948717948717949 | 0.9797979797979798 | 0.9872773536895675 |\n",
            "|    Logreg 0.1    | 0.9224489795918367 | 0.9124423963133641 |        1.0         | 0.9542168674698795 |\n",
            "|     SVM 0.1      | 0.9755102040816327 | 0.9848484848484849 | 0.9848484848484849 | 0.9848484848484849 |\n",
            "|  LSTM beta 0.1   | 0.9015317286652079 | 0.9297297297297298 | 0.9476584022038568 | 0.9386084583901774 |\n",
            "|   GRU beta 0.1   | 0.9431072210065645 | 0.9667590027700831 | 0.9614325068870524 | 0.9640883977900552 |\n",
            "| XGBoost beta 0.1 | 0.949671772428884  | 0.9913294797687862 | 0.9449035812672176 | 0.9675599435825105 |\n",
            "| logreg beta 0.1  | 0.9452954048140044 | 0.9355670103092784 |        1.0         | 0.966711051930759  |\n",
            "|   svm beta 0.1   | 0.9387308533916849 | 0.9539295392953929 | 0.9696969696969697 | 0.9617486338797815 |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 15ms/step - loss: 0.6291 - accuracy: 0.6852 - val_loss: 0.5324 - val_accuracy: 0.8082\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.5801 - accuracy: 0.7254 - val_loss: 0.4301 - val_accuracy: 0.7469\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.3014 - accuracy: 0.8817 - val_loss: 0.0895 - val_accuracy: 0.9673\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.1774 - accuracy: 0.9479 - val_loss: 0.0917 - val_accuracy: 0.9653\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.2024 - accuracy: 0.9296 - val_loss: 0.0844 - val_accuracy: 0.9673\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 18ms/step - loss: 0.6262 - accuracy: 0.6775 - val_loss: 0.5003 - val_accuracy: 0.8082\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.4185 - accuracy: 0.8124 - val_loss: 0.1008 - val_accuracy: 0.9592\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.1548 - accuracy: 0.9491 - val_loss: 0.0955 - val_accuracy: 0.9490\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.1603 - accuracy: 0.9420 - val_loss: 0.0787 - val_accuracy: 0.9571\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.1319 - accuracy: 0.9556 - val_loss: 0.0702 - val_accuracy: 0.9694\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.991403\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.991605\n",
            "[2]\tvalidation_0-auc:0.991605\n",
            "[3]\tvalidation_0-auc:0.991565\n",
            "[4]\tvalidation_0-auc:0.992048\n",
            "[5]\tvalidation_0-auc:0.991712\n",
            "[6]\tvalidation_0-auc:0.991645\n",
            "[7]\tvalidation_0-auc:0.993123\n",
            "[8]\tvalidation_0-auc:0.993002\n",
            "[9]\tvalidation_0-auc:0.993109\n",
            "[10]\tvalidation_0-auc:0.993029\n",
            "[11]\tvalidation_0-auc:0.997945\n",
            "[12]\tvalidation_0-auc:0.998106\n",
            "[13]\tvalidation_0-auc:0.99816\n",
            "[14]\tvalidation_0-auc:0.998214\n",
            "[15]\tvalidation_0-auc:0.998321\n",
            "[16]\tvalidation_0-auc:0.99824\n",
            "[17]\tvalidation_0-auc:0.998187\n",
            "[18]\tvalidation_0-auc:0.99824\n",
            "[19]\tvalidation_0-auc:0.99824\n",
            "[20]\tvalidation_0-auc:0.998214\n",
            "[21]\tvalidation_0-auc:0.998187\n",
            "[22]\tvalidation_0-auc:0.998133\n",
            "[23]\tvalidation_0-auc:0.998133\n",
            "[24]\tvalidation_0-auc:0.998187\n",
            "[25]\tvalidation_0-auc:0.998106\n",
            "[26]\tvalidation_0-auc:0.998106\n",
            "[27]\tvalidation_0-auc:0.99816\n",
            "[28]\tvalidation_0-auc:0.99824\n",
            "[29]\tvalidation_0-auc:0.998187\n",
            "[30]\tvalidation_0-auc:0.998133\n",
            "[31]\tvalidation_0-auc:0.998106\n",
            "[32]\tvalidation_0-auc:0.99816\n",
            "[33]\tvalidation_0-auc:0.998133\n",
            "[34]\tvalidation_0-auc:0.9982\n",
            "[35]\tvalidation_0-auc:0.9982\n",
            "[36]\tvalidation_0-auc:0.998254\n",
            "[37]\tvalidation_0-auc:0.998375\n",
            "[38]\tvalidation_0-auc:0.998402\n",
            "[39]\tvalidation_0-auc:0.998348\n",
            "[40]\tvalidation_0-auc:0.998321\n",
            "[41]\tvalidation_0-auc:0.998321\n",
            "[42]\tvalidation_0-auc:0.998348\n",
            "[43]\tvalidation_0-auc:0.998321\n",
            "[44]\tvalidation_0-auc:0.998402\n",
            "[45]\tvalidation_0-auc:0.998402\n",
            "[46]\tvalidation_0-auc:0.998509\n",
            "[47]\tvalidation_0-auc:0.998482\n",
            "[48]\tvalidation_0-auc:0.998455\n",
            "[49]\tvalidation_0-auc:0.998442\n",
            "[50]\tvalidation_0-auc:0.998442\n",
            "[51]\tvalidation_0-auc:0.998469\n",
            "[52]\tvalidation_0-auc:0.998388\n",
            "[53]\tvalidation_0-auc:0.998442\n",
            "[54]\tvalidation_0-auc:0.998388\n",
            "[55]\tvalidation_0-auc:0.998415\n",
            "[56]\tvalidation_0-auc:0.998415\n",
            "[57]\tvalidation_0-auc:0.998522\n",
            "[58]\tvalidation_0-auc:0.998522\n",
            "[59]\tvalidation_0-auc:0.998334\n",
            "[60]\tvalidation_0-auc:0.998361\n",
            "[61]\tvalidation_0-auc:0.998361\n",
            "[62]\tvalidation_0-auc:0.998254\n",
            "[63]\tvalidation_0-auc:0.998281\n",
            "[64]\tvalidation_0-auc:0.998281\n",
            "[65]\tvalidation_0-auc:0.998281\n",
            "[66]\tvalidation_0-auc:0.998281\n",
            "[67]\tvalidation_0-auc:0.998281\n",
            "[68]\tvalidation_0-auc:0.998281\n",
            "[69]\tvalidation_0-auc:0.998281\n",
            "[70]\tvalidation_0-auc:0.998308\n",
            "[71]\tvalidation_0-auc:0.998334\n",
            "[72]\tvalidation_0-auc:0.998334\n",
            "[73]\tvalidation_0-auc:0.998334\n",
            "[74]\tvalidation_0-auc:0.998227\n",
            "[75]\tvalidation_0-auc:0.9982\n",
            "[76]\tvalidation_0-auc:0.9982\n",
            "[77]\tvalidation_0-auc:0.998227\n",
            "[78]\tvalidation_0-auc:0.998227\n",
            "[79]\tvalidation_0-auc:0.998254\n",
            "[80]\tvalidation_0-auc:0.998227\n",
            "[81]\tvalidation_0-auc:0.998254\n",
            "[82]\tvalidation_0-auc:0.998254\n",
            "[83]\tvalidation_0-auc:0.998254\n",
            "[84]\tvalidation_0-auc:0.998227\n",
            "[85]\tvalidation_0-auc:0.9982\n",
            "[86]\tvalidation_0-auc:0.998173\n",
            "[87]\tvalidation_0-auc:0.998146\n",
            "[88]\tvalidation_0-auc:0.998119\n",
            "[89]\tvalidation_0-auc:0.998119\n",
            "[90]\tvalidation_0-auc:0.998119\n",
            "[91]\tvalidation_0-auc:0.998066\n",
            "[92]\tvalidation_0-auc:0.998093\n",
            "[93]\tvalidation_0-auc:0.998066\n",
            "[94]\tvalidation_0-auc:0.998066\n",
            "[95]\tvalidation_0-auc:0.998039\n",
            "[96]\tvalidation_0-auc:0.998039\n",
            "[97]\tvalidation_0-auc:0.998066\n",
            "[98]\tvalidation_0-auc:0.998119\n",
            "[99]\tvalidation_0-auc:0.998119\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 16ms/step - loss: 0.6358 - accuracy: 0.6783 - val_loss: 0.5688 - val_accuracy: 0.7943\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.5153 - accuracy: 0.7489 - val_loss: 0.3125 - val_accuracy: 0.8775\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3191 - accuracy: 0.9010 - val_loss: 0.2880 - val_accuracy: 0.8972\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.2882 - accuracy: 0.9053 - val_loss: 0.2897 - val_accuracy: 0.9015\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 12ms/step - loss: 0.2706 - accuracy: 0.9095 - val_loss: 0.2595 - val_accuracy: 0.9103\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 14ms/step - loss: 0.5606 - accuracy: 0.7290 - val_loss: 0.3464 - val_accuracy: 0.8972\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2960 - accuracy: 0.9083 - val_loss: 0.2165 - val_accuracy: 0.9234\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.2313 - accuracy: 0.9276 - val_loss: 0.2128 - val_accuracy: 0.9278\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.1902 - accuracy: 0.9439 - val_loss: 0.2620 - val_accuracy: 0.9147\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.1754 - accuracy: 0.9493 - val_loss: 0.2098 - val_accuracy: 0.9365\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.948186\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.948186\n",
            "[2]\tvalidation_0-auc:0.967733\n",
            "[3]\tvalidation_0-auc:0.967733\n",
            "[4]\tvalidation_0-auc:0.983764\n",
            "[5]\tvalidation_0-auc:0.984775\n",
            "[6]\tvalidation_0-auc:0.984673\n",
            "[7]\tvalidation_0-auc:0.984643\n",
            "[8]\tvalidation_0-auc:0.985068\n",
            "[9]\tvalidation_0-auc:0.98797\n",
            "[10]\tvalidation_0-auc:0.986695\n",
            "[11]\tvalidation_0-auc:0.986431\n",
            "[12]\tvalidation_0-auc:0.986724\n",
            "[13]\tvalidation_0-auc:0.986871\n",
            "[14]\tvalidation_0-auc:0.987134\n",
            "[15]\tvalidation_0-auc:0.98794\n",
            "[16]\tvalidation_0-auc:0.987823\n",
            "[17]\tvalidation_0-auc:0.987559\n",
            "[18]\tvalidation_0-auc:0.987501\n",
            "[19]\tvalidation_0-auc:0.988951\n",
            "[20]\tvalidation_0-auc:0.988805\n",
            "[21]\tvalidation_0-auc:0.988805\n",
            "[22]\tvalidation_0-auc:0.989039\n",
            "[23]\tvalidation_0-auc:0.98901\n",
            "[24]\tvalidation_0-auc:0.989332\n",
            "[25]\tvalidation_0-auc:0.99005\n",
            "[26]\tvalidation_0-auc:0.989406\n",
            "[27]\tvalidation_0-auc:0.990607\n",
            "[28]\tvalidation_0-auc:0.990256\n",
            "[29]\tvalidation_0-auc:0.991428\n",
            "[30]\tvalidation_0-auc:0.991193\n",
            "[31]\tvalidation_0-auc:0.991428\n",
            "[32]\tvalidation_0-auc:0.991472\n",
            "[33]\tvalidation_0-auc:0.99156\n",
            "[34]\tvalidation_0-auc:0.991618\n",
            "[35]\tvalidation_0-auc:0.991853\n",
            "[36]\tvalidation_0-auc:0.991911\n",
            "[37]\tvalidation_0-auc:0.991794\n",
            "[38]\tvalidation_0-auc:0.991823\n",
            "[39]\tvalidation_0-auc:0.991823\n",
            "[40]\tvalidation_0-auc:0.991853\n",
            "[41]\tvalidation_0-auc:0.991941\n",
            "[42]\tvalidation_0-auc:0.991823\n",
            "[43]\tvalidation_0-auc:0.991941\n",
            "[44]\tvalidation_0-auc:0.991941\n",
            "[45]\tvalidation_0-auc:0.992029\n",
            "[46]\tvalidation_0-auc:0.992058\n",
            "[47]\tvalidation_0-auc:0.992117\n",
            "[48]\tvalidation_0-auc:0.992117\n",
            "[49]\tvalidation_0-auc:0.992263\n",
            "[50]\tvalidation_0-auc:0.992175\n",
            "[51]\tvalidation_0-auc:0.992292\n",
            "[52]\tvalidation_0-auc:0.992292\n",
            "[53]\tvalidation_0-auc:0.99238\n",
            "[54]\tvalidation_0-auc:0.992498\n",
            "[55]\tvalidation_0-auc:0.992351\n",
            "[56]\tvalidation_0-auc:0.992351\n",
            "[57]\tvalidation_0-auc:0.992292\n",
            "[58]\tvalidation_0-auc:0.992585\n",
            "[59]\tvalidation_0-auc:0.992351\n",
            "[60]\tvalidation_0-auc:0.992556\n",
            "[61]\tvalidation_0-auc:0.99241\n",
            "[62]\tvalidation_0-auc:0.992322\n",
            "[63]\tvalidation_0-auc:0.992322\n",
            "[64]\tvalidation_0-auc:0.992234\n",
            "[65]\tvalidation_0-auc:0.992204\n",
            "[66]\tvalidation_0-auc:0.992204\n",
            "[67]\tvalidation_0-auc:0.992204\n",
            "[68]\tvalidation_0-auc:0.992087\n",
            "[69]\tvalidation_0-auc:0.992117\n",
            "[70]\tvalidation_0-auc:0.992117\n",
            "[71]\tvalidation_0-auc:0.992087\n",
            "[72]\tvalidation_0-auc:0.992117\n",
            "[73]\tvalidation_0-auc:0.992087\n",
            "[74]\tvalidation_0-auc:0.992029\n",
            "[75]\tvalidation_0-auc:0.992058\n",
            "[76]\tvalidation_0-auc:0.992117\n",
            "[77]\tvalidation_0-auc:0.992146\n",
            "[78]\tvalidation_0-auc:0.992117\n",
            "[79]\tvalidation_0-auc:0.992146\n",
            "[80]\tvalidation_0-auc:0.992175\n",
            "[81]\tvalidation_0-auc:0.992146\n",
            "[82]\tvalidation_0-auc:0.992234\n",
            "[83]\tvalidation_0-auc:0.992234\n",
            "[84]\tvalidation_0-auc:0.992175\n",
            "[85]\tvalidation_0-auc:0.992146\n",
            "[86]\tvalidation_0-auc:0.992146\n",
            "[87]\tvalidation_0-auc:0.992263\n",
            "[88]\tvalidation_0-auc:0.992292\n",
            "[89]\tvalidation_0-auc:0.992263\n",
            "[90]\tvalidation_0-auc:0.992322\n",
            "[91]\tvalidation_0-auc:0.99241\n",
            "[92]\tvalidation_0-auc:0.992615\n",
            "[93]\tvalidation_0-auc:0.992585\n",
            "[94]\tvalidation_0-auc:0.992527\n",
            "[95]\tvalidation_0-auc:0.992498\n",
            "[96]\tvalidation_0-auc:0.992498\n",
            "[97]\tvalidation_0-auc:0.992615\n",
            "[98]\tvalidation_0-auc:0.992644\n",
            "[99]\tvalidation_0-auc:0.992615\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.9673469387755103 | 0.9822335025380711 | 0.9772727272727273 | 0.979746835443038  |\n",
            "|      GRU 0.15     | 0.9693877551020408 | 0.9872122762148338 | 0.9747474747474747 | 0.9809402795425667 |\n",
            "|    XGBoost 0.15   | 0.9795918367346939 | 0.9948717948717949 | 0.9797979797979798 | 0.9872773536895675 |\n",
            "|    Logreg 0.15    | 0.9224489795918367 | 0.9124423963133641 |        1.0         | 0.9542168674698795 |\n",
            "|      SVM 0.15     | 0.9755102040816327 | 0.9848484848484849 | 0.9848484848484849 | 0.9848484848484849 |\n",
            "|   LSTM beta 0.15  | 0.9102844638949672 |       0.9375       | 0.9504132231404959 | 0.9439124487004104 |\n",
            "|   GRU beta 0.15   | 0.936542669584245  | 0.9587912087912088 | 0.9614325068870524 | 0.9601100412654747 |\n",
            "| XGBoost beta 0.15 | 0.949671772428884  | 0.9913294797687862 | 0.9449035812672176 | 0.9675599435825105 |\n",
            "|  logreg beta 0.15 | 0.9452954048140044 | 0.9355670103092784 |        1.0         | 0.966711051930759  |\n",
            "|   svm beta 0.15   | 0.9387308533916849 | 0.9539295392953929 | 0.9696969696969697 | 0.9617486338797815 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979
        },
        "id": "6mWRVqUbCs-B",
        "outputId": "6af020d1-f37d-4179-d05e-08840ae11ce7"
      },
      "source": [
        "Result_cross.to_csv('AAPL_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.984334</td>\n",
              "      <td>0.948980</td>\n",
              "      <td>0.967908</td>\n",
              "      <td>0.952020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.987374</td>\n",
              "      <td>0.979592</td>\n",
              "      <td>0.987374</td>\n",
              "      <td>0.987374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.994872</td>\n",
              "      <td>0.979592</td>\n",
              "      <td>0.987277</td>\n",
              "      <td>0.979798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.912442</td>\n",
              "      <td>0.922449</td>\n",
              "      <td>0.954217</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.984848</td>\n",
              "      <td>0.975510</td>\n",
              "      <td>0.984848</td>\n",
              "      <td>0.984848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.934426</td>\n",
              "      <td>0.901532</td>\n",
              "      <td>0.938272</td>\n",
              "      <td>0.942149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.976879</td>\n",
              "      <td>0.927790</td>\n",
              "      <td>0.953456</td>\n",
              "      <td>0.931129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.991329</td>\n",
              "      <td>0.949672</td>\n",
              "      <td>0.967560</td>\n",
              "      <td>0.944904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.935567</td>\n",
              "      <td>0.945295</td>\n",
              "      <td>0.966711</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.953930</td>\n",
              "      <td>0.938731</td>\n",
              "      <td>0.961749</td>\n",
              "      <td>0.969697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.956416</td>\n",
              "      <td>0.961224</td>\n",
              "      <td>0.976514</td>\n",
              "      <td>0.997475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.984810</td>\n",
              "      <td>0.973469</td>\n",
              "      <td>0.983565</td>\n",
              "      <td>0.982323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.994872</td>\n",
              "      <td>0.979592</td>\n",
              "      <td>0.987277</td>\n",
              "      <td>0.979798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.912442</td>\n",
              "      <td>0.922449</td>\n",
              "      <td>0.954217</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.984848</td>\n",
              "      <td>0.975510</td>\n",
              "      <td>0.984848</td>\n",
              "      <td>0.984848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.929730</td>\n",
              "      <td>0.901532</td>\n",
              "      <td>0.938608</td>\n",
              "      <td>0.947658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.966759</td>\n",
              "      <td>0.943107</td>\n",
              "      <td>0.964088</td>\n",
              "      <td>0.961433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.991329</td>\n",
              "      <td>0.949672</td>\n",
              "      <td>0.967560</td>\n",
              "      <td>0.944904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.935567</td>\n",
              "      <td>0.945295</td>\n",
              "      <td>0.966711</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.953930</td>\n",
              "      <td>0.938731</td>\n",
              "      <td>0.961749</td>\n",
              "      <td>0.969697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.982234</td>\n",
              "      <td>0.967347</td>\n",
              "      <td>0.979747</td>\n",
              "      <td>0.977273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.987212</td>\n",
              "      <td>0.969388</td>\n",
              "      <td>0.980940</td>\n",
              "      <td>0.974747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.994872</td>\n",
              "      <td>0.979592</td>\n",
              "      <td>0.987277</td>\n",
              "      <td>0.979798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.912442</td>\n",
              "      <td>0.922449</td>\n",
              "      <td>0.954217</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.984848</td>\n",
              "      <td>0.975510</td>\n",
              "      <td>0.984848</td>\n",
              "      <td>0.984848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.910284</td>\n",
              "      <td>0.943912</td>\n",
              "      <td>0.950413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.958791</td>\n",
              "      <td>0.936543</td>\n",
              "      <td>0.960110</td>\n",
              "      <td>0.961433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.991329</td>\n",
              "      <td>0.949672</td>\n",
              "      <td>0.967560</td>\n",
              "      <td>0.944904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.935567</td>\n",
              "      <td>0.945295</td>\n",
              "      <td>0.966711</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.953930</td>\n",
              "      <td>0.938731</td>\n",
              "      <td>0.961749</td>\n",
              "      <td>0.969697</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0          LSTM 0.05  AAPL  0.984334  0.948980  0.967908  0.952020\n",
              "1           GRU 0.05  AAPL  0.987374  0.979592  0.987374  0.987374\n",
              "2       XGBoost 0.05  AAPL  0.994872  0.979592  0.987277  0.979798\n",
              "3        Logreg 0.05  AAPL  0.912442  0.922449  0.954217  1.000000\n",
              "4           SVM 0.05  AAPL  0.984848  0.975510  0.984848  0.984848\n",
              "5     LSTM beta 0.05  AAPL  0.934426  0.901532  0.938272  0.942149\n",
              "6      GRU beta 0.05  AAPL  0.976879  0.927790  0.953456  0.931129\n",
              "7  XGBoost beta 0.05  AAPL  0.991329  0.949672  0.967560  0.944904\n",
              "8   logreg beta 0.05  AAPL  0.935567  0.945295  0.966711  1.000000\n",
              "9      svm beta 0.05  AAPL  0.953930  0.938731  0.961749  0.969697\n",
              "0           LSTM 0.1  AAPL  0.956416  0.961224  0.976514  0.997475\n",
              "1            GRU 0.1  AAPL  0.984810  0.973469  0.983565  0.982323\n",
              "2        XGBoost 0.1  AAPL  0.994872  0.979592  0.987277  0.979798\n",
              "3         Logreg 0.1  AAPL  0.912442  0.922449  0.954217  1.000000\n",
              "4            SVM 0.1  AAPL  0.984848  0.975510  0.984848  0.984848\n",
              "5      LSTM beta 0.1  AAPL  0.929730  0.901532  0.938608  0.947658\n",
              "6       GRU beta 0.1  AAPL  0.966759  0.943107  0.964088  0.961433\n",
              "7   XGBoost beta 0.1  AAPL  0.991329  0.949672  0.967560  0.944904\n",
              "8    logreg beta 0.1  AAPL  0.935567  0.945295  0.966711  1.000000\n",
              "9       svm beta 0.1  AAPL  0.953930  0.938731  0.961749  0.969697\n",
              "0          LSTM 0.15  AAPL  0.982234  0.967347  0.979747  0.977273\n",
              "1           GRU 0.15  AAPL  0.987212  0.969388  0.980940  0.974747\n",
              "2       XGBoost 0.15  AAPL  0.994872  0.979592  0.987277  0.979798\n",
              "3        Logreg 0.15  AAPL  0.912442  0.922449  0.954217  1.000000\n",
              "4           SVM 0.15  AAPL  0.984848  0.975510  0.984848  0.984848\n",
              "5     LSTM beta 0.15  AAPL  0.937500  0.910284  0.943912  0.950413\n",
              "6      GRU beta 0.15  AAPL  0.958791  0.936543  0.960110  0.961433\n",
              "7  XGBoost beta 0.15  AAPL  0.991329  0.949672  0.967560  0.944904\n",
              "8   logreg beta 0.15  AAPL  0.935567  0.945295  0.966711  1.000000\n",
              "9      svm beta 0.15  AAPL  0.953930  0.938731  0.961749  0.969697"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhE6Y93kCs-C"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-Cry4swCs-C"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whq58hnjCs-C",
        "outputId": "867197ed-6c24-487a-8fa0-a85d50f944f8"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  historical = Train_data(dfs[col_name], train_start=100, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"AAPL\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.05\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 15ms/step - loss: 0.6276 - accuracy: 0.6858 - val_loss: 0.5333 - val_accuracy: 0.8082\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.6250 - accuracy: 0.6899 - val_loss: 0.5143 - val_accuracy: 0.8082\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.4456 - accuracy: 0.8012 - val_loss: 0.1430 - val_accuracy: 0.9429\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.2109 - accuracy: 0.9272 - val_loss: 0.0845 - val_accuracy: 0.9755\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.1707 - accuracy: 0.9420 - val_loss: 0.0847 - val_accuracy: 0.9755\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 13ms/step - loss: 0.6255 - accuracy: 0.6923 - val_loss: 0.5862 - val_accuracy: 0.8082\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.4835 - accuracy: 0.7817 - val_loss: 0.1955 - val_accuracy: 0.9286\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.1731 - accuracy: 0.9444 - val_loss: 0.0788 - val_accuracy: 0.9714\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.1447 - accuracy: 0.9527 - val_loss: 0.0768 - val_accuracy: 0.9673\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.1332 - accuracy: 0.9521 - val_loss: 0.0705 - val_accuracy: 0.9673\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.991403\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.991605\n",
            "[2]\tvalidation_0-auc:0.991605\n",
            "[3]\tvalidation_0-auc:0.991565\n",
            "[4]\tvalidation_0-auc:0.992048\n",
            "[5]\tvalidation_0-auc:0.991712\n",
            "[6]\tvalidation_0-auc:0.991645\n",
            "[7]\tvalidation_0-auc:0.993123\n",
            "[8]\tvalidation_0-auc:0.993002\n",
            "[9]\tvalidation_0-auc:0.993109\n",
            "[10]\tvalidation_0-auc:0.993029\n",
            "[11]\tvalidation_0-auc:0.997945\n",
            "[12]\tvalidation_0-auc:0.998106\n",
            "[13]\tvalidation_0-auc:0.99816\n",
            "[14]\tvalidation_0-auc:0.998214\n",
            "[15]\tvalidation_0-auc:0.998321\n",
            "[16]\tvalidation_0-auc:0.99824\n",
            "[17]\tvalidation_0-auc:0.998187\n",
            "[18]\tvalidation_0-auc:0.99824\n",
            "[19]\tvalidation_0-auc:0.99824\n",
            "[20]\tvalidation_0-auc:0.998214\n",
            "[21]\tvalidation_0-auc:0.998187\n",
            "[22]\tvalidation_0-auc:0.998133\n",
            "[23]\tvalidation_0-auc:0.998133\n",
            "[24]\tvalidation_0-auc:0.998187\n",
            "[25]\tvalidation_0-auc:0.998106\n",
            "[26]\tvalidation_0-auc:0.998106\n",
            "[27]\tvalidation_0-auc:0.99816\n",
            "[28]\tvalidation_0-auc:0.99824\n",
            "[29]\tvalidation_0-auc:0.998187\n",
            "[30]\tvalidation_0-auc:0.998133\n",
            "[31]\tvalidation_0-auc:0.998106\n",
            "[32]\tvalidation_0-auc:0.99816\n",
            "[33]\tvalidation_0-auc:0.998133\n",
            "[34]\tvalidation_0-auc:0.9982\n",
            "[35]\tvalidation_0-auc:0.9982\n",
            "[36]\tvalidation_0-auc:0.998254\n",
            "[37]\tvalidation_0-auc:0.998375\n",
            "[38]\tvalidation_0-auc:0.998402\n",
            "[39]\tvalidation_0-auc:0.998348\n",
            "[40]\tvalidation_0-auc:0.998321\n",
            "[41]\tvalidation_0-auc:0.998321\n",
            "[42]\tvalidation_0-auc:0.998348\n",
            "[43]\tvalidation_0-auc:0.998321\n",
            "[44]\tvalidation_0-auc:0.998402\n",
            "[45]\tvalidation_0-auc:0.998402\n",
            "[46]\tvalidation_0-auc:0.998509\n",
            "[47]\tvalidation_0-auc:0.998482\n",
            "[48]\tvalidation_0-auc:0.998455\n",
            "[49]\tvalidation_0-auc:0.998442\n",
            "[50]\tvalidation_0-auc:0.998442\n",
            "[51]\tvalidation_0-auc:0.998469\n",
            "[52]\tvalidation_0-auc:0.998388\n",
            "[53]\tvalidation_0-auc:0.998442\n",
            "[54]\tvalidation_0-auc:0.998388\n",
            "[55]\tvalidation_0-auc:0.998415\n",
            "[56]\tvalidation_0-auc:0.998415\n",
            "[57]\tvalidation_0-auc:0.998522\n",
            "[58]\tvalidation_0-auc:0.998522\n",
            "[59]\tvalidation_0-auc:0.998334\n",
            "[60]\tvalidation_0-auc:0.998361\n",
            "[61]\tvalidation_0-auc:0.998361\n",
            "[62]\tvalidation_0-auc:0.998254\n",
            "[63]\tvalidation_0-auc:0.998281\n",
            "[64]\tvalidation_0-auc:0.998281\n",
            "[65]\tvalidation_0-auc:0.998281\n",
            "[66]\tvalidation_0-auc:0.998281\n",
            "[67]\tvalidation_0-auc:0.998281\n",
            "[68]\tvalidation_0-auc:0.998281\n",
            "[69]\tvalidation_0-auc:0.998281\n",
            "[70]\tvalidation_0-auc:0.998308\n",
            "[71]\tvalidation_0-auc:0.998334\n",
            "[72]\tvalidation_0-auc:0.998334\n",
            "[73]\tvalidation_0-auc:0.998334\n",
            "[74]\tvalidation_0-auc:0.998227\n",
            "[75]\tvalidation_0-auc:0.9982\n",
            "[76]\tvalidation_0-auc:0.9982\n",
            "[77]\tvalidation_0-auc:0.998227\n",
            "[78]\tvalidation_0-auc:0.998227\n",
            "[79]\tvalidation_0-auc:0.998254\n",
            "[80]\tvalidation_0-auc:0.998227\n",
            "[81]\tvalidation_0-auc:0.998254\n",
            "[82]\tvalidation_0-auc:0.998254\n",
            "[83]\tvalidation_0-auc:0.998254\n",
            "[84]\tvalidation_0-auc:0.998227\n",
            "[85]\tvalidation_0-auc:0.9982\n",
            "[86]\tvalidation_0-auc:0.998173\n",
            "[87]\tvalidation_0-auc:0.998146\n",
            "[88]\tvalidation_0-auc:0.998119\n",
            "[89]\tvalidation_0-auc:0.998119\n",
            "[90]\tvalidation_0-auc:0.998119\n",
            "[91]\tvalidation_0-auc:0.998066\n",
            "[92]\tvalidation_0-auc:0.998093\n",
            "[93]\tvalidation_0-auc:0.998066\n",
            "[94]\tvalidation_0-auc:0.998066\n",
            "[95]\tvalidation_0-auc:0.998039\n",
            "[96]\tvalidation_0-auc:0.998039\n",
            "[97]\tvalidation_0-auc:0.998066\n",
            "[98]\tvalidation_0-auc:0.998119\n",
            "[99]\tvalidation_0-auc:0.998119\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 15ms/step - loss: 0.6255 - accuracy: 0.6759 - val_loss: 0.4969 - val_accuracy: 0.7943\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3715 - accuracy: 0.8612 - val_loss: 0.3141 - val_accuracy: 0.8972\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.4059 - accuracy: 0.8642 - val_loss: 0.3052 - val_accuracy: 0.8950\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3260 - accuracy: 0.8986 - val_loss: 0.3160 - val_accuracy: 0.8972\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.2843 - accuracy: 0.9089 - val_loss: 0.3077 - val_accuracy: 0.8972\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 13ms/step - loss: 0.5554 - accuracy: 0.7218 - val_loss: 0.2432 - val_accuracy: 0.9190\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2759 - accuracy: 0.9131 - val_loss: 0.1753 - val_accuracy: 0.9387\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2110 - accuracy: 0.9282 - val_loss: 0.1917 - val_accuracy: 0.9409\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.1804 - accuracy: 0.9457 - val_loss: 0.1777 - val_accuracy: 0.9431\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.1553 - accuracy: 0.9565 - val_loss: 0.1521 - val_accuracy: 0.9453\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.948186\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.948186\n",
            "[2]\tvalidation_0-auc:0.967733\n",
            "[3]\tvalidation_0-auc:0.967733\n",
            "[4]\tvalidation_0-auc:0.983764\n",
            "[5]\tvalidation_0-auc:0.984775\n",
            "[6]\tvalidation_0-auc:0.984673\n",
            "[7]\tvalidation_0-auc:0.984643\n",
            "[8]\tvalidation_0-auc:0.985068\n",
            "[9]\tvalidation_0-auc:0.98797\n",
            "[10]\tvalidation_0-auc:0.986695\n",
            "[11]\tvalidation_0-auc:0.986431\n",
            "[12]\tvalidation_0-auc:0.986724\n",
            "[13]\tvalidation_0-auc:0.986871\n",
            "[14]\tvalidation_0-auc:0.987134\n",
            "[15]\tvalidation_0-auc:0.98794\n",
            "[16]\tvalidation_0-auc:0.987823\n",
            "[17]\tvalidation_0-auc:0.987559\n",
            "[18]\tvalidation_0-auc:0.987501\n",
            "[19]\tvalidation_0-auc:0.988951\n",
            "[20]\tvalidation_0-auc:0.988805\n",
            "[21]\tvalidation_0-auc:0.988805\n",
            "[22]\tvalidation_0-auc:0.989039\n",
            "[23]\tvalidation_0-auc:0.98901\n",
            "[24]\tvalidation_0-auc:0.989332\n",
            "[25]\tvalidation_0-auc:0.99005\n",
            "[26]\tvalidation_0-auc:0.989406\n",
            "[27]\tvalidation_0-auc:0.990607\n",
            "[28]\tvalidation_0-auc:0.990256\n",
            "[29]\tvalidation_0-auc:0.991428\n",
            "[30]\tvalidation_0-auc:0.991193\n",
            "[31]\tvalidation_0-auc:0.991428\n",
            "[32]\tvalidation_0-auc:0.991472\n",
            "[33]\tvalidation_0-auc:0.99156\n",
            "[34]\tvalidation_0-auc:0.991618\n",
            "[35]\tvalidation_0-auc:0.991853\n",
            "[36]\tvalidation_0-auc:0.991911\n",
            "[37]\tvalidation_0-auc:0.991794\n",
            "[38]\tvalidation_0-auc:0.991823\n",
            "[39]\tvalidation_0-auc:0.991823\n",
            "[40]\tvalidation_0-auc:0.991853\n",
            "[41]\tvalidation_0-auc:0.991941\n",
            "[42]\tvalidation_0-auc:0.991823\n",
            "[43]\tvalidation_0-auc:0.991941\n",
            "[44]\tvalidation_0-auc:0.991941\n",
            "[45]\tvalidation_0-auc:0.992029\n",
            "[46]\tvalidation_0-auc:0.992058\n",
            "[47]\tvalidation_0-auc:0.992117\n",
            "[48]\tvalidation_0-auc:0.992117\n",
            "[49]\tvalidation_0-auc:0.992263\n",
            "[50]\tvalidation_0-auc:0.992175\n",
            "[51]\tvalidation_0-auc:0.992292\n",
            "[52]\tvalidation_0-auc:0.992292\n",
            "[53]\tvalidation_0-auc:0.99238\n",
            "[54]\tvalidation_0-auc:0.992498\n",
            "[55]\tvalidation_0-auc:0.992351\n",
            "[56]\tvalidation_0-auc:0.992351\n",
            "[57]\tvalidation_0-auc:0.992292\n",
            "[58]\tvalidation_0-auc:0.992585\n",
            "[59]\tvalidation_0-auc:0.992351\n",
            "[60]\tvalidation_0-auc:0.992556\n",
            "[61]\tvalidation_0-auc:0.99241\n",
            "[62]\tvalidation_0-auc:0.992322\n",
            "[63]\tvalidation_0-auc:0.992322\n",
            "[64]\tvalidation_0-auc:0.992234\n",
            "[65]\tvalidation_0-auc:0.992204\n",
            "[66]\tvalidation_0-auc:0.992204\n",
            "[67]\tvalidation_0-auc:0.992204\n",
            "[68]\tvalidation_0-auc:0.992087\n",
            "[69]\tvalidation_0-auc:0.992117\n",
            "[70]\tvalidation_0-auc:0.992117\n",
            "[71]\tvalidation_0-auc:0.992087\n",
            "[72]\tvalidation_0-auc:0.992117\n",
            "[73]\tvalidation_0-auc:0.992087\n",
            "[74]\tvalidation_0-auc:0.992029\n",
            "[75]\tvalidation_0-auc:0.992058\n",
            "[76]\tvalidation_0-auc:0.992117\n",
            "[77]\tvalidation_0-auc:0.992146\n",
            "[78]\tvalidation_0-auc:0.992117\n",
            "[79]\tvalidation_0-auc:0.992146\n",
            "[80]\tvalidation_0-auc:0.992175\n",
            "[81]\tvalidation_0-auc:0.992146\n",
            "[82]\tvalidation_0-auc:0.992234\n",
            "[83]\tvalidation_0-auc:0.992234\n",
            "[84]\tvalidation_0-auc:0.992175\n",
            "[85]\tvalidation_0-auc:0.992146\n",
            "[86]\tvalidation_0-auc:0.992146\n",
            "[87]\tvalidation_0-auc:0.992263\n",
            "[88]\tvalidation_0-auc:0.992292\n",
            "[89]\tvalidation_0-auc:0.992263\n",
            "[90]\tvalidation_0-auc:0.992322\n",
            "[91]\tvalidation_0-auc:0.99241\n",
            "[92]\tvalidation_0-auc:0.992615\n",
            "[93]\tvalidation_0-auc:0.992585\n",
            "[94]\tvalidation_0-auc:0.992527\n",
            "[95]\tvalidation_0-auc:0.992498\n",
            "[96]\tvalidation_0-auc:0.992498\n",
            "[97]\tvalidation_0-auc:0.992615\n",
            "[98]\tvalidation_0-auc:0.992644\n",
            "[99]\tvalidation_0-auc:0.992615\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.05     | 0.9755102040816327 | 0.9776119402985075 | 0.9924242424242424 | 0.9849624060150376 |\n",
            "|      GRU 0.05     | 0.9673469387755103 | 0.9871794871794872 | 0.9722222222222222 | 0.9796437659033079 |\n",
            "|    XGBoost 0.05   | 0.9795918367346939 | 0.9948717948717949 | 0.9797979797979798 | 0.9872773536895675 |\n",
            "|    Logreg 0.05    | 0.9224489795918367 | 0.9124423963133641 |        1.0         | 0.9542168674698795 |\n",
            "|      SVM 0.05     | 0.9755102040816327 | 0.9848484848484849 | 0.9848484848484849 | 0.9848484848484849 |\n",
            "|   LSTM beta 0.05  | 0.8971553610503282 | 0.9202127659574468 | 0.953168044077135  | 0.9364005412719892 |\n",
            "|   GRU beta 0.05   | 0.9452954048140044 | 0.9543010752688172 | 0.977961432506887  | 0.9659863945578231 |\n",
            "| XGBoost beta 0.05 | 0.949671772428884  | 0.9913294797687862 | 0.9449035812672176 | 0.9675599435825105 |\n",
            "|  logreg beta 0.05 | 0.9452954048140044 | 0.9355670103092784 |        1.0         | 0.966711051930759  |\n",
            "|   svm beta 0.05   | 0.9387308533916849 | 0.9539295392953929 | 0.9696969696969697 | 0.9617486338797815 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 15ms/step - loss: 0.6301 - accuracy: 0.6870 - val_loss: 0.5520 - val_accuracy: 0.8082\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.6225 - accuracy: 0.6899 - val_loss: 0.5016 - val_accuracy: 0.8082\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.4879 - accuracy: 0.7686 - val_loss: 0.1061 - val_accuracy: 0.9755\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.2240 - accuracy: 0.9219 - val_loss: 0.1014 - val_accuracy: 0.9633\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.2406 - accuracy: 0.9142 - val_loss: 0.2814 - val_accuracy: 0.9510\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 13ms/step - loss: 0.6262 - accuracy: 0.6846 - val_loss: 0.4983 - val_accuracy: 0.8082\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.4997 - accuracy: 0.7621 - val_loss: 0.0845 - val_accuracy: 0.9673\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.1795 - accuracy: 0.9402 - val_loss: 0.0994 - val_accuracy: 0.9490\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.1487 - accuracy: 0.9503 - val_loss: 0.1002 - val_accuracy: 0.9449\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.1438 - accuracy: 0.9497 - val_loss: 0.0931 - val_accuracy: 0.9510\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.991403\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.991605\n",
            "[2]\tvalidation_0-auc:0.991605\n",
            "[3]\tvalidation_0-auc:0.991565\n",
            "[4]\tvalidation_0-auc:0.992048\n",
            "[5]\tvalidation_0-auc:0.991712\n",
            "[6]\tvalidation_0-auc:0.991645\n",
            "[7]\tvalidation_0-auc:0.993123\n",
            "[8]\tvalidation_0-auc:0.993002\n",
            "[9]\tvalidation_0-auc:0.993109\n",
            "[10]\tvalidation_0-auc:0.993029\n",
            "[11]\tvalidation_0-auc:0.997945\n",
            "[12]\tvalidation_0-auc:0.998106\n",
            "[13]\tvalidation_0-auc:0.99816\n",
            "[14]\tvalidation_0-auc:0.998214\n",
            "[15]\tvalidation_0-auc:0.998321\n",
            "[16]\tvalidation_0-auc:0.99824\n",
            "[17]\tvalidation_0-auc:0.998187\n",
            "[18]\tvalidation_0-auc:0.99824\n",
            "[19]\tvalidation_0-auc:0.99824\n",
            "[20]\tvalidation_0-auc:0.998214\n",
            "[21]\tvalidation_0-auc:0.998187\n",
            "[22]\tvalidation_0-auc:0.998133\n",
            "[23]\tvalidation_0-auc:0.998133\n",
            "[24]\tvalidation_0-auc:0.998187\n",
            "[25]\tvalidation_0-auc:0.998106\n",
            "[26]\tvalidation_0-auc:0.998106\n",
            "[27]\tvalidation_0-auc:0.99816\n",
            "[28]\tvalidation_0-auc:0.99824\n",
            "[29]\tvalidation_0-auc:0.998187\n",
            "[30]\tvalidation_0-auc:0.998133\n",
            "[31]\tvalidation_0-auc:0.998106\n",
            "[32]\tvalidation_0-auc:0.99816\n",
            "[33]\tvalidation_0-auc:0.998133\n",
            "[34]\tvalidation_0-auc:0.9982\n",
            "[35]\tvalidation_0-auc:0.9982\n",
            "[36]\tvalidation_0-auc:0.998254\n",
            "[37]\tvalidation_0-auc:0.998375\n",
            "[38]\tvalidation_0-auc:0.998402\n",
            "[39]\tvalidation_0-auc:0.998348\n",
            "[40]\tvalidation_0-auc:0.998321\n",
            "[41]\tvalidation_0-auc:0.998321\n",
            "[42]\tvalidation_0-auc:0.998348\n",
            "[43]\tvalidation_0-auc:0.998321\n",
            "[44]\tvalidation_0-auc:0.998402\n",
            "[45]\tvalidation_0-auc:0.998402\n",
            "[46]\tvalidation_0-auc:0.998509\n",
            "[47]\tvalidation_0-auc:0.998482\n",
            "[48]\tvalidation_0-auc:0.998455\n",
            "[49]\tvalidation_0-auc:0.998442\n",
            "[50]\tvalidation_0-auc:0.998442\n",
            "[51]\tvalidation_0-auc:0.998469\n",
            "[52]\tvalidation_0-auc:0.998388\n",
            "[53]\tvalidation_0-auc:0.998442\n",
            "[54]\tvalidation_0-auc:0.998388\n",
            "[55]\tvalidation_0-auc:0.998415\n",
            "[56]\tvalidation_0-auc:0.998415\n",
            "[57]\tvalidation_0-auc:0.998522\n",
            "[58]\tvalidation_0-auc:0.998522\n",
            "[59]\tvalidation_0-auc:0.998334\n",
            "[60]\tvalidation_0-auc:0.998361\n",
            "[61]\tvalidation_0-auc:0.998361\n",
            "[62]\tvalidation_0-auc:0.998254\n",
            "[63]\tvalidation_0-auc:0.998281\n",
            "[64]\tvalidation_0-auc:0.998281\n",
            "[65]\tvalidation_0-auc:0.998281\n",
            "[66]\tvalidation_0-auc:0.998281\n",
            "[67]\tvalidation_0-auc:0.998281\n",
            "[68]\tvalidation_0-auc:0.998281\n",
            "[69]\tvalidation_0-auc:0.998281\n",
            "[70]\tvalidation_0-auc:0.998308\n",
            "[71]\tvalidation_0-auc:0.998334\n",
            "[72]\tvalidation_0-auc:0.998334\n",
            "[73]\tvalidation_0-auc:0.998334\n",
            "[74]\tvalidation_0-auc:0.998227\n",
            "[75]\tvalidation_0-auc:0.9982\n",
            "[76]\tvalidation_0-auc:0.9982\n",
            "[77]\tvalidation_0-auc:0.998227\n",
            "[78]\tvalidation_0-auc:0.998227\n",
            "[79]\tvalidation_0-auc:0.998254\n",
            "[80]\tvalidation_0-auc:0.998227\n",
            "[81]\tvalidation_0-auc:0.998254\n",
            "[82]\tvalidation_0-auc:0.998254\n",
            "[83]\tvalidation_0-auc:0.998254\n",
            "[84]\tvalidation_0-auc:0.998227\n",
            "[85]\tvalidation_0-auc:0.9982\n",
            "[86]\tvalidation_0-auc:0.998173\n",
            "[87]\tvalidation_0-auc:0.998146\n",
            "[88]\tvalidation_0-auc:0.998119\n",
            "[89]\tvalidation_0-auc:0.998119\n",
            "[90]\tvalidation_0-auc:0.998119\n",
            "[91]\tvalidation_0-auc:0.998066\n",
            "[92]\tvalidation_0-auc:0.998093\n",
            "[93]\tvalidation_0-auc:0.998066\n",
            "[94]\tvalidation_0-auc:0.998066\n",
            "[95]\tvalidation_0-auc:0.998039\n",
            "[96]\tvalidation_0-auc:0.998039\n",
            "[97]\tvalidation_0-auc:0.998066\n",
            "[98]\tvalidation_0-auc:0.998119\n",
            "[99]\tvalidation_0-auc:0.998119\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 15ms/step - loss: 0.5878 - accuracy: 0.7037 - val_loss: 0.3481 - val_accuracy: 0.8950\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.4148 - accuracy: 0.8564 - val_loss: 0.3060 - val_accuracy: 0.8928\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3042 - accuracy: 0.8962 - val_loss: 0.3149 - val_accuracy: 0.8993\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.2998 - accuracy: 0.9022 - val_loss: 0.2765 - val_accuracy: 0.9015\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.2887 - accuracy: 0.9077 - val_loss: 0.3297 - val_accuracy: 0.8972\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 13ms/step - loss: 0.5421 - accuracy: 0.7290 - val_loss: 0.2286 - val_accuracy: 0.9125\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2757 - accuracy: 0.9167 - val_loss: 0.1715 - val_accuracy: 0.9344\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2175 - accuracy: 0.9318 - val_loss: 0.2621 - val_accuracy: 0.9256\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.1811 - accuracy: 0.9378 - val_loss: 0.1616 - val_accuracy: 0.9365\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.1625 - accuracy: 0.9487 - val_loss: 0.1723 - val_accuracy: 0.9322\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.948186\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.948186\n",
            "[2]\tvalidation_0-auc:0.967733\n",
            "[3]\tvalidation_0-auc:0.967733\n",
            "[4]\tvalidation_0-auc:0.983764\n",
            "[5]\tvalidation_0-auc:0.984775\n",
            "[6]\tvalidation_0-auc:0.984673\n",
            "[7]\tvalidation_0-auc:0.984643\n",
            "[8]\tvalidation_0-auc:0.985068\n",
            "[9]\tvalidation_0-auc:0.98797\n",
            "[10]\tvalidation_0-auc:0.986695\n",
            "[11]\tvalidation_0-auc:0.986431\n",
            "[12]\tvalidation_0-auc:0.986724\n",
            "[13]\tvalidation_0-auc:0.986871\n",
            "[14]\tvalidation_0-auc:0.987134\n",
            "[15]\tvalidation_0-auc:0.98794\n",
            "[16]\tvalidation_0-auc:0.987823\n",
            "[17]\tvalidation_0-auc:0.987559\n",
            "[18]\tvalidation_0-auc:0.987501\n",
            "[19]\tvalidation_0-auc:0.988951\n",
            "[20]\tvalidation_0-auc:0.988805\n",
            "[21]\tvalidation_0-auc:0.988805\n",
            "[22]\tvalidation_0-auc:0.989039\n",
            "[23]\tvalidation_0-auc:0.98901\n",
            "[24]\tvalidation_0-auc:0.989332\n",
            "[25]\tvalidation_0-auc:0.99005\n",
            "[26]\tvalidation_0-auc:0.989406\n",
            "[27]\tvalidation_0-auc:0.990607\n",
            "[28]\tvalidation_0-auc:0.990256\n",
            "[29]\tvalidation_0-auc:0.991428\n",
            "[30]\tvalidation_0-auc:0.991193\n",
            "[31]\tvalidation_0-auc:0.991428\n",
            "[32]\tvalidation_0-auc:0.991472\n",
            "[33]\tvalidation_0-auc:0.99156\n",
            "[34]\tvalidation_0-auc:0.991618\n",
            "[35]\tvalidation_0-auc:0.991853\n",
            "[36]\tvalidation_0-auc:0.991911\n",
            "[37]\tvalidation_0-auc:0.991794\n",
            "[38]\tvalidation_0-auc:0.991823\n",
            "[39]\tvalidation_0-auc:0.991823\n",
            "[40]\tvalidation_0-auc:0.991853\n",
            "[41]\tvalidation_0-auc:0.991941\n",
            "[42]\tvalidation_0-auc:0.991823\n",
            "[43]\tvalidation_0-auc:0.991941\n",
            "[44]\tvalidation_0-auc:0.991941\n",
            "[45]\tvalidation_0-auc:0.992029\n",
            "[46]\tvalidation_0-auc:0.992058\n",
            "[47]\tvalidation_0-auc:0.992117\n",
            "[48]\tvalidation_0-auc:0.992117\n",
            "[49]\tvalidation_0-auc:0.992263\n",
            "[50]\tvalidation_0-auc:0.992175\n",
            "[51]\tvalidation_0-auc:0.992292\n",
            "[52]\tvalidation_0-auc:0.992292\n",
            "[53]\tvalidation_0-auc:0.99238\n",
            "[54]\tvalidation_0-auc:0.992498\n",
            "[55]\tvalidation_0-auc:0.992351\n",
            "[56]\tvalidation_0-auc:0.992351\n",
            "[57]\tvalidation_0-auc:0.992292\n",
            "[58]\tvalidation_0-auc:0.992585\n",
            "[59]\tvalidation_0-auc:0.992351\n",
            "[60]\tvalidation_0-auc:0.992556\n",
            "[61]\tvalidation_0-auc:0.99241\n",
            "[62]\tvalidation_0-auc:0.992322\n",
            "[63]\tvalidation_0-auc:0.992322\n",
            "[64]\tvalidation_0-auc:0.992234\n",
            "[65]\tvalidation_0-auc:0.992204\n",
            "[66]\tvalidation_0-auc:0.992204\n",
            "[67]\tvalidation_0-auc:0.992204\n",
            "[68]\tvalidation_0-auc:0.992087\n",
            "[69]\tvalidation_0-auc:0.992117\n",
            "[70]\tvalidation_0-auc:0.992117\n",
            "[71]\tvalidation_0-auc:0.992087\n",
            "[72]\tvalidation_0-auc:0.992117\n",
            "[73]\tvalidation_0-auc:0.992087\n",
            "[74]\tvalidation_0-auc:0.992029\n",
            "[75]\tvalidation_0-auc:0.992058\n",
            "[76]\tvalidation_0-auc:0.992117\n",
            "[77]\tvalidation_0-auc:0.992146\n",
            "[78]\tvalidation_0-auc:0.992117\n",
            "[79]\tvalidation_0-auc:0.992146\n",
            "[80]\tvalidation_0-auc:0.992175\n",
            "[81]\tvalidation_0-auc:0.992146\n",
            "[82]\tvalidation_0-auc:0.992234\n",
            "[83]\tvalidation_0-auc:0.992234\n",
            "[84]\tvalidation_0-auc:0.992175\n",
            "[85]\tvalidation_0-auc:0.992146\n",
            "[86]\tvalidation_0-auc:0.992146\n",
            "[87]\tvalidation_0-auc:0.992263\n",
            "[88]\tvalidation_0-auc:0.992292\n",
            "[89]\tvalidation_0-auc:0.992263\n",
            "[90]\tvalidation_0-auc:0.992322\n",
            "[91]\tvalidation_0-auc:0.99241\n",
            "[92]\tvalidation_0-auc:0.992615\n",
            "[93]\tvalidation_0-auc:0.992585\n",
            "[94]\tvalidation_0-auc:0.992527\n",
            "[95]\tvalidation_0-auc:0.992498\n",
            "[96]\tvalidation_0-auc:0.992498\n",
            "[97]\tvalidation_0-auc:0.992615\n",
            "[98]\tvalidation_0-auc:0.992644\n",
            "[99]\tvalidation_0-auc:0.992615\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.9510204081632653 | 0.9869109947643979 | 0.952020202020202  | 0.9691516709511568 |\n",
            "|     GRU 0.1      | 0.9510204081632653 | 0.9894736842105263 | 0.9494949494949495 | 0.9690721649484537 |\n",
            "|   XGBoost 0.1    | 0.9795918367346939 | 0.9948717948717949 | 0.9797979797979798 | 0.9872773536895675 |\n",
            "|    Logreg 0.1    | 0.9224489795918367 | 0.9124423963133641 |        1.0         | 0.9542168674698795 |\n",
            "|     SVM 0.1      | 0.9755102040816327 | 0.9848484848484849 | 0.9848484848484849 | 0.9848484848484849 |\n",
            "|  LSTM beta 0.1   | 0.8971553610503282 | 0.9202127659574468 | 0.953168044077135  | 0.9364005412719892 |\n",
            "|   GRU beta 0.1   | 0.9321663019693655 | 0.9689265536723164 | 0.9449035812672176 | 0.9567642956764295 |\n",
            "| XGBoost beta 0.1 | 0.949671772428884  | 0.9913294797687862 | 0.9449035812672176 | 0.9675599435825105 |\n",
            "| logreg beta 0.1  | 0.9452954048140044 | 0.9355670103092784 |        1.0         | 0.966711051930759  |\n",
            "|   svm beta 0.1   | 0.9387308533916849 | 0.9539295392953929 | 0.9696969696969697 | 0.9617486338797815 |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 15ms/step - loss: 0.6313 - accuracy: 0.6858 - val_loss: 0.5042 - val_accuracy: 0.8082\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.6283 - accuracy: 0.6899 - val_loss: 0.5238 - val_accuracy: 0.8082\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.4361 - accuracy: 0.7994 - val_loss: 0.1083 - val_accuracy: 0.9633\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.2425 - accuracy: 0.9142 - val_loss: 0.1244 - val_accuracy: 0.9633\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.1952 - accuracy: 0.9367 - val_loss: 0.1181 - val_accuracy: 0.9469\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 13ms/step - loss: 0.6157 - accuracy: 0.6876 - val_loss: 0.4564 - val_accuracy: 0.8082\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.3698 - accuracy: 0.8320 - val_loss: 0.0863 - val_accuracy: 0.9571\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.1782 - accuracy: 0.9379 - val_loss: 0.0770 - val_accuracy: 0.9714\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.1577 - accuracy: 0.9503 - val_loss: 0.0874 - val_accuracy: 0.9592\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.1444 - accuracy: 0.9503 - val_loss: 0.1162 - val_accuracy: 0.9490\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.991403\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.991605\n",
            "[2]\tvalidation_0-auc:0.991605\n",
            "[3]\tvalidation_0-auc:0.991565\n",
            "[4]\tvalidation_0-auc:0.992048\n",
            "[5]\tvalidation_0-auc:0.991712\n",
            "[6]\tvalidation_0-auc:0.991645\n",
            "[7]\tvalidation_0-auc:0.993123\n",
            "[8]\tvalidation_0-auc:0.993002\n",
            "[9]\tvalidation_0-auc:0.993109\n",
            "[10]\tvalidation_0-auc:0.993029\n",
            "[11]\tvalidation_0-auc:0.997945\n",
            "[12]\tvalidation_0-auc:0.998106\n",
            "[13]\tvalidation_0-auc:0.99816\n",
            "[14]\tvalidation_0-auc:0.998214\n",
            "[15]\tvalidation_0-auc:0.998321\n",
            "[16]\tvalidation_0-auc:0.99824\n",
            "[17]\tvalidation_0-auc:0.998187\n",
            "[18]\tvalidation_0-auc:0.99824\n",
            "[19]\tvalidation_0-auc:0.99824\n",
            "[20]\tvalidation_0-auc:0.998214\n",
            "[21]\tvalidation_0-auc:0.998187\n",
            "[22]\tvalidation_0-auc:0.998133\n",
            "[23]\tvalidation_0-auc:0.998133\n",
            "[24]\tvalidation_0-auc:0.998187\n",
            "[25]\tvalidation_0-auc:0.998106\n",
            "[26]\tvalidation_0-auc:0.998106\n",
            "[27]\tvalidation_0-auc:0.99816\n",
            "[28]\tvalidation_0-auc:0.99824\n",
            "[29]\tvalidation_0-auc:0.998187\n",
            "[30]\tvalidation_0-auc:0.998133\n",
            "[31]\tvalidation_0-auc:0.998106\n",
            "[32]\tvalidation_0-auc:0.99816\n",
            "[33]\tvalidation_0-auc:0.998133\n",
            "[34]\tvalidation_0-auc:0.9982\n",
            "[35]\tvalidation_0-auc:0.9982\n",
            "[36]\tvalidation_0-auc:0.998254\n",
            "[37]\tvalidation_0-auc:0.998375\n",
            "[38]\tvalidation_0-auc:0.998402\n",
            "[39]\tvalidation_0-auc:0.998348\n",
            "[40]\tvalidation_0-auc:0.998321\n",
            "[41]\tvalidation_0-auc:0.998321\n",
            "[42]\tvalidation_0-auc:0.998348\n",
            "[43]\tvalidation_0-auc:0.998321\n",
            "[44]\tvalidation_0-auc:0.998402\n",
            "[45]\tvalidation_0-auc:0.998402\n",
            "[46]\tvalidation_0-auc:0.998509\n",
            "[47]\tvalidation_0-auc:0.998482\n",
            "[48]\tvalidation_0-auc:0.998455\n",
            "[49]\tvalidation_0-auc:0.998442\n",
            "[50]\tvalidation_0-auc:0.998442\n",
            "[51]\tvalidation_0-auc:0.998469\n",
            "[52]\tvalidation_0-auc:0.998388\n",
            "[53]\tvalidation_0-auc:0.998442\n",
            "[54]\tvalidation_0-auc:0.998388\n",
            "[55]\tvalidation_0-auc:0.998415\n",
            "[56]\tvalidation_0-auc:0.998415\n",
            "[57]\tvalidation_0-auc:0.998522\n",
            "[58]\tvalidation_0-auc:0.998522\n",
            "[59]\tvalidation_0-auc:0.998334\n",
            "[60]\tvalidation_0-auc:0.998361\n",
            "[61]\tvalidation_0-auc:0.998361\n",
            "[62]\tvalidation_0-auc:0.998254\n",
            "[63]\tvalidation_0-auc:0.998281\n",
            "[64]\tvalidation_0-auc:0.998281\n",
            "[65]\tvalidation_0-auc:0.998281\n",
            "[66]\tvalidation_0-auc:0.998281\n",
            "[67]\tvalidation_0-auc:0.998281\n",
            "[68]\tvalidation_0-auc:0.998281\n",
            "[69]\tvalidation_0-auc:0.998281\n",
            "[70]\tvalidation_0-auc:0.998308\n",
            "[71]\tvalidation_0-auc:0.998334\n",
            "[72]\tvalidation_0-auc:0.998334\n",
            "[73]\tvalidation_0-auc:0.998334\n",
            "[74]\tvalidation_0-auc:0.998227\n",
            "[75]\tvalidation_0-auc:0.9982\n",
            "[76]\tvalidation_0-auc:0.9982\n",
            "[77]\tvalidation_0-auc:0.998227\n",
            "[78]\tvalidation_0-auc:0.998227\n",
            "[79]\tvalidation_0-auc:0.998254\n",
            "[80]\tvalidation_0-auc:0.998227\n",
            "[81]\tvalidation_0-auc:0.998254\n",
            "[82]\tvalidation_0-auc:0.998254\n",
            "[83]\tvalidation_0-auc:0.998254\n",
            "[84]\tvalidation_0-auc:0.998227\n",
            "[85]\tvalidation_0-auc:0.9982\n",
            "[86]\tvalidation_0-auc:0.998173\n",
            "[87]\tvalidation_0-auc:0.998146\n",
            "[88]\tvalidation_0-auc:0.998119\n",
            "[89]\tvalidation_0-auc:0.998119\n",
            "[90]\tvalidation_0-auc:0.998119\n",
            "[91]\tvalidation_0-auc:0.998066\n",
            "[92]\tvalidation_0-auc:0.998093\n",
            "[93]\tvalidation_0-auc:0.998066\n",
            "[94]\tvalidation_0-auc:0.998066\n",
            "[95]\tvalidation_0-auc:0.998039\n",
            "[96]\tvalidation_0-auc:0.998039\n",
            "[97]\tvalidation_0-auc:0.998066\n",
            "[98]\tvalidation_0-auc:0.998119\n",
            "[99]\tvalidation_0-auc:0.998119\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 4s 15ms/step - loss: 0.5982 - accuracy: 0.7133 - val_loss: 0.3873 - val_accuracy: 0.8271\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3284 - accuracy: 0.8817 - val_loss: 0.2842 - val_accuracy: 0.9059\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 12ms/step - loss: 0.2947 - accuracy: 0.9010 - val_loss: 0.2878 - val_accuracy: 0.9081\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.2696 - accuracy: 0.9071 - val_loss: 0.2770 - val_accuracy: 0.9059\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.2493 - accuracy: 0.9203 - val_loss: 0.3840 - val_accuracy: 0.8643\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 14ms/step - loss: 0.5859 - accuracy: 0.7158 - val_loss: 0.3582 - val_accuracy: 0.7943\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.2939 - accuracy: 0.8938 - val_loss: 0.2140 - val_accuracy: 0.9125\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.1980 - accuracy: 0.9360 - val_loss: 0.1801 - val_accuracy: 0.9344\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.1634 - accuracy: 0.9499 - val_loss: 0.1450 - val_accuracy: 0.9519\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.1600 - accuracy: 0.9481 - val_loss: 0.1205 - val_accuracy: 0.9540\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.948186\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.948186\n",
            "[2]\tvalidation_0-auc:0.967733\n",
            "[3]\tvalidation_0-auc:0.967733\n",
            "[4]\tvalidation_0-auc:0.983764\n",
            "[5]\tvalidation_0-auc:0.984775\n",
            "[6]\tvalidation_0-auc:0.984673\n",
            "[7]\tvalidation_0-auc:0.984643\n",
            "[8]\tvalidation_0-auc:0.985068\n",
            "[9]\tvalidation_0-auc:0.98797\n",
            "[10]\tvalidation_0-auc:0.986695\n",
            "[11]\tvalidation_0-auc:0.986431\n",
            "[12]\tvalidation_0-auc:0.986724\n",
            "[13]\tvalidation_0-auc:0.986871\n",
            "[14]\tvalidation_0-auc:0.987134\n",
            "[15]\tvalidation_0-auc:0.98794\n",
            "[16]\tvalidation_0-auc:0.987823\n",
            "[17]\tvalidation_0-auc:0.987559\n",
            "[18]\tvalidation_0-auc:0.987501\n",
            "[19]\tvalidation_0-auc:0.988951\n",
            "[20]\tvalidation_0-auc:0.988805\n",
            "[21]\tvalidation_0-auc:0.988805\n",
            "[22]\tvalidation_0-auc:0.989039\n",
            "[23]\tvalidation_0-auc:0.98901\n",
            "[24]\tvalidation_0-auc:0.989332\n",
            "[25]\tvalidation_0-auc:0.99005\n",
            "[26]\tvalidation_0-auc:0.989406\n",
            "[27]\tvalidation_0-auc:0.990607\n",
            "[28]\tvalidation_0-auc:0.990256\n",
            "[29]\tvalidation_0-auc:0.991428\n",
            "[30]\tvalidation_0-auc:0.991193\n",
            "[31]\tvalidation_0-auc:0.991428\n",
            "[32]\tvalidation_0-auc:0.991472\n",
            "[33]\tvalidation_0-auc:0.99156\n",
            "[34]\tvalidation_0-auc:0.991618\n",
            "[35]\tvalidation_0-auc:0.991853\n",
            "[36]\tvalidation_0-auc:0.991911\n",
            "[37]\tvalidation_0-auc:0.991794\n",
            "[38]\tvalidation_0-auc:0.991823\n",
            "[39]\tvalidation_0-auc:0.991823\n",
            "[40]\tvalidation_0-auc:0.991853\n",
            "[41]\tvalidation_0-auc:0.991941\n",
            "[42]\tvalidation_0-auc:0.991823\n",
            "[43]\tvalidation_0-auc:0.991941\n",
            "[44]\tvalidation_0-auc:0.991941\n",
            "[45]\tvalidation_0-auc:0.992029\n",
            "[46]\tvalidation_0-auc:0.992058\n",
            "[47]\tvalidation_0-auc:0.992117\n",
            "[48]\tvalidation_0-auc:0.992117\n",
            "[49]\tvalidation_0-auc:0.992263\n",
            "[50]\tvalidation_0-auc:0.992175\n",
            "[51]\tvalidation_0-auc:0.992292\n",
            "[52]\tvalidation_0-auc:0.992292\n",
            "[53]\tvalidation_0-auc:0.99238\n",
            "[54]\tvalidation_0-auc:0.992498\n",
            "[55]\tvalidation_0-auc:0.992351\n",
            "[56]\tvalidation_0-auc:0.992351\n",
            "[57]\tvalidation_0-auc:0.992292\n",
            "[58]\tvalidation_0-auc:0.992585\n",
            "[59]\tvalidation_0-auc:0.992351\n",
            "[60]\tvalidation_0-auc:0.992556\n",
            "[61]\tvalidation_0-auc:0.99241\n",
            "[62]\tvalidation_0-auc:0.992322\n",
            "[63]\tvalidation_0-auc:0.992322\n",
            "[64]\tvalidation_0-auc:0.992234\n",
            "[65]\tvalidation_0-auc:0.992204\n",
            "[66]\tvalidation_0-auc:0.992204\n",
            "[67]\tvalidation_0-auc:0.992204\n",
            "[68]\tvalidation_0-auc:0.992087\n",
            "[69]\tvalidation_0-auc:0.992117\n",
            "[70]\tvalidation_0-auc:0.992117\n",
            "[71]\tvalidation_0-auc:0.992087\n",
            "[72]\tvalidation_0-auc:0.992117\n",
            "[73]\tvalidation_0-auc:0.992087\n",
            "[74]\tvalidation_0-auc:0.992029\n",
            "[75]\tvalidation_0-auc:0.992058\n",
            "[76]\tvalidation_0-auc:0.992117\n",
            "[77]\tvalidation_0-auc:0.992146\n",
            "[78]\tvalidation_0-auc:0.992117\n",
            "[79]\tvalidation_0-auc:0.992146\n",
            "[80]\tvalidation_0-auc:0.992175\n",
            "[81]\tvalidation_0-auc:0.992146\n",
            "[82]\tvalidation_0-auc:0.992234\n",
            "[83]\tvalidation_0-auc:0.992234\n",
            "[84]\tvalidation_0-auc:0.992175\n",
            "[85]\tvalidation_0-auc:0.992146\n",
            "[86]\tvalidation_0-auc:0.992146\n",
            "[87]\tvalidation_0-auc:0.992263\n",
            "[88]\tvalidation_0-auc:0.992292\n",
            "[89]\tvalidation_0-auc:0.992263\n",
            "[90]\tvalidation_0-auc:0.992322\n",
            "[91]\tvalidation_0-auc:0.99241\n",
            "[92]\tvalidation_0-auc:0.992615\n",
            "[93]\tvalidation_0-auc:0.992585\n",
            "[94]\tvalidation_0-auc:0.992527\n",
            "[95]\tvalidation_0-auc:0.992498\n",
            "[96]\tvalidation_0-auc:0.992498\n",
            "[97]\tvalidation_0-auc:0.992615\n",
            "[98]\tvalidation_0-auc:0.992644\n",
            "[99]\tvalidation_0-auc:0.992615\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.9469387755102041 | 0.9868421052631579 | 0.946969696969697  | 0.9664948453608249 |\n",
            "|      GRU 0.15     | 0.9489795918367347 | 0.9946666666666667 | 0.9419191919191919 | 0.9675745784695201 |\n",
            "|    XGBoost 0.15   | 0.9795918367346939 | 0.9948717948717949 | 0.9797979797979798 | 0.9872773536895675 |\n",
            "|    Logreg 0.15    | 0.9224489795918367 | 0.9124423963133641 |        1.0         | 0.9542168674698795 |\n",
            "|      SVM 0.15     | 0.9755102040816327 | 0.9848484848484849 | 0.9848484848484849 | 0.9848484848484849 |\n",
            "|   LSTM beta 0.15  | 0.8643326039387309 | 0.963076923076923  | 0.8622589531680441 | 0.9098837209302325 |\n",
            "|   GRU beta 0.15   | 0.9540481400437637 |       0.975        | 0.9669421487603306 | 0.970954356846473  |\n",
            "| XGBoost beta 0.15 | 0.949671772428884  | 0.9913294797687862 | 0.9449035812672176 | 0.9675599435825105 |\n",
            "|  logreg beta 0.15 | 0.9452954048140044 | 0.9355670103092784 |        1.0         | 0.966711051930759  |\n",
            "|   svm beta 0.15   | 0.9387308533916849 | 0.9539295392953929 | 0.9696969696969697 | 0.9617486338797815 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979
        },
        "id": "F57CidjyCs-D",
        "outputId": "af7c0413-dd62-45d8-9062-9628e4b7fd0b"
      },
      "source": [
        "Result_purging.to_csv('AAPL_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.977612</td>\n",
              "      <td>0.975510</td>\n",
              "      <td>0.984962</td>\n",
              "      <td>0.992424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.987179</td>\n",
              "      <td>0.967347</td>\n",
              "      <td>0.979644</td>\n",
              "      <td>0.972222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.994872</td>\n",
              "      <td>0.979592</td>\n",
              "      <td>0.987277</td>\n",
              "      <td>0.979798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.912442</td>\n",
              "      <td>0.922449</td>\n",
              "      <td>0.954217</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.984848</td>\n",
              "      <td>0.975510</td>\n",
              "      <td>0.984848</td>\n",
              "      <td>0.984848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.920213</td>\n",
              "      <td>0.897155</td>\n",
              "      <td>0.936401</td>\n",
              "      <td>0.953168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.954301</td>\n",
              "      <td>0.945295</td>\n",
              "      <td>0.965986</td>\n",
              "      <td>0.977961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.991329</td>\n",
              "      <td>0.949672</td>\n",
              "      <td>0.967560</td>\n",
              "      <td>0.944904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.935567</td>\n",
              "      <td>0.945295</td>\n",
              "      <td>0.966711</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.953930</td>\n",
              "      <td>0.938731</td>\n",
              "      <td>0.961749</td>\n",
              "      <td>0.969697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.986911</td>\n",
              "      <td>0.951020</td>\n",
              "      <td>0.969152</td>\n",
              "      <td>0.952020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.989474</td>\n",
              "      <td>0.951020</td>\n",
              "      <td>0.969072</td>\n",
              "      <td>0.949495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.994872</td>\n",
              "      <td>0.979592</td>\n",
              "      <td>0.987277</td>\n",
              "      <td>0.979798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.912442</td>\n",
              "      <td>0.922449</td>\n",
              "      <td>0.954217</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.984848</td>\n",
              "      <td>0.975510</td>\n",
              "      <td>0.984848</td>\n",
              "      <td>0.984848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.920213</td>\n",
              "      <td>0.897155</td>\n",
              "      <td>0.936401</td>\n",
              "      <td>0.953168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.968927</td>\n",
              "      <td>0.932166</td>\n",
              "      <td>0.956764</td>\n",
              "      <td>0.944904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.991329</td>\n",
              "      <td>0.949672</td>\n",
              "      <td>0.967560</td>\n",
              "      <td>0.944904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.935567</td>\n",
              "      <td>0.945295</td>\n",
              "      <td>0.966711</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.953930</td>\n",
              "      <td>0.938731</td>\n",
              "      <td>0.961749</td>\n",
              "      <td>0.969697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.946939</td>\n",
              "      <td>0.966495</td>\n",
              "      <td>0.946970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.994667</td>\n",
              "      <td>0.948980</td>\n",
              "      <td>0.967575</td>\n",
              "      <td>0.941919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.994872</td>\n",
              "      <td>0.979592</td>\n",
              "      <td>0.987277</td>\n",
              "      <td>0.979798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.912442</td>\n",
              "      <td>0.922449</td>\n",
              "      <td>0.954217</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.984848</td>\n",
              "      <td>0.975510</td>\n",
              "      <td>0.984848</td>\n",
              "      <td>0.984848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.963077</td>\n",
              "      <td>0.864333</td>\n",
              "      <td>0.909884</td>\n",
              "      <td>0.862259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.975000</td>\n",
              "      <td>0.954048</td>\n",
              "      <td>0.970954</td>\n",
              "      <td>0.966942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.991329</td>\n",
              "      <td>0.949672</td>\n",
              "      <td>0.967560</td>\n",
              "      <td>0.944904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.935567</td>\n",
              "      <td>0.945295</td>\n",
              "      <td>0.966711</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.953930</td>\n",
              "      <td>0.938731</td>\n",
              "      <td>0.961749</td>\n",
              "      <td>0.969697</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0          LSTM 0.05  AAPL  0.977612  0.975510  0.984962  0.992424\n",
              "1           GRU 0.05  AAPL  0.987179  0.967347  0.979644  0.972222\n",
              "2       XGBoost 0.05  AAPL  0.994872  0.979592  0.987277  0.979798\n",
              "3        Logreg 0.05  AAPL  0.912442  0.922449  0.954217  1.000000\n",
              "4           SVM 0.05  AAPL  0.984848  0.975510  0.984848  0.984848\n",
              "5     LSTM beta 0.05  AAPL  0.920213  0.897155  0.936401  0.953168\n",
              "6      GRU beta 0.05  AAPL  0.954301  0.945295  0.965986  0.977961\n",
              "7  XGBoost beta 0.05  AAPL  0.991329  0.949672  0.967560  0.944904\n",
              "8   logreg beta 0.05  AAPL  0.935567  0.945295  0.966711  1.000000\n",
              "9      svm beta 0.05  AAPL  0.953930  0.938731  0.961749  0.969697\n",
              "0           LSTM 0.1  AAPL  0.986911  0.951020  0.969152  0.952020\n",
              "1            GRU 0.1  AAPL  0.989474  0.951020  0.969072  0.949495\n",
              "2        XGBoost 0.1  AAPL  0.994872  0.979592  0.987277  0.979798\n",
              "3         Logreg 0.1  AAPL  0.912442  0.922449  0.954217  1.000000\n",
              "4            SVM 0.1  AAPL  0.984848  0.975510  0.984848  0.984848\n",
              "5      LSTM beta 0.1  AAPL  0.920213  0.897155  0.936401  0.953168\n",
              "6       GRU beta 0.1  AAPL  0.968927  0.932166  0.956764  0.944904\n",
              "7   XGBoost beta 0.1  AAPL  0.991329  0.949672  0.967560  0.944904\n",
              "8    logreg beta 0.1  AAPL  0.935567  0.945295  0.966711  1.000000\n",
              "9       svm beta 0.1  AAPL  0.953930  0.938731  0.961749  0.969697\n",
              "0          LSTM 0.15  AAPL  0.986842  0.946939  0.966495  0.946970\n",
              "1           GRU 0.15  AAPL  0.994667  0.948980  0.967575  0.941919\n",
              "2       XGBoost 0.15  AAPL  0.994872  0.979592  0.987277  0.979798\n",
              "3        Logreg 0.15  AAPL  0.912442  0.922449  0.954217  1.000000\n",
              "4           SVM 0.15  AAPL  0.984848  0.975510  0.984848  0.984848\n",
              "5     LSTM beta 0.15  AAPL  0.963077  0.864333  0.909884  0.862259\n",
              "6      GRU beta 0.15  AAPL  0.975000  0.954048  0.970954  0.966942\n",
              "7  XGBoost beta 0.15  AAPL  0.991329  0.949672  0.967560  0.944904\n",
              "8   logreg beta 0.15  AAPL  0.935567  0.945295  0.966711  1.000000\n",
              "9      svm beta 0.15  AAPL  0.953930  0.938731  0.961749  0.969697"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH__8B42Cs-G"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vqBsjlEUbos"
      },
      "source": [
        "## DVN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "id": "ScbH1woyUbot",
        "outputId": "da511fe2-27df-43bd-bd8a-909727553da7"
      },
      "source": [
        "dfs = pd.read_csv(\"DVN.csv\")\n",
        "# dfs = dfs[::-1].reset_index()\n",
        "# Denoise prices\n",
        "dfs = denoise_data(dfs, '<CLOSE>')\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>US1.DVN</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>65.00</td>\n",
              "      <td>65.520</td>\n",
              "      <td>63.85</td>\n",
              "      <td>75.894667</td>\n",
              "      <td>2465941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>US1.DVN</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>65.14</td>\n",
              "      <td>65.550</td>\n",
              "      <td>64.68</td>\n",
              "      <td>75.894667</td>\n",
              "      <td>2226445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>US1.DVN</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>65.37</td>\n",
              "      <td>66.500</td>\n",
              "      <td>64.99</td>\n",
              "      <td>75.894667</td>\n",
              "      <td>3011060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>US1.DVN</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>66.67</td>\n",
              "      <td>66.940</td>\n",
              "      <td>65.22</td>\n",
              "      <td>75.894667</td>\n",
              "      <td>2097798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US1.DVN</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>65.84</td>\n",
              "      <td>66.980</td>\n",
              "      <td>65.74</td>\n",
              "      <td>75.894667</td>\n",
              "      <td>2343172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>US1.DVN</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>34.05</td>\n",
              "      <td>35.090</td>\n",
              "      <td>33.92</td>\n",
              "      <td>28.368300</td>\n",
              "      <td>826165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>US1.DVN</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>35.58</td>\n",
              "      <td>35.970</td>\n",
              "      <td>34.87</td>\n",
              "      <td>28.368300</td>\n",
              "      <td>723106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>US1.DVN</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>35.15</td>\n",
              "      <td>36.040</td>\n",
              "      <td>34.55</td>\n",
              "      <td>28.368300</td>\n",
              "      <td>483024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>US1.DVN</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>35.38</td>\n",
              "      <td>35.905</td>\n",
              "      <td>34.91</td>\n",
              "      <td>28.368300</td>\n",
              "      <td>633147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>US1.DVN</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>35.97</td>\n",
              "      <td>37.165</td>\n",
              "      <td>35.87</td>\n",
              "      <td>28.368300</td>\n",
              "      <td>661181</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2769 rows Ã— 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     <TICKER> <PER>    <DATE>  <TIME>  ...  <HIGH>  <LOW>    <CLOSE>    <VOL>\n",
              "0     US1.DVN     D  20101004       0  ...  65.520  63.85  75.894667  2465941\n",
              "1     US1.DVN     D  20101005       0  ...  65.550  64.68  75.894667  2226445\n",
              "2     US1.DVN     D  20101006       0  ...  66.500  64.99  75.894667  3011060\n",
              "3     US1.DVN     D  20101007       0  ...  66.940  65.22  75.894667  2097798\n",
              "4     US1.DVN     D  20101008       0  ...  66.980  65.74  75.894667  2343172\n",
              "...       ...   ...       ...     ...  ...     ...    ...        ...      ...\n",
              "2764  US1.DVN     D  20210927       0  ...  35.090  33.92  28.368300   826165\n",
              "2765  US1.DVN     D  20210928       0  ...  35.970  34.87  28.368300   723106\n",
              "2766  US1.DVN     D  20210929       0  ...  36.040  34.55  28.368300   483024\n",
              "2767  US1.DVN     D  20210930       0  ...  35.905  34.91  28.368300   633147\n",
              "2768  US1.DVN     D  20211001       0  ...  37.165  35.87  28.368300   661181\n",
              "\n",
              "[2769 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 240
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr8AAAHiCAYAAADh4aRaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXiU5b0+8PuZyWRfIAsBErKwBZMAQRYXENnd2CyLyvHYuvcItD1qXWqx1P4stHrqkSJSRVutG+ARqwJaF9wRCIpAIEgS0CSQhWxkzyzP74/J+/rOZCaZJJNZMvfnurxIZt7lyWTi3PPM9/0+QkoJIiIiIqJAoPP2AIiIiIiIPIXhl4iIiIgCBsMvEREREQUMhl8iIiIiChgMv0REREQUMBh+iYiIiChgMPwSEXmIEOI/hBD/dmG7zUKINR4a02khxBxPnIuIyBcI9vklokAihDgNIBGACYAZwDEALwJ4Rkpp8eLQvKL98bhNSvmBC9tKAKOklAV9PjAioj7CmV8iCkQLpJRRAFIBrAdwP4DnvDskIiLyBIZfIgpYUso6KeVbAK4D8FMhRDYACCFChBCPCyF+EEKUt5chhLXfN0MIUSKEuEcIUSGEOCuEuFk5phAiRgjxohCiUgjxvRDit0IIXft9PxNCfN7+tRBCPNF+jPNCiCOa8/9DCPH/XDxfnBDi7fZjHBBC/D/lHI4IIf6zfVxVQoiH7O6bIoTYK4SobT/PRiFEcPt9n7Zv9q0QokEIcZ0QYqAQ4p32n7Wm/evkXv9iiIj6EMMvEQU8KeV+ACUALmu/aT2A0QByAIwEkATgYc0ugwHEtN9+K4CnhBAD2+/7a/t9wwFcDuAmADejo3kAprefJwbAcgBVTobY2fmeAtDYvs1P2/9zSAiRCeBpAP8JYCiAOADasGoG8N8A4gFcAmA2gLsAQEo5vX2b8VLKSCnlVlhfQ/4O6wx6CoBmABudnZ+IyBcw/BIRWZ0BECuEEADuAPDfUspqKWU9gD8CuF6zrRHAI1JKo5RyF4AGABlCCH37dg9KKeullKcB/A+sYdOeEUAUgDGwXn9xXEp51snYOjvfEgC/k1I2SSmPAXihk59xKYB3pJSfSilbAawBoNY5SykPSim/klKa2sf+N1gDvENSyiop5f+1n7sewKOdbU9E5AuCvD0AIiIfkQSgGkACgHAAB605GAAgAOg121ZJKU2a75sARMI6Y2oA8L3mvu/bj21DSvmREGIjrDO3qUKINwDcK6U872Bszs6XAOv/x4s192m/tjdUe7+UslEIoc42CyFGA/gLgEmwPgZBAA46O5gQIhzAEwCuBKDMREcJIfRSSnMn4yAi8hrO/BJRwBNCTIY1oH4O4BysH99nSSkHtP8XI6WMdOFQ52CdpU3V3JYCoNTRxlLKDVLKiQAyYS1/+HU3h14Ja9cKbenCsE62P6u9vz28xmnufxpAPqwdHaIB/AbW4O/MPQAyAFzUvr1SGtHZPkREXsXwS0QBSwgRLYSYD+A1AC9JKY+0tzt7FsATQohB7dslCSGu6Op47bOd2wA8KoSIEkKkArgbwEsOzj1ZCHGREMIAa81uCzQlCK5oP98bANYKIcKFEGNgrTF25nUA84UQ09ovZHsEtq8DUQDOA2hoP9Z/2e1fDmsts3b7ZgC1QohYAL/rzviJiLyB4ZeIAtHbQoh6WEsAHoL1o37tRWn3AygA8JUQ4jyAD2Cd4XTFaljDbBGsM8mvAHjewXbRsIbsGlhLI6oAPNbtnwRYBevFcGUA/gngVQCtjjaUUuYBWNk+prPt5y7RbHIvgBUA6tvHttXuEGsBvNDeDWI5gP8FEAbrjPdXAN7twfiJiDyKi1wQEfUjQog/ARgspXTa9YGIKJBx5peIyI8JIcYIIca19w2eAmsrtB3eHhcRka9itwciIv8WBWupw1BYa3L/B8C/vDoiIiIfxrIHIiIiIgoYLHsgIiIiooDB8EtEREREAcOjNb/x8fEyLS3Nk6ckIiIiogBz8ODBc1LKBEf3eTT8pqWlITc315OnJCIiIqIAI4T43tl9LHsgIiIiooDB8EtEREREAYPhl4iIiIgCBsMvEREREQUMhl8iIiIiChgMv0REREQUMBh+iYiIiChgMPwSERERUcBg+CUiIiKigMHwS0REREQBg+GXiIiIiAIGwy8RERERBQyGXyIiIiIKGAy/RERERBQwGH6JiIiIKGAw/BIRERFRwGD4dSMppbeHQERERESdYPh1k1OnTiEvLw8Wi8XbQyEiIiIiJxh+3aSxsREAUF5e7uWREBEREZEzDL9uJoTw9hCIiIiIyAmGXzfQ1vrq9XovjoSIiIiIOsPw62as+SUiIiLyXQy/bqCd+a2srERlZaUXR0NEREREzjD8uoF9izNe9EZERETkmxh+e0hKidraWkgpHfb3NZvNXhgVEREREXWG4beHzp07h5KSEtTV1anhV3ux25kzZ7w1NCIiIiJyguG3h4xGIwCgpKREvcgtMTERoaGhAID6+noAQENDA5qbm70zSCIiIiKywfDbQ9pSh6amJgDWHr8GgwEAEBQUBAA4ffo0CgsLPT9AIiIiIuqA4dcNlPpeIQSSkpIAQJ0BJiIiIiLfwfDbQ9qZX6UEQgiBoKAghISEeGtYRERERNQJhl830IZf5V9HHSCIiIiIyLsYfntIG27b2toAMPwSERER+TqG3x5SujkAQGtrKwDbVmcMv0RERES+h+HXRRaLBWfOnFE7OyjtzQB06POrzAATERERkW9h+HVRc3MzqqurUVFR4XQbbfiVUqq1wERERETkGxh+XWQymQCg00BrX/N74sQJj4yNiIiIiFzD8OsipZev2Wzusp5XCKGGZSIiIiLyHQy/LlJqfC0Wi9Pwq631ZfglIiIi8j0Mvy5SAq+U0ib8BgcHd9iWrc6IiIiIfBPDr4u04ff48ePq7dquD9qaX2f7ExEREZH3MPy6yFF4jYmJsQm/CoZfIiIiIt/E8OtGSuhVLo4DgKioKAAMv0RERES+gOHXRY7Cq339r0JZ8Q0AdDqd0/2JiIiIyLMYfnth0KBBXW6jLHzB8EtERETkfQy/LnIUXkNDQ9Xbd+/ejWHDhmHp0qWorq5Wt+HMLxEREZHvYPh1g1deeQX3338/goOD8fbbb2P16tXqSnAMv0RERES+g+HXRc7C69NPP41169bh8ssvx5EjR7Blyxbk5ubilVdeAQAEBQV1uj8REREReQ7Dr4schddvvvkGmzZtwvz58/G///u/CA8Px4033ojp06dj06ZNOHfuHMMvERERkQ9h+O0G5eI1xUMPPYTo6Gg8+OCD6n1CCDzwwANoamrC9u3b1bIHR/2AiYiIiMizGH5dJKVUgywAlJWVYffu3fjFL36B6Ohom21HjRqFqVOnYseOHTCZTOr+RERERORdDL8uaG5uRm1tLYxGI0JDQ5GYmIjHH38cQ4YMwYMPPthh+9TUVPziF7/A2bNn8eSTTwJg+CUiIiLyBQy/LigsLFS/HjlyJMxmM95//33ccsstCA8PR3x8PCIiItRtDAYDli9fjmXLluHRRx9FXl4evv/+e5Y+EBEREXkZw28PbN++HRaLBTfccAMAYPDgwUhPT++w3ebNmxEXF4f169dDSomWlhZPD5WIiIiINBh+u8lisWD9+vUYO3YssrKyOt02NjYWv/3tb3Ho0CHs2bOHpQ9EREREXsbw201vv/02zpw5g6VLl7q0/c0334y0tDQ8/vjjqK+v7+PREREREVFnGH67aefOndDpdHjggQdc2j44OBgPP/wwSktLsXLlSs7+EhEREXkRw283SCmxe/duLF68GMHBwS7tI4TA5MmTsXLlSrzxxhv45JNPutyntrZWbZFGRERERO7D8OuCqKgoAEBBQQFKSkpw1VVXubyvEAIAcNNNNyE+Ph6PPfYYjEaj0+2NRiNKSkpQXFzcu0ETERERUQcMv12oqqpSa3X3798PAJg3b57L+yvhNzQ0FLfffjt27dqFd999F7W1tQ63N5vNAIC2trbeDJuIiIiIHGD47cK5c+fUr7/77jsMGTIEw4YNc3l/JfwCwE9/+lMIIfDuu++iubnZ4fZK+GVtMBEREZH7Mfx2QQmhQUFB2L9/Py655BKbQNsdCQkJGDduHPbv3+/0GEr4JSIiIiL3Y/jtghJ+jx49ipKSEixcuLDbxwgKClKPdemll+Lbb791uuAFZ3yJiIiI+g7Dr4s+/fRT6HQ6zJ8/v9v7jho1CgDQ1NSEcePGwWg04uDBgw63VcIvQzARERGR+zH8dkEJofv378eECRMQFxfX7WMoJQ51dXUYO3YsACA3N9fhthaLxea8REREROQ+DL9dkFKitbUV33zzDWbMmNGjY+h0Pz7MMTExSE5OxqFDh5yeT/svEREREbkPw68Ljhw5gra2Nlx++eVuOV5mZiaOHj3q8D5nXSCIiIiIqPcYfrsQHByMAwcOQAiByy67rMfH0XZ3yMzMRElJCaqrqztsp/T/5cwvERERkfsx/HZCKXn4+uuvkZOTgwEDBvT4WHq9Xv06MzMTAJxe9KY9PxERERG5D8NvJ4xGI9ra2npV76uIiYlRv87KygLg+KK34OBg9Wvl4jciIiIico8gbw/Al1ksFhw7dgytra2YPn16r441aNAg1NfXo62tDbGxsUhKSlKXSwaA8vJyVFZWIjQ01Ob82hljIiIiIuodzvx2wmKxIC8vDwAwZcqUXh1Lr9erZRNBQUEYM2YMvv32W/V+ZRll7QpvLHsgIiIici+G304oM7+JiYkYOnSoW44HAAaDAQkJCaiqqnK6jf3XRERERNR7DL+dUMJvTk6OW46nzOQGBQUhNjYW58+fR1tbG4Afu0GYzWa1LzBnfomIiIjcy6XwK4T4byFEnhDiqBDiVSFEqBAiXQixTwhRIITYKoQI7vpI/qWxsRFFRUW48MIL3Xpcg8GApKQkAEBhYWGH+5Xwy5lfIiIiIvfqMvwKIZIA/ALAJCllNgA9gOsB/AnAE1LKkQBqANzalwP1huPHj8NisWDcuHFuOd6gQYMQHx+PuLg4ZGRkAAAOHz7cYTuTyQSAM79ERERE7uZq2UMQgDAhRBCAcABnAcwC8Hr7/S8AWOz+4XmXcrHb2LFj3XI8vV6PwYMHQ6fTIT09HUFBQTYXvdnjzC8RERGRe3UZfqWUpQAeB/ADrKG3DsBBALVSSlP7ZiUAkvpqkN6Sl5eHsLAwDB8+3K3HFUIgODgYaWlpDmd+FZz5JSIiInIvV8oeBgJYBCAdwFAAEQCudPUEQog7hBC5QojcysrKHg/UG44dO4YRI0YgKMj97ZANBgMyMjLUmV/tLO/AgQM73EZEREREvedK2cMcAKeklJVSSiOANwBMBTCgvQwCAJIBlDraWUr5jJRykpRyUkJCglsG7QlSShw7dgyjRo1SOzG4U2xsLEaPHo2SkhIcOnTI5r7w8HB1DETeYDabUVhYiKamJrUGnYiIqD9wJfz+AOBiIUS4sKbA2QCOAdgDYGn7Nj8F8K++GaJ3lJeXo6qqCqNHj+6T8CuEwNSpUwEAb7/9ts19yqpuDL/kLcXFxWhubkZRURHy8/NtFl8hIiLyZ67U/O6D9cK2rwEcad/nGQD3A7hbCFEAIA7Ac304To87ePAgACAjI6PPwm9GRgZGjhxps8wxwFZn5H1NTU0231ssFtTV1XEWmIiI/J5LxaxSyt8B+J3dzUUAerfmrw/bu3cv9Ho9srKy+vQ8mZmZ2LdvH6SUashm+CVvMpvNHZ57JpMJxcXFCA8Pd/sFoERERJ7EFd6c2LNnDzIyMhAeHt4nM7+K7OxslJeXo6ysTL1NCAEhBMMveYWy6qBWa2ur0/uIiIj8CcOvA21tbTh48KC6sltfhl9lAY0jR46otwkhYDAY+BEzeUVn4bcv/xaIiIg8geHXgdzcXLS2trp9WWMt5WK20aNHw2Aw4OjRo+p9QggEBQXBaDT22fmJHJFSori4uMPtDL9ERNRfMPw6sGfPHgDAhAkT+ry+0WAw4IILLrCZ+QUYMsg77N9wBQcHAwDOnz8PgM9LIiLyfwy/Dnz++edITU3FpZdeqvbc7UvZ2dnIy8tT20np9XoIIdjqjDzOvtQmNTXV5nu2PCMiIn/H8GtHSol9+/YhJycHBoPBI+fMzs5Gc3MzGhoakJqaiqCgIIZf8gr7cKt0HlGYTCY+L4mIyK8x/No5deoUampqMG7cuD79iFdZwhgAxo4dCwA4ceIEoqKi+uycRF1ROowkJycjLS2tQ/gFgJaWFk8Pi4iIyG0Yfu3k5uYCQJ/399Xr9QgKsrZZTklJQXR0tM1iF5z5JW9Qwm9YWBgiIyMdvgFk+CUiIn/G8GsnNzcXBoMBkyZN6vNzKWUVOp0O48aNY/glr1PCr7LEtqOZ3+bmZo+OiYiIyJ0Yfu3k5uZi1KhRiImJ6fNzDR48WP06KysLR48etWkpxfBLnqZc8KaEXy3lk4rq6mo+N4mIyG8x/GpIKfH1118jKyvL4YyXu0VERGDIkCEArMscG43GDi3PiDzJZDKp3Ubsaf8mGH6JiMhfMfxqFBYWoq6uDllZWR7rZ6pc4JaZmQkAOHjwIADO/JJ3WCyWDrO+Srszhl8iIuoPgrw9gL6Wm5trU0vbGWXWNTMz0yMzv8CPHy+np6dj4MCBDL/kVVLKDm/8lL8Fhl8iIuoP+n343b17Nx5++GGXt09ISMDIkSM9Gn5TU1MRFhaGiRMnquHXbDbDbDbDYrF4bCzOnDp1Cnq9HikpKV4dB/U9R883R0GX4ZeIiPxVvw+/d999N+68806XtxdCoLy83KOBUyl9yMnJwV//+leYTCbU1dUBABoaGhAdHe2xsTjS2Njo1fOT5zia+Q0PD0dMTAwGDRqEkydPqtsp/xqNRnUZZCIiIl/X78NvREQEIiIiXN5eCZ2Ornbva1lZWWhtbUVRUZF6mzfG0dDQAIvFgqioKI/VPpNvcFTzq9PpMGzYMJvblPBbU1ODM2fOYMSIEQgLC/PYOImIiHqKF7zZUfqceqPUQFlYIy8vDwkJCQDg8fBpMplw+vRp/PDDD6itrfXoucn7HM38OtsO+PFTAaVFHxERka9j+LXjzfB7wQUXAACOHTumzlZ7urayqqpK/Vrp+UqBobq6Gi0tLepzX0qJtrY2m+egsiy3lBIVFRXqJyX8hICIiPwFw68db4bfyMhIpKamIi8vT73N0+FXWcgAsF50R4HjzJkzAKx/A0eOHEFKSgpCQkKg0+kQFRWF3//+94iMjATwY/hVMPwSEZG/6Pc1v92lhF9vvZhnZ2fjyJEjPhEmlMeC+j/tm6yqqiosXboURqMRjzzyCEwmE44cOYK1a9fi2LFjWLNmDWpqapzuT0RE5MsYfu0orZ68FT5zcnLw3nvvoaWlBYDnQ4VyPiEEzp8/j6FDh3r0/OQd2t/7Qw89hDNnzuDTTz/FlClT1PvvuusubN68GZdddhlmzJhhs399fb1HlgQnIiLqLZY9tKutrUVjYyPMZrNXOiwoxo8fD5PJhPz8fK+NAbCGHZPJBKPR6NVxkGe0tbUBsC4K8+6772L9+vVq8AWsoXjDhg1IT0/HK6+80mF/XhxJRET+guEX1trWkpISnDp1CrW1tV5dVCInJwcAcPjwYQDem/lVsPQhMBQXF6OpqQkPPfQQxo0bh1WrVnXYxmAwYPny5di/f796oRtgrY/3di9qIiIiVzH8Ah1mN70ZfkeMGIGIiAh1qWVPsw+/2oveWNfZP1VXV6O1tRVbtmxBaWkpNm7caHPho9aiRYtgNpvxxRdfAABSU1MRHBzM5wYREfmNgA+/ZrO5Q0uv5uZmL43GGrzHjx/vtZlfewy//d+ZM2dw7tw5vPjii7jxxhtx2WWXOd12ypQpiI2NxccffwwACA0NhRCCzw0iIvIbAR1+pZQ4fvw4SkpKvD0UGzk5OThy5IhXAoVyziFDhgCw7fXLgNO/1NTUqG/0tm3bhtbWVvz2t7/tdB+9Xo/58+fj888/h9FohMFgYPglIiK/ErDh12KxoLq6GkDHxRxSU1O9MSRVTk4Ozp8/j9LSUq+FCmWRDW1JCOt/+5fS0lIUFhaisrISzz77LK644gpkZGR0ud+CBQtQX1+PgwcPArBeDMfnBhER+YuADb+VlZU4e/Zsh9uHDBmCqKgoL4zoR8pFbydOnPD4uZWwHRwcDABqyzXA+phR/7Np0yaYTCb86le/cmn7K664AiEhIdizZw8A6xLHzc3NXBSFiIj8QsCGX0czVcOHD0dsbKwXRmMrOzsbOp0O+fn5Xpn5FUKovY61s+LV1dVerYcm92tqasKuXbswd+5czJ0716V9oqKiMGPGDHzyySc2z0/O/hIRkT8I2PDraBGL4OBgn1hZLSwsDKNHj/ZKr19tmJFSoqmpyeb+wsJCTw+J+tDu3bvR1NSEVatWdau/9XXXXYfS0lJ888036m2s+yUiIn8QsOHXEW+2OLM3fvx4nDhxwmszv9R/NTc3q7XcX3zxBYYOHWqzoIUr5s+fD51Oh3/961/qKoAmk4mzv0RE5PN8J+15mKNQ6Uuhb/z48Th79ixqamo8el5XwjZn+PxbYWEhTp48CYvFgm+//RZZWVndXtUwISEB06ZNw5tvvqn2BC4qKsKpU6f6YshERERuE7Dht6qqqsNtvhR+L7zwQgCw+VjZU7p6HDi757+UNy4WiwWHDh1CRUUFZs2a1aNPPRYtWoTDhw/jhx9+UG9rbm7mmyMiIvJpARl+/SG8TZkyBUIIHDhwwKPndSW4tLW1eWAk1Be0z/233noLYWFhPQ6/11xzDQDggw8+sLmdXR+IiMiXMfz6qJiYGIwaNcrj4Rfoeua3rKzMQyMhd1OCaVNTE959913MmzcP4eHhPQq/o0ePRnp6Ot5//32b2+37ZhMREfkShl8fJYTA+PHjkZub69HxSinV8KsEosTERJttDAaDx8ZD7qUE0w8//BCNjY1YvHgxgJ5d7CmEwJVXXolPPvnE5tMAhl8iIvJlDL8+SgiBnJwc1NfX49ixY14Zw5gxY5CZmYm4uDiEhoaqtyuPn8lkQkFBAVpbW70yPuq+oqIiAMB7772HIUOGqLXlPa13v/rqq9HY2Ij9+/ert/nD3xcREQUuhl8fNmHCBADWdlSeYj/zq/w3YsQIdeU75fE7f/48WlpauPKbn2lqasJXX32FWbNmYcCAARg0aJC6nHV3zZkzB1FRUepqbwBrfomIyLcFZPhVXpyV9k5JSUkYMWKEN4fkUGpqKgYMGOCVjg/2hBBITU1FeHi4Gn6Vi+N8qT8ydU4Igb1796K1tRUzZ86EXq/HoEGDejzzGxoaikWLFuHf//632jtY+ZeIiMgXBWRqUcJbWloaRo0ahYEDByIsLMzLo+pIp9Nh9OjRyMvL89g5tTO/zsakPH7Kvwy//kOv1+Ozzz5DVFQULrzwQsTFxfX6mMuWLUNtba1a+lBRUdHrYxIREfWVgEwtSmjT6/UICQnx8micE0IgPT0dR44c8ZmPknU6nTrjq4zJl/ojU+eMRiM+/vhjTJ8+vUMtd0/NmzcPUVFR+Oijj9wwQiIior4V0OHX12cshRC48MILUVdXh3379nnknN2Z+W1paQEAdYUv8m1SShQUFKCqqgpLlizBsGHD3HLc0NBQLFy4EB988AEGDBgAAKwDJyIin+Xb6a+PKOHNH2Ysp06diqCgIGzcuNHlfdra2lBcXNwnF/Zpw68y88sVvfyDxWJRSxNmzpzp1uf/smXLUF1djS+//BIAUF5e7rZjExERuVNAhl9/uVBLCIGYmBhMmjQJr776KkpKSlza78yZM6irq0NjY2O3z9nVzK8QAmazGdXV1R0ufCPfZrFYcODAAaSkpCAtLc2tx77iiisQFRWFf/3rX249bk9IKVFXV+c3XV2IiMizfDv9uZHFYkF1dTWklH4X1n75y18CALZv397n5+rqsVHuLysrY/j1MyaTCbm5ubjsssvcfuzQ0FAsWLAAu3bt8nq3h7y8PBQXF/PCOyIicihgwm9lZaU6I6rMbvp62YOyeER2djYmTpyIZ5991iNBs6uZX8D6ZkIZC8Ovfzhw4ADq6ur6JPwCP3Z98MaS3I74ykWiRETkWwIm/CpLriqhzdeDr72bb74Zx48fx65du1ze5/vvv+/2eboTZBl+/cfRo0exc+dOAMCMGTP65BxXXnklwsPDsWfPHp9oHcgLMYmIyJGACb9KQBNC+GX4XbJkCdLS0rBu3bpOtzOZTL2udXT1sWHZg39Qfk/79+9HSkoKUlJS+uQ8oaGhuPzyy7F//36feE4w/BIRkSMBF34Baxjwt/BrMBjwi1/8Al988QUOHjzodLv8/Hw0NTX1+DxSSpSXl+Ott95CZmYmYmJiMHHiRHWhDe3jxplf/2A2m2E2m3Hw4EFMnjy5Ty/0nDt3LoqKily+OLMv9ebvgIiI+q+ACb/a9mZSSp/v9GBPSok5c+YgIiICTz75ZJ+d58CBA5g7dy4WLVqEmpoa3HDDDfjhhx8wf/58p71bGX59m9FoRH5+Purr6zFlypQ+fe7PmzcPAPD555/32TlcVVdXh7q6Om8Pg4iIfIx/JcBeqK+vB2CdBfPHsgfAGtwXLlyI1157DefOnXP78T/44APcfPPNSExMxI4dO3D8+HFs3rwZO3fuRFlZGa655hqHV/Iz/Pq2lpYWdZGUKVOm9Gk5QGZmJgYPHowvvviiz87RGfvnYnFxMUpLS73egYKIiHxHQIRf7Qvi2bNn/TL8KleuL1iwAEajEe+//77bjt3W1oZNmzZh/vz5SEtLw9atW7F48WJ1ta4pU6Zg69atOHLkCO68884OAYPh17eZzWbs378fI0aMQFZWVp8+94UQmDp1KnJzc50+L6SUaicTd3NU715TU8NFN4iISBUQ4beqqsrme3+s+VXCb2ZmJmJjYx2G385CaH19vcP7y8vLMXnyZKxcuRKXXnopXnzxRcTHx3fYbuHChfjTn/6E3bt345133nH5vOR9jY2NOHjwIK6++mokJCT0+fkmTZqEc+fO4fTp0w7vr6qqwsmTJ9Hc3IfayRUAACAASURBVOz2czu72JPPUSIiUgRE+LUvEfDnmV+9Xo9Zs2bhgw8+6LCNoxf41tZWNDY24vvvv+/Q9L+lpQVXXnklCgoKsGPHDnz44YcYMGCA08dm5cqVuPjii/HnP/8Z1dXVnZ6XfMfevXvR0tKCq666yiPnmzJlCgCoSx3bU0qQlPaD7iClxIkTJzq80dXeT0REBARI+A0JCVG/1uv1fhl+tTNal19+OYqLizv08XUWfpXgXFtba3Pf6tWrcejQIbXMQQjR6ay4Xq/Hli1bUF9fj6effhoA1H3Id+3ZswfBwcG4/PLLPXK+cePGISwszGndb0+eL01NTWhpaXF6v8VigdFo7JNaeCIi6l8CIvzqdDoEBwcjOjoaQUFBftntQRsYpk6dCgD4+OOPnW6j0K5ypb3oZ//+/diyZQvuvvtuzJ8/X93fbDZ3+thkZWXhtttuw/bt21FUVKQ+nuSbpJT46KOPMHnyZISHh3vknKGhocjOzlYvsrOnXSXQFVJKFBUVoaCgwOk2Xa3mxucoEREp/CsB9oDFYkF9fT3a2tqg0+n8doU3bVCIjo5GUlIS/u///s9mG0cv8EqPV/tt1qxZg7i4OKxduxbNzc0wmUwoLi6GxWLp8o3B2rVrERISgn/84x/qTDr5piNHjqCwsBBz58712Dn1ej3Gjx+Pw4cPO6zr7W74dWU7+/A7ZswYm+/5HCUiIkW/D7/aF8X+En6bm5sxd+5c7N69u8uPeS0Wi82+Ukp8+umn+Pe//4277roLUVFRKCwsxMmTJ9VazK4em8TERMydOxe7du1CWVkZg4UP27p1K3Q6ncfqfQHr39n48eNhMpmQm5vb4f6+CL/abYQQCAoKQnR0tHpbX3WXICIi/9Pvw6/2RVEJv67MbvoSR3W111xzDUwmE7Zv367e5iiESik7zPxu3LgRERERuPrqq9U6Su02XYVfIQRWrVoFnU6HX//616z59VGVlZX44x//iIsvvhiJiYkeO69Op8O4ceMAOL7orbvht6uSBsD2ua/8bWsvqHPlGEREFBj8JwH2kPICm5CQAJ1OByklTCYT9Hq9l0fmOkfhNyMjA1lZWXjppZc63VdKifPnz6vfl5aWYseOHbj22msRGRnZ4xWwBg8ejKlTp+Krr77CqVOnenQMsmptbVVn3d3pH//4BwDgpptu8uibPZ1Oh9jYWGRmZnZoi6fVk5lfZ0sWa7dR/rZjY2Nt7ucnFEREBARQ+I2IiLAJAH25ylVfsA8KQggsX74cX375JcrKygB0nPkVQsBsNttcJb9lyxaYzWasWLHC4XGV/VyxevVqAL6xlK0/O3nyZIfOHb0lpcTzzz+PSy+9FJdffrnHwy9g7Q39+eef44cffugwNsC18CultGlfVlRU5PQTDoUSfpVFWhTubK1GRET+K2DCr06ns5nt9YfwO3z4cAwbNgxCCIcv+FdeeSUA4N///jcA2wCQnJzscMZ4+/btmDZtGoYNGwag4wIg3ZGeno5hw4Zh7969PT4GuZ/ZbMauXbuQn5+Pm266yeNlPkIICCGwYMECAMC2bdsAWMNnY2Njt8JvU1NTh08nHP0taI/lrMSjLxbVICIi/9Pvw69S66fT6RAaGqrerv3aV4WHhyMmJsbp/aNGjcLAgQPx2WefdbjPYDB0CM2FhYXIz8/H4sWLOz2vKzO/F1xwAS644AJMnz4d+/fvt2mjRt7V1NSEXbt2AbDWhnujxl2v1yM1NRWTJk3Cq6++CsA6a3vq1KluhV9HzytH4be0tBSAtRwoMjLS4bEaGhpcHj8REfVf/T78Ki+wer0ewcHB6u3ar32ds4t1SktLMXXqVLXsQBsKlNk3bcDYs2cPAKgzcs64EpT0ej2CgoIwY8YM1NfX49NPP+1yH+pcbz+WN5lMaGlpwblz5/D6669j2rRpGDJkCADXfqfupFxcumLFCnz99dc4ceIE2tra1HECrrUfcxSQO9vP/o2b9hMe7aqEREQUuAIm/CplD9HR0RgyZIhfdXtwJjQ0FNOmTUN+fj4qKytRXFys3qeEX21Q2L9/P7Kysjq98l+v12PgwIEuj2HatGkQQmDHjh28oKiX8vPzbX6H3VVYWIiCggJs374dFRUVuOWWW2yWxfYkJfxed911EEKos78A1BDc3RZmrp5XKyMjA5mZmd06BhER9W/+nwC7oA2/AJCSkoK4uDhvDsltoqOj1dXevvjiiw4fEWtnftva2vDNN99g+vTpnR4zKSmpW28MhgwZgqysLHzyyScsfXCDnnbfAH4sEXj55ZeRmpqK2bNnq0HTW+F36NChmDFjBl599dUOb456Gn4dvckSQiA+Pr7Dc1cIAZ1Op5YBERER9fvwO2DAAKSlpfXLFz4pJSZNmoTg4OAOHRfsZ36//fZbtLS0YPr06Z3O0HZ3Rjw6Ohrz5s3D0aNHUVhY2P0fgjrozQz64cOHcfjwYdxwww0AgNOnTwPwfPiVUqoXt61YsQLfffddh5Z4roTfiooKh8d2dFtnf+PR0dH98v8BRETUff0+/AYHBzu9AKY/CA0NxeTJkx2G36CgIHXmb+/evdDr9Zg2bVqnx+tu+BVC4LrrrgMAvP76693alxzrzYIML730EiIjI7F48WKb44SFhbljaC5TOiu0trZiyZIlMBgM2Llzp8023Qn52k9r7PdTvu8s3Cqfghw9epSLshARBbh+H377M+VFf9q0afj6669t+vkCQEhIiPpCv3fvXowbNw6RkZHdumDIFWPHjkV2djZ27NjR7X3tSSlRV1cXcPXD2gswexrOTp8+jffeew/Lli3DgAED0NjYCMC6IIm3FnURQmDgwIGYN28edu7cqf5etUsPdyY4OBgxMTEYPHgwQkJCOtzf0tKCRx99FHfffTemT5+OX/7ylw4fP21JjvKGkIiIAhPDrx9wFEi1JQ0zZ86E0WjEP/7xD3W2T6l1BICamhrk5eXhkksugdlsdhgsk5OTAVhbpHWXTqfDFVdcgW+++QZFRUXd3l+ruroaxcXFqK2t7dVx/IXyu9B2Jehp8H/uuecQHByMdevWITw8XL3dG4s7KBdVKj/L8uXLUVpaiiNHjgCA097V9pQ2bUIIDBo0yOaYVVVVuOSSS7BmzRocP34czc3N2LBhAx577LEOx9H+DXHml4gosDH8+gHlhXvEiBHq7Jf2xXzu3Lm47LLL8NRTT+H666/Hrl278OWXX6qh57PPPoOUElOnTnUYOIKCgjBgwABkZ2f3aPEPIQTmzZsHwLqIRm8owcR+Fru/Un5ebVlCT8LvqVOn8M477+DGG29EYmKize+xs17RfUV5nio/y8KFCxEcHIz33nuvW88xbY9i5TkvpURLSwsWLVqE48ePY+PGjdi9ezc+/fRTLFu2DGvWrEF+fn6nxyQiosDF8OsHIiIiAFg/Ah4+fDhGjx5tM3Om0+mwY8cOrFmzBjU1Nbj//vsxc+ZMpKSkYPr06XjooYcQFxeHrKwsNDQ04OzZswCsnS9iY2ORkZHRq/HpdDoMHToUY8eOxdatW3t9LMA2AEopUVNT0y9LIZSP48PCwjB06FAA1p+3ra2tWz/vunXroNPpsGrVKgA/zuCHhoZ6vN4XsA2qgPXC08suuwzvv/9+h+dvZxxdyGY2m/Gzn/0MX3zxBf75z3/i6quvBgAMHDgQGzduhMFgwLp162z20bb3601NNRER+T+GXz+QnJyM4cOHQ6/Xq4t12IeHsLAwLF++HDt37sT27duxdetW3HrrrZg6dSruuece7Ny5E3q9Hg0NDTazjUOHDu31VfDK/krpg7u7PjQ0NKC0tBTl5eVuPa4vUOpPg4OD1cDa2tqK7777DiUlJS4d49tvv8Xzzz+PJUuWqAFa+Z14u8OB9jl6zTXX4OzZs9i3b5/L+0opO8z8/u53v8PWrVvx5z//GcuWLYMQAmFhYYiOjsagQYNw++2345VXXsH333+vHktbzsPwS0QU2Bh+/YBer7ep4XREeUEPCQnBmDFj8JOf/ASPP/441q1bh5/97GfIysrqs/Ep4WTu3LkAelf6oIQlR7OCTU1NPT6ur9KGXyXcKbPBrvT8NZvNuPPOOxEXF4eVK1eqvwulw4mywpun2c/8AsCsWbMQHByM1157rcN9jtjfL4TAnj178Je//AU///nPce+99wJAh+Wb77nnHggh8Pjjjzs87pkzZ7r/AxERUb/B8Oun7Gd+tRc1GQwG6PV6hw3/O/u+N2MBgKFDh2L8+PHYtm1bj4/V2aIG/bHsQbsCm6PA2FV96rPPPot9+/bh4YcftqntDQkJQXZ2dpdvmvqKo+dWREQEZs+ejVdffdWl2VflOa0cS6/X4+WXX0ZKSgr++te/qrfbh99hw4bhxhtvxJYtWxz2CQb653OJiIhcw/Drp+zDrzZMjBw50qbbg7K9Jz4Cv/rqq/HNN9+goKCgR/srP4c29Clf98cLlZSaVu3vx9HPrmhra1NvKysrwwMPPIBZs2Zh5syZ6vF8gbMgf+WVV6KyshInTpzo8hjfffedzbGKioqwb98+3HTTTTYXzdmHXwC477770Nraig0bNqi3aWuf++NziYiIXMPw209oZ36VIGAffu31Re/Xq666CkIIvPDCCz3a31HQDYTwC6DL8Nvc3Iwvv/wSFRUVkFLil7/8JZqbm/H0008jNjYWANR/vc1R+DWbzbj00ksRFhaGtWvXqgthOKL9uZVjPf/88wgKCsLs2bNtnu+Owu+YMWNw7bXXYuPGjTh//jwAYPjw4Q6PT0REgYXh1091NvOrhAVtIFB6pdofw53jAawLKlx99dXYsmVLjxYT0M78VlVVoaSkRK3R7O2sZmtrq1d63nbGUSsvbTBrampSuz8sWrQIM2fOxAUXXIBRo0Zh27ZtePjhhzF69GjU1NQA6P4KfX3F2cxvcnIynnzySezduxd33HGH+vtoaWmxCcPaiyaFEGhpacGLL76IGTNmID4+3mbRCkfhFwAefPBB1NXVYfPmzTZjAvpn/TgREbnGN14pqdschV+dToe0tDSbbRSOwq87KbWlUkrcddddKCsrw5tvvunSvm1tbWqY0Ybfs2fP2ix20dvZupMnT7r0cbsndTXzW1JSgvz8fNx00014//33ceONN2LmzJkYOXIknnrqKfzmN78B4HsdDByFX+Vnvf322/HII48gNzcXd955J9ra2lBQUIDCwkI1DLe2ttoca/369Th37hxWrFhhcx4ppdPwO2nSJMyZMwdPPPGE2jda+fsoLi72uceMiIg8o/srGpBPklIiODhYvcof8Gz4HTJkCAoKCtDW1oaJEyciPT0dmzZtwvLly7vcV6ntzM7OVoNfZx+J94av1MQqLBZLh/BrHxjXrl2Lbdu24e6778bNN98MwLowyZgxYzps72ucje22227DmTNnsHnzZhQUFODRRx9FTEwMDh06hIkTJ6Kurg6NjY34/vvvUV5ejnXr1uGGG27A5MmTAfwY9pXjO3tuP/jgg5g9ezZeeOEF3HnnnWrPbMD6pssbPZCJiMi7GH79lP3Mr6PFAOzDrzI7FhoaimHDhrl1PKGhoYiJiUFdXR0qKyvx85//HPfffz/y8vJcbrPW0tLS6eyuL4e8npBSoqmpSe1Bq/x+6uvrAViXet60aRO2bduG1atXq8EXsJ0drq6uBoAerc7XV+yDvKPf3cqVKzFmzBjcd999uOaaaxAUFITq6mrEx8ejtrbWpkQlMTERf/nLX3Du3DkAHevAnZV7zJw5E1OmTMGf//xn3HrrrTaPkdFoZPglIgpALr1aCiEGANgCIBuABHALgBMAtgJIA3AawHIpZU2fjJI60IZfi8WChoYGh9tov1YucIuMjFSXn3UnbQC55ZZb8PDDD+Ppp5/Gxo0bXdrfYrHAYrEgODjYYb2wsuhBb2ew3XEMd2hra4PJZEJCQoJ620cffYTt27ejuroaRUVFaG1txYoVK3D77bfb7KsNv0rJyMCBAz0zcBc4msXW3q78O3v2bLz77rv4n//5H+j1egwePFhd3W7QoEFIT0/H9OnTkZ6ejvDwcDQ2NqK5uVn9+bvqhSyEwAMPPICf/OQn2L59O2644QYkJyejpKSEZQ9ERAHK1amiJwG8K6VcKoQIBhAO4DcAPpRSrhdCPADgAQD399E4qRPOLiyzD799veqXNvzGx8dj+fLlePHFF7Fu3TpERUV12L6mpsZmHyX8RkVFdfiZIiIi0NjY2OPgqq0hra6uRlxcXLeP4W7KzGZISAhOnz6NlStXYteuXUhKSsKIESMwduxYrFixQu1SoKzup5SEVFRUIDIyUn08Bg0a5J0fxIHu/I4mTZqE9evXO71/2LBhak35sGHD8N1336mhWln1T3sBnL1FixbhggsuwPr163H99derz8WKigqfesNARESe0eUFb0KIGADTATwHAFLKNillLYBFAJR+Vi8AWNxXg6SOtDO/zjoYOAsgfTXjZX++u+66C/X19Xj55Zcdbl9aWori4mL1+8rKSlgsFocf3yuBpaelD9ruAdog7AopJc6dO+f2ThEmkwlSSmzcuBGZmZn45JNPcO+99+Kdd97BU089heeffx5Tp05VtzebzTaPTUVFBYqKitQLvnxhNlvhrOzB0Ruwrn6n9uU7gPWxKCkpUb/XLl9sT6fT4f7778fhw4fx3nvvqft0FpiJiKj/cqXbQzqASgB/F0J8I4TYIoSIAJAopTzbvk0ZgMS+GiR1pA2/p0+fdmmf6OjoPhyR7czv8ePHcdFFF2HChAnYsGGDS6FVKWtwVL/p7GN0VznqG+uqxsZGlJWVoaysrEfn1lJ+RmVMf/vb3/Dggw9izpw5yMvLw09/+lM14AYFBXXo25uUlNThzYGzbgfepDzGDQ0NOHr0KM6ePWtzf3dWsbP/BAMAamtrbeqCu5rJX7FiBWJjY/Hyyy9DCIGIiAivrX5HRETe5corZhCACwE8LaWcAKAR1hIHlbS+kjlMJUKIO4QQuUKI3MrKyt6Ol7rB0zOB2gBiNpshhMDq1atx/PhxLFy40GFdspZSk6wNctHR0Rg8eLDDNmA91d3HpSf9ip05ceIE8vLy0NjYiMceewxPPfUUbrzxRrz55ptITU3tsL39WIOCgjqUNxiNRp+a9QVswy8AtQ+xQtvNQ2lDNnToUIfHsu9XDXR8HnT18xsMBixevBhvvfUWWltbO1wwSkREgcOV8FsCoERKua/9+9dhDcPlQoghAND+b4WjnaWUz0gpJ0kpJ2kv7KHe6c2Ld1+96Ov1eptWawBw00034dFHH8WuXbtwww03dLq/MounDTtJSUmIj4+3mfErLS3t1Ti7GxSVBRHcETBNJhPOnTuHSy65BOvWrcNVV12FZ555Rv2Z4+LiEBYWhsTERKcf5duXXzQ0NPjczK8zjt7EKGU4zlYcdDTz25Pn8NKlS3H+/Hl8/PHHAPqunR4REfm2Li94k1KWCSGKhRAZUsoTAGYDONb+308BrG//9199OlKy4Sj8JiUldblPXwsJCVFn+6SU0Ov1+M1vfgODwYD77rsPb7/9NubPn++w7lgJI46WZVaCUUWF9T1WVz+rvfDwcDXEdic41dXVqQtt9PZNg5QSlZWVuPXWW1FWVoZnn30WF198MYKDg9VthgwZ4tJx7HW3jrmvKRdYOnvMtOFX2caV8AtYnx89+QRg+vTp0Ov1+Oyzz9TnT1NTE8sfiIgCjKvTRasBvCyEOAwgB8AfYQ29c4UQJwHMaf+eApw2qGgD7q9+9StkZmZi1apVyM3NRX5+vtP9tL1XHS3V3Fvd6YerhO3eMpvN+Oyzz3DLLbegrKwMmzZtwsKFCwF0/aZEqftVyiKchUR/4GjmV/na2c9l/7vv6aceERERyMnJwZdffqnextIHIqLA41IKkFIeAjDJwV2z3TsccpWjANBVw35ldasBAwb02bi0QcVkMqlB02Aw4LnnnsPcuXMxe/ZsLF++HEII7NmzB0II6HQ6LFy4EP/xH/8Bg8GAyMhINDQ0dJj57SkpJUJCQtDa2tqtIK3dtjdBqbS0FGvXrsXp06exefNmTJo0SQ3WXYXfIUOGIDExUX0M4uLiUFFR4Zb6577k6DmqfB8aGqp2W+hqoQpHM7/aTg3deT5fdNFF+Oc//+mTHTKIiMgz/KNQkDoQQqCtrQ1NTU3Q6/WIiIhAaGhop/uEhIQgOzvbZonXvmRf2pCdnY1t27Zh7Nix+Pvf/47nnnsO8fHxGDp0KCIjI/H444/j2muvxYEDB5CSkoKMjAx1396GX3d0RKirq1NLJzojpURxcTEaGxvV2w4cOIA9e/bgtttuw7x582y27yqAaRcoUb6/4IILbHrU2neF8AWOfi4l/CYnJ6sX7nUVRB3N/Gp1p4vJ5MmTUV9fr3af4MwvEVHg8Z31UKlHioqKADif9TUYDG7vT9sZRy2samtrUVlZifj4eAwbNgx/+9vf0NraipaWFsTExACwBpgdO3bgsccew9y5c/Hmm29i1qxZ6rG0dbE9YbFY1AvIXA08LS0tHS6KOnXqVJfLNVssFtTV1aGurg7Z2dkAgBdffBFhYWH42c9+1mlPWlcJITB48GDU1tb6bIDrLPzq9Xq11lbpDOJq+O0qDHdmypQpAIAffvgBSUlJPvvYERFR3+HMr59y9FGwI6NHj0ZmZqYnhgTANlgqX5eWlqK1tdXmoqyQkBDExMRAp9Nh8ODB0Ov1mD17Nl566SWkpKTgqquuwh/+8Ae3hROTyeTwMeqsdKCgoKDDba72K1Yotb7vvPMOlixZov7M7qAsB+zquDyts/Brf5uj8JuQkIDY2NgOt3f1fWcyMjIQGRmJgwcPAnBP6zwiIvIvDL/9hLMA0NmMWl/QzjIrwUIJPI46EowYMQLx8fFqIBwyZAg+++wzLFiwAA8//DDWrl2rbutKJwRHKioqbJZFVsbT0tKCY8eO4fz5853ury0p6Kz8QlmxTRvwDhw4gKVLlyIxMRGrV68GYA3+ivj4+O7/QH7ClXIO4Mfwa/+mIDY21mHv365mgjuj1+sxadIkNfz64psGIiLqWwy/fsr+RdtXLtxJSEhQSxRKS0vxww8/qPc5Wk5WCS7KQgcmkwkDBw7E9u3bccstt+CRRx7Bpk2bAPT8Qj3lwjL7HsRKSYPSysxeaGgoQkJCMHToUHV5ZWedIoxGI/Lz81FZWWnzu9m4cSMqKyvxxBNPIC0tDRkZGTYlKomJvVsYUSmh6G1ZSF/oTvh1tL2zYKo8V1w9j72JEyfi6NGjMJlMKC8v79a+RETk/xh+/ZR9MHAULL0hJCQEw4cPB2Ado3ZW1dEYleDiKAht3rwZCxYswMqVK/Hss89Cp9OpYdnVGTvtOe1rfh0tqqGldIgArBdoAdbZa0cflSvHqqmpUY9/8uRJvPbaa1i2bBmysrIghIDBYEBYWBgGDBiAUaNG9fpNS1RUFFJSUnxyBrk7ZQ/22ycnJzsN9NoLKVNSUrq80NNeTk4OWlpacPr0aZY9EBEFIIbffsKXPr51FugcLWyhbKuESy2DwYDt27fjyiuvxB133IE33nij2yFPG26czSzW1tY6XO1L2yFCW+7Q2TLNRqMRUkpYLBY88sgjiIyMVMsdlGMIIZCcnGxT/tBTQghER0f7zMx/dyhjVi54U8TFxbk8y9+Tnz0nJwcAkJ+fr5aqEBFR4GD49VO+/ILdkz66zmppQ0JC8Oabb+Liiy/GzTffjJKSEgCu//za8KucQ9lXe4zCwkKHPWkdBStH3TPsVyx75ZVXcOjQIfz6179Wg5w7wq4/UR477Qpq2sfY0bLF2dnZLtd2d9XX2pmMjAyEhITgxIkTAHjRGxFRoGGrMz/ly+G3OzNx9mUPjmpqQ0JC8PLLL2PChAn4yU9+gmeeecblDhbKbHNMTEyH8Gn/GFosFpsQ7qw3cE1Njc1FcC0tLdiwYQM+/PBDBAcHIywsDG+++SZmzJihruIGIOCW0VV+p5GRkYiOjkZZWZlbjqssgKKseNddBoMBY8eOxfHjxwH49t8SERG5H8Mv+YwxY8Y4Dc7Dhw/H+++/j3nz5uGWW27Bvn37kJSU1OUxlVm9uLi4DnXFyr8GgwFGoxFms7lDP2DteBISElBZWYnm5mY1GJvNZixduhQ7d+7EsGHDIKWETqfD/Pnz8fvf/97h7GagcFTLa98GTtGdxyYlJQVGo7Fby1Tby8nJweuvv96hOwcREfV/LHvwU86WjfV1er3eaYlDUFBQp63EpkyZgu3bt6O6uhpz5szBr3/9a3z99dcunddRAJNSQq/XIyQkBBs3bsTixYtxww034PTp02oo0u6n7cyglD48+eST2LlzJx599FHs2rULu3fvxldffYVHH31UDWchISE2q9UFGmert/W0Q4VOp+t1CUlOTg5qa2tRXl7uN387RETkHgy/fsr+Bbu3LbPczb4eUykfCAkJwZgxY3p83AsvvBBPPPEEpJR48sknMXHiREyaNAlvvfWWw+3tZx91Oh2klGhtbUV1dTX27duHiRMn4plnnkFhYSHeeustzJ8/X21/5qx+2Ww248MPP8R9992HRYsWYfny5ep9VVVVAKAuPyyldMuqbv7K2cyvwWBQA7CnZ8UnTJgAADhx4gTa2to8em4iIvIuht9+QKfT9eoj4L6QlpaGhIQE9XtlfO5YdGPq1Kk4evQoKioqsGHDBtTV1WHx4sUOA7B9+BVCQEqJiooKHDp0CHfddRcSExOxfft27N69Gxs3bsSJEyewZMkSGI1G6HQ6GI1GlJaWwmg0oq6uDkePHsXixYsxZ84cjBw5Es8991yH3rPAj4tj9PTCrP6is9+5t0pBxo4dCyEEjh8/jtOnTztcgIWIiPon30pM5DLtDJq7lst1J71eb/OxtjK71tuwo63bHTBgAFavXo3bbrsN06dPx3/+53/icS1KNwAAIABJREFUwIEDGD16tLq9s/Cbl5eH//qv/8KgQYPw7rvv4vz582htbcWll16KjRs34uc//znuv/9+zJgxAxs2bEB5eTl0Op1aQxwSEoI//OEPuOOOO1BTUwPAOtOrfA1YP9YfMWJEwHV5sKfT6RAVFYWWlhYMGjTI5j5vhd+oqCgMHz5c7fhgMpkC/vdERBQoGH77AV+9kMrRuNzVVkob/sPCwvD6669j4sSJuPbaa7Fv3z51NTdH4bewsBArVqxAdHQ0nn/+eSQlJeH8+fPqBVjXX389KisrsWbNGrz//vuYPn061qxZg4qKCgQFBSEmJgYzZszAuHHjAPy4glxYWJgafiMiIqDT6QJ61lf7mOt0OodLFdtv60lZWVlqzbiv/g0REZH7Mfz6Ke1sr6++cDta1KKzC9pc4exnTU1NxWuvvYYrrrgCq1atwt///nd1lle7nxACf/zjH9Hc3IxXXnkFM2bMUD+W114Id++992LEiBEIDw/HggUL1MfbbDbj+PHjiIyMRFtbm83s9sCBA3HmzBkA1hpsX/29eFpnn0x48zGaMGEC3n77bTQ1NfGiNyKiAOJ7n5eTS5KTk9XZTV+lneVVVmZTwk56erpNeYKr7NuVac2ZMwf33HMPXnjhBSxduhSff/65uoLX+fPn0dTUhD/96U947733sHr1agwfPlztRKANaBaLBSaTCWPHjsWsWbNs7lO+bmhoULtC2I/N/utA5cpFfvZ9nj1p3LhxkFLi1KlTXOiCiCiAcObXTwUFBSEhIaHTpXa9TRsMQ0NDbe6LiIjo0TGV8OksrKxfvx56vR5PPvkk3njjDaSkpMBsNqO0tFTdZtmyZbjrrrtgNBrV2/R6vTpTLaVUL2CzL1vQhjRlKWOgY7eN3s5w9wdDhw5FSEhIp4t7ePNNQnZ2NgDg5MmTDL9ERAGE4deP+frsYlxcnFoP62jBg57obOYXsIbj3/3ud/jVr36FF198Ec8//zxCQ0OxatUqtLS0YPjw4bjwwgvVxSgU2rCqhN+goKBOu2jo9Xo1QNtvF8itzRR6vb7DBW72vDnzq1yMWFBQwPBLRBRAGH79mK+HX22gjI6Oxvnz53vdj7irmV8AOHXqFMxmM5YsWYKrrroKgPXiJiEEzp07h7KyMpjNZqd101JKmM3mLmdvlX7BANROASNHjnRLO7dA4c3wq9frkZmZiYKCAptPAYiIqH9jza8f86eApdfrkZqa2uNVvRTOZn6llDhz5gwaGhrU8oWmpiYA1llZ7SIXANDS0uL0QiyLxaIuX+yIUmutLG+snAOwlnewZZbrvN2mLzs7GwUFBeqKfURE1P8x/Poxfwi/o0eP7tWKbva0F5xp1dfXo7q6GqdPn+6wz/DhwzvsD8AmpGrDdGVlZYeyCK2UlBQA1tIGJfyyxrdntIufeEN2djbKy8vVVfmIiKj/Y/ilPhUcHOzW1eeUY9mHFWc1wPHx8TazzdqxaAOrfRlFZzO/Op0OkZGRanmEcht1n7ffNCgXvR0/ftyr4yAiIs/hKzb5FW14bW5uVr92Fj7tu0pouzdoA6/9zKPJZOo00Op0OphMJrV22B9m4X2REn69dcHZ2LFjAQD5+fleOT8REXkewy/5HWWlsMLCQvU2RzO/BoMBUVFRNrfp9XqMGTMG4eHhiIuLU2+3n502mUydtuhqaGiA0WhEY2Oj12cv/ZnyuHsr/CYnJyMqKgrfffedV85PRESex24Pfiw4OBgRERFdtpPqb7Rh89SpU057BmtrfbWCgoI63JecnIyamhqUl5ert9kHZy0lrLW2tnboYUyu8/bMrxACo0ePZvglIgognPn1Y0IIpKen93jBCH+lnaVtbGxERUWF2k9YkZSU1K1eu8qiIVqu7s+Z357z9swvAGRkZOC7777jEsdERAGC4Zf8jiths6cXoGlLIVyt42X47TlX+jb3tYyMDNTV1aGsrMxrYyAiIs9h+CW/40r3CHeE386kpaX16Phky2AwICQkRK3j9oaMjAwAwLFjx7w2BiIi8hyGX/I7er0eAwYMwJAhQzrdpidcDc29XayDrIQQGDVqFKKjo702BiX85uXleW0MRETkOQy/5HeEEEhOTu50lrY79b72x6bAEhUVhejoaBw+fNjbQyEiIg9g+KV+ITw83Ca49nRhDVfDr8FgUAM2lzP2by0tLRg5ciRnfomIAgTDL/UL6enpyMzMVL/v6Qyuq/spLbKSk5M7dIkg/5KSkoIRI0YgPz+fHR+IiAIAwy/5tdGjR2PMmDEQQrilZEE5hiuzuUIIDBgwgEsb+7mQkBCMGDECtbW17PhARBQA+KpNfi04OLjHJQ7OjBo1yukCGdT/6HQ6jBgxAgDw5Zdfenk0RETU1xh+qV9JTU1FcnJyr44REhLC3r0BRK/XY+TIkQCA3NxcL4+GiIj6Gpc3pn6lsyWJiRwRQiApKQmxsbHs9UtEFAA480tEBGDixInYv38/L3ojIurnGH6JiABcdNFFKCsrQ2FhobeHQkREfYjhl4gCnpQSkydPBgDs2bPHy6MhIqK+xPBLRAFPSon09HQkJCQw/BIR9XMMv0REsF74NnnyZHz00Ues+yUi6scYfoko4Clhd8qUKSgvL0d+fr6XR0RERH2F4ZeIAp6yst+UKVMAAB999JE3h+OXjEYjZ8yJyC8w/BJRwFOWqE5OTkZKSgrrfrupra0NJ06cQGVlpbeHQkTUJYZfIgp4cXFxAKwzwJdeeik+/vhjWCwWL4/KfyiPVU1NjZdHQkTUNYZfIgp4sbGxagAeN24cqqqqcOTI/2fvzuOiqtc/gH++swDDvqMgbiCuReaaSwm4pYJr5tbVrF9amak3vXXLsmtZ3bKsa1aaW4aJV3OJ1PSKa2rXtLoJroBAiMrOsM9yfn/gOZ3ZYFhmY57369UrmLPOMM585pnveb6/2/is7BPHcQbDG/jf6QMDIcQRUPglhBAA7u7uAED9fhtQXFyM1NRUVFVVoaSkBABorC8hxKFQ+CWEEADe3t4AgDZt2iAyMpIuejOhrKwMAJCeno4//vgDarVaqPjyFw4SQog9o/BLCCHQDW7R0dE4efIkNBqN2duXlpaiqqrKEqdmV2Qymc7vWq2WKr+EEIdC4ZcQQvQMGDAApaWl+OWXX8xaX61WIycnB+np6RY+M9tTKBQ6v1dXV1PllxDiUCj8EkKIHn7cr7lDH5wh9JqSnZ0thF+VSoWamhobnxEhhNSPwi8hhOgJDAxE9+7dzb7oTaVSCT+Xl5db6rTsgrEhDuL7X1JSgqysLFRUVFjztAghxGwUfgkhxIjY2FicOnVKJ9iZ4+bNm5Y5ITvTsWNH4ee7d+8KP2s0GiiVSty8eZNanxFC7BKFX0IIMSIuLg4VFRU4efJkg+vqXwTmDBeAKRQKBAUFGdzOXyTIcRzS0tKsfVqEENIgCr+EEHKPq6ur8PPw4cPh6emJpKQko+tWVlaitLQUAODi4qKzLDU1FUVFRZY7URvigz1jDF5eXgbL+ceEEELsFYVfQgi5JzAwUPi5sLAQ48ePx65du1BbW2uwbkZGBnJycgAYn9ns1q1bljtRO+Hu7m60+ksIIfaMwi8hhNzj5+cntPKqqanBtGnTUFxcjCNHjtS7nTMMc+Dp31dfX1/h586dOxusX1lZ6VSPDyHE/lH4JYQQkeDgYACAm5sbRo4cCT8/P+zYscPk+hzHOWW443v6iod86PcABuoq5IWFhVY7L0IIaQiFX0IIEfHy8oKbmxuAumA3efJk7N27F5WVlUbXLy0tdequBnwIZoyZnOTC2LARYlp2drZTDJshxFYo/BJCiB7GmBBop02bhvLychw4cEBYLp722BG/1tdqtcJ9UCqVjQrvxu5rt27d0LVrVwBARESEwXKJhN5qGqOsrAxFRUXQarXUL5kQC6BXJEII0SORSISQN2zYMISEhGD79u3C8suXLws/u7q6Olz4zc7OxuXLl1FWVoasrCydPr2m8MM7+L7H4iqvTCYT2r0ZG/rgiB8QbEX8OBUXFyMzMxO3b9+24RkR0vpQ+CWEED2MMSGESKVSPPHEE9i/f7/RCSzy8vLAcRykUqmVz7Lp+FnosrOzAZg3LCEnJwepqakoKSkBUBdo7969i+LiYoNg6+/vr/N7ZWVlq5j5TqPRoLq62uLH4PEfNAoKCujDAyEtiMIvIYToEYdfAHjxxRfBGMNHH31kdH2O4+Dv74+ePXuaHPdqL4yFN/1zNnYRX1lZmfDz+fPnER4ejpCQEPj7+2Pw4MFISkrCnTt38OOPP+Lq1atQq9U625eUlDR6tjx788cff+DGjRsWHeMtDr/iv5UzjysnpKVR+CWEED364bddu3aYOXMmvvzyS6HyqU8ikYAxhsjISGEf9sjYhXvic+U4DqmpqUJVWN+ePXvwzDPPICAgAO+88w7efPNNZGZmYtq0aWjTpg2GDBmC2NhY9O7dG6+88opQVS4tLUVGRoZl7pSV8GHU1HOgJYhDrrhaTpVfQlqOrOFVCCHEuUgkEoNK2/z587F161Z899136N27t8E2fIB0dXWFj48PqqqqrHKujaU/FTPw57mr1WpcuXIFQN2FcDytVotz585h69atOH36NAYNGoTvv/9e6PH76quv4syZM/j111/Rtm1b+Pv7Izk5GR999BFKS0vx6aefgjHm8JVfT09PFBcXW/R+6FfMeVT5JaTlUPglhBA9+pVfAOjfvz/atGmD/fv31xt+TW1vL4ydFx+IMzMzdW6vqqrC6tWrsX79euTk5CAoKAgLFizAU089pTO5hVQqxdChQzF06FDhtn79+kEmk+H9999HSkoK4uLiLHSPrKOsrAzFxcUAdIcmtDRTFXe1Wm0wjTYhpGlo2AMhhOgxFl4lEgkefvhhXLx40eQ29W1vL/gKoo+Pj8Gympoand+nTJmC5cuXo0ePHli1ahUOHjyIefPmGa0e6/P09MSSJUvQtWtXvP766ygoKAAAh73wTRxKTfV8bq7q6mqD501AQAAAICsryyLHJMQZUfglhBA94j6/YlFRUcjKyhLCj4eHh7BMv5etvYZf/rxcXV0NbhPbtGkTDhw4gLVr1yIpKQnx8fE62zSEMYY2bdpg7dq1KCsrw5o1awC0jgkvLHUfxEMewsLC4OLiInzQ0Gg0dvucIsTRUPglhBA94j6/YkOHDoVGo8HFixcRFRWl89W/fuXXXvGhXhzu9e/r5cuX8a9//QtTpkzBc88916zZxoYPH46ZM2ciOTkZOTk5Dhng9MfharVai4zBvXPnDoC6VnF+fn6IiooSZhs0dh6EkKah8EsIIXr48Kof1IYOHQpXV1ecPXsWcrlcp9rraMMe/Pz84ObmBolEInQx4O/P2rVr4enpic8//xyMsWb3MJ47dy6kUik2bNhgsKyqqspguIW94S8CFGvpcb9KpVK4SDI0NFS43cvLS/iZD79KpRKFhYUtenxCnAmFX0II0WMq/CoUCgwYMAApKSlgjOkEXv0gbM/hlzEGV1dXREZGwsPDQwhycrkcV65cwcmTJ/HEE08IHR+aOz1xcHAwpkyZgv379+u0O9NqtUhPT8f169ebtX9LMlXhbem/rzlV3crKSmi1WmRlZSEvL486QBDSRBR+CSFEDx9qy8vLDcZ3Dh48GH/88QcOHDjgkOFXo9GYPFetVouNGzfCw8MD06ZNQ3l5eYt81S6Xy/H0009DJpPhgw8+EG7nuyfYs4qKCqO3t2TwVKlU9bbGi4iIAFA3m2BaWppwOw2DIKRpKPwSQogePhxmZ2cbVCVnzJgBADh06JDJ8MuzxwCs1Wp1hjGIw29mZiYOHz6MuXPnwtvbG0BdMOPvZ1PHMkdERGDQoEGYNm0adu3ahatXrwKom/jCnpWVlel0WVAoFGjfvj2A5v1tNRqNTq/gq1evoqioyOT6prprUPglpGko/BJCiB79Gc/EvL29MXjwYBw/frzeyq+9UqlUOmGKD7/l5eVYv349XFxcsHDhQmG5eGxrU++XTCaDi4sLnnzySbi5ueHNN98EYLmWYS1FfyyyRqMRHgPxJCCNdfnyZVy9ehXFxcUmA+xvv/2GqVOn4qmnnsLx48eNrmPJfsOEtGYUfgkhRE99IY/jOAwYMAC///670LsWMB5+7bHyq1KpIJfLhd/5mddOnDiB/fv3Y9asWQgJCRGW63+97+7u3uRjBwQE4KmnnsKOHTuQmpqqs8wRxq8GBAQIf9u7d+82e3+5ubk6zyGgrtfv3//+d/Tr1w9HjhzBnj17MHLkSOzbt09YJzAwEEDdsBxHnzWPEFug8EsIIXoausBr4MCBAIAff/zR6Db2Gn45joNKpdKZKYw/13Xr1sHFxQUvvviizjbir+jlcjk6deqEHj16NPrY/HHmz58PuVyOjRs3GhzH3og/BHXr1k0n/LYUcfhVq9VYunQp3nnnHUyaNAk3btzArVu3MHz4cKxYsUJ4vvF/v8LCQru+WJAQe0XhlxBC9DRU+b3vvvvg4eGBEydOGN3GXsOvOMTyGGM4f/48Dhw4gBkzZqBNmzZQKBTCuGC+py1jDB06dABjrMndHxhj8Pf3x+jRo5GUlKRT7bX3yi//eLRE+DU1TfE333yD48ePY+3atdixYwcCAgLg5uaG3bt3o1u3bnjttddQWlqq8/fTarXIz8+nCjAhjUDhlxBC9DQUfl1cXBAbG4sffvhBCLjGtrG38Mt3rhCHr6qqKrzxxhsIDw/HvHnzwBiDTCZDt27dAPw5s1hAQIDJ0GYufnzx9OnTcevWLZ2pom0VfjUajdDnWJ/4nPi/rzlTO9dHrVYbnSFOqVRi/fr1iIuLw/PPP6+zzNvbG+vXr0dxcTFWr15t0Hf5zp07NP0xIY1A4ZcQQvToB1mNRiO0ouI4DowxJCQkICsrC9euXWtwe3vBX1wlDnAfffQRcnJy8Oabb0KhUOh0dmCMCeG3Je4TYwxlZWWIiYmBQqHAoUOH0KFDBwC2G/aQnZ2NGzduGP2gwodfvsMDYLpqay5THS6++uorlJSU4K233jK6/MEHH8SsWbOwd+9eo0MdWsO00YRYC4VfQgjRox/0bt26hfT0dKjVaiEkjRs3DgCMXonPDwuwt3Gs/Lnz53fjxg2sX78eCQkJ6Nevn8H6UqlUuA/NnegC+HP8cH5+PoYPH47Dhw8LATM7O7vZ+28Kvo+vsb+VRqOBVCoV2r7x/Pz8mlwB5jtI+Pn5CbdVV1djx44diImJwYABA4xuxxjD3Llz4ebmhvfff9/ockKIecx+NWOMSRljvzDGku/93okx9hNj7AZjLIkx1ryPw4QQYif0gwRfraupqRGqoG3atMGAAQPw008/ISoqSmd9PhjZa/hljKGoqAgJCQmQy+VYtGiRsI5++za+WtzS4So+Ph7FxcU4duwYgLoqq6nhB9aQk5NjcJtWqzUa+hljTR6mUVlZCXd3d53tDx48iJKSErzwwgsmH2d+vPTUqVPxzTffIDMzU2e5RqNBWVlZk86JEGfTmI/yLwK4LPr9PQAfcRwXCaAYwFMteWKEEGIrpgIIH2b55QkJCTh//jwKCwt11uPHZNpr+K2srMTUqVORnp6OLVu2ICgoSFhHfN81Gg3Ky8uFn1tSnz594OHhodPC68aNGzabuMHYTG76E4LwmjODn1arhVwuh0KhAFD3N/n666/RpUsXDB482OR2/N9lzpw5kEqlSEpKMljHVtVzQhyNWeGXMdYOwFgAX977nQGIBbDr3ipbAUywxAkSQoi9yM7O1hn/Gh8fDwBITk7WWY+vFtpbB4PCwkKsXr0anTp1QkpKCjZs2IAhQ4borKMffnmmpvltDPG+XVxcMGLECOzduxeurq7C7fb0mJmq/EokkiaHX3566YCAAISGhiIlJQXXrl3D7NmzjQZtnlQqRUBAAPr164fp06dj7969RscP29sHLkLskbmV3zUAlgHgX5UCAJRwHMd/RP8DQFgLnxshhNhEQ90e+OW9evVCx44dDcKvPeA4Dnfv3hXCEMdx+Oijj7BlyxYMGjQIx44dw1/+8heD8aymiKvDTdWpUyed38eNG4eSkhKcPn1auM3a4ZcfosJXYsX4oKqvOa3s+EDNGIOLiwu++OILtG/fHjNnzoSvr6/J7RhjaNu2LTw8PPDXv/4VFRUVWLt2rcF6+rPSEUIMNRh+GWPjANzlOO5CUw7AGHuGMfYzY+zn/Pz8puyCEEKsytzwyxjDqFGjcOzYMZ0+q/bQ51epVOLu3bvIy8sDAPzxxx/YunUrRo8ejeTkZDzyyCMA6mZs69Wrl8594oWF/VnTaM7MbjyFQqFzodeQIUPg5+eHAwcOCLdZO/zy4dZUt4eWDL8cx4HjOGGfKSkpuHz5MubNm4eOHTvWW/kVu//++/HAAw8gKSkJxcXFOsso/BLSMHMqv4MBJDDGbgLYgbrhDh8D8GWM8Ze7tgOQa2xjjuPWcxzXl+O4vi1ROSCEEFvSDzwjR46EUqnETz/9ZKMzMo4PaPwY2rfffhsqlQovvPCC0fWN9SsWB7+W6PbAGENYWJgwzMHV1RXx8fE4cuSIcJ7WHvPL329jwwXqG/PLL28Mfn1+nx988AGCg4ORkJDQqP0AwIYNG8BxnEG3EX6MNiHEtAZfzTiOe4XjuHYcx3UEMA1ACsdxMwEcAzDl3mqzAewzsQtCCHEo4gDYtm1b4WeZTGbQ8zY2NhZSqVSnemkPlV9xQEtPT8fGjRsxefJkdO3a1ex9GJuyuSXwQw2kUinGjBmD0tJSXL16FYD1L9riA6k1Kr/8sSQSCc6cOYNTp05h9uzZOjO2matPnz4IDw/H0aNHdW431UeYEPKn5nyU/xuAJYyxG6gbA7yxgfUJIcQhiIOesYuxxMt9fX3xyCOP4NtvvzUIQ/YQfjmOw4oVKyCXyzFv3jydMC/m5eUlrK+/j5bGhz2pVIqHHnoIAPDbb78BaP4Mao2h0WiEiq9+FZfjuHoveOPXaQxx+H3nnXcQEBCA6dOnm/yb1IcxhrFjx+Ls2bNQqVQtMiabEGfRqPDLcdxxjuPG3fs5g+O4/hzHRXIc9xjHcTTQiBDSKohDn/hrb61Wa3S2s8mTJ+Pq1atIS0sz2N7atFqtcJ4AcPfuXSQlJeEvf/kLgoKCTI4rNVbNbImhDsa4ubkJxwoPD0f79u3x66+/wsfHx2LHNIYfD82fi0qlwpkzZ3Dy5Elh+IUlKr/Xrl1DcnIyFi5ciN69e5t90aG+UaNGoba2Fr/99htCQkKatA9CnBHN8EYIIfWQy+XCVL9AXYDRD7cTJ04EAHz77bc6t1ur8ltQUIBLly5Bq9UiLS1NZ7renTt3QqVSYc6cOQBgV+HX1dUVjDFMnToVR44cQWlpqVXH/PL3TyKRoLa2FgMGDMDgwYPxyCOP4PHHH29wzG9j/758lTkxMREymQzPPvtss85/6NChUCgUOt0yCCENo/BLCCF6xOFWJpOhR48e8PHxMbocqBsXPHDgQOzdu9focksrKCgA8Ge4qq2tBcdxKC8vR2JiImJjY9G5c2cApgOtsUBnqfvh4eGBiIgIBAQEAABmzZoFtVqNf/7zn9Bqtbh06RKUSiVqa2stcnweP6TF398fiYmJ+OWXX7BkyRI888wz2LNnD95//32jjxcfiBsb1LVaLdRqNZKSkjB27NhmD1UICAjAsGHDcOLECQAt05GDEGdA4ZcQQvTohz7GWIOTB0ycOBEXL17UuWDL2mN+xcfjOA6ffvopCgoK8Prrr+Pu3bsATIdf/qt3vipb37rNxRiDQqEQHufo6GhERUVh586dwmQaWVlZuHbtmkWOz+Mfr8LCQqxbtw7Dhg3Dk08+iQULFmDWrFn4+uuv8emnnxr8HfnQ3Ji2YhzH4c6dOzh37hzu3LmD2bNnt8h9GDNmDG7cuIH09HQoFAqrDhshxFHRvxJCCDGDh4eH8LOxiuiECXWTXO7du9dmY37FPV/37duH7du3Y+rUqWZVGL29vdGjRw+dyR6sGaQ2btyImpoag9Zdluz7y+87KSkJVVVVWLx4MYC6v+/SpUsRFxeH1157Db169cLhw4eF7ZpywVt1dTVqamqwf/9++Pv7Y8yYMS1yH0aNGgUA+OGHH5o17TIhzoTCLyGE6DEWXvmv6E0tj4qKQo8ePXTCr7WCiH7f2fT0dDz77LO47777sHjxYrMDpH7YtWaIHzRoEEJDQ3Ho0CGd26urqy16XI7jsH37dvTp00cYGgLUPRarV6/G+vXrUVNTg1GjRuGpp57SGfPd2L+vUqlESkoKpk2bptNFpDkiIyPRuXNnHDp0SAi/FIAJqR+FX0IIMQNjTBjraaoiOmHCBJw8eVIYg2ttfMj97LPPIJfL8dFHH+lUrBuLv5/WCMESiQSTJk3Cjz/+iLKyMuF2S14Ax3Ecfv75Z6Snp2PSpEkGy6VSKf7v//4PqampWLp0KTZt2oRt27Y1aZILxhg2b96MmpqaFhvywO939OjRSElJER4rCr+E1I/CLyGEmEl/hi59jz32GDQaDXbv3g3AeiGE75tbWVmJ/Px8HD16FOPHj29S/1h9UVFRiIqKavZ+zDFlyhSoVCqdiRsaGmvdHBzHYc+ePfD29sb48eMNlkdERACoG+P77rvvon///njllVdQUVHR6CEGFRUVSExMREhICPr169di9wGoG/pQUVGB8+fPA6DwS0hDKPwSQoiZ+FBhKvxGR0eje/fu2L59uzVPSzgflUqFb7/9Fmq1GlOnToVUKm12BwAXF5cmzUDWFP3790dERASSkpLqnXa4KbKzs1FYWKhzW2lpKY4cOYLp06cjMjLSYBv98c8ff/wx8vLy8Pbbbzc6/H755ZeorKzEP/7xjxavpMfExEAulyMlJQVA3YcgQohpFH4JIaSR6uuVO3XqVJw6dQrl5eUtXoGrqKgwCHDAn0Nf+sIgAAAgAElEQVQDVCoVdu3ahYEDB6JDhw6oqamBn59fi56DJUmlUkybNg2pqan4/fffAbRM+NVqtSgrK9OZ1AKouzixuroaTz31lMEYXGN/44EDB2L27Nn44IMP8N1335n99+U4Dp9//jn69u2LKVOmNP2OmODl5YUhQ4bg2LFjAOo6ZRBiD2pqahrVFcVaKPwSQkgjmQq/ANC3b18AwI0bN1r8uJmZmQYBTqVSoaqqCgBw6dIl3L592+hX+I7AxcUFzz77LLy8vLBr1y4ALfMVvvjNlx+6Ultbi/feew9RUVHC34yfWlkqlSIsLMzovtasWYMhQ4bg5ZdfxtNPP41z584hLS0N8+fPR1RUFAYOHIilS5fqtGk7ffo0bt68iYSEhBa70E3fqFGjcOnSJeTn51tk/4Q0RW5uLm7dumXr0zBA4ZcQQoxwc3MzebEYH5KMue+++wAA169ft9jYS36/HMcJ/XsB4MSJE5BKpRg6dCgAIDAw0CLHt6SwsDA8+eSTSE5ORl5eXou0OhNPllFSUgIA2LRpE4qLi/H4448LwxAiIyMRFRWF7t27m5xy2NfXF4cPH8aCBQvw448/4qGHHkLPnj2xefNmoVXcmjVr0LVrV9x3331Yv3495s+fj5CQEIwaNcpiFw/yLc/Onj0LgMb9Evug1Wrtsve0/Z0RIYTYgcjISHTq1EnntpCQEAD1979t3749QkJCcOHCBYuHX6VSqdPb99ixY3jggQeE2ejs8U3HHMuWLYNUKsW6deta5DEUD524deuWMAyha9euOp0XZDIZXFxcGtyfXC7HwoUL8eOPP2L9+vX4/PPPcfXqVezduxfHjh1DTk4OVq9eDZVKhXnz5iEjIwPvv/8+3N3dLRZ+77//fvj6+uLChQsALNsfmRBzcRxns77n9XHMV0ZCCLGBoKAg9OrVq94Xc8YYYmNj8dNPP1msUwG/X3EwPHnyJG7cuCFUfYG66W7rG6Jhr8LCwrBgwQLs27cPFy9ebPb+9IPgkSNH8Ntvv2HmzJlNfnwYY/Dy8sL//d//Yd68eejYsaOwrE2bNliyZAl+//13nDt3DgcPHkSfPn0AwGLDHiQSCQYOHEjhl9gVqvwSQoiTiIuLQ0FBAa5evWqR/fPBRvymwl/pz19Q1bZtW3h6esLLy0uY4c2RLn577bXXEBQUhJdeeqnZHyL0t9+xYwckEgliYmLMqvQawxiDUqmsN2TK5XJ06NABgYGB0Gq1kEqlFg0CsbGxyMrKQkFBAYVfYnMcx0GlUlHllxBCnEFsbCwA4NNPP7XI0Af9fQYGBuLs2bOIiYkRhjzw45IZYwgJCUGXLl0QGhra4udiKT4+PsKFY1999VWz9qXRaCCRSBAaGor8/HwkJSVhxIgR8PX1bXIllr/IMD09vd71xG3iLB1Ihw0bBgC4cOGCRfsjE2KO8vJyALrTrtsLCr+EENLCOnXqhICAAJw6dQoVFRUtvn8+RPEhuKCgALdu3cKgQYOEdfSrLa6urnZZganPzJkzcd9992H58uVC2GwKlUoFuVwOT09PvPrqq+A4DgsWLADQ/HHRDbVxEu+/TZs2zTpWQ3r37g0PDw9cuHCBKr/E5vjXJ3G/bHtB4ZcQQixg0aJFKCwsxKVLl1pkf9XV1cLP4m4PAPDzzz8DqAs/PEcLusZIpVIsXrwYubm5+Pjjj5u8H7VaDblcjkWLFuHs2bNYs2aNMEbX0uMRxVX6+rqEtASZTCaM+7VW+L19+zZu3rxplWMRx8J/+9CuXTsbn4khCr+EEGIBCQkJAKAzVW9z8C26gD8rv/z/f//9d7i4uKBz587COq0h/EokEvTr1w9jx47FO++8g4KCgibth5/847PPPsOyZcswefJkYZmlLwgUh19rXHz48MMP4/r168jNzbX4sYC6bx34r7cJEausrIREIrHaDJGNQeGXEEIsoEuXLujcubNwIVpziYOfVquFSqUSQsfx48fRu3dvu3yTaQ4+wL/11ltQKpVYs2ZNo/fBcRyuX7+O5cuXY8SIEVi1ahW8vLyE5S1R+TV3fK01wu+wYcPAcRxOnTpl8WOJUV9hok+j0UAul1O3B0IIcRaMMQwYMADnzp0Tph9uKdXV1bh58yZKS0uRmZmJixcvYurUqejSpQvc3Nxa9Fi2xL9p9ujRA5MmTcLatWtRVlbWqH2o1Wq89tprUCgU+OqrryCVSnU6PDR1KIK4tVl9VVZxKLRGCOjfvz9cXFxw/vx5ix9LTKVSWfV4xP7Za49fgMIvIYRYBGMM0dHRqKysbLFxvzyNRiNcaHXgwAEwxjBt2jS4uro6ZF9fU/j7UlxcjL/97W8oLS3FF1980ah9rFy5Ev/73//w3nvv6Vxw5unpCaDp4dfT01OotNcX/Pjw6+vr2+S2ao3h5uaG6Ohoq4df8Zh0QgAKv4QQ4nT48AsA586da9F9cxwHiUQCjuNw8OBBxMTEOFQbM3Px4Tc/Px/9+vVDXFwcPvzwQ7OD1qFDh7By5UokJCTojPMFgPDwcERERDSrGstPgVzfcBM+/IaFhVktCAwcOBCpqalWGYvL3yfqLkH0UfglhBAnwxhDWFgYgoKCcPbs2Rbdt1arhVwux6VLl5CVlYWZM2e26P7thXgIR1VVFV555RXcvn0b27Zta3Db6upqzJs3D5GRkVi+fLlBdVYqlTa7BRM/eYi7u7vR5Vqt1iKt7hoycOBAqNVqnDlzBpmZmaisrLTYsSj8ElMo/BJCiJNhjIExhj59+rRI5Vcmk8HLywuurq7gOA5yuRzff/89XF1ddaqa/CxulppG15rEQziKi4sRExODvn374t1339Xp+6vVanHp0iWdiwI/+eQTZGdn4+WXX4abm5tQpW1JDb2x5+TkCOHXmiGgf//+kMlkOHToECoqKizW+UGj0QiVbZpUg+ij8EsIIU6Gf9Hv27cvrl27hsLCwmbtjw+8jDFh2tAffvgBY8eOFWZ1A+rGlvbq1avVdX4oKipCcXExVq1ahYyMDCxbtgwAUFpairS0NAB1PWcBoLCwEKtWrcKYMWMwYMAAeHh4WLTRvqlOB0qlEoD1P4h4eXmhf//+2L9/v0W7MFy+fFnYP1V+iT6tVkvhlxBCnAn/ot+rVy8AwOnTp5u1P76KwhiDVqvFmTNnUFBQ0GqHPBhTXl6OESNGYPHixVi7di3WrVtn0PtXrVZj7ty5UCqVePvttwHUfSCwBHPf2K3dgUMikSAuLg7p6em4ceOGVY5J4Zfoo8ovIYQ4Gf5Fv1OnTpBIJDh+/HizWp5ptVpIJBLhQrcDBw7A09MTY8aMaalTtkvioQ/ivr+jRo3C888/jx49emDJkiVISUlBbm4u5syZg/379+Pjjz9G9+7ddbazFL76WVNTg8rKSty5c8emfW8lEgliY2PBGMPRo0cbnIK5JdCwB6JPo9HYbfcZy861SAghTooPXO7u7oiKisK5c+dw5coVoRLcGHyQ4iu/NTU1SElJwSOPPNKq+voa06lTJ6F6yRiDWq1GRkYGNm3ahPPnz2P37t34/vvvceTIEWGbN998E88//zyys7MBWK6/rjhU19bW4vr168LvgYGBkMlkUKvVFhlvXB+tVovAwEBER0cjJSUF8+fPt/gxS0pK7HIaW2IbGo0GGo3Gbq89oPBLCCEWwofV+++/H99//32Tq2P8V8oSiQSMMVy4cAEFBQUYOnRoS56uXRJXjqRSqVA9r6iowLBhwxAREYElS5bg3LlzyM/PR0xMDAYPHozy8nJhzK01Kr/iC/CAujDI910Wj8m2Bv7DUlxcHFavXo3c3NwmfehqynHt9WtuYl3i1yx7ZJ9nRQghrQAfBO6//35UVFQgIyOjSfsRV34lEgmOHTsGmUyGJ554osXO1V6JwxTHcTqPRU5ODoC6ThhDhgzBxIkTMWTIEOGiQJ61Kr9ieXl5qKiosMnwBz54xMXFAUCLTbEtxod9Nzc3YfIQGvdLeOJ/p/aIwi8hhFgIH8J69+4NALh48WKT9qNf+T1z5gyio6N1ZixrrcRvnlqtVngsxLeLK6t8dZ2v+gKWrT7xf+O7d+8aXa4fiq2B7/QRHh6Obt264T//+U+Lh/D09HQAgEKhEKrzLT2NN3FcFH4JIcRJ8X1Qw8PDERISgv/+979N2o84/JaUlODKlSsYMGCA3b6xtCTxfSwtLRXCpDjQVlZWokOHDgD+rEiKh5hYelrhgoICm17gps/f31/4edy4cfjll19w586dFtm3SqVCamqq8DtjTJgimi56IzwKv4QQ4uQYYxgwYAD++9//Nikg8BU1qVSKc+fOgeM4PPTQQ3b7xtKS9Ku2fLgVVxmDg4OFC2v4cCx+nC15xbk9hV6e+HkRHx8PjuOwd+/eZu+3oqICV69e1bnP4vB7584dqv62AhzHGUwa05R9ABR+CSHEqQ0dOhQlJSX48ccfG70tH+hkMhlOnz4NhUKBnj17tvQpOgT+sRC37/Lz84NcLodEIhFut5cqpK0nG+nZsyfat2+Pb7/9ttn7KioqMrhNrVYLHy4qKipw5coVg4v/iGPhP8CYGspjDgq/hBDipAIDA4Wfhw4dChcXF+zZs6fR+6moqIBUKoWrqytOnz6NPn362DxUWVNUVJTQRquyslJnWVhYGIC6N1kXFxfU1NSgsLAQ1dXV8PDwQJcuXax+vmKdO3e26fHlcjni4uJw7NgxFBcXN2tfxj5QlJaWCpVfXnp6OlWAHRj/t2tOcKXwSwghTkr8lb2HhwcGDRqEPXv2NOqrco7jUFZWBldXV2RlZSE9PR0PPfSQJU7Xbrm4uAi9csUdBWQyGfz8/ITfXV1dUVFRgby8PAB1j7m1+4wGBwcLgRywfeVXLpdj1KhRUKvV+Pe//92sfRl73nbs2NHoBYV8j2XieFQqFQAKv4QQQppA/MLPV+CysrLwv//9z+x98O295HI5Dh48CABO0d9Xn7E3Uf0L2VxdXXUCmqWmNTYlLCwMwcHBVj+uMREREQgPD4dMJkOPHj3QtWtXbNmypVn71A+/wcHB8PT0BAAEBQXpLOMDFHE8fOW3OdV7Cr+EEOKkxBUxLy8vPPzww2CM4bvvvjN7H/ybiEKhwIEDB9CpUyd07NixpU/V7hl7E9Wv6op/j4iIsHiXB33iKrStKRQK+Pj4QCaTgTGGcePG4ezZs7hy5UqT9ykecuLj44OAgADh95CQEKvPZEcsQ/zBpbHfUvHrU/glhBAnJX7hZ4whICAAvXv31pmKtyH81/z8lMZjxoyx2zcUa9MfUiAOu9YabsBPL823WgP+/Lvbw9Su/LkkJCRAKpVi69atZm+bkZGBu3fvCi37eJ6enggPDzfooiFex14uOCSNJ674mjtxiVarRWpqKu7evQulUikMe7HX1yoKv4QQYiHiyq9EIgHHcRg+fDjOnj2L8vJys/bBB4pDhw6hsrISkyZNssi5OhKFQgHAsIUZHzbd3NwMLsKylIiICPTs2RNeXl46t3fp0sXmF7vxvL290bZtW4wePRpfffWV2cG0srISd+/exeXLl3W6a4SHhxtdXxx+abY3xyV+fvA/19bW1jsMgv97FxQUoKKiQridwi8hhDgZ/covAAwfPhwqlQqnTp0yax98oNi5cyfatWuHYcOGITQ0VKfS6CzatGkjfJUPGPYAlkgk6N69OyIiIqx2Towxk0MyLNlfuDEkEgm0Wi3mzJmDW7du4T//+U+j91FdXQ0ACA0NNXm/xJVu8ax7xLHoV/ArKipw7do1XL161eQ2fPjlOE7n3yWFX0IIcTLGroIfNGgQXF1dzR76wHEcLl++jCNHjuDJJ5+ERCKBv7+/QaXRGQQGBiI8PFx4ozUWwqRSqd2+4doKx3FQqVQYM2YM/P39sXnz5kbvw1jVT6PR4NSpU3jhhRfQvXt3rFy5Em3btoVMJrPolNLEssThNy8vD5mZmQa317eNsQ/99oaenYQQYiHG3gTc3NwwZMgQs6tvGo0Gr7zyCjw9PfHXv/7VIufpaPhuCu7u7jY+E8fCGMOMGTOwd+/eBnv+6geds2fP4vHHH0dAQAD8/PzQpUsXtGnTBg8//DA2bNiAoKAgfP7554iPj0dqaioNe3Bw/IcX/b7apoj/3uIPShR+CSHEyZj6+m/EiBH4/fffcfv27Qb3cfToUaSnp2PmzJn0VfI9fn5+6Nmzp9XG9To6/sMCx3GYO3cuampq8MUXX9S7DR9+vb29UVZWhjlz5uD27dt46aWXMGvWLPTr1w/jxo3DN998g/z8fJw8eRLbt2/HtWvXMGXKFPzlL38RhkoQx8JxXKM7pYinQi4sLBR+ttfwS68chBBiIcYqv/xFb0BdsJ05c2a9+/jss88QEBCAt99+23In6oDs9U3VHomfe71798aoUaOwevVqvPDCC/Dw8DC6DR9+3d3dcfjwYRQVFWH37t31XnA5bdo0jBw5EitXrsSaNWvw9NNPY9u2bcLx+dngZDKZXXTCIMbpj9sVu379Ojp16mTwwVN8QaSYvf47pcovIYRYiLjdljiAPPDAA/D3929w6ENxcTGOHDmCCRMmOOUYX9Iy+OeeUqnEpUuX8Le//Q0FBQXYsGGDyW348JuZmYm1a9eiR48eePDBBxs8lr+/P55//nksXrwYiYmJOHDggHDsnJwcZGZm4vr16zT9sZ1jjKFNmzYGt9fU1Oh0c+DxHViM7cceUeWXEEIsRCaTwd3dXWe4Qk1NDTw9PREbG4vDhw9Dq9WarLIkJydDo9Fg2LBh9BU/aTI+gPBfTffu3RvDhg3D+++/j/nz5wu9isX4vq0vvvgiOI7DO++8Y3b3CsYY5s+fjwMHDmDx4sUYMWIEsrKydNZRKpV2NSkI+RPHcWCMCbP36ZNKpUhPT0d5eTlqa2tRXl5uMhRrNBp4enpatQOLOejVlBBCLIjv9Zqbmyv8v2vXrpg4cSJ27dqF06dP4+GHHza67ZYtW9C+fXtER0fbbQWF2D/9545Wq8Vrr72G4cOHY9OmTXjuuecMttm3bx/mzJmDwMBAJCYmIiQkpFHhVyaTYc2aNXj00Uexbt06YaiP+ByI/WKMmfxQvm3bNqPPGVNiY2Nx9OjRljq1FkHhlxBCrEB/us+EhAR4eHggMTHRIPyq1WpcuHABKSkpWLp0KVV9SbPoh1+NRoPY2FgMHToUy5cvx6RJk3S+4s7Ly8Ozzz6Ldu3a4ejRo3BxcUFBQYHZH8AYY9BqtRg9ejTi4uLw1ltvYeDAgTqVxIYm2qiqqkJ6ejo6d+5MXT2sjK/8Ggu/SqUSr7/+OgYOHIhly5YhJydHWN9YK7T27dsjKCjIGqfdKPSKSgghVhASEoKSkhL4+PiA4zjcvHkTjz76KL755ht88MEHOmN6S0pKsHHjRjDGMGXKFOqZSlqUVqsFYwzr168XhkCMHj0aDz74IB566CHMmzcPlZWV2LRpE8LCwoSr980NvxKJRAi37777Lvr164ctW7ZgwYIFOudQH34GxLKyMgq/NmLs771hwwYUFhbi0KFD6NOnD1JTU4XQ6+npiQ4dOiA/Px9eXl5wcXGxm4le9NErKiGEWAF/8RvHccIb/+TJk6FUKrF161addTMzM5GYmIhHH30UQUFBFH5Js+hX5PjnX7du3bB9+3Z4eHjgyy+/xOzZsxEVFYVjx47hvffeQ+fOnZs0W5e4Cti3b1/Ex8fjq6++0mmH1VD4FV8gSqzLWOW3Xbt2OH/+PL7++mtMmzYNffr0EdblSSQSMMYQHBwMhUJht8EXoPBLCCFWI5FIdMJvdHQ0+vfvj9WrV6OwsBAcx2HHjh0YO3YsJBIJnn/+eVRUVDS65yYhYvqhtbq6WngOTpw4ERcuXEBZWRl++uknvPrqq9ixYwcee+wxYermxgZQ/W2WLVuG2tpabNmyBUDdhaANDXvgUfi1Pv0hWgBQW1uLv/3tb2jXrh3+8Y9/GN3Oka5LoPBLCCFWwo+FFFe9Vq9ejdzcXPTs2RMxMTGYPn06fHx8sG3bNrRv3x4AqCcqaRb9bg4VFRXCBZg8lUoFd3d3TJs2DT179oRKpTL4xqEplV+g7qLPSZMmYfv27ZDL5ZDJZMK/AVPh1pGCVGuRl5cnzPyn//i//PLLKCwsxKpVq3QmTRFzpG+oHOdMCSHEwelXfhljGDJkCE6fPo3Q0FCcP38eq1evRnJyMiIjI3W2I6QllZaW6vyekZFhsJz/xqGx1Ve5XA6VSoXa2loAdeN3FyxYAKlUilWrVgljgpVKJVJTU4X1xGjYg/UVFhYiNzdXGPbAO3nyJDZv3oxly5ahV69eQtW+rKxMZ3tH+sBCr6iEEGIlEolEp/LL/79///64ePEilEollixZYvAm4khvKsQxGRuGoN9lxNznIV9pvnHjhrDv4OBgLFy4ENu2bUNaWhq0Wi3y8/MBGJ8djMKvdYkfZ5VKJbw2lZaWYsWKFejVqxdWrFghfHDhg7KYI31Id5wzJYQQB8d/Hcy/0Wi1Wp3qCf/m4chfJxL71KFDB53fxc8pUwGTXycgIADu7u5mT0rBV4y1Wi04joNGo4Gfnx9efvllhIaGYuHChSgqKkJlZSUA46Gav81YVdgcOTk5yMvLwx9//AGlUil0jyDG6T8H+L/NJ598guLiYmzduhWurq6QSqUoKipCXl6ewUWLjvQhnV5RCSHESvjKr/iNpqqqymA9qnaRlubl5YWoqCjhCnxx+E1LS9NZlx9jzq8rl8vRuXNns/tN81Pdent7CxVlmUwGPz8/JCYmIisrC2+//bawvrHOD/y/gaaEX47jUFpaisLCQpSUlCArKws3b96kf1cmcBwnVOF5YWFh+Pjjj7Fz5068+uqrwtTWxjo48G0aHelDuuOcKSGEODjGGCorKxt8E9Zfbs8tg4jjcHFxESrApiq/3bp1Eyp4zQkzCoUCZWVluHLlCoA/n8OPPPIIFi5ciP379yM5Odng+PrUajXUanWjjm2qkwSFX+MqKip0wq9KpcLMmTOxaNEixMfHY/ny5cKy+qr0VPklhBBioKKiAhzHoaioqN71xJUwNzc3eHt7W/rUiJPghy/ojzsH6gKqTCZrkfBb31fiCxcuRO/evbFq1SqUlJQYDaXi2/iv4M1lKvya217NmanVavzzn//Ed999hxUrVmD37t06H76NBVz+eUKVX0IIISaJxx/qv/GLxwQDQEREhEO9qRD7J5PJoFarwXGccLGZt7c3IiIiAEBnqEJT6QdNfigEAPj6+mL58uWoqKjAhg0bGgy/jQ2tFH7/VFhY2OB4Z61Wi5qaGqxduxajRo3Cjh078Mwzz+CNN94QJufhOVJ1tz70ikoIIXYoODgYvXr1ajVvNsR+8BekqVQqYcx5SEiIzoVqQPPCr37lVzxRi6+vL0aMGIGEhAR88803yMrKatS+GkLh9095eXm4efNmvetkZGTgiSeewBdffIHu3btj48aNWLdundnH4P8+jjQ8i8IvIYTYkKmpZ6naSyyFf25pNBrhgjJxOOVDon7VrzH453GnTp3QtWtXnWWMMXh6euKDDz6ARCLBypUrDbYX/7tobPhVqVQAgKioKLi7uwu3O2P4bUhlZSUmTpyI3NxcfPLJJ1i7di3Gjx/fYJAVjx0PDAyEXC6Hh4eHNU65RdCrKyGE2JCxYQ9A6/l6kdgfcUs9rVYLqVSq83zjlzenktepUycEBATAw8PDZIju3Lkz5s6di2+//RaffvqpzrLbt28LPzc2/PKBXi6Xw9/fX7idwq+hN954Azdv3sSaNWsQExODgIAAncdMH/8YSqVS9OjRA926dYO7uzu6du3arG8KrI3CLyGEWImnp6fBbfpv7OI3fUIsgQ+3/IQr+t8ydOjQAT4+Ps2q/Hp4eKBt27b1rsMYw6JFixAbG4uFCxdi7969wnmJz7W6urpRx1apVJDL5WCMwdfXF927dwfg3OHXWEvFtLQ0fPjhh5g+fTr69esHoG5sdn0fvMPCwuDt7Q03NzdIJBKH/YbKMc+aEEIcUHh4uMFt+pXfkpISAM79Rk0sSxx+NRqNQYXX3d0d4eHhVvn2QS6X45133kGfPn0wZcoU7NixQ/g3ANR1O6mqqmpUmzKVSqUzjIO/v40N0a1Jenq6wW3vvvsu3Nzc8NJLL8HFxQWBgYHw8fGpdz8KhQLt27d32NDLc+yzJ4QQB6IfMlxdXXWqXOLA29ivegkxl3jMr7HKr7W5u7vj008/xeDBgzFjxgwkJiYCqKsy+vj4QK1WC+N4zVFbW6tTteZDvDhUOwNTQ6qAuhnwtm/fjnnz5sHPzw8ymQxt2rRxmuFWFH4JIcRGJBKJzhuS+KpsZ3kTItbHj820h/BbUVEBoK6iePDgQYwcORILFy7Ezp074eXlJVywxq/XkNraWqjV6mYN2Wgt6gu/n332GTiOw5NPPomKigqne72h8EsIITbCT3fME4/LCwoKssUpEScgkUjAGINKpTI67MGa+K4Bbm5ucHd3x759+zBy5EisWrUKP//8M9zc3ADA7MrvtWvXAOh2rwDqWrkBzjWcyFQnGaVSiS+++ALx8fFOF3p5FH4JIcRGGGNGxzK6u7vb/Kto0noxxiCVSk1e8GZNfHW3uroaGo0Grq6u+Ne//gV3d3e88MILUCqVYIyZHAZUWlpqNNDqdx4Qd7hwFvr3taysDBqNBp988gmKiorwyiuvCMsaO4ueo6NXV0IIsRH9yi8h1sIHSltXfoE/Q9qNGzdQWVmJ2tparFy5Er/++ivGjBmDqqoqg38nBQUFmDx5Mnx9fREWFoa///3vUCqVwnL9YQ98+C0sLEReXp7R7getDf+Y8bPrVVRU4MKFC/jnP/+JMWPGYMCAAcK6zvShAKDwSwghNkPhl9iKRCJBbW0tOI4zGCJgbfyQBJVKhcLCQgBAXFwcduzYgbNnz2LJkiXIzc0FUBfSNm/ejJ49e0aTTnAAABkrSURBVCI5ORmLFi1C79698e6772LIkCG4dOkSFAqFMFyCx4ff/Px8FBYWIj09XegH3Frxry183143Nzds27YNZWVlePPNNwH8eREuP7W1s6DwSwghNmIq/KrVahucDXEmjDGh+mnri8PEwy7UajWkUin8/f0xefJkfPbZZzh79iyGDBmCiRMnIioqCnPnzkXnzp2RmJiIZcuW4eDBg0hOTsadO3cwY8YMbN682eAYxsa2tvZ/Z/rTVJeXl2Pr1q0YNmwY+vbtC6Au/Pr4+AjVYWdB4ZcQQmzEVPht7RUpYnviwGnrYQ/iYKo/FOOZZ57BoUOHMGHCBKSmpqJ9+/bYsmULEhMT0a1bNxQWFuLu3bvo0qULUlJSEB8fj7fffhtr1641eQzenTt3LHvHbEw8GxsAJCcnQ6lUYsaMGULPY47jnPKiN8eZi44QQlqRzp07o7y8XJhili5wI9YkHuNpT+GXr0aLzykkJATLly9Hu3bt4OvrCwDIyMgQlt+9e1f4+a233gIALFy4EF26dMGoUaMM9sczt32ao+I/RPOV3/3798PPzw99+vRBRkYGOnbs6LThl15tCSHEBtzc3IQ3ZP3qr63HYJLWTxx4bP18M/bBz1hYNadNGWMM33zzDSIjI/HGG28It/P9gp1FTU0N8vLyIJPJIJfLUVBQgOPHjyMhIQEuLi7QarXIyMig8EsIIcR6GGM608yK8ReoEGIpfLgMDQ21efgxdnxjgVhcra6vO4G7uzvmzZuHn376CVevXjVY7u3tbdZ+HFlOTg6AuseWMYbk5GSo1WpMnjxZZz2NRmPzv78tUPglhBAb0A+/4jdhZ3wzItbFP/fs4blmrMorvgCrXbt2AHQrv6aq1a6urgCAGTNmQCKR4KuvvhKWtWnTBkDdvzcvLy/h59ZGq9UKY3qlUikqKyuxdetWDB48GJ06dTJY39YXPNoChV9CCLERPoBkZGS0+ivPiX2xp0kf9Ku8fn5+OuHW19cXEolECL/8OHljoY0PzW3btsXIkSOxbds2IeDy++Q4Dp6ensLPrU1WVpbO75s2bUJBQQGefvppAIbfLNl62IstUPglhBAratu2rfAGzV+IotVqdcKvM1ZiiHXxFV97CX+hoaHw8/MDYLzbiXhGutTUVCiVSkilUqGCayzAzZ49Gzk5OTh+/DgA3ZDN3//WWPkVX8in0Wjw/vvvY9CgQZg1axZ69eqFtm3bIjg4WFjHGV9vqNsDIYRYUUBAAAICAgDovunwIcTX11d4QyfEUuwt/Pr7+0OtVqO4uNjomHeO41BSUqITYDUaDSIjI1FZWQk3Nzfk5+cL/7YAYPz48fD29sa2bdsQGxsr3C4ecmQv999STpw4gezsbKxbt074sM0Yg6+vr9Algyq/hBBCrEYmkwkzUfFvwj4+PnYxDpO0bvb4HJPJZOjVqxd8fHwMlvHfjBQVFQm38ffB3d0dEokEISEhQsAD6oZATJgwAXv37kVtbS08PDzg5+enc5Ffaw6/np6eWLNmDe6//36MGTNGZ5k99Xm2BQq/hBBiQ3yVl38TtsdQQlqfoKAg+Pv7O0xnEf5CtsZ67LHHUFJSguPHj4MxhrCwMLi4uJjstNKapKSk4PLly3jllVcMXlekUik8PT0RFBRko7OzrQbDL2MsnDF2jDGWxhhLZYy9eO92f8bYEcbY9Xv/97P86RJCSOvCV134i3ko/BJrkEqlCA0NdZjJVbp06WIQ1Mz5txITEwOpVIoTJ04Y3ba1Vn45jsO//vUvREZG4rHHHjNYzhhDx44dERISYoOzsz1znvVqAH/lOK4HgIEAnmeM9QDwMoCjHMd1AXD03u+EEEIagQ8f4r6chBBD+mPhzfm34uHhgT59+iAlJUXn9tZS+VWpVLh165bO/ZBKpTh48CB+/fVX/P3vf3fKYQ0NaTD8chyXx3HcxXs/KwFcBhAGYDyArfdW2wpggqVOkhBCWiv9NyYKv4QYJ75AVKFQCH17GxIfH49z584hLy9PuK21VH7z8vJQVFSEgoIC4bbKykqsWbMGDz74IGbPnm3Ds7Nfjfq+gzHWEUBvAD8BCOE4jn8m3QZgtHbOGHuGMfYzY+zn/Pz8ZpwqIYS0PvpfO1P4JcQ48QfFiIgIoVdvQyZOnAgA2Ldvn3Bbawm//PnznRs4jsPGjRuRl5eHNWvWOMywFmsz+1FhjHkC2A1gEcdxZeJlXN2jb/QZxHHceo7j+nIc19dZB1YTQogpVPklxDwSiQRubm5o27Zto7br0aMHunTpgqVLl0KlUgn7Ahx/2INSqdT5/fjx49i0aRPi4+MxdOhQG52V/TMr/DLG5KgLvokcx3177+Y7jLG295a3BXDXMqdICCGtF1V+CTFfZGSkTi9fczDGMHv2bJSXl+PgwYPCbYDjV37FVCoVnnvuOXh4eOCNN96w9enYNXO6PTAAGwFc5jjuQ9Gi/QD4wSSzAezT35YQQkj99Cu/renNmBB7sWzZMgQFBWHr1rpLlVpL5Zcnk8nw2GOP4cqVK3j99dfRuXNnW5+SXTOn8jsYwBMAYhljv977bwyAdwGMYIxdBzD83u+EEEIaQVz5DQoKcsrZlgixNLlcjlmzZuG7775DYWFhq5ve+Nq1a9i3bx8ef/xxxMXFwdfX19anZNfM6fZwmuM4xnHc/RzHPXDvvwMcxxVyHBfHcVwXjuOGcxxX1NC+CCGE6BKH3+DgYBr2QIiFzJ49GyqVCjt27Gg1wx74b4727t0LmUyGpUuXwsXFhV5HGkCXARJCiA3xb1I0rTEhlhUdHY3o6Ghs3boVjDFIJBKHrvwWFRVBo9FArVbju+++w7hx4+Dp6QmFQmHrU7N7FH4JIcTGevbsiXbt2tn6NAhp9WbPno3z58/j8uXLDh9+b926BQA4c+YMCgoKMGvWLKhUKprUwgwUfgkhxMYYY1T1JcQKZsyYAalUii1btoAx5tDhl7d//374+/sjOjoaAIR2bsQ0Cr+EEEIIcQohISFISEjAxo0bUVNT49Dh193dHSUlJTh27BjGjBkDtVoNwPHHMVsDhV9CCCGEOI3FixejsLAQ+/btc+igyHEcjh49itraWkyYMEG4PSwszIZn5Rgo/BJCCCHEaQwZMgR9+/bF5s2bhWqpI9Jqtdi9ezeio6PRtWtXqNVqyGQyyOVyW5+a3aPwSwghhBCnwRjD4sWLkZmZiWPHjtn6dJrs8uXL+P333zFnzhxbn4rDofBLCCGEEKfy2GOPISgoCElJSbY+lSbbs2cPZDIZZs6cKdzmyJVsa6LwSwghhBCnIpfLMXnyZBw/fhy3b9+29ek0SK1WIzc3V7hAT6vV4uDBg4iJiUFQUJCwnnjSHGIaPUqEEEIIcTrTp0+HRqPBtm3bbH0qDcrPz0dxcTHS0tKg0Wjw3//+F3fu3EFCQgIAIDw8HABoenQzUfglhBBCiNOJiorCAw88gC+++MKhuj5UV1dj586dkMlkePTRRwEAHh4eAOpauZGGUfglhBBCiNOpra3FxIkTkZ6ejtOnT9v6dMyWkZGBnTt34qGHHoKfnx8AQCaToVevXvDy8rLx2TkGCr+EEEIIcToajQajRo2CQqHAxo0bG719dXU1NBqNBc7MkHgGyLS0NOTm5mLkyJE0M2QTUfglhBBCiNORSCTw8PDAqFGjsHv3bpSXl5u9LcdxuHHjBrKysix4hsb98MMPkMlkiImJoUpvE1H4JYQQQojTCQ0NhaenJyZNmoTy8nL8+9//NntbfoxwZWWlpU5PUFtbi9raWgB1XR4OHz6MAQMGwMfHBzKZzOLHb40o/BJCCCHE6cjlcgQFBeGBBx5AREQEtm7dava21rxA7tq1aygrK4NcLsf//vc/5ObmIj4+Hu7u7lY7h9aGwi8hhBBCnBJjDIwxzJw5EydOnEBGRoZZ2/H9dq1JIpHg4MGD8PLyQlxcHDp06GD1c2gtKPwSQgghxCnxF4w9/vjjYIyZXf21RWu0kpIS7N69G/Hx8XBzc4NUKrX6ObQWFH4JIYQQ4pT48BsWFoYRI0Zg69atZlV1bVH53bNnD2pqarBkyRJERUVZ/fitCYVfQgghhDg1juMwZ84cZGVl4fjx42atbw38cTiOQ1JSEgYMGIA+ffrQTG7NROGXEEIIIU6Jr/xyHIcJEybAx8cHW7ZsaXA7a4VfvsJ88eJFZGRkYN68eVY5bmtH4ZcQQgghTkk8SYRCocC0adOwa9culJWV1budtYY98MfZunUrfHx88Pjjj1vluK0dhV9CCCGEOCVx5RcAnnzySVRVVTXY89dald/a2lpcvnwZx44dw5IlS6i9WQuh8EsIIYQQp6Qffvv3749u3brhyy+/rHc7ceVXpVJZ7Pxu3ryJzz//HD4+PnjxxRctdhxnQ+GXEEIIIU5JP/wyxvD888/j3LlzOHXqlMH6paWl0Gg0OpXfq1evWuz8fvvtN6SkpGD+/Pnw8fGx2HGcDYVfQgghhJB75s6di6CgIKxatUrndpVKhZycHOTk5BgMe6ipqWnx86iqqsJrr72GkJAQvPTSSy2+f2dG4ZcQQggh5B53d3csWrQIhw4dwi+//IKqqirU1NSgqKgIQF3Q1b/gLS8vz+i+OI5DbW1tk87j9ddfR2ZmJlauXInAwMAm7YMYR+GXEEIIIU5Jf9gD77nnnoOXlxdWr16N9PR0XL9+Hfn5+QDqKsDV1dU666vVaqP7LykpwbVr14RtzXXp0iV89NFHmDRpEmJiYhq1LWkYhV9CCCGEEBFfX188/fTTSEpKwp07dwyWFxcX6/xuKvzyIdnYPkzhOA7PPvssfH19sXjxYgQFBTXizIk5KPwSQgghxCmZqvwCwIIFC6DRaLBz58569+Hp6QmNRmN0mbhfsLm9gbds2YLTp0/jH//4B3x9fSGVSs3ajpiPwi8hhBBCiJ7OnTtj3Lhx2LVrl9EL2gICAtCtWzcoFApwHGcQoDmO02mDZs7Y36ysLCxatAhDhgzBhAkTAAAymayZ94Too/BLCCGEEKdmatKKBQsWoKioCD/88INwG99yTCqVQiaTCa3PcnJydLbVr/Q2FH6vXr2K0aNHQ6vVYsuWLcIFdlT5bXkUfgkhhBDilMTTGxsTExODiIgIJCYmCgFZIqmLTlKpFFeuXMHChQsRExODDh06ICgoCH/961+RkZEhhN+QkBAA9bdDu3jxIgYNGoTCwkIkJycjLCxMWEbht+VR+CWEEEKIUzNV+eU4DjNmzEBaWhp+++039OjRQ1g3OTkZvXv3xqFDhzBgwAC88MILGDx4MNasWYP77rsPH374IUpKSiCXyyGVSlFbW4vKykqDY/3000+IjY2Fp6cnzp49i0ceeUTnAjo+bJOWQ48oIYQQQpxWfdVfjuMwbtw4eHl54dtvvxWC6PHjx/HUU0/hgQcewPXr1/Hee+/hmWeewapVq3DkyBFERkbi5ZdfxqOPPoqPP/4YQF2HiIyMDJ2ewKdOncLw4cMRGBiIkydPIiIiQjguUNd1grQ8Cr+EEEIIIUbU1tbC3d0dzz77LPbs2YNPPvkEycnJWLJkCaKjo3H48GG0bdsWXl5eAOouTgsODsbXX3+NpKQk9O3bFytWrEBCQoIwJpgfy3vs2DGMHj0aYWFhOHHiBDp06CAclw+/AQEBVr7HzoGZKvVbQt++fbmff/7ZascjhBBCCKlPWloa/Pz80LZtW4Nlly5dAgBERUVh2rRp2LNnDwBg8ODB2Ldvn044zc7O1mltxsvKysKsWbPg7e2NDRs2IDQ0FGVlZRg7dixCQ0ORkpIijAvmlZSU4I8//kBkZCTc3Nxa8u46DcbYBY7j+hpbRv0zCCGEEELq4eLigsTERKxYsQJ+fn5YsmQJXFxcdNYxNXxi7Nix2LlzJ8aPH4/x48cjMjISly9fRmhoKJKTkxEcHIzs7GwEBQVBoVAAgMHFdaRlUfglhBBCiNNijJm84E0qlQqtzRQKBd577z2T+3F1dRV+5ie+aNOmDQBgxIgRSEtLw3vvvYfz58/juef+v737CbHrLOM4/n1mMpNMMmmcJrlD0wQnSmEos6hBpKCUrtLaTXQjdaFBBAVb0IWL6MaiGxV04SKCYqCCWgpa7MJ/WQiurK0S7UxKbYwJJsRGSYgJEwy587i4507uTGbSTJy599z7fj9wuWfec5nzDk/ewy/vee85n+PIkSPs2bOHkydPsrCwwPz8PNPT0wBcvXp1sW9af4ZfSZKkZTKTZrN517caa8/aAkxNTd22f2pqiqNHjzI3N8fQ0BCXLl1ibGxs8ZZonXd4aC+fcOZ3Yxh+JUmSOpw9e3bxvrx3G37ba3M779G7XEQwNDS0GHjPnz+/ZH9mLnk4hjO/G8PwK0mSirV82UNmLi47gLuffR0ZGWFmZuYdP7d582auX7++4r5z584tLp/Yu3evM78bxPArSZJUWf5Y4vV+wtpK4Xd8fJxr165x5cqVxbZt27at63F1i/+lkCRJxbp58yaXL19e/LnZbC7Zv97ht3Ntb9vExMRtbZs2OT+5UQy/kiRJlc7ZV1j/GdjOL8ZBa63wjh07lqzvjQjX+24gw68kSSre/Pz8kve29Q6hjUaD6elpGo0GcOvOEO1HGwOr3npN68PwK0mSinf69GmgFXY3cslB+/c3Gg1mZmYWj7Vly5YVb5Gm9Wf4lSRJxVq+3rbZbDIyMtKTvmzduhXw/r4bzdXUkiSpWGNjY0u+8LawsMDw8DC7d+9eDKPdMjQ0RKPR4L777uvqcUtj+JUkSao0m01GR0eZnJzsyfHba4G1cZxXlyRJAmZnZ7lx44bLDgac1ZUkScVaaX3vet/bV/Vi+JUkScUaHx+/7XZmzvwONqsrSZKKFRHs2LFjSdtKT2HT4DD8SpKkoi2f+d25c2ePeqJuMPxKkqSiLZ/pHR0d7VFP1A2GX0mSVLTONb6NRmPdH2msejH8SpIkAdu3b/c+uwUw/EqSpKK1Z3p9sloZDL+SJEkqho83liRJRZucnCQzb7vlmQaT4VeSJBVtZGSEffv29bob6hKXPUiSJKkYhl9JkiQVw/ArSZKkYhh+JUmSVAzDryRJkoph+JUkSVIxDL+SJEkqhuFXkiRJxTD8SpIkqRiGX0mSJBXD8CtJkqRiGH4lSZJUDMOvJEmSimH4lSRJUjEMv5IkSSqG4VeSJEnFMPxKkiSpGIZfSZIkFSMys3sHi/gXcLZrB7xlF/DvHhxX/x/r1p+sW3+ybv3JuvUn67bx3p2Zu1fa0dXw2ysR8Vpmvr/X/dDaWLf+ZN36k3XrT9atP1m33nLZgyRJkoph+JUkSVIxSgm/3+t1B3RPrFt/sm79ybr1J+vWn6xbDxWx5leSJEmCcmZ+JUmSpMEPvxHxZES8GRGnIuJIr/ujpSLiTES8HhEnIuK1qu3+iDgeEW9V7xNVe0TEd6pa/iUiDvS29+WIiGMRcTEiZjva1lyniDhcff6tiDjci7+lJKvU7bmIOF+NuRMR8VTHvi9VdXszIp7oaPc82iURsS8ifhsRJyNiLiI+X7U73mrsDnVzvNVRZg7sCxgG/ga8BxgF/gw83Ot++VpSozPArmVt3wSOVNtHgG9U208BvwQCeBR4pdf9L+UFPAYcAGbvtU7A/cDp6n2i2p7o9d82yK9V6vYc8MUVPvtwdY7cDOyvzp3Dnke7XrMHgAPV9nbgr1VtHG81ft2hbo63Gr4Gfeb3A8CpzDydmTeAF4BDPe6T3tkh4Plq+3ngIx3tP8yW3wPviogHetHB0mTm74BLy5rXWqcngOOZeSkzLwPHgSc3vvflWqVuqzkEvJCZ/83MvwOnaJ1DPY92UWZeyMw/VdtXgTeAB3G81dod6rYax1sPDXr4fRD4R8fP57jzP0Z1XwK/iYg/RsRnqrbJzLxQbf8TmKy2rWe9rLVO1q8+nq0ukR9rXz7HutVOREwB7wNewfHWN5bVDRxvtTPo4Vf196HMPAB8GHgmIh7r3Jmt60PekqTmrFNf+S7wXuAR4ALwrd52RyuJiHHgp8AXMvM/nfscb/W1Qt0cbzU06OH3PLCv4+e9VZtqIjPPV+8XgZdoXfJ5u72coXq/WH3cetbLWutk/WogM9/OzGZmLgDfpzXmwLrVRkSM0ApQP8rMn1XNjreaW6lujrd6GvTw+yrwUETsj4hR4Gng5R73SZWI2BYR29vbwEFgllaN2t9MPgz8vNp+Gfhk9e3mR4ErHZcB1X1rrdOvgYMRMVFd+jtYtamLlq2T/yitMQetuj0dEZsjYj/wEPAHPI92VUQE8APgjcz8dscux1uNrVY3x1s9bep1BzZSZt6MiGdpDfhh4FhmzvW4W7plEnipdc5gE/DjzPxVRLwKvBgRnwbOAh+rPv8LWt9sPgXMA5/qfpfLFBE/AR4HdkXEOeArwNdZQ50y81JEfI3WyR3gq5l5t1/G0j1YpW6PR8QjtC6bnwE+C5CZcxHxInASuAk8k5nN6vd4Hu2eDwKfAF6PiBNV25dxvNXdanX7uOOtfnzCmyRJkoox6MseJEmSpEWGX0mSJBXD8CtJkqRiGH4lSZJUDMOvJEmSimH4lSRJUjEMv5IkSSqG4VeSJEnF+B+MQZjVCY39MAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uG9VxqmUbou",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d377ade1-126f-4489-8158-70e920bad1a7"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 100, 2300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"6be78e4c-6feb-4de8-bcb2-607f5f4992ca\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"6be78e4c-6feb-4de8-bcb2-607f5f4992ca\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '6be78e4c-6feb-4de8-bcb2-607f5f4992ca',\n",
              "                        [{\"type\": \"scatter\", \"x\": [20110225, 20110228, 20110301, 20110302, 20110303, 20110304, 20110307, 20110308, 20110309, 20110310, 20110311, 20110314, 20110315, 20110316, 20110317, 20110318, 20110321, 20110322, 20110323, 20110324, 20110325, 20110328, 20110329, 20110330, 20110331, 20110401, 20110404, 20110405, 20110406, 20110407, 20110408, 20110411, 20110412, 20110413, 20110414, 20110415, 20110418, 20110419, 20110420, 20110421, 20110425, 20110426, 20110427, 20110428, 20110429, 20110502, 20110503, 20110504, 20110505, 20110506, 20110509, 20110510, 20110511, 20110512, 20110513, 20110516, 20110517, 20110518, 20110519, 20110520, 20110523, 20110524, 20110525, 20110526, 20110527, 20110531, 20110601, 20110602, 20110603, 20110606, 20110607, 20110608, 20110609, 20110610, 20110613, 20110614, 20110615, 20110616, 20110617, 20110620, 20110621, 20110622, 20110623, 20110624, 20110627, 20110628, 20110629, 20110630, 20110701, 20110705, 20110706, 20110707, 20110708, 20110711, 20110712, 20110713, 20110714, 20110715, 20110718, 20110719, 20110720, 20110721, 20110722, 20110725, 20110726, 20110727, 20110728, 20110729, 20110801, 20110802, 20110803, 20110804, 20110805, 20110808, 20110809, 20110810, 20110811, 20110812, 20110815, 20110816, 20110817, 20110818, 20110819, 20110822, 20110823, 20110824, 20110825, 20110826, 20110829, 20110830, 20110831, 20110901, 20110902, 20110906, 20110907, 20110908, 20110909, 20110912, 20110913, 20110914, 20110915, 20110916, 20110919, 20110920, 20110921, 20110922, 20110923, 20110926, 20110927, 20110928, 20110929, 20110930, 20111003, 20111004, 20111005, 20111006, 20111007, 20111010, 20111011, 20111012, 20111013, 20111014, 20111017, 20111018, 20111019, 20111020, 20111021, 20111024, 20111025, 20111026, 20111027, 20111028, 20111031, 20111101, 20111102, 20111103, 20111104, 20111107, 20111108, 20111109, 20111110, 20111111, 20111114, 20111115, 20111116, 20111117, 20111118, 20111121, 20111122, 20111123, 20111125, 20111128, 20111129, 20111130, 20111201, 20111202, 20111205, 20111206, 20111207, 20111208, 20111209, 20111212, 20111213, 20111214, 20111215, 20111216, 20111219, 20111220, 20111221, 20111222, 20111223, 20111227, 20111228, 20111229, 20111230, 20120103, 20120104, 20120105, 20120106, 20120109, 20120110, 20120111, 20120112, 20120113, 20120117, 20120118, 20120119, 20120120, 20120123, 20120124, 20120125, 20120126, 20120127, 20120130, 20120131, 20120201, 20120202, 20120203, 20120206, 20120207, 20120208, 20120209, 20120210, 20120213, 20120214, 20120215, 20120216, 20120217, 20120221, 20120222, 20120223, 20120224, 20120227, 20120228, 20120229, 20120301, 20120302, 20120305, 20120306, 20120307, 20120308, 20120309, 20120312, 20120313, 20120314, 20120315, 20120316, 20120319, 20120320, 20120321, 20120322, 20120323, 20120326, 20120327, 20120328, 20120329, 20120330, 20120402, 20120403, 20120404, 20120405, 20120409, 20120410, 20120411, 20120412, 20120413, 20120416, 20120417, 20120418, 20120419, 20120420, 20120423, 20120424, 20120425, 20120426, 20120427, 20120430, 20120501, 20120502, 20120503, 20120504, 20120507, 20120508, 20120509, 20120510, 20120511, 20120514, 20120515, 20120516, 20120517, 20120518, 20120521, 20120522, 20120523, 20120524, 20120525, 20120529, 20120530, 20120531, 20120601, 20120604, 20120605, 20120606, 20120607, 20120608, 20120611, 20120612, 20120613, 20120614, 20120615, 20120618, 20120619, 20120620, 20120621, 20120622, 20120625, 20120626, 20120627, 20120628, 20120629, 20120702, 20120703, 20120705, 20120706, 20120709, 20120710, 20120711, 20120712, 20120713, 20120716, 20120717, 20120718, 20120719, 20120720, 20120723, 20120724, 20120725, 20120726, 20120727, 20120730, 20120731, 20120801, 20120802, 20120803, 20120806, 20120807, 20120808, 20120809, 20120810, 20120813, 20120814, 20120815, 20120816, 20120817, 20120820, 20120821, 20120822, 20120823, 20120824, 20120827, 20120828, 20120829, 20120830, 20120831, 20120904, 20120905, 20120906, 20120907, 20120910, 20120911, 20120912, 20120913, 20120914, 20120917, 20120918, 20120919, 20120920, 20120921, 20120924, 20120925, 20120926, 20120927, 20120928, 20121001, 20121002, 20121003, 20121004, 20121005, 20121008, 20121009, 20121010, 20121011, 20121012, 20121015, 20121016, 20121017, 20121018, 20121019, 20121022, 20121023, 20121024, 20121025, 20121026, 20121031, 20121101, 20121102, 20121105, 20121106, 20121107, 20121108, 20121109, 20121112, 20121113, 20121114, 20121115, 20121116, 20121119, 20121120, 20121121, 20121123, 20121126, 20121127, 20121128, 20121129, 20121130, 20121203, 20121204, 20121205, 20121206, 20121207, 20121210, 20121211, 20121212, 20121213, 20121214, 20121217, 20121218, 20121219, 20121220, 20121221, 20121224, 20121226, 20121227, 20121228, 20121231, 20130102, 20130103, 20130104, 20130107, 20130108, 20130109, 20130110, 20130111, 20130114, 20130115, 20130116, 20130117, 20130118, 20130122, 20130123, 20130124, 20130125, 20130128, 20130129, 20130130, 20130131, 20130201, 20130204, 20130205, 20130206, 20130207, 20130208, 20130211, 20130212, 20130213, 20130214, 20130215, 20130219, 20130220, 20130221, 20130222, 20130225, 20130226, 20130227, 20130228, 20130301, 20130304, 20130305, 20130306, 20130307, 20130308, 20130311, 20130312, 20130313, 20130314, 20130315, 20130318, 20130319, 20130320, 20130321, 20130322, 20130325, 20130326, 20130327, 20130328, 20130401, 20130402, 20130403, 20130404, 20130405, 20130408, 20130409, 20130410, 20130411, 20130412, 20130415, 20130416, 20130417, 20130418, 20130419, 20130422, 20130423, 20130424, 20130425, 20130426, 20130429, 20130430, 20130501, 20130502, 20130503, 20130506, 20130507, 20130508, 20130509, 20130510, 20130513, 20130514, 20130515, 20130516, 20130517, 20130520, 20130521, 20130522, 20130523, 20130524, 20130528, 20130529, 20130530, 20130531, 20130603, 20130604, 20130605, 20130606, 20130607, 20130610, 20130611, 20130612, 20130613, 20130614, 20130617, 20130618, 20130619, 20130620, 20130621, 20130624, 20130625, 20130626, 20130627, 20130628, 20130701, 20130702, 20130703, 20130705, 20130708, 20130709, 20130710, 20130711, 20130712, 20130715, 20130716, 20130717, 20130718, 20130719, 20130722, 20130723, 20130724, 20130725, 20130726, 20130729, 20130730, 20130731, 20130801, 20130802, 20130805, 20130806, 20130807, 20130808, 20130809, 20130812, 20130813, 20130814, 20130815, 20130816, 20130819, 20130820, 20130821, 20130822, 20130823, 20130826, 20130827, 20130828, 20130829, 20130830, 20130903, 20130904, 20130905, 20130906, 20130909, 20130910, 20130911, 20130912, 20130913, 20130916, 20130917, 20130918, 20130919, 20130920, 20130923, 20130924, 20130925, 20130926, 20130927, 20130930, 20131001, 20131002, 20131003, 20131004, 20131007, 20131008, 20131009, 20131010, 20131011, 20131014, 20131015, 20131016, 20131017, 20131018, 20131021, 20131022, 20131023, 20131024, 20131025, 20131028, 20131029, 20131030, 20131031, 20131101, 20131104, 20131105, 20131106, 20131107, 20131108, 20131111, 20131112, 20131113, 20131114, 20131115, 20131118, 20131119, 20131120, 20131121, 20131122, 20131125, 20131126, 20131127, 20131129, 20131202, 20131203, 20131204, 20131205, 20131206, 20131209, 20131210, 20131211, 20131212, 20131213, 20131216, 20131217, 20131218, 20131219, 20131220, 20131223, 20131224, 20131226, 20131227, 20131230, 20131231, 20140102, 20140103, 20140106, 20140107, 20140108, 20140109, 20140110, 20140113, 20140114, 20140115, 20140116, 20140117, 20140121, 20140122, 20140123, 20140124, 20140127, 20140128, 20140129, 20140130, 20140131, 20140203, 20140204, 20140205, 20140206, 20140207, 20140210, 20140211, 20140212, 20140213, 20140214, 20140218, 20140219, 20140220, 20140221, 20140224, 20140225, 20140226, 20140227, 20140228, 20140303, 20140304, 20140305, 20140306, 20140307, 20140310, 20140311, 20140312, 20140313, 20140314, 20140317, 20140318, 20140319, 20140320, 20140321, 20140324, 20140325, 20140326, 20140327, 20140328, 20140331, 20140401, 20140402, 20140403, 20140404, 20140407, 20140408, 20140409, 20140410, 20140411, 20140414, 20140415, 20140416, 20140417, 20140421, 20140422, 20140423, 20140424, 20140425, 20140428, 20140429, 20140430, 20140501, 20140502, 20140505, 20140506, 20140507, 20140508, 20140509, 20140512, 20140513, 20140514, 20140515, 20140516, 20140519, 20140520, 20140521, 20140522, 20140523, 20140527, 20140528, 20140529, 20140530, 20140602, 20140603, 20140604, 20140605, 20140606, 20140609, 20140610, 20140611, 20140612, 20140613, 20140616, 20140617, 20140618, 20140619, 20140620, 20140623, 20140624, 20140625, 20140626, 20140627, 20140630, 20140701, 20140702, 20140703, 20140707, 20140708, 20140709, 20140710, 20140711, 20140714, 20140715, 20140716, 20140717, 20140718, 20140721, 20140722, 20140723, 20140724, 20140725, 20140728, 20140729, 20140730, 20140731, 20140801, 20140804, 20140805, 20140806, 20140807, 20140808, 20140811, 20140812, 20140813, 20140814, 20140815, 20140818, 20140819, 20140820, 20140821, 20140822, 20140825, 20140826, 20140827, 20140828, 20140829, 20140902, 20140903, 20140904, 20140905, 20140908, 20140909, 20140910, 20140911, 20140912, 20140915, 20140916, 20140917, 20140918, 20140919, 20140922, 20140923, 20140924, 20140925, 20140926, 20140929, 20140930, 20141001, 20141002, 20141003, 20141006, 20141007, 20141008, 20141009, 20141010, 20141013, 20141014, 20141015, 20141016, 20141017, 20141020, 20141021, 20141022, 20141023, 20141024, 20141027, 20141028, 20141029, 20141030, 20141031, 20141103, 20141104, 20141105, 20141106, 20141107, 20141110, 20141111, 20141112, 20141113, 20141114, 20141117, 20141118, 20141119, 20141120, 20141121, 20141124, 20141125, 20141126, 20141128, 20141201, 20141202, 20141203, 20141204, 20141205, 20141208, 20141209, 20141210, 20141211, 20141212, 20141215, 20141216, 20141217, 20141218, 20141219, 20141222, 20141223, 20141224, 20141226, 20141229, 20141230, 20141231, 20150102, 20150105, 20150106, 20150107, 20150108, 20150109, 20150112, 20150113, 20150114, 20150115, 20150116, 20150120, 20150121, 20150122, 20150123, 20150126, 20150127, 20150128, 20150129, 20150130, 20150202, 20150203, 20150204, 20150205, 20150206, 20150209, 20150210, 20150211, 20150212, 20150213, 20150217, 20150218, 20150219, 20150220, 20150223, 20150224, 20150225, 20150226, 20150227, 20150302, 20150303, 20150304, 20150305, 20150306, 20150309, 20150310, 20150311, 20150312, 20150313, 20150316, 20150317, 20150318, 20150319, 20150320, 20150323, 20150324, 20150325, 20150326, 20150327, 20150330, 20150331, 20150401, 20150402, 20150406, 20150407, 20150408, 20150409, 20150410, 20150413, 20150414, 20150415, 20150416, 20150417, 20150420, 20150421, 20150422, 20150423, 20150424, 20150427, 20150428, 20150429, 20150430, 20150501, 20150504, 20150505, 20150506, 20150507, 20150508, 20150511, 20150512, 20150513, 20150514, 20150515, 20150518, 20150519, 20150520, 20150521, 20150522, 20150526, 20150527, 20150528, 20150529, 20150601, 20150602, 20150603, 20150604, 20150605, 20150608, 20150609, 20150610, 20150611, 20150612, 20150615, 20150616, 20150617, 20150618, 20150619, 20150622, 20150623, 20150624, 20150625, 20150626, 20150629, 20150630, 20150701, 20150702, 20150706, 20150707, 20150708, 20150709, 20150710, 20150713, 20150714, 20150715, 20150716, 20150717, 20150720, 20150721, 20150722, 20150723, 20150724, 20150727, 20150728, 20150729, 20150730, 20150731, 20150803, 20150804, 20150805, 20150806, 20150807, 20150810, 20150811, 20150812, 20150813, 20150814, 20150817, 20150818, 20150819, 20150820, 20150821, 20150824, 20150825, 20150826, 20150827, 20150828, 20150831, 20150901, 20150902, 20150903, 20150904, 20150908, 20150909, 20150910, 20150911, 20150914, 20150915, 20150916, 20150917, 20150918, 20150921, 20150922, 20150923, 20150924, 20150925, 20150928, 20150929, 20150930, 20151001, 20151002, 20151005, 20151006, 20151007, 20151008, 20151009, 20151012, 20151013, 20151014, 20151015, 20151016, 20151019, 20151020, 20151021, 20151022, 20151023, 20151026, 20151027, 20151028, 20151029, 20151030, 20151102, 20151103, 20151104, 20151105, 20151106, 20151109, 20151110, 20151111, 20151112, 20151113, 20151116, 20151117, 20151118, 20151119, 20151120, 20151123, 20151124, 20151125, 20151127, 20151130, 20151201, 20151202, 20151203, 20151204, 20151207, 20151208, 20151209, 20151210, 20151211, 20151214, 20151215, 20151216, 20151217, 20151218, 20151221, 20151222, 20151223, 20151224, 20151228, 20151229, 20151230, 20151231, 20160104, 20160105, 20160106, 20160107, 20160108, 20160111, 20160112, 20160113, 20160114, 20160115, 20160119, 20160120, 20160121, 20160122, 20160125, 20160126, 20160127, 20160128, 20160129, 20160201, 20160202, 20160203, 20160204, 20160205, 20160208, 20160209, 20160210, 20160211, 20160212, 20160216, 20160217, 20160218, 20160219, 20160222, 20160223, 20160224, 20160225, 20160226, 20160229, 20160301, 20160302, 20160303, 20160304, 20160307, 20160308, 20160309, 20160310, 20160311, 20160314, 20160315, 20160316, 20160317, 20160318, 20160321, 20160322, 20160323, 20160324, 20160328, 20160329, 20160330, 20160331, 20160401, 20160404, 20160405, 20160406, 20160407, 20160408, 20160411, 20160412, 20160413, 20160414, 20160415, 20160418, 20160419, 20160420, 20160421, 20160422, 20160425, 20160426, 20160427, 20160428, 20160429, 20160502, 20160503, 20160504, 20160505, 20160506, 20160509, 20160510, 20160511, 20160512, 20160513, 20160516, 20160517, 20160518, 20160519, 20160520, 20160523, 20160524, 20160525, 20160526, 20160527, 20160531, 20160601, 20160602, 20160603, 20160606, 20160607, 20160608, 20160609, 20160610, 20160613, 20160614, 20160615, 20160616, 20160617, 20160620, 20160621, 20160622, 20160623, 20160624, 20160627, 20160628, 20160629, 20160630, 20160701, 20160705, 20160706, 20160707, 20160708, 20160711, 20160712, 20160713, 20160714, 20160715, 20160718, 20160719, 20160720, 20160721, 20160722, 20160725, 20160726, 20160727, 20160728, 20160729, 20160801, 20160802, 20160803, 20160804, 20160805, 20160808, 20160809, 20160810, 20160811, 20160812, 20160815, 20160816, 20160817, 20160818, 20160819, 20160822, 20160823, 20160824, 20160825, 20160826, 20160829, 20160830, 20160831, 20160901, 20160902, 20160906, 20160907, 20160908, 20160909, 20160912, 20160913, 20160914, 20160915, 20160916, 20160919, 20160920, 20160921, 20160922, 20160923, 20160926, 20160927, 20160928, 20160929, 20160930, 20161003, 20161004, 20161005, 20161006, 20161007, 20161010, 20161011, 20161012, 20161013, 20161014, 20161017, 20161018, 20161019, 20161020, 20161021, 20161024, 20161025, 20161026, 20161027, 20161028, 20161031, 20161101, 20161102, 20161103, 20161104, 20161107, 20161108, 20161109, 20161110, 20161111, 20161114, 20161115, 20161116, 20161117, 20161118, 20161121, 20161122, 20161123, 20161125, 20161128, 20161129, 20161130, 20161201, 20161202, 20161205, 20161206, 20161207, 20161208, 20161209, 20161212, 20161213, 20161214, 20161215, 20161216, 20161219, 20161220, 20161221, 20161222, 20161223, 20161227, 20161228, 20161229, 20161230, 20170103, 20170104, 20170105, 20170106, 20170109, 20170110, 20170111, 20170112, 20170113, 20170117, 20170118, 20170119, 20170120, 20170123, 20170124, 20170125, 20170126, 20170127, 20170130, 20170131, 20170201, 20170202, 20170203, 20170206, 20170207, 20170208, 20170209, 20170210, 20170213, 20170214, 20170215, 20170216, 20170217, 20170221, 20170222, 20170223, 20170224, 20170227, 20170228, 20170301, 20170302, 20170303, 20170306, 20170307, 20170308, 20170309, 20170310, 20170313, 20170314, 20170315, 20170316, 20170317, 20170320, 20170321, 20170322, 20170323, 20170324, 20170327, 20170328, 20170329, 20170330, 20170331, 20170403, 20170404, 20170405, 20170406, 20170407, 20170410, 20170411, 20170412, 20170413, 20170417, 20170418, 20170419, 20170420, 20170421, 20170424, 20170425, 20170426, 20170427, 20170428, 20170501, 20170502, 20170503, 20170504, 20170505, 20170508, 20170509, 20170510, 20170511, 20170512, 20170515, 20170516, 20170517, 20170518, 20170519, 20170522, 20170523, 20170524, 20170525, 20170526, 20170530, 20170531, 20170601, 20170602, 20170605, 20170606, 20170607, 20170608, 20170609, 20170612, 20170613, 20170614, 20170615, 20170616, 20170619, 20170620, 20170621, 20170622, 20170623, 20170626, 20170627, 20170628, 20170629, 20170630, 20170703, 20170705, 20170706, 20170707, 20170710, 20170711, 20170712, 20170713, 20170714, 20170717, 20170718, 20170719, 20170720, 20170721, 20170724, 20170725, 20170726, 20170727, 20170728, 20170731, 20170801, 20170802, 20170803, 20170804, 20170807, 20170808, 20170809, 20170810, 20170811, 20170814, 20170815, 20170816, 20170817, 20170818, 20170821, 20170822, 20170823, 20170824, 20170825, 20170828, 20170829, 20170830, 20170831, 20170901, 20170905, 20170906, 20170907, 20170908, 20170911, 20170912, 20170913, 20170914, 20170915, 20170918, 20170919, 20170920, 20170921, 20170922, 20170925, 20170926, 20170927, 20170928, 20170929, 20171002, 20171003, 20171004, 20171005, 20171006, 20171009, 20171010, 20171011, 20171012, 20171013, 20171016, 20171017, 20171018, 20171019, 20171020, 20171023, 20171024, 20171025, 20171026, 20171027, 20171030, 20171031, 20171101, 20171102, 20171103, 20171106, 20171107, 20171108, 20171109, 20171110, 20171113, 20171114, 20171115, 20171116, 20171117, 20171120, 20171121, 20171122, 20171124, 20171127, 20171128, 20171129, 20171130, 20171201, 20171204, 20171205, 20171206, 20171207, 20171208, 20171211, 20171212, 20171213, 20171214, 20171215, 20171218, 20171219, 20171220, 20171221, 20171222, 20171226, 20171227, 20171228, 20171229, 20180102, 20180103, 20180104, 20180105, 20180108, 20180109, 20180110, 20180111, 20180112, 20180116, 20180117, 20180118, 20180119, 20180122, 20180123, 20180124, 20180125, 20180126, 20180129, 20180130, 20180131, 20180201, 20180202, 20180205, 20180206, 20180207, 20180208, 20180209, 20180212, 20180213, 20180214, 20180215, 20180216, 20180220, 20180221, 20180222, 20180223, 20180226, 20180227, 20180228, 20180301, 20180302, 20180305, 20180306, 20180307, 20180308, 20180309, 20180312, 20180313, 20180314, 20180315, 20180316, 20180319, 20180320, 20180321, 20180322, 20180323, 20180326, 20180327, 20180328, 20180329, 20180402, 20180403, 20180404, 20180405, 20180406, 20180409, 20180410, 20180411, 20180412, 20180413, 20180416, 20180417, 20180418, 20180419, 20180420, 20180423, 20180424, 20180425, 20180426, 20180427, 20180430, 20180501, 20180502, 20180503, 20180504, 20180507, 20180508, 20180509, 20180510, 20180511, 20180514, 20180515, 20180516, 20180517, 20180518, 20180521, 20180522, 20180523, 20180524, 20180525, 20180529, 20180530, 20180531, 20180601, 20180604, 20180605, 20180606, 20180607, 20180608, 20180611, 20180612, 20180613, 20180614, 20180615, 20180618, 20180619, 20180620, 20180621, 20180622, 20180625, 20180626, 20180627, 20180628, 20180629, 20180702, 20180703, 20180705, 20180706, 20180709, 20180710, 20180711, 20180712, 20180713, 20180716, 20180717, 20180718, 20180719, 20180720, 20180723, 20180724, 20180725, 20180726, 20180727, 20180730, 20180731, 20180801, 20180802, 20180803, 20180806, 20180807, 20180808, 20180809, 20180810, 20180813, 20180814, 20180815, 20180816, 20180817, 20180820, 20180821, 20180822, 20180823, 20180824, 20180827, 20180828, 20180829, 20180830, 20180831, 20180904, 20180905, 20180906, 20180907, 20180910, 20180911, 20180912, 20180913, 20180914, 20180917, 20180918, 20180919, 20180920, 20180921, 20180924, 20180925, 20180926, 20180927, 20180928, 20181001, 20181002, 20181003, 20181004, 20181005, 20181008, 20181009, 20181010, 20181011, 20181012, 20181015, 20181016, 20181017, 20181018, 20181019, 20181022, 20181023, 20181024, 20181025, 20181026, 20181029, 20181030, 20181031, 20181101, 20181102, 20181105, 20181106, 20181107, 20181108, 20181109, 20181112, 20181113, 20181114, 20181115, 20181116, 20181119, 20181120, 20181121, 20181123, 20181126, 20181127, 20181128, 20181129, 20181130, 20181203, 20181204, 20181206, 20181207, 20181210, 20181211, 20181212, 20181213, 20181214, 20181217, 20181218, 20181219, 20181220, 20181221, 20181224, 20181226, 20181227, 20181228, 20181231, 20190102, 20190103, 20190104, 20190107, 20190108, 20190109, 20190110, 20190111, 20190114, 20190115, 20190116, 20190117, 20190118, 20190122, 20190123, 20190124, 20190125, 20190128, 20190129, 20190130, 20190131, 20190201, 20190204, 20190205, 20190206, 20190207, 20190208, 20190211, 20190212, 20190213, 20190214, 20190215, 20190219, 20190220, 20190221, 20190222, 20190225, 20190226, 20190227, 20190228, 20190301, 20190304, 20190305, 20190306, 20190307, 20190308, 20190311, 20190312, 20190313, 20190314, 20190315, 20190318, 20190319, 20190320, 20190321, 20190322, 20190325, 20190326, 20190327, 20190328, 20190329, 20190401, 20190402, 20190403, 20190404, 20190405, 20190408, 20190409, 20190410, 20190411, 20190412, 20190415, 20190416, 20190417, 20190418, 20190422, 20190423, 20190424, 20190425, 20190426, 20190429, 20190430, 20190501, 20190502, 20190503, 20190506, 20190507, 20190508, 20190509, 20190510, 20190513, 20190514, 20190515, 20190516, 20190517, 20190520, 20190521, 20190522, 20190523, 20190524, 20190528, 20190529, 20190530, 20190531, 20190603, 20190604, 20190605, 20190606, 20190607, 20190610, 20190611, 20190612, 20190613, 20190614, 20190617, 20190618, 20190619, 20190620, 20190621, 20190624, 20190625, 20190626, 20190627, 20190628, 20190701, 20190702, 20190703, 20190705, 20190708, 20190709, 20190710, 20190711, 20190712, 20190715, 20190716, 20190717, 20190718, 20190719, 20190722, 20190723, 20190724, 20190725, 20190726, 20190729, 20190730, 20190731, 20190801, 20190802, 20190805, 20190806, 20190807, 20190808, 20190809, 20190812, 20190813, 20190814, 20190815, 20190816, 20190819, 20190820, 20190821, 20190822, 20190823, 20190826, 20190827, 20190828, 20190829, 20190830, 20190903, 20190904, 20190905, 20190906, 20190909, 20190910, 20190911, 20190912, 20190913, 20190916, 20190917, 20190918, 20190919, 20190920, 20190923, 20190924, 20190925, 20190926, 20190927, 20190930, 20191001, 20191002, 20191003, 20191004, 20191007, 20191008, 20191009, 20191010, 20191011, 20191014, 20191015, 20191016, 20191017, 20191018, 20191021, 20191022, 20191023, 20191024, 20191025, 20191028, 20191029, 20191030, 20191031, 20191101, 20191104, 20191105, 20191106, 20191107, 20191108, 20191111, 20191112, 20191113, 20191114, 20191115, 20191118, 20191119, 20191120], \"y\": [85.91681999999999, 86.03642000000002, 86.15932, 86.25751999999999, 86.34791999999999, 86.42841999999999, 86.50211999999999, 86.55992, 86.61582, 86.66932000000001, 86.72202000000003, 86.74832, 86.78172, 86.82802, 86.86911999999998, 86.92091999999998, 86.97442, 87.01982, 87.06272000000001, 87.10262, 87.10951999999999, 87.09732, 87.07792, 87.06452000000002, 87.01972000000002, 86.96992000000002, 86.93771999999998, 86.87371999999999, 86.78792000000001, 86.71222000000002, 86.62171999999998, 86.56471999999998, 86.49462000000001, 86.38242, 86.25431999999999, 86.11962, 86.00922, 85.90181999999999, 85.81981999999999, 85.74112000000001, 85.68011999999999, 85.61111999999999, 85.56232, 85.48331999999999, 85.38511999999999, 85.30042000000002, 85.21211999999997, 85.10851999999998, 85.03822, 84.94142, 84.88132, 84.79782000000002, 84.72041999999999, 84.67662000000001, 84.62032, 84.54652, 84.45202, 84.34662, 84.23451999999999, 84.12122000000001, 84.01311999999999, 83.88982, 83.72092, 83.54412, 83.34072, 83.13362000000001, 82.89452, 82.65111999999999, 82.39991, 82.16811, 81.92951000000001, 81.68651, 81.41731, 80.84951000000001, 80.57841, 80.30741, 80.02181, 79.75551, 79.50701, 79.50701000000002, 79.26571, 79.02971, 78.80360999999999, 78.58940999999999, 78.35091, 78.13691, 77.9149, 77.6852, 77.45309999999999, 77.2057, 76.9623, 76.74029999999999, 76.50880000000001, 76.2724, 76.0331, 75.73850000000002, 75.4137, 75.09479999999999, 74.8196, 74.5716, 74.29979999999999, 74.021, 73.72045, 73.42205, 73.15655000000001, 72.89995, 72.65924999999999, 72.40535, 72.16295, 71.92564999999999, 71.69864999999999, 71.48244999999999, 71.27404999999999, 71.03925, 70.82685000000001, 70.61655, 70.40365, 70.02995000000001, 69.82315, 69.63494999999999, 69.49125, 69.37235000000001, 69.20745000000001, 69.20745, 69.03615, 68.91555, 68.78484999999999, 68.66804999999998, 68.55585, 68.47964999999999, 68.36945, 68.26005, 68.16365, 68.05775, 67.96095, 67.86264999999999, 67.72285, 67.58105, 67.42025000000001, 67.24965000000002, 67.04684999999999, 66.83255, 66.62535000000001, 66.44145, 66.31345, 66.18675000000002, 66.05775, 65.94274999999999, 65.80845000000001, 65.67384999999999, 65.48774999999999, 65.33305000000001, 65.13885, 64.92915, 64.70215, 64.47855, 64.26674999999999, 64.05735, 63.882149999999996, 63.59914999999999, 63.46695, 63.37365, 63.28664999999999, 63.25604999999999, 63.18885, 63.187850000000005, 63.18784999999999, 63.15834999999999, 63.13225000000001, 63.08225, 63.05134999999999, 63.02165, 63.01305, 63.011050000000004, 63.01244999999999, 63.00255, 62.99815, 63.00555, 62.99465, 62.987350000000006, 62.957249999999995, 62.94065, 62.92315000000001, 62.92745000000001, 62.93495, 62.90965, 62.89045, 62.89755, 62.89464999999999, 62.89814999999999, 62.91105, 62.91405, 62.91565, 62.917049999999996, 62.92465, 62.97795, 63.12635, 63.32285000000001, 63.498050000000006, 63.66384999999998, 63.84725, 64.02215, 64.21979999999999, 64.4384, 64.6223, 64.79220000000001, 64.9562, 65.1219, 65.2611, 65.38329999999999, 65.5037, 65.6349, 65.74610000000001, 65.86039999999998, 65.96640000000001, 66.0567, 66.1544, 66.2476, 66.3681, 66.49080000000001, 66.59200000000001, 66.6367, 66.6762, 66.75319999999999, 66.84089999999999, 66.8968, 66.9423, 66.9992, 67.0637, 67.0821, 67.1379, 67.17750000000001, 67.19569999999999, 67.21589999999999, 67.2237, 67.25319999999999, 67.292, 67.32449999999999, 67.37929999999999, 67.42909999999999, 67.4926, 67.5589, 67.60979999999999, 67.65810000000002, 67.6804, 67.71990000000001, 67.79729999999999, 67.8338, 67.8491, 67.87650000000001, 67.86059999999999, 67.86110000000002, 67.8611, 67.8723, 67.902, 67.94369999999999, 67.9835, 68.0306, 68.04180000000001, 68.03450000000001, 68.0192, 68.00829999999999, 68.0134, 68.0181, 68.0066, 67.9905, 67.9293, 67.8869, 67.8304, 67.7781, 67.69959999999999, 67.62590000000002, 67.57970000000002, 67.54690000000001, 67.5088, 67.45909999999999, 67.38779999999998, 67.3218, 67.2523, 67.1665, 67.0984, 67.0058, 66.9357, 66.85549999999999, 66.75819999999999, 66.6753, 66.58151, 66.48831, 66.41551, 66.33921000000001, 66.16731, 66.08971000000001, 66.02291, 65.93910999999999, 65.83761, 65.67380999999999, 65.67380999999999, 65.48190999999998, 65.27901, 65.08051, 64.89491000000001, 64.72491, 64.56021, 64.39791, 64.24801, 64.09810999999999, 63.92581000000001, 63.76191000000001, 63.61460999999999, 63.49360999999999, 63.36360999999999, 63.23330999999999, 63.08491, 62.92781, 62.76991, 62.63091, 62.49591000000001, 62.35291, 62.199210000000015, 62.058810000000015, 61.92071, 61.799609999999994, 61.66320999999999, 61.52561000000001, 61.403410000000015, 61.285610000000005, 61.18121000000001, 61.082609999999995, 60.96456, 60.86316, 60.75246, 60.643159999999995, 60.415060000000004, 60.316159999999996, 60.19926, 60.09075999999999, 60.00505999999999, 59.91666, 59.84346000000001, 59.84346000000001, 59.790859999999995, 59.75316, 59.71576, 59.68285999999999, 59.63726, 59.56656, 59.47926000000001, 59.390560000000015, 59.29496000000001, 59.21676, 59.14656, 59.08686000000001, 59.04966, 59.009660000000004, 58.977059999999994, 58.94676, 58.90366000000001, 58.87425999999999, 58.851220000000005, 58.83402, 58.830920000000006, 58.82162, 58.809020000000004, 58.80632000000001, 58.80441999999999, 58.815020000000004, 58.84331999999999, 58.85831999999999, 58.88102, 58.89922000000001, 58.91550300000001, 58.91980300000001, 58.917703, 58.899902999999995, 58.889003, 58.88700299999999, 58.892503000000005, 58.895303000000006, 58.927702999999994, 58.91480299999999, 58.881203, 58.850303, 58.811603, 58.778802999999996, 58.760303, 58.729803000000004, 58.710893, 58.699793, 58.66519300000001, 58.62559300000001, 58.573792999999995, 58.531393, 58.47779299999999, 58.429293, 58.38279299999999, 58.33209299999999, 58.29739299999999, 58.257993000000006, 58.23739300000001, 58.214393, 58.181293000000004, 58.122992999999994, 58.06019299999999, 57.999692999999986, 57.933093, 57.868793000000004, 57.82149299999999, 57.796793, 57.75509299999999, 57.70799299999998, 57.650592999999986, 57.588093, 57.541793, 57.510293000000004, 57.458593, 57.404493, 57.338693000000006, 57.277793, 57.21939299999999, 57.11869299999999, 57.064793, 57.019593000000015, 56.97319300000001, 56.92169300000001, 56.865793000000004, 56.865793000000004, 56.80259299999999, 56.731593000000004, 56.668793000000015, 56.608392999999985, 56.54909299999999, 56.48919299999999, 56.45159300000001, 56.420293, 56.411493, 56.414293, 56.416193, 56.412293000000005, 56.390592999999996, 56.377793000000004, 56.352593000000006, 56.333293000000005, 56.31519300000001, 56.27749300000001, 56.24119300000001, 56.236093000000004, 56.233093, 56.215993, 56.210592999999996, 56.17389299999999, 56.128493, 56.08839299999999, 56.022393, 55.95489299999999, 55.88839299999999, 55.815193, 55.75279299999999, 55.679993, 55.620533, 55.56403300000001, 55.511533, 55.46903299999999, 55.41233299999999, 55.304732999999985, 55.26693299999999, 55.219533, 55.17263300000001, 55.12523300000001, 55.082933000000004, 55.082933000000004, 55.05605, 55.041850000000004, 55.02355000000001, 55.012750000000004, 55.00205000000001, 54.98135, 54.95844999999999, 54.929649999999995, 54.872249999999994, 54.85174999999998, 54.851749999999996, 54.858349999999994, 54.87125, 54.89225, 54.92725, 54.95286, 54.95965999999999, 54.96515999999999, 54.96586, 54.968059999999994, 54.954260000000005, 54.94206, 54.940259999999995, 54.94316, 54.95196, 54.97156, 55.00016000000001, 55.03176000000001, 55.05826, 55.09385999999999, 55.14576, 55.206659999999985, 55.26325999999999, 55.31856, 55.37896, 55.521460000000005, 55.57446, 55.62416, 55.664559999999994, 55.717861, 55.787861, 55.86616100000001, 55.86616100000001, 55.93516100000001, 56.006661, 56.06486099999999, 56.122561, 56.175461000000006, 56.213661, 56.236161, 56.269561, 56.308460999999994, 56.33646100000001, 56.352261, 56.382560999999995, 56.408061, 56.42206100000001, 56.42606099999999, 56.433810999999984, 56.427111000000004, 56.43431100000001, 56.444210999999996, 56.43571099999999, 56.41751099999998, 56.382311, 56.325011, 56.28411100000001, 56.23391100000001, 56.185811, 56.123411000000004, 56.065211000000005, 56.00041100000001, 55.93131100000001, 55.879911, 55.833211, 55.774710999999996, 55.720710999999994, 55.687711, 55.638511, 55.633911, 55.646611, 55.658211, 55.685811, 55.72711100000001, 55.75856100000001, 55.790361000000004, 55.81496100000001, 55.844561000000006, 55.86216100000001, 55.866361000000005, 55.85736099999999, 55.845060999999994, 55.848961, 55.850061000000004, 55.85316100000001, 55.83436100000001, 55.820461, 55.827561, 55.839861, 55.847660999999995, 55.853961, 55.860161, 55.86976100000001, 55.871361, 55.86436100000001, 55.865460999999996, 55.871261, 55.894960999999995, 55.93786099999999, 55.984860999999995, 56.014561, 56.045061000000004, 56.060361, 56.073161, 56.07966100000001, 56.104350999999994, 56.160451, 56.20385100000001, 56.266150999999994, 56.322051, 56.39885099999999, 56.47115099999999, 56.572650999999986, 56.626450999999996, 56.68875100000001, 56.737851000000006, 56.78305099999999, 56.82585099999999, 56.825850999999986, 56.85675099999999, 56.87065099999999, 56.881251, 56.887251, 56.88065100000001, 56.88325100000001, 56.874351, 56.86235099999999, 56.856850999999985, 56.852951000000004, 56.852751000000005, 56.84785000000001, 56.85175, 56.858050000000006, 56.875949999999996, 56.90275, 56.94905, 56.99634999999999, 57.04165, 57.11045, 57.196949999999994, 57.262350000000005, 57.33485, 57.413450000000005, 57.49555, 57.57235000000001, 57.64405000000001, 57.723650000000006, 57.814949999999996, 57.90925, 58.00405, 58.06795, 58.11285000000001, 58.17314999999999, 58.252849999999995, 58.31675, 58.40255, 58.554748999999994, 58.629059, 58.738259000000006, 58.842859000000004, 58.938659, 59.029458999999996, 59.029458999999996, 59.099759000000006, 59.164159, 59.21365900000001, 59.267259, 59.313058999999996, 59.371058999999995, 59.41835900000001, 59.470059, 59.513259000000005, 59.552559, 59.581959, 59.604909000000006, 59.624809000000006, 59.653709, 59.692708999999994, 59.73070899999999, 59.77680899999999, 59.835409, 59.894859000000004, 59.93965900000001, 59.992559, 60.043959, 60.102959, 60.15395900000001, 60.185959000000004, 60.212659, 60.243959000000004, 60.270959000000005, 60.30605900000001, 60.339059000000006, 60.373009, 60.41390900000001, 60.440209, 60.477109, 60.49590899999999, 60.518009, 60.545109000000004, 60.57450900000001, 60.608109000000006, 60.632408999999996, 60.64720900000001, 60.664908999999994, 60.664908999999994, 60.667909, 60.68080899999999, 60.68270899999999, 60.674209, 60.67630899999999, 60.67820899999999, 60.698709, 60.727509, 60.746809000000006, 60.763509000000006, 60.778608999999996, 60.805709, 60.830809, 60.862409, 60.90990899999999, 60.961009000000004, 61.01080899999999, 61.076809000000004, 61.12970899999999, 61.18430899999999, 61.247909, 61.302909, 61.35730900000001, 61.42150900000001, 61.483309000000006, 61.52590899999999, 61.569109, 61.605009, 61.630809, 61.636709, 61.633109000000005, 61.628509, 61.603609, 61.583709000000006, 61.577209, 61.565109, 61.56060900000001, 61.55930900000001, 61.55360900000001, 61.559609, 61.574309, 61.59530900000001, 61.61990900000001, 61.65880900000002, 61.723709, 61.810908999999995, 61.88960899999999, 61.958209000000004, 62.038409, 62.116209, 62.193608999999995, 62.26411000000001, 62.343999999999994, 62.403, 62.4742, 62.555200000000006, 62.6343, 62.724700000000006, 62.82500000000001, 62.9384, 63.036300000000004, 63.133600000000015, 63.22380000000001, 63.3153, 63.40079999999999, 63.498899999999985, 63.59549999999999, 63.689099999999996, 63.824, 63.939699999999995, 64.05469999999998, 64.16099999999999, 64.2888, 64.4127, 64.5125, 64.60965000000002, 64.71435000000001, 64.82715, 64.95114999999998, 65.16995, 65.28115, 65.40374999999999, 65.5323, 65.6687, 65.791, 65.791, 65.92490000000001, 66.06585, 66.20844999999998, 66.36345, 66.51005, 66.66755, 66.83795, 67.01775, 67.20105000000001, 67.37904999999999, 67.55885, 67.74495, 67.94285, 68.13945, 68.33805, 68.51714999999999, 68.71945000000001, 68.92715, 69.12885, 69.33194999999999, 69.51825, 69.69405, 69.87825000000001, 70.04585, 70.21534999999999, 70.38654999999999, 70.54705, 70.68934999999999, 70.82504999999999, 70.94445, 71.08655, 71.20025, 71.33134999999999, 71.46504999999999, 71.60324999999999, 71.74435000000001, 71.89665000000001, 72.16715, 72.29575000000001, 72.41745, 72.52705, 72.64255, 72.77945000000001, 72.77945000000001, 72.89905, 73.01535000000001, 73.12174999999998, 73.23044999999999, 73.33765, 73.43495, 73.52055, 73.60794999999999, 73.70205, 73.79754999999999, 73.89335, 73.97425000000001, 74.05404999999999, 74.11874999999999, 74.18695, 74.24765000000001, 74.31005, 74.38095, 74.45664999999998, 74.51384999999999, 74.56995, 74.61765, 74.66695, 74.69234999999999, 74.70125, 74.70095, 74.70240000000001, 74.6956, 74.68520000000001, 74.6847, 74.69655, 74.70875, 74.71374999999999, 74.71364999999999, 74.713849, 74.69504900000001, 74.686049, 74.65124899999999, 74.62329899999999, 74.586499, 74.54179900000001, 74.471799, 74.471799, 74.402399, 74.330999, 74.263799, 74.16409900000001, 74.032899, 73.863099, 73.68849900000001, 73.51349900000001, 73.34889900000002, 73.19609899999999, 73.037149, 72.89854899999999, 72.74804899999998, 72.60104899999999, 72.451349, 72.276849, 72.12594899999999, 71.97874900000001, 71.812449, 71.64804899999999, 71.46346899999999, 71.24346899999999, 71.076969, 70.92126900000001, 70.790269, 70.64747, 70.50357, 70.35547000000001, 70.21237, 70.05947, 69.91167, 69.76167, 69.61607, 69.49277000000001, 69.37907, 69.24996999999999, 69.11497, 68.97377, 68.76947, 68.58797, 68.41566999999999, 68.24667, 68.07677, 67.89677, 67.70536999999999, 67.50731999999999, 67.28112, 67.03792000000001, 66.78302, 66.51921999999999, 66.27732, 66.08372, 65.91272, 65.75282, 65.59992, 65.46382, 65.29822, 65.16102, 65.02872, 64.90512000000001, 64.77352, 64.64141999999998, 64.49851999999998, 64.34692, 64.19812, 64.06281999999999, 63.93172000000001, 63.780719999999995, 63.63102, 63.46701999999999, 63.30382, 63.16371999999999, 63.014619999999994, 62.88811999999999, 62.75411999999999, 62.601620000000004, 62.47792000000001, 62.357120000000016, 62.21822, 61.96902, 61.889920000000004, 61.82362000000001, 61.75647000000001, 61.70657000000002, 61.65657000000001, 61.65657000000001, 61.58527000000001, 61.51072000000001, 61.44182000000001, 61.38971999999999, 61.35451899999999, 61.321218, 61.260018, 61.217217999999995, 61.162417999999995, 61.106018000000006, 61.07236800000001, 61.056368, 61.01646800000001, 60.978168000000004, 60.93856800000001, 60.915168000000016, 60.894168000000015, 60.889268, 60.886868000000014, 60.90176800000001, 60.919067999999996, 60.94596799999999, 60.952268000000004, 60.947767999999996, 60.955268, 60.93541499999999, 60.95111500000001, 60.933715, 60.92141499999999, 60.92761499999999, 60.910315, 60.909515, 60.92691500000001, 60.91861500000001, 60.940295, 60.983195, 60.98309499999999, 60.967095, 60.958294, 60.943194000000005, 60.951994000000006, 60.967893999999994, 60.972094000000006, 60.972094000000006, 60.995293999999994, 61.019493999999995, 61.036894000000004, 61.028694, 61.023494, 61.013293999999995, 61.023793999999995, 61.046794, 61.12209399999999, 61.188294000000006, 61.26429400000001, 61.34439400000001, 61.431094, 61.51809400000001, 61.642394, 61.73674399999999, 61.86684399999999, 61.993544, 62.142144000000016, 62.280044000000004, 62.41434400000001, 62.488944000000004, 62.543044, 62.58854399999999, 62.649043999999996, 62.68204400000001, 62.738144000000005, 62.80504400000001, 62.86844399999999, 62.920544, 62.969044000000004, 63.013444, 63.078444, 63.14854400000001, 63.219044, 63.286544000000006, 63.332744000000005, 63.368744, 63.405044, 63.464744, 63.487344, 63.509644, 63.509644, 63.509944, 63.522244, 63.536844, 63.534043999999994, 63.522543999999996, 63.552043999999995, 63.579044, 63.587244, 63.56274400000001, 63.52684400000001, 63.47594400000002, 63.42114400000002, 63.345844000000014, 63.277144000000014, 63.202444000000014, 63.12984400000001, 63.02794399999999, 62.92274499999999, 62.815347, 62.729747, 62.646847, 62.557247000000004, 62.461547, 62.349447, 62.21332700000001, 62.10832700000001, 62.00382700000001, 61.906926999999996, 61.78832700000001, 61.65812700000001, 61.549927000000004, 61.469826999999995, 61.39532700000001, 61.319727, 61.224927, 61.14052699999999, 61.033327, 60.928927, 60.81248000000001, 60.68908, 60.57958000000001, 60.47178000000001, 60.35228000000001, 60.23458000000001, 60.10228, 59.963879999999996, 59.81898, 59.639979999999994, 59.44818, 59.21298000000001, 58.95398000000001, 58.704179999999994, 58.47878000000001, 58.27058000000001, 58.047380000000004, 57.80218, 57.563480000000006, 57.310080000000006, 57.042379999999994, 56.781580000000005, 56.51887999999999, 56.25208000000001, 55.987780000000015, 55.71838000000002, 55.44698000000001, 55.19248, 54.93488, 54.65758000000001, 54.371579999999994, 54.08447999999999, 53.77997999999999, 53.475779999999986, 53.19388, 52.88658, 52.585780000000014, 52.27778, 51.99098, 51.72257999999999, 51.49238, 51.052780000000006, 50.83688, 50.64137999999999, 50.423079999999985, 50.18477999999999, 49.95647999999999, 49.95647999999999, 49.75038000000001, 49.55408000000001, 49.35298000000001, 49.157579999999996, 48.95157999999999, 48.75088000000001, 48.561280000000004, 48.35838, 48.13268, 47.92328, 47.712080000000014, 47.49338000000001, 47.295280000000005, 47.128569999999996, 46.97706999999998, 46.82727, 46.67977, 46.53516999999999, 46.40327, 46.24597000000001, 46.08227000000001, 45.92197000000001, 45.78317000000001, 45.63767, 45.520070000000004, 45.384969999999996, 45.260070000000006, 45.13076999999999, 45.02947, 44.91526999999999, 44.80611499999999, 44.706815, 44.613114999999986, 44.49441499999999, 44.344615000000005, 44.201315, 44.02201499999999, 43.67263499999999, 43.50693499999999, 43.331035, 43.134935000000006, 42.959635000000006, 42.776935, 42.776935, 42.566935, 42.34193500000001, 42.124335, 41.92843499999999, 41.77683499999999, 41.61653499999999, 41.463635000000004, 41.303035, 41.149134999999994, 40.990285, 40.840385, 40.689885, 40.532185, 40.35198499999999, 40.172785, 39.980284999999995, 39.790884999999996, 39.610884999999996, 39.459285, 39.32368499999999, 39.19198499999999, 39.023284999999994, 38.857384999999994, 38.691485, 38.492884999999994, 38.33508500000001, 38.172585000000005, 38.032185000000005, 37.909785, 37.765285000000006, 37.615585, 37.481285, 37.358485, 37.217885, 37.062885, 36.687485, 36.502285, 36.315885, 36.13348500000001, 35.955185, 35.766284999999996, 35.568084999999996, 35.568084999999996, 35.403185, 35.22808499999999, 35.051285, 34.869285, 34.664784999999995, 34.432485, 34.18718499999999, 33.963885, 33.73718499999999, 33.517885, 33.319835, 33.097435, 32.880135, 32.648135, 32.431635, 32.22663500000001, 32.010735000000004, 31.833835000000004, 31.660234999999997, 31.486135, 31.338334999999997, 31.206235000000003, 31.048335, 30.902834999999996, 30.741834999999995, 30.571334999999998, 30.381344999999996, 30.190944999999996, 29.982045000000003, 29.773845000000005, 29.567995, 29.366095000000005, 29.185695, 29.022693999999998, 28.862594000000005, 28.706993999999998, 28.557494000000002, 28.400294000000002, 28.254494, 28.121894, 28.008794, 27.891094000000002, 27.771294, 27.676749, 27.556349, 27.437649000000008, 27.371449000000002, 27.321249, 27.263149, 27.244349000000003, 27.219849, 27.179648999999998, 27.136549000000006, 27.114949000000003, 27.100048, 27.096747999999998, 27.116547999999998, 27.153748, 27.192248, 27.241748, 27.289747999999996, 27.299048, 27.311048, 27.336547999999993, 27.359548, 27.394348, 27.438598, 27.474297999999997, 27.504897999999997, 27.56354799999999, 27.644447999999997, 27.727548000000002, 27.816947999999993, 27.935347999999994, 28.074948, 28.191149, 28.320649000000003, 28.437049, 28.579249, 28.787549000000002, 28.907849000000006, 29.022149000000002, 29.150749, 29.264799000000004, 29.365099, 29.365099000000004, 29.492499, 29.604099, 29.675798999999998, 29.769999, 29.888499000000003, 30.014099, 30.161899, 30.307098999999997, 30.464399, 30.615699, 30.778899000000003, 30.952298999999996, 31.155099, 31.358598999999998, 31.539698999999995, 31.733399, 31.92029900000001, 32.102298999999995, 32.27919899999999, 32.455799, 32.641099, 32.786899, 32.941549, 33.069849, 33.189299000000005, 33.357699000000004, 33.487699, 33.62429900000001, 33.756399, 33.894799, 34.048899, 34.188798999999996, 34.313899, 34.434299, 34.567299000000006, 34.703799000000004, 34.86589899999999, 35.20084899999999, 35.387549, 35.568749000000004, 35.735248999999996, 35.921449, 36.106148999999995, 36.106149, 36.28589899999999, 36.454599, 36.628799, 36.78549, 36.92828999999999, 37.05569, 37.188739999999996, 37.317339999999994, 37.445539999999994, 37.57844, 37.67594, 37.76014, 37.832139999999995, 37.882039999999996, 37.951240000000006, 38.01514, 38.054539999999996, 38.09994, 38.17464, 38.24784, 38.315850000000005, 38.39905, 38.47105, 38.572939999999996, 38.702641, 38.82824099999999, 38.944741, 39.050441, 39.167041, 39.263941, 39.345541000000004, 39.448141, 39.550441, 39.653541, 39.747141, 39.886691, 39.94681599999999, 40.017416, 40.08576600000001, 40.151166, 40.200165999999996, 40.247866, 40.247866, 40.274166, 40.290966, 40.306965000000005, 40.311465000000005, 40.337165, 40.37486500000001, 40.424715, 40.476015, 40.54631499999999, 40.606415, 40.669315, 40.718864999999994, 40.757864999999995, 40.798264999999994, 40.889664999999994, 40.99216499999999, 41.060964999999996, 41.132464999999996, 41.219065, 41.28926499999999, 41.37786499999999, 41.45036499999999, 41.508765000000004, 41.554764999999996, 41.661364999999996, 41.73986500000001, 41.828365000000005, 41.92166499999999, 42.01186500000001, 42.106265, 42.206165, 42.307165, 42.414865, 42.519265000000004, 42.62856500000001, 42.72931500000001, 42.841615000000004, 42.95061499999999, 43.034115, 43.140707, 43.248207, 43.333707000000004, 43.415907, 43.48320699999999, 43.540806999999994, 43.598607, 43.673407, 43.735307000000006, 43.80880700000001, 43.875807, 43.91680800000001, 43.95075799999999, 43.97705800000001, 43.99835800000001, 44.021558, 44.033958000000005, 44.052358, 44.069357999999994, 44.087458000000005, 44.093558, 44.11486799999999, 44.146167999999996, 44.18076800000001, 44.204418000000004, 44.213018000000005, 44.22631799999999, 44.229017999999996, 44.25181799999999, 44.287718, 44.32911799999999, 44.36861800000001, 44.400318000000006, 44.43781800000001, 44.48331799999999, 44.52941799999999, 44.567018000000004, 44.59051800000001, 44.63610800000001, 44.68120799999999, 44.76961800000001, 44.77921800000001, 44.766118, 44.75481800000001, 44.755418, 44.759918, 44.759918, 44.75981800000002, 44.76541800000001, 44.760918000000004, 44.755018, 44.716817999999996, 44.68561800000001, 44.65711800000001, 44.64906800000001, 44.63024299999999, 44.611843, 44.589642999999995, 44.568042999999996, 44.556392999999986, 44.547592999999985, 44.539093, 44.52549300000001, 44.52169300000001, 44.531593, 44.550993, 44.567493000000006, 44.57674299999999, 44.591843, 44.593843, 44.598643, 44.57764300000001, 44.57504300000001, 44.58209299999999, 44.584492999999995, 44.559892999999995, 44.53859299999999, 44.51649300000001, 44.488393000000016, 44.444893, 44.392593, 44.339392999999994, 44.294543, 44.266142999999985, 44.173143, 44.088243000000006, 44.00464300000001, 43.92054399999999, 43.836044, 43.742343999999996, 43.742343999999996, 43.634843999999994, 43.534344000000004, 43.43484399999999, 43.33194399999999, 43.246944000000006, 43.15124399999999, 43.056093999999995, 42.96729399999998, 42.879994, 42.787902, 42.687902, 42.606302, 42.518801999999994, 42.43480200000001, 42.34645, 42.24695, 42.134350000000005, 42.00405, 41.853849999999994, 41.70995, 41.575449, 41.438749, 41.30364900000001, 41.158148999999995, 41.008649, 40.869649, 40.733248999999994, 40.607349000000006, 40.46674900000001, 40.325449000000006, 40.17734900000001, 40.02584900000001, 39.865949, 39.70114900000001, 39.550549000000004, 39.251449, 39.098549000000006, 38.936249000000004, 38.801549, 38.679849, 38.567249000000004, 38.430749000000006, 38.430749000000006, 38.278549, 38.121249, 37.959449000000006, 37.822349, 37.689249, 37.55984900000001, 37.425749, 37.301449, 37.175249, 37.07224899999999, 36.961648999999994, 36.847649000000004, 36.719049, 36.614149000000005, 36.510549, 36.405749, 36.30239900000001, 36.228199000000004, 36.150099, 36.081599, 35.994699, 35.931049, 35.843949, 35.75814900000001, 35.673549, 35.58589900000001, 35.503899000000004, 35.421049, 35.339349, 35.258049, 35.171049, 35.078849, 34.969949, 34.85934900000001, 34.751948999999996, 34.642749, 34.529599000000005, 34.423699, 34.304899000000006, 34.189149, 34.072249, 33.968049, 33.870649, 33.790849, 33.70454899999999, 33.61374899999999, 33.539499, 33.464149, 33.401799000000004, 33.34259900000001, 33.273099, 33.20689899999999, 33.150899, 33.104749, 33.065148, 33.034398, 33.019298, 33.018598, 33.010197999999995, 32.994398, 32.982598, 32.963198000000006, 32.948798000000004, 32.928948, 32.91074799999999, 32.889247999999995, 32.874398, 32.865498, 32.842301, 32.817501, 32.797901, 32.785253000000004, 32.785253, 32.779453, 32.784152999999996, 32.793553, 32.797353, 32.814453, 32.827653, 32.877653, 32.923053, 32.966252999999995, 33.029903, 33.083403000000004, 33.154103000000006, 33.154103000000006, 33.247693000000005, 33.342793, 33.432293, 33.52889300000001, 33.631493000000006, 33.727993, 33.805443, 33.877443, 33.955443, 34.036742999999994, 34.092743, 34.148593, 34.20039299999999, 34.270610999999995, 34.336811, 34.413211000000004, 34.486560999999995, 34.566661, 34.647061, 34.721360999999995, 34.788760999999994, 34.844260999999996, 34.902060999999996, 34.957761, 35.02266099999999, 35.087361, 35.150961, 35.201261, 35.247961000000004, 35.294160999999995, 35.349011, 35.414311, 35.493811, 35.575611, 35.678811, 35.760661000000006, 35.85426100000001, 36.048761, 36.15816099999999, 36.274511000000004, 36.394960999999995, 36.518161, 36.636761, 36.636761, 36.756761000000004, 36.883461000000004, 37.020461, 37.149061, 37.273461000000005, 37.394361, 37.511210999999996, 37.650111, 37.785311, 37.920911000000004, 38.046311, 38.16391099999999, 38.275811, 38.364311, 38.454111000000005, 38.561710999999995, 38.643861, 38.708811, 38.757511, 38.786011, 38.798911, 38.806011, 38.826211, 38.823260999999995, 38.825461, 38.815110999999995, 38.78971099999999, 38.767011, 38.700661, 38.638961, 38.586161, 38.536461, 38.489861, 38.437811, 38.379978, 38.29712800000001, 38.25052800000001, 38.199425000000005, 38.167224999999995, 38.135625000000005, 38.105325, 38.075925, 38.07592499999999, 38.049825, 38.017525, 37.987825, 37.952825, 37.919925, 37.905625, 37.879325, 37.843025000000004, 37.807725, 37.755925000000005, 37.677075, 37.606775, 37.51777500000001, 37.42288500000001, 37.323485000000005, 37.243885000000006, 37.15018499999999, 37.060784999999996, 37.000184999999995, 36.955234999999995, 36.904534999999996, 36.858934999999995, 36.808234999999996, 36.771035, 36.756485, 36.733385000000006, 36.70616700000001, 36.692367000000004, 36.66546700000001, 36.646716999999995, 36.619217, 36.588317, 36.565217, 36.543817000000004, 36.557317000000005, 36.557817, 36.556017000000004, 36.550017000000004, 36.55611700000001, 36.577017, 36.605767, 36.638566999999995, 36.670967, 36.696767, 36.71256700000001, 36.726967, 36.730866999999996, 36.728267, 36.728066999999996, 36.732267, 36.735066999999994, 36.718517, 36.700216999999995, 36.687166999999995, 36.671867, 36.653267, 36.628867, 36.600367000000006, 36.58476700000001, 36.563967, 36.552941999999994, 36.54134200000001, 36.52874200000001, 36.52634200000001, 36.495141999999994, 36.452442000000005, 36.417542000000005, 36.394542, 36.382042, 36.36044199999999, 36.377942000000004, 36.383042, 36.386742, 36.421141999999996, 36.470341999999995, 36.526241999999996, 36.594091999999996, 36.684792, 36.777892, 36.972392000000006, 37.072891999999996, 37.166692, 37.264792, 37.365392, 37.50214199999999, 37.50214199999999, 37.638842, 37.762142, 37.887242, 38.009941999999995, 38.145292, 38.285425000000004, 38.424525, 38.555924999999995, 38.688125, 38.829325, 38.955425, 39.063024999999996, 39.17422499999999, 39.276825, 39.387225, 39.504425, 39.611575, 39.72067500000001, 39.833575, 39.924974999999996, 40.026375, 40.108774999999994, 40.19087499999999, 40.279675000000005, 40.377975000000006, 40.47837500000001, 40.601974999999996, 40.713575000000006, 40.826874999999994, 40.932575, 41.043275, 41.158075, 41.253275, 41.345275, 41.440475, 41.52177499999999, 41.59197499999999, 41.703975, 41.762175, 41.827675, 41.88227499999999, 41.93427500000001, 41.970175, 41.97017499999999, 42.005575, 42.04687500000001, 42.08477499999999, 42.127574999999986, 42.160775, 42.202775, 42.220875, 42.235775, 42.244875, 42.244675, 42.234325, 42.231325, 42.219525000000004, 42.207225, 42.180825, 42.152525000000004, 42.105925, 42.043125, 41.991325, 41.930524999999996, 41.884325, 41.83847500000001, 41.780725000000004, 41.710924999999996, 41.643924999999996, 41.567525, 41.487824999999994, 41.423024999999996, 41.331624999999995, 41.221824999999995, 41.12350000000001, 41.0248, 40.934, 40.8309, 40.744800000000005, 40.599399999999996, 40.5104, 40.4143, 40.318200000000004, 40.1944, 40.081100000000006, 39.9615, 39.9615, 39.822399999999995, 39.6827, 39.517399999999995, 39.36745, 39.195809999999994, 39.02711, 38.84811, 38.663509999999995, 38.47630999999999, 38.305409999999995, 38.153909999999996, 37.98861, 37.823910000000005, 37.655210000000004, 37.48021, 37.30651000000001, 37.13801, 36.96471, 36.77751, 36.579910000000005, 36.37241, 36.165409999999994, 35.939409999999995, 35.70861, 35.483509999999995, 35.27101, 35.06951, 34.857310000000005, 34.648410000000005, 34.45366, 34.268159999999995, 34.08546, 33.91846, 33.74976, 33.60316, 33.45906, 33.31676, 33.16796, 33.013459999999995, 32.84506, 32.68356, 32.528259999999996, 32.365759999999995, 32.203759999999996, 32.04015999999999, 31.88146, 31.71726, 31.562060000000002, 31.421660000000003, 31.281859999999998, 31.147160000000003, 31.013759999999998, 30.867559999999997, 30.715860000000003, 30.560060000000004, 30.412560000000003, 30.278859999999998, 30.145359999999997, 30.01576, 29.88786, 29.766859999999998, 29.634460000000004, 29.523560000000003, 29.425159999999995, 29.32786, 29.223359999999992, 29.117159999999995, 29.01746, 28.90616, 28.806160000000006, 28.700160000000004, 28.60506, 28.496959999999998, 28.40446, 28.310959999999994, 28.22256, 28.149260000000005, 28.07186, 28.006859999999996, 27.907809999999998, 27.861509999999996, 27.836609999999997, 27.832009999999997, 27.806909999999995, 27.791509999999995, 27.79151, 27.797710000000002, 27.78786000000001, 27.78256, 27.769460000000002, 27.76196, 27.74316, 27.72116, 27.692960000000003, 27.687660000000005, 27.677960000000006, 27.673060000000007, 27.677960000000002, 27.68426, 27.71279, 27.742890000000006, 27.775190000000002, 27.832890000000003, 27.88329, 27.96683, 28.04153, 28.120230000000003, 28.187330000000006, 28.245130000000003, 28.302029999999995, 28.335630000000002, 28.372329999999998, 28.40413, 28.444230000000008, 28.496530000000003, 28.539430000000007, 28.582030000000003, 28.625830000000004, 28.680430000000005, 28.72443, 28.78803, 28.85873, 28.947129999999998, 29.124730000000003, 29.208730000000006, 29.27283, 29.319930000000003, 29.364430000000002, 29.398480000000003, 29.398480000000003, 29.43178, 29.443880000000004, 29.44188, 29.44138, 29.44293, 29.434980000000007, 29.42773, 29.42313, 29.420380000000005, 29.422680000000003, 29.413680000000003, 29.40358, 29.395080000000004, 29.395979999999998, 29.399079999999998, 29.39558000000001, 29.411180000000005, 29.425179999999997, 29.42768, 29.44028, 29.457480000000004, 29.468680000000003, 29.485080000000007, 29.50628, 29.528679999999994, 29.554299999999998, 29.578999999999997, 29.598699999999994, 29.6104, 29.6255, 29.627799999999997, 29.6312, 29.6046, 29.574149999999996, 29.5369, 29.466575, 29.431875, 29.398924999999995, 29.362424999999998, 29.32372499999999, 29.286424999999998, 29.254324999999998, 29.254324999999998, 29.242624999999997, 29.245675000000002, 29.219374999999996, 29.182075, 29.129375, 29.076375, 29.021375, 28.960974999999994, 28.899075000000003, 28.821775, 28.741374999999998, 28.658375000000007, 28.566375000000004, 28.47597500000001, 28.395275000000005, 28.310375000000004, 28.228675000000003, 28.143575000000002, 28.048275, 27.956875, 27.859974999999995, 27.757174999999997, 27.662174999999998, 27.571375, 27.478275000000004, 27.394875000000003, 27.295045000000005, 27.202845000000003, 27.120745000000003, 27.034145000000002, 26.948045, 26.841845, 26.746545000000012, 26.682945, 26.614344999999993, 26.552744999999994, 26.485145, 26.424795, 26.364694999999998, 26.302694999999993, 26.237595000000002, 26.166795, 26.098695, 26.023995, 25.942895, 25.850295000000003, 25.777995, 25.696295, 25.605795000000004, 25.501895000000005, 25.406495, 25.314595000000004, 25.215994999999992, 25.132394999999995, 25.070594999999997, 25.007194999999996, 24.944844999999997, 24.874345, 24.818944999999996, 24.774144999999997, 24.731945000000007, 24.686595, 24.646495, 24.600745000000003, 24.557945, 24.499295000000004, 24.441045, 24.397945, 24.362545, 24.335345000000007, 24.292145000000005, 24.249945000000004, 24.210244999999997, 24.154744999999995, 24.096044999999993, 24.032944999999994, 23.972344999999997, 23.910745000000006, 23.847445000000004, 23.707653, 23.659952999999994, 23.612633000000002, 23.561033000000002, 23.504233, 23.448533, 23.44853300000001, 23.379033000000003, 23.310733, 23.238433, 23.185933, 23.138683, 23.110832999999992, 23.085257999999996, 23.056258, 23.026358000000002, 23.003907999999996, 22.973808000000005, 22.956708000000003, 22.943008000000006, 22.934907999999997, 22.917208000000002, 22.901958, 22.912657999999997, 22.927657999999997, 22.950657999999997, 22.965957999999997, 22.979858, 22.994858, 23.012107999999994, 23.038008, 23.066008, 23.110608000000003, 23.149908, 23.183008, 23.201358000000006, 23.221058, 23.249558000000007, 23.281758, 23.329308, 23.372107999999997, 23.408707999999997, 23.432507999999995, 23.451058, 23.475958000000006, 23.474758, 23.461558, 23.452958, 23.423907999999997, 23.390157999999996, 23.390158]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6be78e4c-6feb-4de8-bcb2-607f5f4992ca');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"e57cfc00-9992-4932-8115-e243b204c137\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"e57cfc00-9992-4932-8115-e243b204c137\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'e57cfc00-9992-4932-8115-e243b204c137',\n",
              "                        [{\"type\": \"scatter\", \"x\": [20110225, 20110228, 20110301, 20110302, 20110303, 20110304, 20110307, 20110308, 20110309, 20110310, 20110311, 20110314, 20110315, 20110316, 20110317, 20110318, 20110321, 20110322, 20110323, 20110324, 20110325, 20110328, 20110329, 20110330, 20110331, 20110401, 20110404, 20110405, 20110406, 20110407, 20110408, 20110411, 20110412, 20110413, 20110414, 20110415, 20110418, 20110419, 20110420, 20110421, 20110425, 20110426, 20110427, 20110428, 20110429, 20110502, 20110503, 20110504, 20110505, 20110506, 20110509, 20110510, 20110511, 20110512, 20110513, 20110516, 20110517, 20110518, 20110519, 20110520, 20110523, 20110524, 20110525, 20110526, 20110527, 20110531, 20110601, 20110602, 20110603, 20110606, 20110607, 20110608, 20110609, 20110610, 20110613, 20110614, 20110615, 20110616, 20110617, 20110620, 20110621, 20110622, 20110623, 20110624, 20110627, 20110628, 20110629, 20110630, 20110701, 20110705, 20110706, 20110707, 20110708, 20110711, 20110712, 20110713, 20110714, 20110715, 20110718, 20110719, 20110720, 20110721, 20110722, 20110725, 20110726, 20110727, 20110728, 20110729, 20110801, 20110802, 20110803, 20110804, 20110805, 20110808, 20110809, 20110810, 20110811, 20110812, 20110815, 20110816, 20110817, 20110818, 20110819, 20110822, 20110823, 20110824, 20110825, 20110826, 20110829, 20110830, 20110831, 20110901, 20110902, 20110906, 20110907, 20110908, 20110909, 20110912, 20110913, 20110914, 20110915, 20110916, 20110919, 20110920, 20110921, 20110922, 20110923, 20110926, 20110927, 20110928, 20110929, 20110930, 20111003, 20111004, 20111005, 20111006, 20111007, 20111010, 20111011, 20111012, 20111013, 20111014, 20111017, 20111018, 20111019, 20111020, 20111021, 20111024, 20111025, 20111026, 20111027, 20111028, 20111031, 20111101, 20111102, 20111103, 20111104, 20111107, 20111108, 20111109, 20111110, 20111111, 20111114, 20111115, 20111116, 20111117, 20111118, 20111121, 20111122, 20111123, 20111125, 20111128, 20111129, 20111130, 20111201, 20111202, 20111205, 20111206, 20111207, 20111208, 20111209, 20111212, 20111213, 20111214, 20111215, 20111216, 20111219, 20111220, 20111221, 20111222, 20111223, 20111227, 20111228, 20111229, 20111230, 20120103, 20120104, 20120105, 20120106, 20120109, 20120110, 20120111, 20120112, 20120113, 20120117, 20120118, 20120119, 20120120, 20120123, 20120124, 20120125, 20120126, 20120127, 20120130, 20120131, 20120201, 20120202, 20120203, 20120206, 20120207, 20120208, 20120209, 20120210, 20120213, 20120214, 20120215, 20120216, 20120217, 20120221, 20120222, 20120223, 20120224, 20120227, 20120228, 20120229, 20120301, 20120302, 20120305, 20120306, 20120307, 20120308, 20120309, 20120312, 20120313, 20120314, 20120315, 20120316, 20120319, 20120320, 20120321, 20120322, 20120323, 20120326, 20120327, 20120328, 20120329, 20120330, 20120402, 20120403, 20120404, 20120405, 20120409, 20120410, 20120411, 20120412, 20120413, 20120416, 20120417, 20120418, 20120419, 20120420, 20120423, 20120424, 20120425, 20120426, 20120427, 20120430, 20120501, 20120502, 20120503, 20120504, 20120507, 20120508, 20120509, 20120510, 20120511, 20120514, 20120515, 20120516, 20120517, 20120518, 20120521, 20120522, 20120523, 20120524, 20120525, 20120529, 20120530, 20120531, 20120601, 20120604, 20120605, 20120606, 20120607, 20120608, 20120611, 20120612, 20120613, 20120614, 20120615, 20120618, 20120619, 20120620, 20120621, 20120622, 20120625, 20120626, 20120627, 20120628, 20120629, 20120702, 20120703, 20120705, 20120706, 20120709, 20120710, 20120711, 20120712, 20120713, 20120716, 20120717, 20120718, 20120719, 20120720, 20120723, 20120724, 20120725, 20120726, 20120727, 20120730, 20120731, 20120801, 20120802, 20120803, 20120806, 20120807, 20120808, 20120809, 20120810, 20120813, 20120814, 20120815, 20120816, 20120817, 20120820, 20120821, 20120822, 20120823, 20120824, 20120827, 20120828, 20120829, 20120830, 20120831, 20120904, 20120905, 20120906, 20120907, 20120910, 20120911, 20120912, 20120913, 20120914, 20120917, 20120918, 20120919, 20120920, 20120921, 20120924, 20120925, 20120926, 20120927, 20120928, 20121001, 20121002, 20121003, 20121004, 20121005, 20121008, 20121009, 20121010, 20121011, 20121012, 20121015, 20121016, 20121017, 20121018, 20121019, 20121022, 20121023, 20121024, 20121025, 20121026, 20121031, 20121101, 20121102, 20121105, 20121106, 20121107, 20121108, 20121109, 20121112, 20121113, 20121114, 20121115, 20121116, 20121119, 20121120, 20121121, 20121123, 20121126, 20121127, 20121128, 20121129, 20121130, 20121203, 20121204, 20121205, 20121206, 20121207, 20121210, 20121211, 20121212, 20121213, 20121214, 20121217, 20121218, 20121219, 20121220, 20121221, 20121224, 20121226, 20121227, 20121228, 20121231, 20130102, 20130103, 20130104, 20130107, 20130108, 20130109, 20130110, 20130111, 20130114, 20130115, 20130116, 20130117, 20130118, 20130122, 20130123, 20130124, 20130125, 20130128, 20130129, 20130130, 20130131, 20130201, 20130204, 20130205, 20130206, 20130207, 20130208, 20130211, 20130212, 20130213, 20130214, 20130215, 20130219, 20130220, 20130221, 20130222, 20130225, 20130226, 20130227, 20130228, 20130301, 20130304, 20130305, 20130306, 20130307, 20130308, 20130311, 20130312, 20130313, 20130314, 20130315, 20130318, 20130319, 20130320, 20130321, 20130322, 20130325, 20130326, 20130327, 20130328, 20130401, 20130402, 20130403, 20130404, 20130405, 20130408, 20130409, 20130410, 20130411, 20130412, 20130415, 20130416, 20130417, 20130418, 20130419, 20130422, 20130423, 20130424, 20130425, 20130426, 20130429, 20130430, 20130501, 20130502, 20130503, 20130506, 20130507, 20130508, 20130509, 20130510, 20130513, 20130514, 20130515, 20130516, 20130517, 20130520, 20130521, 20130522, 20130523, 20130524, 20130528, 20130529, 20130530, 20130531, 20130603, 20130604, 20130605, 20130606, 20130607, 20130610, 20130611, 20130612, 20130613, 20130614, 20130617, 20130618, 20130619, 20130620, 20130621, 20130624, 20130625, 20130626, 20130627, 20130628, 20130701, 20130702, 20130703, 20130705, 20130708, 20130709, 20130710, 20130711, 20130712, 20130715, 20130716, 20130717, 20130718, 20130719, 20130722, 20130723, 20130724, 20130725, 20130726, 20130729, 20130730, 20130731, 20130801, 20130802, 20130805, 20130806, 20130807, 20130808, 20130809, 20130812, 20130813, 20130814, 20130815, 20130816, 20130819, 20130820, 20130821, 20130822, 20130823, 20130826, 20130827, 20130828, 20130829, 20130830, 20130903, 20130904, 20130905, 20130906, 20130909, 20130910, 20130911, 20130912, 20130913, 20130916, 20130917, 20130918, 20130919, 20130920, 20130923, 20130924, 20130925, 20130926, 20130927, 20130930, 20131001, 20131002, 20131003, 20131004, 20131007, 20131008, 20131009, 20131010, 20131011, 20131014, 20131015, 20131016, 20131017, 20131018, 20131021, 20131022, 20131023, 20131024, 20131025, 20131028, 20131029, 20131030, 20131031, 20131101, 20131104, 20131105, 20131106, 20131107, 20131108, 20131111, 20131112, 20131113, 20131114, 20131115, 20131118, 20131119, 20131120, 20131121, 20131122, 20131125, 20131126, 20131127, 20131129, 20131202, 20131203, 20131204, 20131205, 20131206, 20131209, 20131210, 20131211, 20131212, 20131213, 20131216, 20131217, 20131218, 20131219, 20131220, 20131223, 20131224, 20131226, 20131227, 20131230, 20131231, 20140102, 20140103, 20140106, 20140107, 20140108, 20140109, 20140110, 20140113, 20140114, 20140115, 20140116, 20140117, 20140121, 20140122, 20140123, 20140124, 20140127, 20140128, 20140129, 20140130, 20140131, 20140203, 20140204, 20140205, 20140206, 20140207, 20140210, 20140211, 20140212, 20140213, 20140214, 20140218, 20140219, 20140220, 20140221, 20140224, 20140225, 20140226, 20140227, 20140228, 20140303, 20140304, 20140305, 20140306, 20140307, 20140310, 20140311, 20140312, 20140313, 20140314, 20140317, 20140318, 20140319, 20140320, 20140321, 20140324, 20140325, 20140326, 20140327, 20140328, 20140331, 20140401, 20140402, 20140403, 20140404, 20140407, 20140408, 20140409, 20140410, 20140411, 20140414, 20140415, 20140416, 20140417, 20140421, 20140422, 20140423, 20140424, 20140425, 20140428, 20140429, 20140430, 20140501, 20140502, 20140505, 20140506, 20140507, 20140508, 20140509, 20140512, 20140513, 20140514, 20140515, 20140516, 20140519, 20140520, 20140521, 20140522, 20140523, 20140527, 20140528, 20140529, 20140530, 20140602, 20140603, 20140604, 20140605, 20140606, 20140609, 20140610, 20140611, 20140612, 20140613, 20140616, 20140617, 20140618, 20140619, 20140620, 20140623, 20140624, 20140625, 20140626, 20140627, 20140630, 20140701, 20140702, 20140703, 20140707, 20140708, 20140709, 20140710, 20140711, 20140714, 20140715, 20140716, 20140717, 20140718, 20140721, 20140722, 20140723, 20140724, 20140725, 20140728, 20140729, 20140730, 20140731, 20140801, 20140804, 20140805, 20140806, 20140807, 20140808, 20140811, 20140812, 20140813, 20140814, 20140815, 20140818, 20140819, 20140820, 20140821, 20140822, 20140825, 20140826, 20140827, 20140828, 20140829, 20140902, 20140903, 20140904, 20140905, 20140908, 20140909, 20140910, 20140911, 20140912, 20140915, 20140916, 20140917, 20140918, 20140919, 20140922, 20140923, 20140924, 20140925, 20140926, 20140929, 20140930, 20141001, 20141002, 20141003, 20141006, 20141007, 20141008, 20141009, 20141010, 20141013, 20141014, 20141015, 20141016, 20141017, 20141020, 20141021, 20141022, 20141023, 20141024, 20141027, 20141028, 20141029, 20141030, 20141031, 20141103, 20141104, 20141105, 20141106, 20141107, 20141110, 20141111, 20141112, 20141113, 20141114, 20141117, 20141118, 20141119, 20141120, 20141121, 20141124, 20141125, 20141126, 20141128, 20141201, 20141202, 20141203, 20141204, 20141205, 20141208, 20141209, 20141210, 20141211, 20141212, 20141215, 20141216, 20141217, 20141218, 20141219, 20141222, 20141223, 20141224, 20141226, 20141229, 20141230, 20141231, 20150102, 20150105, 20150106, 20150107, 20150108, 20150109, 20150112, 20150113, 20150114, 20150115, 20150116, 20150120, 20150121, 20150122, 20150123, 20150126, 20150127, 20150128, 20150129, 20150130, 20150202, 20150203, 20150204, 20150205, 20150206, 20150209, 20150210, 20150211, 20150212, 20150213, 20150217, 20150218, 20150219, 20150220, 20150223, 20150224, 20150225, 20150226, 20150227, 20150302, 20150303, 20150304, 20150305, 20150306, 20150309, 20150310, 20150311, 20150312, 20150313, 20150316, 20150317, 20150318, 20150319, 20150320, 20150323, 20150324, 20150325, 20150326, 20150327, 20150330, 20150331, 20150401, 20150402, 20150406, 20150407, 20150408, 20150409, 20150410, 20150413, 20150414, 20150415, 20150416, 20150417, 20150420, 20150421, 20150422, 20150423, 20150424, 20150427, 20150428, 20150429, 20150430, 20150501, 20150504, 20150505, 20150506, 20150507, 20150508, 20150511, 20150512, 20150513, 20150514, 20150515, 20150518, 20150519, 20150520, 20150521, 20150522, 20150526, 20150527, 20150528, 20150529, 20150601, 20150602, 20150603, 20150604, 20150605, 20150608, 20150609, 20150610, 20150611, 20150612, 20150615, 20150616, 20150617, 20150618, 20150619, 20150622, 20150623, 20150624, 20150625, 20150626, 20150629, 20150630, 20150701, 20150702, 20150706, 20150707, 20150708, 20150709, 20150710, 20150713, 20150714, 20150715, 20150716, 20150717, 20150720, 20150721, 20150722, 20150723, 20150724, 20150727, 20150728, 20150729, 20150730, 20150731, 20150803, 20150804, 20150805, 20150806, 20150807, 20150810, 20150811, 20150812, 20150813, 20150814, 20150817, 20150818, 20150819, 20150820, 20150821, 20150824, 20150825, 20150826, 20150827, 20150828, 20150831, 20150901, 20150902, 20150903, 20150904, 20150908, 20150909, 20150910, 20150911, 20150914, 20150915, 20150916, 20150917, 20150918, 20150921, 20150922, 20150923, 20150924, 20150925, 20150928, 20150929, 20150930, 20151001, 20151002, 20151005, 20151006, 20151007, 20151008, 20151009, 20151012, 20151013, 20151014, 20151015, 20151016, 20151019, 20151020, 20151021, 20151022, 20151023, 20151026, 20151027, 20151028, 20151029, 20151030, 20151102, 20151103, 20151104, 20151105, 20151106, 20151109, 20151110, 20151111, 20151112, 20151113, 20151116, 20151117, 20151118, 20151119, 20151120, 20151123, 20151124, 20151125, 20151127, 20151130, 20151201, 20151202, 20151203, 20151204, 20151207, 20151208, 20151209, 20151210, 20151211, 20151214, 20151215, 20151216, 20151217, 20151218, 20151221, 20151222, 20151223, 20151224, 20151228, 20151229, 20151230, 20151231, 20160104, 20160105, 20160106, 20160107, 20160108, 20160111, 20160112, 20160113, 20160114, 20160115, 20160119, 20160120, 20160121, 20160122, 20160125, 20160126, 20160127, 20160128, 20160129, 20160201, 20160202, 20160203, 20160204, 20160205, 20160208, 20160209, 20160210, 20160211, 20160212, 20160216, 20160217, 20160218, 20160219, 20160222, 20160223, 20160224, 20160225, 20160226, 20160229, 20160301, 20160302, 20160303, 20160304, 20160307, 20160308, 20160309, 20160310, 20160311, 20160314, 20160315, 20160316, 20160317, 20160318, 20160321, 20160322, 20160323, 20160324, 20160328, 20160329, 20160330, 20160331, 20160401, 20160404, 20160405, 20160406, 20160407, 20160408, 20160411, 20160412, 20160413, 20160414, 20160415, 20160418, 20160419, 20160420, 20160421, 20160422, 20160425, 20160426, 20160427, 20160428, 20160429, 20160502, 20160503, 20160504, 20160505, 20160506, 20160509, 20160510, 20160511, 20160512, 20160513, 20160516, 20160517, 20160518, 20160519, 20160520, 20160523, 20160524, 20160525, 20160526, 20160527, 20160531, 20160601, 20160602, 20160603, 20160606, 20160607, 20160608, 20160609, 20160610, 20160613, 20160614, 20160615, 20160616, 20160617, 20160620, 20160621, 20160622, 20160623, 20160624, 20160627, 20160628, 20160629, 20160630, 20160701, 20160705, 20160706, 20160707, 20160708, 20160711, 20160712, 20160713, 20160714, 20160715, 20160718, 20160719, 20160720, 20160721, 20160722, 20160725, 20160726, 20160727, 20160728, 20160729, 20160801, 20160802, 20160803, 20160804, 20160805, 20160808, 20160809, 20160810, 20160811, 20160812, 20160815, 20160816, 20160817, 20160818, 20160819, 20160822, 20160823, 20160824, 20160825, 20160826, 20160829, 20160830, 20160831, 20160901, 20160902, 20160906, 20160907, 20160908, 20160909, 20160912, 20160913, 20160914, 20160915, 20160916, 20160919, 20160920, 20160921, 20160922, 20160923, 20160926, 20160927, 20160928, 20160929, 20160930, 20161003, 20161004, 20161005, 20161006, 20161007, 20161010, 20161011, 20161012, 20161013, 20161014, 20161017, 20161018, 20161019, 20161020, 20161021, 20161024, 20161025, 20161026, 20161027, 20161028, 20161031, 20161101, 20161102, 20161103, 20161104, 20161107, 20161108, 20161109, 20161110, 20161111, 20161114, 20161115, 20161116, 20161117, 20161118, 20161121, 20161122, 20161123, 20161125, 20161128, 20161129, 20161130, 20161201, 20161202, 20161205, 20161206, 20161207, 20161208, 20161209, 20161212, 20161213, 20161214, 20161215, 20161216, 20161219, 20161220, 20161221, 20161222, 20161223, 20161227, 20161228, 20161229, 20161230, 20170103, 20170104, 20170105, 20170106, 20170109, 20170110, 20170111, 20170112, 20170113, 20170117, 20170118, 20170119, 20170120, 20170123, 20170124, 20170125, 20170126, 20170127, 20170130, 20170131, 20170201, 20170202, 20170203, 20170206, 20170207, 20170208, 20170209, 20170210, 20170213, 20170214, 20170215, 20170216, 20170217, 20170221, 20170222, 20170223, 20170224, 20170227, 20170228, 20170301, 20170302, 20170303, 20170306, 20170307, 20170308, 20170309, 20170310, 20170313, 20170314, 20170315, 20170316, 20170317, 20170320, 20170321, 20170322, 20170323, 20170324, 20170327, 20170328, 20170329, 20170330, 20170331, 20170403, 20170404, 20170405, 20170406, 20170407, 20170410, 20170411, 20170412, 20170413, 20170417, 20170418, 20170419, 20170420, 20170421, 20170424, 20170425, 20170426, 20170427, 20170428, 20170501, 20170502, 20170503, 20170504, 20170505, 20170508, 20170509, 20170510, 20170511, 20170512, 20170515, 20170516, 20170517, 20170518, 20170519, 20170522, 20170523, 20170524, 20170525, 20170526, 20170530, 20170531, 20170601, 20170602, 20170605, 20170606, 20170607, 20170608, 20170609, 20170612, 20170613, 20170614, 20170615, 20170616, 20170619, 20170620, 20170621, 20170622, 20170623, 20170626, 20170627, 20170628, 20170629, 20170630, 20170703, 20170705, 20170706, 20170707, 20170710, 20170711, 20170712, 20170713, 20170714, 20170717, 20170718, 20170719, 20170720, 20170721, 20170724, 20170725, 20170726, 20170727, 20170728, 20170731, 20170801, 20170802, 20170803, 20170804, 20170807, 20170808, 20170809, 20170810, 20170811, 20170814, 20170815, 20170816, 20170817, 20170818, 20170821, 20170822, 20170823, 20170824, 20170825, 20170828, 20170829, 20170830, 20170831, 20170901, 20170905, 20170906, 20170907, 20170908, 20170911, 20170912, 20170913, 20170914, 20170915, 20170918, 20170919, 20170920, 20170921, 20170922, 20170925, 20170926, 20170927, 20170928, 20170929, 20171002, 20171003, 20171004, 20171005, 20171006, 20171009, 20171010, 20171011, 20171012, 20171013, 20171016, 20171017, 20171018, 20171019, 20171020, 20171023, 20171024, 20171025, 20171026, 20171027, 20171030, 20171031, 20171101, 20171102, 20171103, 20171106, 20171107, 20171108, 20171109, 20171110, 20171113, 20171114, 20171115, 20171116, 20171117, 20171120, 20171121, 20171122, 20171124, 20171127, 20171128, 20171129, 20171130, 20171201, 20171204, 20171205, 20171206, 20171207, 20171208, 20171211, 20171212, 20171213, 20171214, 20171215, 20171218, 20171219, 20171220, 20171221, 20171222, 20171226, 20171227, 20171228, 20171229, 20180102, 20180103, 20180104, 20180105, 20180108, 20180109, 20180110, 20180111, 20180112, 20180116, 20180117, 20180118, 20180119, 20180122, 20180123, 20180124, 20180125, 20180126, 20180129, 20180130, 20180131, 20180201, 20180202, 20180205, 20180206, 20180207, 20180208, 20180209, 20180212, 20180213, 20180214, 20180215, 20180216, 20180220, 20180221, 20180222, 20180223, 20180226, 20180227, 20180228, 20180301, 20180302, 20180305, 20180306, 20180307, 20180308, 20180309, 20180312, 20180313, 20180314, 20180315, 20180316, 20180319, 20180320, 20180321, 20180322, 20180323, 20180326, 20180327, 20180328, 20180329, 20180402, 20180403, 20180404, 20180405, 20180406, 20180409, 20180410, 20180411, 20180412, 20180413, 20180416, 20180417, 20180418, 20180419, 20180420, 20180423, 20180424, 20180425, 20180426, 20180427, 20180430, 20180501, 20180502, 20180503, 20180504, 20180507, 20180508, 20180509, 20180510, 20180511, 20180514, 20180515, 20180516, 20180517, 20180518, 20180521, 20180522, 20180523, 20180524, 20180525, 20180529, 20180530, 20180531, 20180601, 20180604, 20180605, 20180606, 20180607, 20180608, 20180611, 20180612, 20180613, 20180614, 20180615, 20180618, 20180619, 20180620, 20180621, 20180622, 20180625, 20180626, 20180627, 20180628, 20180629, 20180702, 20180703, 20180705, 20180706, 20180709, 20180710, 20180711, 20180712, 20180713, 20180716, 20180717, 20180718, 20180719, 20180720, 20180723, 20180724, 20180725, 20180726, 20180727, 20180730, 20180731, 20180801, 20180802, 20180803, 20180806, 20180807, 20180808, 20180809, 20180810, 20180813, 20180814, 20180815, 20180816, 20180817, 20180820, 20180821, 20180822, 20180823, 20180824, 20180827, 20180828, 20180829, 20180830, 20180831, 20180904, 20180905, 20180906, 20180907, 20180910, 20180911, 20180912, 20180913, 20180914, 20180917, 20180918, 20180919, 20180920, 20180921, 20180924, 20180925, 20180926, 20180927, 20180928, 20181001, 20181002, 20181003, 20181004, 20181005, 20181008, 20181009, 20181010, 20181011, 20181012, 20181015, 20181016, 20181017, 20181018, 20181019, 20181022, 20181023, 20181024, 20181025, 20181026, 20181029, 20181030, 20181031, 20181101, 20181102, 20181105, 20181106, 20181107, 20181108, 20181109, 20181112, 20181113, 20181114, 20181115, 20181116, 20181119, 20181120, 20181121, 20181123, 20181126, 20181127, 20181128, 20181129, 20181130, 20181203, 20181204, 20181206, 20181207, 20181210, 20181211, 20181212, 20181213, 20181214, 20181217, 20181218, 20181219, 20181220, 20181221, 20181224, 20181226, 20181227, 20181228, 20181231, 20190102, 20190103, 20190104, 20190107, 20190108, 20190109, 20190110, 20190111, 20190114, 20190115, 20190116, 20190117, 20190118, 20190122, 20190123, 20190124, 20190125, 20190128, 20190129, 20190130, 20190131, 20190201, 20190204, 20190205, 20190206, 20190207, 20190208, 20190211, 20190212, 20190213, 20190214, 20190215, 20190219, 20190220, 20190221, 20190222, 20190225, 20190226, 20190227, 20190228, 20190301, 20190304, 20190305, 20190306, 20190307, 20190308, 20190311, 20190312, 20190313, 20190314, 20190315, 20190318, 20190319, 20190320, 20190321, 20190322, 20190325, 20190326, 20190327, 20190328, 20190329, 20190401, 20190402, 20190403, 20190404, 20190405, 20190408, 20190409, 20190410, 20190411, 20190412, 20190415, 20190416, 20190417, 20190418, 20190422, 20190423, 20190424, 20190425, 20190426, 20190429, 20190430, 20190501, 20190502, 20190503, 20190506, 20190507, 20190508, 20190509, 20190510, 20190513, 20190514, 20190515, 20190516, 20190517, 20190520, 20190521, 20190522, 20190523, 20190524, 20190528, 20190529, 20190530, 20190531, 20190603, 20190604, 20190605, 20190606, 20190607, 20190610, 20190611, 20190612, 20190613, 20190614, 20190617, 20190618, 20190619, 20190620, 20190621, 20190624, 20190625, 20190626, 20190627, 20190628, 20190701, 20190702, 20190703, 20190705, 20190708, 20190709, 20190710, 20190711, 20190712, 20190715, 20190716, 20190717, 20190718, 20190719, 20190722, 20190723, 20190724, 20190725, 20190726, 20190729, 20190730, 20190731, 20190801, 20190802, 20190805, 20190806, 20190807, 20190808, 20190809, 20190812, 20190813, 20190814, 20190815, 20190816, 20190819, 20190820, 20190821, 20190822, 20190823, 20190826, 20190827, 20190828, 20190829, 20190830, 20190903, 20190904, 20190905, 20190906, 20190909, 20190910, 20190911, 20190912, 20190913, 20190916, 20190917, 20190918, 20190919, 20190920, 20190923, 20190924, 20190925, 20190926, 20190927, 20190930, 20191001, 20191002, 20191003, 20191004, 20191007, 20191008, 20191009, 20191010, 20191011, 20191014, 20191015, 20191016, 20191017, 20191018, 20191021, 20191022, 20191023, 20191024, 20191025, 20191028, 20191029, 20191030, 20191031, 20191101, 20191104, 20191105, 20191106, 20191107, 20191108, 20191111, 20191112, 20191113, 20191114, 20191115, 20191118, 20191119, 20191120], \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e57cfc00-9992-4932-8115-e243b204c137');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSTadIOfUbou"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH5xT-hWUbou",
        "outputId": "b20ad15d-c6c1-44d6-e496-a805cfa602d0"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.15, .1, .05]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values) \n",
        "  historical = Train_data(dfs[col_name], train_start=100, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"DVN\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 15ms/step - loss: 0.6715 - accuracy: 0.6130 - val_loss: 0.6294 - val_accuracy: 0.6878\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.6660 - accuracy: 0.6189 - val_loss: 0.6437 - val_accuracy: 0.6878\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.5311 - accuracy: 0.7485 - val_loss: 0.6436 - val_accuracy: 0.6898\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.4003 - accuracy: 0.8207 - val_loss: 0.5683 - val_accuracy: 0.6653\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.3705 - accuracy: 0.8337 - val_loss: 0.7807 - val_accuracy: 0.6939\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 13ms/step - loss: 0.6683 - accuracy: 0.6172 - val_loss: 0.6364 - val_accuracy: 0.6878\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.5353 - accuracy: 0.7101 - val_loss: 0.5816 - val_accuracy: 0.6796\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.3476 - accuracy: 0.8426 - val_loss: 0.6716 - val_accuracy: 0.6857\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.3364 - accuracy: 0.8574 - val_loss: 0.6748 - val_accuracy: 0.6673\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.3452 - accuracy: 0.8509 - val_loss: 0.6745 - val_accuracy: 0.7122\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.726392\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.724714\n",
            "[2]\tvalidation_0-auc:0.726828\n",
            "[3]\tvalidation_0-auc:0.727051\n",
            "[4]\tvalidation_0-auc:0.727633\n",
            "[5]\tvalidation_0-auc:0.732094\n",
            "[6]\tvalidation_0-auc:0.733364\n",
            "[7]\tvalidation_0-auc:0.733025\n",
            "[8]\tvalidation_0-auc:0.733869\n",
            "[9]\tvalidation_0-auc:0.733966\n",
            "[10]\tvalidation_0-auc:0.733985\n",
            "[11]\tvalidation_0-auc:0.733423\n",
            "[12]\tvalidation_0-auc:0.733597\n",
            "[13]\tvalidation_0-auc:0.733655\n",
            "[14]\tvalidation_0-auc:0.739978\n",
            "[15]\tvalidation_0-auc:0.740055\n",
            "[16]\tvalidation_0-auc:0.739745\n",
            "[17]\tvalidation_0-auc:0.73798\n",
            "[18]\tvalidation_0-auc:0.73734\n",
            "[19]\tvalidation_0-auc:0.736884\n",
            "[20]\tvalidation_0-auc:0.739474\n",
            "[21]\tvalidation_0-auc:0.738931\n",
            "[22]\tvalidation_0-auc:0.736749\n",
            "[23]\tvalidation_0-auc:0.737175\n",
            "[24]\tvalidation_0-auc:0.739813\n",
            "[25]\tvalidation_0-auc:0.739784\n",
            "[26]\tvalidation_0-auc:0.743469\n",
            "[27]\tvalidation_0-auc:0.747086\n",
            "[28]\tvalidation_0-auc:0.747047\n",
            "[29]\tvalidation_0-auc:0.747231\n",
            "[30]\tvalidation_0-auc:0.74696\n",
            "[31]\tvalidation_0-auc:0.747406\n",
            "[32]\tvalidation_0-auc:0.747581\n",
            "[33]\tvalidation_0-auc:0.748657\n",
            "[34]\tvalidation_0-auc:0.748114\n",
            "[35]\tvalidation_0-auc:0.747871\n",
            "[36]\tvalidation_0-auc:0.749404\n",
            "[37]\tvalidation_0-auc:0.750034\n",
            "[38]\tvalidation_0-auc:0.750015\n",
            "[39]\tvalidation_0-auc:0.750548\n",
            "[40]\tvalidation_0-auc:0.750141\n",
            "[41]\tvalidation_0-auc:0.750625\n",
            "[42]\tvalidation_0-auc:0.750936\n",
            "[43]\tvalidation_0-auc:0.750587\n",
            "[44]\tvalidation_0-auc:0.754388\n",
            "[45]\tvalidation_0-auc:0.754417\n",
            "[46]\tvalidation_0-auc:0.755561\n",
            "[47]\tvalidation_0-auc:0.755891\n",
            "[48]\tvalidation_0-auc:0.755028\n",
            "[49]\tvalidation_0-auc:0.754795\n",
            "[50]\tvalidation_0-auc:0.754213\n",
            "[51]\tvalidation_0-auc:0.754049\n",
            "[52]\tvalidation_0-auc:0.753826\n",
            "[53]\tvalidation_0-auc:0.753874\n",
            "[54]\tvalidation_0-auc:0.75368\n",
            "[55]\tvalidation_0-auc:0.753399\n",
            "[56]\tvalidation_0-auc:0.753981\n",
            "[57]\tvalidation_0-auc:0.753302\n",
            "[58]\tvalidation_0-auc:0.753535\n",
            "[59]\tvalidation_0-auc:0.753573\n",
            "[60]\tvalidation_0-auc:0.753864\n",
            "[61]\tvalidation_0-auc:0.753506\n",
            "[62]\tvalidation_0-auc:0.756454\n",
            "[63]\tvalidation_0-auc:0.756803\n",
            "[64]\tvalidation_0-auc:0.759625\n",
            "[65]\tvalidation_0-auc:0.75913\n",
            "[66]\tvalidation_0-auc:0.758684\n",
            "[67]\tvalidation_0-auc:0.760546\n",
            "[68]\tvalidation_0-auc:0.760623\n",
            "[69]\tvalidation_0-auc:0.760429\n",
            "[70]\tvalidation_0-auc:0.760468\n",
            "[71]\tvalidation_0-auc:0.760352\n",
            "[72]\tvalidation_0-auc:0.759722\n",
            "[73]\tvalidation_0-auc:0.760526\n",
            "[74]\tvalidation_0-auc:0.760963\n",
            "[75]\tvalidation_0-auc:0.760875\n",
            "[76]\tvalidation_0-auc:0.760769\n",
            "[77]\tvalidation_0-auc:0.760536\n",
            "[78]\tvalidation_0-auc:0.762379\n",
            "[79]\tvalidation_0-auc:0.762204\n",
            "[80]\tvalidation_0-auc:0.762379\n",
            "[81]\tvalidation_0-auc:0.762611\n",
            "[82]\tvalidation_0-auc:0.761952\n",
            "[83]\tvalidation_0-auc:0.762165\n",
            "[84]\tvalidation_0-auc:0.762359\n",
            "[85]\tvalidation_0-auc:0.762417\n",
            "[86]\tvalidation_0-auc:0.762068\n",
            "[87]\tvalidation_0-auc:0.762204\n",
            "[88]\tvalidation_0-auc:0.761816\n",
            "[89]\tvalidation_0-auc:0.761932\n",
            "[90]\tvalidation_0-auc:0.761971\n",
            "[91]\tvalidation_0-auc:0.761661\n",
            "[92]\tvalidation_0-auc:0.761195\n",
            "[93]\tvalidation_0-auc:0.761409\n",
            "[94]\tvalidation_0-auc:0.761292\n",
            "[95]\tvalidation_0-auc:0.760284\n",
            "[96]\tvalidation_0-auc:0.760245\n",
            "[97]\tvalidation_0-auc:0.76009\n",
            "[98]\tvalidation_0-auc:0.760517\n",
            "[99]\tvalidation_0-auc:0.760206\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 15ms/step - loss: 0.5997 - accuracy: 0.6626 - val_loss: 0.5117 - val_accuracy: 0.7287\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.4555 - accuracy: 0.7924 - val_loss: 0.6006 - val_accuracy: 0.7024\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.4272 - accuracy: 0.8111 - val_loss: 0.5847 - val_accuracy: 0.7133\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3887 - accuracy: 0.8286 - val_loss: 0.5382 - val_accuracy: 0.7046\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3849 - accuracy: 0.8401 - val_loss: 0.4740 - val_accuracy: 0.7046\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 13ms/step - loss: 0.5919 - accuracy: 0.6717 - val_loss: 0.4909 - val_accuracy: 0.7024\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3767 - accuracy: 0.8292 - val_loss: 0.5375 - val_accuracy: 0.6980\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3641 - accuracy: 0.8485 - val_loss: 0.5332 - val_accuracy: 0.6937\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3423 - accuracy: 0.8485 - val_loss: 0.6968 - val_accuracy: 0.6586\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3392 - accuracy: 0.8521 - val_loss: 0.5688 - val_accuracy: 0.6761\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.760893\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.755861\n",
            "[2]\tvalidation_0-auc:0.753895\n",
            "[3]\tvalidation_0-auc:0.753276\n",
            "[4]\tvalidation_0-auc:0.753202\n",
            "[5]\tvalidation_0-auc:0.752745\n",
            "[6]\tvalidation_0-auc:0.748578\n",
            "[7]\tvalidation_0-auc:0.745462\n",
            "[8]\tvalidation_0-auc:0.747169\n",
            "[9]\tvalidation_0-auc:0.749382\n",
            "[10]\tvalidation_0-auc:0.750915\n",
            "[11]\tvalidation_0-auc:0.752819\n",
            "[12]\tvalidation_0-auc:0.753734\n",
            "[13]\tvalidation_0-auc:0.754216\n",
            "[14]\tvalidation_0-auc:0.75544\n",
            "[15]\tvalidation_0-auc:0.75497\n",
            "[16]\tvalidation_0-auc:0.754525\n",
            "[17]\tvalidation_0-auc:0.752547\n",
            "[18]\tvalidation_0-auc:0.754031\n",
            "[19]\tvalidation_0-auc:0.754525\n",
            "[20]\tvalidation_0-auc:0.754748\n",
            "[21]\tvalidation_0-auc:0.754871\n",
            "[22]\tvalidation_0-auc:0.755218\n",
            "[23]\tvalidation_0-auc:0.757109\n",
            "[24]\tvalidation_0-auc:0.757653\n",
            "[25]\tvalidation_0-auc:0.759137\n",
            "[26]\tvalidation_0-auc:0.747577\n",
            "[27]\tvalidation_0-auc:0.7477\n",
            "[28]\tvalidation_0-auc:0.740455\n",
            "[29]\tvalidation_0-auc:0.741444\n",
            "[30]\tvalidation_0-auc:0.741865\n",
            "[31]\tvalidation_0-auc:0.762747\n",
            "[32]\tvalidation_0-auc:0.761758\n",
            "[33]\tvalidation_0-auc:0.751879\n",
            "[34]\tvalidation_0-auc:0.752572\n",
            "[35]\tvalidation_0-auc:0.760163\n",
            "[36]\tvalidation_0-auc:0.760262\n",
            "[37]\tvalidation_0-auc:0.758556\n",
            "[38]\tvalidation_0-auc:0.75727\n",
            "[39]\tvalidation_0-auc:0.757864\n",
            "[40]\tvalidation_0-auc:0.757443\n",
            "[41]\tvalidation_0-auc:0.759199\n",
            "[42]\tvalidation_0-auc:0.758408\n",
            "[43]\tvalidation_0-auc:0.757369\n",
            "[44]\tvalidation_0-auc:0.756751\n",
            "[45]\tvalidation_0-auc:0.757468\n",
            "[46]\tvalidation_0-auc:0.757641\n",
            "[47]\tvalidation_0-auc:0.759026\n",
            "[48]\tvalidation_0-auc:0.759471\n",
            "[49]\tvalidation_0-auc:0.759891\n",
            "[50]\tvalidation_0-auc:0.760831\n",
            "[51]\tvalidation_0-auc:0.758754\n",
            "[52]\tvalidation_0-auc:0.759125\n",
            "[53]\tvalidation_0-auc:0.759545\n",
            "[54]\tvalidation_0-auc:0.775087\n",
            "[55]\tvalidation_0-auc:0.774345\n",
            "[56]\tvalidation_0-auc:0.768855\n",
            "[57]\tvalidation_0-auc:0.768534\n",
            "[58]\tvalidation_0-auc:0.769844\n",
            "[59]\tvalidation_0-auc:0.770734\n",
            "[60]\tvalidation_0-auc:0.777225\n",
            "[61]\tvalidation_0-auc:0.775408\n",
            "[62]\tvalidation_0-auc:0.776298\n",
            "[63]\tvalidation_0-auc:0.776867\n",
            "[64]\tvalidation_0-auc:0.775915\n",
            "[65]\tvalidation_0-auc:0.77003\n",
            "[66]\tvalidation_0-auc:0.770821\n",
            "[67]\tvalidation_0-auc:0.771217\n",
            "[68]\tvalidation_0-auc:0.771118\n",
            "[69]\tvalidation_0-auc:0.773863\n",
            "[70]\tvalidation_0-auc:0.774011\n",
            "[71]\tvalidation_0-auc:0.77458\n",
            "[72]\tvalidation_0-auc:0.77406\n",
            "[73]\tvalidation_0-auc:0.773961\n",
            "[74]\tvalidation_0-auc:0.773195\n",
            "[75]\tvalidation_0-auc:0.773343\n",
            "[76]\tvalidation_0-auc:0.773541\n",
            "[77]\tvalidation_0-auc:0.773788\n",
            "[78]\tvalidation_0-auc:0.774617\n",
            "[79]\tvalidation_0-auc:0.773034\n",
            "[80]\tvalidation_0-auc:0.773133\n",
            "[81]\tvalidation_0-auc:0.773924\n",
            "[82]\tvalidation_0-auc:0.773207\n",
            "[83]\tvalidation_0-auc:0.773801\n",
            "[84]\tvalidation_0-auc:0.774295\n",
            "[85]\tvalidation_0-auc:0.774097\n",
            "[86]\tvalidation_0-auc:0.77432\n",
            "[87]\tvalidation_0-auc:0.774716\n",
            "[88]\tvalidation_0-auc:0.774518\n",
            "[89]\tvalidation_0-auc:0.774345\n",
            "[90]\tvalidation_0-auc:0.773924\n",
            "[91]\tvalidation_0-auc:0.773306\n",
            "[92]\tvalidation_0-auc:0.773356\n",
            "[93]\tvalidation_0-auc:0.77338\n",
            "[94]\tvalidation_0-auc:0.773529\n",
            "[95]\tvalidation_0-auc:0.773059\n",
            "[96]\tvalidation_0-auc:0.773232\n",
            "[97]\tvalidation_0-auc:0.77338\n",
            "[98]\tvalidation_0-auc:0.772713\n",
            "[99]\tvalidation_0-auc:0.773108\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.6938775510204082 |  0.5114503816793893 | 0.43790849673202614 | 0.47183098591549294 |\n",
            "|      GRU 0.15     | 0.7122448979591837 |  0.5535714285714286 | 0.40522875816993464 |  0.4679245283018868 |\n",
            "|    XGBoost 0.15   | 0.6857142857142857 |  0.4967741935483871 |  0.5032679738562091 |         0.5         |\n",
            "|    Logreg 0.15    | 0.7163265306122449 |  0.5660377358490566 | 0.39215686274509803 |  0.4633204633204633 |\n",
            "|      SVM 0.15     | 0.6836734693877551 | 0.49333333333333335 | 0.48366013071895425 |  0.4884488448844884 |\n",
            "|   LSTM beta 0.15  | 0.7045951859956237 |  0.4418604651162791 |        0.475        |  0.4578313253012048 |\n",
            "|   GRU beta 0.15   | 0.6761487964989059 | 0.40540540540540543 |         0.5         |  0.4477611940298507 |\n",
            "| XGBoost beta 0.15 | 0.6849015317286652 | 0.43103448275862066 |        0.625        |  0.5102040816326531 |\n",
            "|  logreg beta 0.15 | 0.7024070021881839 | 0.43846153846153846 |        0.475        |        0.456        |\n",
            "|   svm beta 0.15   | 0.6783369803063457 |  0.4105960264900662 |  0.5166666666666667 |  0.4575645756457565 |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 15ms/step - loss: 0.6693 - accuracy: 0.6148 - val_loss: 0.6485 - val_accuracy: 0.6878\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.6688 - accuracy: 0.6124 - val_loss: 0.6277 - val_accuracy: 0.6878\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.4790 - accuracy: 0.7580 - val_loss: 0.6285 - val_accuracy: 0.6939\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.4001 - accuracy: 0.8201 - val_loss: 0.5700 - val_accuracy: 0.6449\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.3633 - accuracy: 0.8373 - val_loss: 0.5868 - val_accuracy: 0.6816\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 14ms/step - loss: 0.6736 - accuracy: 0.6041 - val_loss: 0.6261 - val_accuracy: 0.6878\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.5088 - accuracy: 0.7379 - val_loss: 0.6013 - val_accuracy: 0.6571\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.3705 - accuracy: 0.8408 - val_loss: 0.6889 - val_accuracy: 0.6857\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.3279 - accuracy: 0.8568 - val_loss: 0.6668 - val_accuracy: 0.6857\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.3346 - accuracy: 0.8556 - val_loss: 0.6822 - val_accuracy: 0.6816\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.726392\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.725219\n",
            "[2]\tvalidation_0-auc:0.724365\n",
            "[3]\tvalidation_0-auc:0.725888\n",
            "[4]\tvalidation_0-auc:0.726421\n",
            "[5]\tvalidation_0-auc:0.728157\n",
            "[6]\tvalidation_0-auc:0.733151\n",
            "[7]\tvalidation_0-auc:0.732841\n",
            "[8]\tvalidation_0-auc:0.732104\n",
            "[9]\tvalidation_0-auc:0.733229\n",
            "[10]\tvalidation_0-auc:0.733403\n",
            "[11]\tvalidation_0-auc:0.732336\n",
            "[12]\tvalidation_0-auc:0.732773\n",
            "[13]\tvalidation_0-auc:0.733859\n",
            "[14]\tvalidation_0-auc:0.734101\n",
            "[15]\tvalidation_0-auc:0.739648\n",
            "[16]\tvalidation_0-auc:0.73928\n",
            "[17]\tvalidation_0-auc:0.738727\n",
            "[18]\tvalidation_0-auc:0.738727\n",
            "[19]\tvalidation_0-auc:0.737777\n",
            "[20]\tvalidation_0-auc:0.736817\n",
            "[21]\tvalidation_0-auc:0.739522\n",
            "[22]\tvalidation_0-auc:0.739755\n",
            "[23]\tvalidation_0-auc:0.739532\n",
            "[24]\tvalidation_0-auc:0.741879\n",
            "[25]\tvalidation_0-auc:0.741171\n",
            "[26]\tvalidation_0-auc:0.74502\n",
            "[27]\tvalidation_0-auc:0.745137\n",
            "[28]\tvalidation_0-auc:0.746378\n",
            "[29]\tvalidation_0-auc:0.750703\n",
            "[30]\tvalidation_0-auc:0.751149\n",
            "[31]\tvalidation_0-auc:0.750286\n",
            "[32]\tvalidation_0-auc:0.750141\n",
            "[33]\tvalidation_0-auc:0.749966\n",
            "[34]\tvalidation_0-auc:0.74985\n",
            "[35]\tvalidation_0-auc:0.751159\n",
            "[36]\tvalidation_0-auc:0.750916\n",
            "[37]\tvalidation_0-auc:0.750325\n",
            "[38]\tvalidation_0-auc:0.749927\n",
            "[39]\tvalidation_0-auc:0.749888\n",
            "[40]\tvalidation_0-auc:0.749908\n",
            "[41]\tvalidation_0-auc:0.75176\n",
            "[42]\tvalidation_0-auc:0.752827\n",
            "[43]\tvalidation_0-auc:0.75688\n",
            "[44]\tvalidation_0-auc:0.756541\n",
            "[45]\tvalidation_0-auc:0.755823\n",
            "[46]\tvalidation_0-auc:0.755523\n",
            "[47]\tvalidation_0-auc:0.755581\n",
            "[48]\tvalidation_0-auc:0.755154\n",
            "[49]\tvalidation_0-auc:0.755629\n",
            "[50]\tvalidation_0-auc:0.755852\n",
            "[51]\tvalidation_0-auc:0.75594\n",
            "[52]\tvalidation_0-auc:0.755125\n",
            "[53]\tvalidation_0-auc:0.755523\n",
            "[54]\tvalidation_0-auc:0.755755\n",
            "[55]\tvalidation_0-auc:0.755697\n",
            "[56]\tvalidation_0-auc:0.756143\n",
            "[57]\tvalidation_0-auc:0.756521\n",
            "[58]\tvalidation_0-auc:0.756463\n",
            "[59]\tvalidation_0-auc:0.756318\n",
            "[60]\tvalidation_0-auc:0.75624\n",
            "[61]\tvalidation_0-auc:0.75561\n",
            "[62]\tvalidation_0-auc:0.75594\n",
            "[63]\tvalidation_0-auc:0.755183\n",
            "[64]\tvalidation_0-auc:0.755144\n",
            "[65]\tvalidation_0-auc:0.754611\n",
            "[66]\tvalidation_0-auc:0.754495\n",
            "[67]\tvalidation_0-auc:0.754863\n",
            "[68]\tvalidation_0-auc:0.758703\n",
            "[69]\tvalidation_0-auc:0.760284\n",
            "[70]\tvalidation_0-auc:0.760517\n",
            "[71]\tvalidation_0-auc:0.76073\n",
            "[72]\tvalidation_0-auc:0.760943\n",
            "[73]\tvalidation_0-auc:0.761932\n",
            "[74]\tvalidation_0-auc:0.761894\n",
            "[75]\tvalidation_0-auc:0.76201\n",
            "[76]\tvalidation_0-auc:0.761884\n",
            "[77]\tvalidation_0-auc:0.761962\n",
            "[78]\tvalidation_0-auc:0.76201\n",
            "[79]\tvalidation_0-auc:0.761991\n",
            "[80]\tvalidation_0-auc:0.761467\n",
            "[81]\tvalidation_0-auc:0.761545\n",
            "[82]\tvalidation_0-auc:0.761079\n",
            "[83]\tvalidation_0-auc:0.761079\n",
            "[84]\tvalidation_0-auc:0.761302\n",
            "[85]\tvalidation_0-auc:0.759392\n",
            "[86]\tvalidation_0-auc:0.759392\n",
            "[87]\tvalidation_0-auc:0.759838\n",
            "[88]\tvalidation_0-auc:0.760517\n",
            "[89]\tvalidation_0-auc:0.760711\n",
            "[90]\tvalidation_0-auc:0.762718\n",
            "[91]\tvalidation_0-auc:0.763106\n",
            "[92]\tvalidation_0-auc:0.763164\n",
            "[93]\tvalidation_0-auc:0.763174\n",
            "[94]\tvalidation_0-auc:0.763154\n",
            "[95]\tvalidation_0-auc:0.762825\n",
            "[96]\tvalidation_0-auc:0.762495\n",
            "[97]\tvalidation_0-auc:0.762223\n",
            "[98]\tvalidation_0-auc:0.761971\n",
            "[99]\tvalidation_0-auc:0.762301\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 15ms/step - loss: 0.6232 - accuracy: 0.6554 - val_loss: 0.6414 - val_accuracy: 0.7133\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.4393 - accuracy: 0.7978 - val_loss: 0.5299 - val_accuracy: 0.7265\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.4084 - accuracy: 0.8123 - val_loss: 0.6498 - val_accuracy: 0.6827\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3855 - accuracy: 0.8298 - val_loss: 0.7322 - val_accuracy: 0.6827\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3823 - accuracy: 0.8352 - val_loss: 0.5437 - val_accuracy: 0.6674\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 13ms/step - loss: 0.5516 - accuracy: 0.6771 - val_loss: 0.5498 - val_accuracy: 0.6674\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3679 - accuracy: 0.8371 - val_loss: 0.5717 - val_accuracy: 0.7024\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3645 - accuracy: 0.8413 - val_loss: 0.5221 - val_accuracy: 0.7112\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3599 - accuracy: 0.8413 - val_loss: 0.5765 - val_accuracy: 0.7002\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3506 - accuracy: 0.8437 - val_loss: 0.5974 - val_accuracy: 0.7024\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.760893\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.755861\n",
            "[2]\tvalidation_0-auc:0.753895\n",
            "[3]\tvalidation_0-auc:0.753276\n",
            "[4]\tvalidation_0-auc:0.753202\n",
            "[5]\tvalidation_0-auc:0.752745\n",
            "[6]\tvalidation_0-auc:0.748578\n",
            "[7]\tvalidation_0-auc:0.745462\n",
            "[8]\tvalidation_0-auc:0.747169\n",
            "[9]\tvalidation_0-auc:0.749382\n",
            "[10]\tvalidation_0-auc:0.750915\n",
            "[11]\tvalidation_0-auc:0.752819\n",
            "[12]\tvalidation_0-auc:0.753734\n",
            "[13]\tvalidation_0-auc:0.754216\n",
            "[14]\tvalidation_0-auc:0.75544\n",
            "[15]\tvalidation_0-auc:0.75497\n",
            "[16]\tvalidation_0-auc:0.754525\n",
            "[17]\tvalidation_0-auc:0.752547\n",
            "[18]\tvalidation_0-auc:0.754031\n",
            "[19]\tvalidation_0-auc:0.754525\n",
            "[20]\tvalidation_0-auc:0.754748\n",
            "[21]\tvalidation_0-auc:0.754871\n",
            "[22]\tvalidation_0-auc:0.755218\n",
            "[23]\tvalidation_0-auc:0.757109\n",
            "[24]\tvalidation_0-auc:0.757653\n",
            "[25]\tvalidation_0-auc:0.759137\n",
            "[26]\tvalidation_0-auc:0.747577\n",
            "[27]\tvalidation_0-auc:0.7477\n",
            "[28]\tvalidation_0-auc:0.740455\n",
            "[29]\tvalidation_0-auc:0.741444\n",
            "[30]\tvalidation_0-auc:0.741865\n",
            "[31]\tvalidation_0-auc:0.762747\n",
            "[32]\tvalidation_0-auc:0.761758\n",
            "[33]\tvalidation_0-auc:0.751879\n",
            "[34]\tvalidation_0-auc:0.752572\n",
            "[35]\tvalidation_0-auc:0.760163\n",
            "[36]\tvalidation_0-auc:0.760262\n",
            "[37]\tvalidation_0-auc:0.758556\n",
            "[38]\tvalidation_0-auc:0.75727\n",
            "[39]\tvalidation_0-auc:0.757864\n",
            "[40]\tvalidation_0-auc:0.757443\n",
            "[41]\tvalidation_0-auc:0.759199\n",
            "[42]\tvalidation_0-auc:0.758408\n",
            "[43]\tvalidation_0-auc:0.757369\n",
            "[44]\tvalidation_0-auc:0.756751\n",
            "[45]\tvalidation_0-auc:0.757468\n",
            "[46]\tvalidation_0-auc:0.757641\n",
            "[47]\tvalidation_0-auc:0.759026\n",
            "[48]\tvalidation_0-auc:0.759471\n",
            "[49]\tvalidation_0-auc:0.759891\n",
            "[50]\tvalidation_0-auc:0.760831\n",
            "[51]\tvalidation_0-auc:0.758754\n",
            "[52]\tvalidation_0-auc:0.759125\n",
            "[53]\tvalidation_0-auc:0.759545\n",
            "[54]\tvalidation_0-auc:0.775087\n",
            "[55]\tvalidation_0-auc:0.774345\n",
            "[56]\tvalidation_0-auc:0.768855\n",
            "[57]\tvalidation_0-auc:0.768534\n",
            "[58]\tvalidation_0-auc:0.769844\n",
            "[59]\tvalidation_0-auc:0.770734\n",
            "[60]\tvalidation_0-auc:0.777225\n",
            "[61]\tvalidation_0-auc:0.775408\n",
            "[62]\tvalidation_0-auc:0.776298\n",
            "[63]\tvalidation_0-auc:0.776867\n",
            "[64]\tvalidation_0-auc:0.775915\n",
            "[65]\tvalidation_0-auc:0.77003\n",
            "[66]\tvalidation_0-auc:0.770821\n",
            "[67]\tvalidation_0-auc:0.771217\n",
            "[68]\tvalidation_0-auc:0.771118\n",
            "[69]\tvalidation_0-auc:0.773863\n",
            "[70]\tvalidation_0-auc:0.774011\n",
            "[71]\tvalidation_0-auc:0.77458\n",
            "[72]\tvalidation_0-auc:0.77406\n",
            "[73]\tvalidation_0-auc:0.773961\n",
            "[74]\tvalidation_0-auc:0.773195\n",
            "[75]\tvalidation_0-auc:0.773343\n",
            "[76]\tvalidation_0-auc:0.773541\n",
            "[77]\tvalidation_0-auc:0.773788\n",
            "[78]\tvalidation_0-auc:0.774617\n",
            "[79]\tvalidation_0-auc:0.773034\n",
            "[80]\tvalidation_0-auc:0.773133\n",
            "[81]\tvalidation_0-auc:0.773924\n",
            "[82]\tvalidation_0-auc:0.773207\n",
            "[83]\tvalidation_0-auc:0.773801\n",
            "[84]\tvalidation_0-auc:0.774295\n",
            "[85]\tvalidation_0-auc:0.774097\n",
            "[86]\tvalidation_0-auc:0.77432\n",
            "[87]\tvalidation_0-auc:0.774716\n",
            "[88]\tvalidation_0-auc:0.774518\n",
            "[89]\tvalidation_0-auc:0.774345\n",
            "[90]\tvalidation_0-auc:0.773924\n",
            "[91]\tvalidation_0-auc:0.773306\n",
            "[92]\tvalidation_0-auc:0.773356\n",
            "[93]\tvalidation_0-auc:0.77338\n",
            "[94]\tvalidation_0-auc:0.773529\n",
            "[95]\tvalidation_0-auc:0.773059\n",
            "[96]\tvalidation_0-auc:0.773232\n",
            "[97]\tvalidation_0-auc:0.77338\n",
            "[98]\tvalidation_0-auc:0.772713\n",
            "[99]\tvalidation_0-auc:0.773108\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.6816326530612244 | 0.49377593360995853 |  0.7777777777777778 |  0.6040609137055838 |\n",
            "|     GRU 0.1      | 0.6816326530612244 | 0.49032258064516127 | 0.49673202614379086 |  0.4935064935064935 |\n",
            "|   XGBoost 0.1    | 0.689795918367347  |  0.5032679738562091 |  0.5032679738562091 |  0.5032679738562091 |\n",
            "|    Logreg 0.1    | 0.7142857142857143 |  0.5585585585585585 | 0.40522875816993464 | 0.46969696969696967 |\n",
            "|     SVM 0.1      | 0.6857142857142857 | 0.49673202614379086 | 0.49673202614379086 | 0.49673202614379086 |\n",
            "|  LSTM beta 0.1   | 0.6673960612691466 |  0.3904109589041096 |        0.475        | 0.42857142857142855 |\n",
            "|   GRU beta 0.1   | 0.7024070021881839 | 0.43846153846153846 |        0.475        |        0.456        |\n",
            "| XGBoost beta 0.1 | 0.6849015317286652 | 0.43103448275862066 |        0.625        |  0.5102040816326531 |\n",
            "| logreg beta 0.1  | 0.7024070021881839 | 0.43846153846153846 |        0.475        |        0.456        |\n",
            "|   svm beta 0.1   | 0.6783369803063457 |  0.4105960264900662 |  0.5166666666666667 |  0.4575645756457565 |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "Threshhold =  0.05\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 15ms/step - loss: 0.6864 - accuracy: 0.5663 - val_loss: 0.6413 - val_accuracy: 0.7265\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.5909 - accuracy: 0.6651 - val_loss: 0.3071 - val_accuracy: 0.8816\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.3190 - accuracy: 0.8692 - val_loss: 0.3540 - val_accuracy: 0.8245\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.2935 - accuracy: 0.8799 - val_loss: 0.2627 - val_accuracy: 0.8694\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.2905 - accuracy: 0.8846 - val_loss: 0.2776 - val_accuracy: 0.8653\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 13ms/step - loss: 0.6794 - accuracy: 0.5746 - val_loss: 0.6361 - val_accuracy: 0.8000\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.3729 - accuracy: 0.8396 - val_loss: 0.2533 - val_accuracy: 0.8776\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.2872 - accuracy: 0.8888 - val_loss: 0.2514 - val_accuracy: 0.8776\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.2707 - accuracy: 0.8964 - val_loss: 0.2475 - val_accuracy: 0.8816\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.2635 - accuracy: 0.8905 - val_loss: 0.2607 - val_accuracy: 0.8857\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.938527\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.938118\n",
            "[2]\tvalidation_0-auc:0.956471\n",
            "[3]\tvalidation_0-auc:0.957477\n",
            "[4]\tvalidation_0-auc:0.95667\n",
            "[5]\tvalidation_0-auc:0.956555\n",
            "[6]\tvalidation_0-auc:0.956282\n",
            "[7]\tvalidation_0-auc:0.958169\n",
            "[8]\tvalidation_0-auc:0.958798\n",
            "[9]\tvalidation_0-auc:0.959186\n",
            "[10]\tvalidation_0-auc:0.958746\n",
            "[11]\tvalidation_0-auc:0.960328\n",
            "[12]\tvalidation_0-auc:0.960265\n",
            "[13]\tvalidation_0-auc:0.961785\n",
            "[14]\tvalidation_0-auc:0.961827\n",
            "[15]\tvalidation_0-auc:0.962058\n",
            "[16]\tvalidation_0-auc:0.962121\n",
            "[17]\tvalidation_0-auc:0.963714\n",
            "[18]\tvalidation_0-auc:0.963609\n",
            "[19]\tvalidation_0-auc:0.964343\n",
            "[20]\tvalidation_0-auc:0.963651\n",
            "[21]\tvalidation_0-auc:0.96384\n",
            "[22]\tvalidation_0-auc:0.963284\n",
            "[23]\tvalidation_0-auc:0.963536\n",
            "[24]\tvalidation_0-auc:0.964175\n",
            "[25]\tvalidation_0-auc:0.964468\n",
            "[26]\tvalidation_0-auc:0.964636\n",
            "[27]\tvalidation_0-auc:0.964825\n",
            "[28]\tvalidation_0-auc:0.964877\n",
            "[29]\tvalidation_0-auc:0.964982\n",
            "[30]\tvalidation_0-auc:0.965108\n",
            "[31]\tvalidation_0-auc:0.965506\n",
            "[32]\tvalidation_0-auc:0.96516\n",
            "[33]\tvalidation_0-auc:0.96473\n",
            "[34]\tvalidation_0-auc:0.964709\n",
            "[35]\tvalidation_0-auc:0.96473\n",
            "[36]\tvalidation_0-auc:0.964877\n",
            "[37]\tvalidation_0-auc:0.964206\n",
            "[38]\tvalidation_0-auc:0.964856\n",
            "[39]\tvalidation_0-auc:0.964594\n",
            "[40]\tvalidation_0-auc:0.964888\n",
            "[41]\tvalidation_0-auc:0.964867\n",
            "[42]\tvalidation_0-auc:0.965192\n",
            "[43]\tvalidation_0-auc:0.964856\n",
            "[44]\tvalidation_0-auc:0.964793\n",
            "[45]\tvalidation_0-auc:0.964709\n",
            "[46]\tvalidation_0-auc:0.964542\n",
            "[47]\tvalidation_0-auc:0.964573\n",
            "[48]\tvalidation_0-auc:0.964951\n",
            "[49]\tvalidation_0-auc:0.965286\n",
            "[50]\tvalidation_0-auc:0.965726\n",
            "[51]\tvalidation_0-auc:0.965726\n",
            "[52]\tvalidation_0-auc:0.965496\n",
            "[53]\tvalidation_0-auc:0.9656\n",
            "[54]\tvalidation_0-auc:0.965642\n",
            "[55]\tvalidation_0-auc:0.965621\n",
            "[56]\tvalidation_0-auc:0.965716\n",
            "[57]\tvalidation_0-auc:0.965779\n",
            "[58]\tvalidation_0-auc:0.965275\n",
            "[59]\tvalidation_0-auc:0.965192\n",
            "[60]\tvalidation_0-auc:0.965317\n",
            "[61]\tvalidation_0-auc:0.965317\n",
            "[62]\tvalidation_0-auc:0.965569\n",
            "[63]\tvalidation_0-auc:0.96538\n",
            "[64]\tvalidation_0-auc:0.965254\n",
            "[65]\tvalidation_0-auc:0.965234\n",
            "[66]\tvalidation_0-auc:0.965234\n",
            "[67]\tvalidation_0-auc:0.965296\n",
            "[68]\tvalidation_0-auc:0.965171\n",
            "[69]\tvalidation_0-auc:0.965171\n",
            "[70]\tvalidation_0-auc:0.965244\n",
            "[71]\tvalidation_0-auc:0.965223\n",
            "[72]\tvalidation_0-auc:0.96516\n",
            "[73]\tvalidation_0-auc:0.965055\n",
            "[74]\tvalidation_0-auc:0.96493\n",
            "[75]\tvalidation_0-auc:0.965244\n",
            "[76]\tvalidation_0-auc:0.96537\n",
            "[77]\tvalidation_0-auc:0.965265\n",
            "[78]\tvalidation_0-auc:0.965286\n",
            "[79]\tvalidation_0-auc:0.965118\n",
            "[80]\tvalidation_0-auc:0.965244\n",
            "[81]\tvalidation_0-auc:0.965275\n",
            "[82]\tvalidation_0-auc:0.965234\n",
            "[83]\tvalidation_0-auc:0.965139\n",
            "[84]\tvalidation_0-auc:0.96516\n",
            "[85]\tvalidation_0-auc:0.965192\n",
            "[86]\tvalidation_0-auc:0.965254\n",
            "[87]\tvalidation_0-auc:0.965317\n",
            "[88]\tvalidation_0-auc:0.965234\n",
            "[89]\tvalidation_0-auc:0.965254\n",
            "[90]\tvalidation_0-auc:0.965108\n",
            "[91]\tvalidation_0-auc:0.965108\n",
            "[92]\tvalidation_0-auc:0.964647\n",
            "[93]\tvalidation_0-auc:0.964751\n",
            "[94]\tvalidation_0-auc:0.96494\n",
            "[95]\tvalidation_0-auc:0.964961\n",
            "[96]\tvalidation_0-auc:0.96494\n",
            "[97]\tvalidation_0-auc:0.965024\n",
            "[98]\tvalidation_0-auc:0.965087\n",
            "[99]\tvalidation_0-auc:0.964605\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 15ms/step - loss: 0.6343 - accuracy: 0.6258 - val_loss: 0.4378 - val_accuracy: 0.8337\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.4102 - accuracy: 0.8286 - val_loss: 0.3527 - val_accuracy: 0.8228\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 12ms/step - loss: 0.3488 - accuracy: 0.8600 - val_loss: 0.3334 - val_accuracy: 0.8709\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3495 - accuracy: 0.8660 - val_loss: 0.3067 - val_accuracy: 0.8753\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3139 - accuracy: 0.8745 - val_loss: 0.2921 - val_accuracy: 0.8490\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 13ms/step - loss: 0.5327 - accuracy: 0.7055 - val_loss: 0.3189 - val_accuracy: 0.8359\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3314 - accuracy: 0.8582 - val_loss: 0.2984 - val_accuracy: 0.8753\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3023 - accuracy: 0.8781 - val_loss: 0.2581 - val_accuracy: 0.8753\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2910 - accuracy: 0.8787 - val_loss: 0.2654 - val_accuracy: 0.8687\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2774 - accuracy: 0.8781 - val_loss: 0.2768 - val_accuracy: 0.8600\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.939132\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.939675\n",
            "[2]\tvalidation_0-auc:0.939675\n",
            "[3]\tvalidation_0-auc:0.954612\n",
            "[4]\tvalidation_0-auc:0.943325\n",
            "[5]\tvalidation_0-auc:0.954542\n",
            "[6]\tvalidation_0-auc:0.956622\n",
            "[7]\tvalidation_0-auc:0.954773\n",
            "[8]\tvalidation_0-auc:0.953422\n",
            "[9]\tvalidation_0-auc:0.955478\n",
            "[10]\tvalidation_0-auc:0.954669\n",
            "[11]\tvalidation_0-auc:0.951643\n",
            "[12]\tvalidation_0-auc:0.952093\n",
            "[13]\tvalidation_0-auc:0.952082\n",
            "[14]\tvalidation_0-auc:0.950314\n",
            "[15]\tvalidation_0-auc:0.950199\n",
            "[16]\tvalidation_0-auc:0.950522\n",
            "[17]\tvalidation_0-auc:0.950453\n",
            "[18]\tvalidation_0-auc:0.950753\n",
            "[19]\tvalidation_0-auc:0.951527\n",
            "[20]\tvalidation_0-auc:0.951989\n",
            "[21]\tvalidation_0-auc:0.951342\n",
            "[22]\tvalidation_0-auc:0.95021\n",
            "[23]\tvalidation_0-auc:0.949771\n",
            "[24]\tvalidation_0-auc:0.948963\n",
            "[25]\tvalidation_0-auc:0.948824\n",
            "[26]\tvalidation_0-auc:0.949309\n",
            "[27]\tvalidation_0-auc:0.949517\n",
            "[28]\tvalidation_0-auc:0.94991\n",
            "[29]\tvalidation_0-auc:0.949217\n",
            "[30]\tvalidation_0-auc:0.94887\n",
            "[31]\tvalidation_0-auc:0.948131\n",
            "[32]\tvalidation_0-auc:0.948293\n",
            "[33]\tvalidation_0-auc:0.948662\n",
            "[34]\tvalidation_0-auc:0.947392\n",
            "[35]\tvalidation_0-auc:0.947357\n",
            "[36]\tvalidation_0-auc:0.946964\n",
            "[37]\tvalidation_0-auc:0.945578\n",
            "[38]\tvalidation_0-auc:0.948466\n",
            "[39]\tvalidation_0-auc:0.949725\n",
            "[40]\tvalidation_0-auc:0.949448\n",
            "[41]\tvalidation_0-auc:0.949633\n",
            "[42]\tvalidation_0-auc:0.949563\n",
            "[43]\tvalidation_0-auc:0.949586\n",
            "[44]\tvalidation_0-auc:0.949702\n",
            "[45]\tvalidation_0-auc:0.94991\n",
            "[46]\tvalidation_0-auc:0.950049\n",
            "[47]\tvalidation_0-auc:0.950487\n",
            "[48]\tvalidation_0-auc:0.951365\n",
            "[49]\tvalidation_0-auc:0.951573\n",
            "[50]\tvalidation_0-auc:0.95125\n",
            "[51]\tvalidation_0-auc:0.950926\n",
            "[52]\tvalidation_0-auc:0.951262\n",
            "[53]\tvalidation_0-auc:0.950776\n",
            "[54]\tvalidation_0-auc:0.947819\n",
            "[55]\tvalidation_0-auc:0.946849\n",
            "[56]\tvalidation_0-auc:0.946733\n",
            "Stopping. Best iteration:\n",
            "[6]\tvalidation_0-auc:0.956622\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.05     | 0.8653061224489796 | 0.7048192771084337 | 0.8731343283582089 |        0.78        |\n",
            "|      GRU 0.05     | 0.8857142857142857 | 0.782608695652174  | 0.8059701492537313 | 0.7941176470588236 |\n",
            "|    XGBoost 0.05   | 0.889795918367347  | 0.7409638554216867 | 0.917910447761194  | 0.8200000000000001 |\n",
            "|    Logreg 0.05    | 0.8755102040816326 | 0.7517241379310344 | 0.8134328358208955 | 0.7813620071684588 |\n",
            "|      SVM 0.05     | 0.8816326530612245 |       0.7375       | 0.8805970149253731 | 0.8027210884353743 |\n",
            "|   LSTM beta 0.05  | 0.849015317286652  | 0.7241379310344828 | 0.7835820895522388 | 0.7526881720430108 |\n",
            "|   GRU beta 0.05   | 0.8599562363238512 | 0.7302631578947368 | 0.8283582089552238 | 0.7762237762237761 |\n",
            "| XGBoost beta 0.05 | 0.8555798687089715 | 0.7023809523809523 | 0.8805970149253731 | 0.7814569536423842 |\n",
            "|  logreg beta 0.05 | 0.8796498905908097 | 0.7687074829931972 | 0.8432835820895522 | 0.8042704626334519 |\n",
            "|   svm beta 0.05   | 0.8840262582056893 | 0.7547169811320755 | 0.8955223880597015 | 0.8191126279863482 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979
        },
        "id": "OAd9FoNiUbov",
        "outputId": "11cc2954-ee4b-4e97-f61f-677f6c114166"
      },
      "source": [
        "Result_cross.to_csv('DVN_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.511450</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>0.471831</td>\n",
              "      <td>0.437908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.553571</td>\n",
              "      <td>0.712245</td>\n",
              "      <td>0.467925</td>\n",
              "      <td>0.405229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.496774</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.503268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.566038</td>\n",
              "      <td>0.716327</td>\n",
              "      <td>0.463320</td>\n",
              "      <td>0.392157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.493333</td>\n",
              "      <td>0.683673</td>\n",
              "      <td>0.488449</td>\n",
              "      <td>0.483660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.441860</td>\n",
              "      <td>0.704595</td>\n",
              "      <td>0.457831</td>\n",
              "      <td>0.475000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.405405</td>\n",
              "      <td>0.676149</td>\n",
              "      <td>0.447761</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.431034</td>\n",
              "      <td>0.684902</td>\n",
              "      <td>0.510204</td>\n",
              "      <td>0.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.438462</td>\n",
              "      <td>0.702407</td>\n",
              "      <td>0.456000</td>\n",
              "      <td>0.475000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.410596</td>\n",
              "      <td>0.678337</td>\n",
              "      <td>0.457565</td>\n",
              "      <td>0.516667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.493776</td>\n",
              "      <td>0.681633</td>\n",
              "      <td>0.604061</td>\n",
              "      <td>0.777778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.490323</td>\n",
              "      <td>0.681633</td>\n",
              "      <td>0.493506</td>\n",
              "      <td>0.496732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.503268</td>\n",
              "      <td>0.689796</td>\n",
              "      <td>0.503268</td>\n",
              "      <td>0.503268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.558559</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.469697</td>\n",
              "      <td>0.405229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.496732</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.496732</td>\n",
              "      <td>0.496732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.390411</td>\n",
              "      <td>0.667396</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.475000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.438462</td>\n",
              "      <td>0.702407</td>\n",
              "      <td>0.456000</td>\n",
              "      <td>0.475000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.431034</td>\n",
              "      <td>0.684902</td>\n",
              "      <td>0.510204</td>\n",
              "      <td>0.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.438462</td>\n",
              "      <td>0.702407</td>\n",
              "      <td>0.456000</td>\n",
              "      <td>0.475000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.410596</td>\n",
              "      <td>0.678337</td>\n",
              "      <td>0.457565</td>\n",
              "      <td>0.516667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.05</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.704819</td>\n",
              "      <td>0.865306</td>\n",
              "      <td>0.780000</td>\n",
              "      <td>0.873134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.05</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.885714</td>\n",
              "      <td>0.794118</td>\n",
              "      <td>0.805970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.05</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.740964</td>\n",
              "      <td>0.889796</td>\n",
              "      <td>0.820000</td>\n",
              "      <td>0.917910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.05</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.751724</td>\n",
              "      <td>0.875510</td>\n",
              "      <td>0.781362</td>\n",
              "      <td>0.813433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.05</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.737500</td>\n",
              "      <td>0.881633</td>\n",
              "      <td>0.802721</td>\n",
              "      <td>0.880597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.05</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.724138</td>\n",
              "      <td>0.849015</td>\n",
              "      <td>0.752688</td>\n",
              "      <td>0.783582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.05</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.730263</td>\n",
              "      <td>0.859956</td>\n",
              "      <td>0.776224</td>\n",
              "      <td>0.828358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.05</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.702381</td>\n",
              "      <td>0.855580</td>\n",
              "      <td>0.781457</td>\n",
              "      <td>0.880597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.05</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.768707</td>\n",
              "      <td>0.879650</td>\n",
              "      <td>0.804270</td>\n",
              "      <td>0.843284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.05</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.754717</td>\n",
              "      <td>0.884026</td>\n",
              "      <td>0.819113</td>\n",
              "      <td>0.895522</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0          LSTM 0.15  DVN  0.511450  0.693878  0.471831  0.437908\n",
              "1           GRU 0.15  DVN  0.553571  0.712245  0.467925  0.405229\n",
              "2       XGBoost 0.15  DVN  0.496774  0.685714  0.500000  0.503268\n",
              "3        Logreg 0.15  DVN  0.566038  0.716327  0.463320  0.392157\n",
              "4           SVM 0.15  DVN  0.493333  0.683673  0.488449  0.483660\n",
              "5     LSTM beta 0.15  DVN  0.441860  0.704595  0.457831  0.475000\n",
              "6      GRU beta 0.15  DVN  0.405405  0.676149  0.447761  0.500000\n",
              "7  XGBoost beta 0.15  DVN  0.431034  0.684902  0.510204  0.625000\n",
              "8   logreg beta 0.15  DVN  0.438462  0.702407  0.456000  0.475000\n",
              "9      svm beta 0.15  DVN  0.410596  0.678337  0.457565  0.516667\n",
              "0           LSTM 0.1  DVN  0.493776  0.681633  0.604061  0.777778\n",
              "1            GRU 0.1  DVN  0.490323  0.681633  0.493506  0.496732\n",
              "2        XGBoost 0.1  DVN  0.503268  0.689796  0.503268  0.503268\n",
              "3         Logreg 0.1  DVN  0.558559  0.714286  0.469697  0.405229\n",
              "4            SVM 0.1  DVN  0.496732  0.685714  0.496732  0.496732\n",
              "5      LSTM beta 0.1  DVN  0.390411  0.667396  0.428571  0.475000\n",
              "6       GRU beta 0.1  DVN  0.438462  0.702407  0.456000  0.475000\n",
              "7   XGBoost beta 0.1  DVN  0.431034  0.684902  0.510204  0.625000\n",
              "8    logreg beta 0.1  DVN  0.438462  0.702407  0.456000  0.475000\n",
              "9       svm beta 0.1  DVN  0.410596  0.678337  0.457565  0.516667\n",
              "0          LSTM 0.05  DVN  0.704819  0.865306  0.780000  0.873134\n",
              "1           GRU 0.05  DVN  0.782609  0.885714  0.794118  0.805970\n",
              "2       XGBoost 0.05  DVN  0.740964  0.889796  0.820000  0.917910\n",
              "3        Logreg 0.05  DVN  0.751724  0.875510  0.781362  0.813433\n",
              "4           SVM 0.05  DVN  0.737500  0.881633  0.802721  0.880597\n",
              "5     LSTM beta 0.05  DVN  0.724138  0.849015  0.752688  0.783582\n",
              "6      GRU beta 0.05  DVN  0.730263  0.859956  0.776224  0.828358\n",
              "7  XGBoost beta 0.05  DVN  0.702381  0.855580  0.781457  0.880597\n",
              "8   logreg beta 0.05  DVN  0.768707  0.879650  0.804270  0.843284\n",
              "9      svm beta 0.05  DVN  0.754717  0.884026  0.819113  0.895522"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6pGj-XwUbov"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crurrFJAUbov"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jTHp7w7Ubov",
        "outputId": "9cbf4535-154f-4d6d-9e7c-48181bdb6a42"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        " \n",
        "  historical = Train_data(dfs[col_name], train_start=100, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"DVN\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 4s 19ms/step - loss: 0.6679 - accuracy: 0.6136 - val_loss: 0.6348 - val_accuracy: 0.6878\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.6590 - accuracy: 0.6189 - val_loss: 0.5904 - val_accuracy: 0.6878\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.5634 - accuracy: 0.7059 - val_loss: 0.6361 - val_accuracy: 0.6878\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.4851 - accuracy: 0.7686 - val_loss: 0.7454 - val_accuracy: 0.6898\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.3586 - accuracy: 0.8450 - val_loss: 0.5929 - val_accuracy: 0.6898\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 14ms/step - loss: 0.6638 - accuracy: 0.6142 - val_loss: 0.5958 - val_accuracy: 0.6878\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.4454 - accuracy: 0.7828 - val_loss: 0.5359 - val_accuracy: 0.6959\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.3543 - accuracy: 0.8343 - val_loss: 0.8058 - val_accuracy: 0.6898\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.3313 - accuracy: 0.8491 - val_loss: 0.7932 - val_accuracy: 0.6776\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.3342 - accuracy: 0.8456 - val_loss: 0.6863 - val_accuracy: 0.6837\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.726392\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.724714\n",
            "[2]\tvalidation_0-auc:0.726828\n",
            "[3]\tvalidation_0-auc:0.727051\n",
            "[4]\tvalidation_0-auc:0.727633\n",
            "[5]\tvalidation_0-auc:0.732094\n",
            "[6]\tvalidation_0-auc:0.733364\n",
            "[7]\tvalidation_0-auc:0.733025\n",
            "[8]\tvalidation_0-auc:0.733869\n",
            "[9]\tvalidation_0-auc:0.733966\n",
            "[10]\tvalidation_0-auc:0.733985\n",
            "[11]\tvalidation_0-auc:0.733423\n",
            "[12]\tvalidation_0-auc:0.733597\n",
            "[13]\tvalidation_0-auc:0.733655\n",
            "[14]\tvalidation_0-auc:0.739978\n",
            "[15]\tvalidation_0-auc:0.740055\n",
            "[16]\tvalidation_0-auc:0.739745\n",
            "[17]\tvalidation_0-auc:0.73798\n",
            "[18]\tvalidation_0-auc:0.73734\n",
            "[19]\tvalidation_0-auc:0.736884\n",
            "[20]\tvalidation_0-auc:0.739474\n",
            "[21]\tvalidation_0-auc:0.738931\n",
            "[22]\tvalidation_0-auc:0.736749\n",
            "[23]\tvalidation_0-auc:0.737175\n",
            "[24]\tvalidation_0-auc:0.739813\n",
            "[25]\tvalidation_0-auc:0.739784\n",
            "[26]\tvalidation_0-auc:0.743469\n",
            "[27]\tvalidation_0-auc:0.747086\n",
            "[28]\tvalidation_0-auc:0.747047\n",
            "[29]\tvalidation_0-auc:0.747231\n",
            "[30]\tvalidation_0-auc:0.74696\n",
            "[31]\tvalidation_0-auc:0.747406\n",
            "[32]\tvalidation_0-auc:0.747581\n",
            "[33]\tvalidation_0-auc:0.748657\n",
            "[34]\tvalidation_0-auc:0.748114\n",
            "[35]\tvalidation_0-auc:0.747871\n",
            "[36]\tvalidation_0-auc:0.749404\n",
            "[37]\tvalidation_0-auc:0.750034\n",
            "[38]\tvalidation_0-auc:0.750015\n",
            "[39]\tvalidation_0-auc:0.750548\n",
            "[40]\tvalidation_0-auc:0.750141\n",
            "[41]\tvalidation_0-auc:0.750625\n",
            "[42]\tvalidation_0-auc:0.750936\n",
            "[43]\tvalidation_0-auc:0.750587\n",
            "[44]\tvalidation_0-auc:0.754388\n",
            "[45]\tvalidation_0-auc:0.754417\n",
            "[46]\tvalidation_0-auc:0.755561\n",
            "[47]\tvalidation_0-auc:0.755891\n",
            "[48]\tvalidation_0-auc:0.755028\n",
            "[49]\tvalidation_0-auc:0.754795\n",
            "[50]\tvalidation_0-auc:0.754213\n",
            "[51]\tvalidation_0-auc:0.754049\n",
            "[52]\tvalidation_0-auc:0.753826\n",
            "[53]\tvalidation_0-auc:0.753874\n",
            "[54]\tvalidation_0-auc:0.75368\n",
            "[55]\tvalidation_0-auc:0.753399\n",
            "[56]\tvalidation_0-auc:0.753981\n",
            "[57]\tvalidation_0-auc:0.753302\n",
            "[58]\tvalidation_0-auc:0.753535\n",
            "[59]\tvalidation_0-auc:0.753573\n",
            "[60]\tvalidation_0-auc:0.753864\n",
            "[61]\tvalidation_0-auc:0.753506\n",
            "[62]\tvalidation_0-auc:0.756454\n",
            "[63]\tvalidation_0-auc:0.756803\n",
            "[64]\tvalidation_0-auc:0.759625\n",
            "[65]\tvalidation_0-auc:0.75913\n",
            "[66]\tvalidation_0-auc:0.758684\n",
            "[67]\tvalidation_0-auc:0.760546\n",
            "[68]\tvalidation_0-auc:0.760623\n",
            "[69]\tvalidation_0-auc:0.760429\n",
            "[70]\tvalidation_0-auc:0.760468\n",
            "[71]\tvalidation_0-auc:0.760352\n",
            "[72]\tvalidation_0-auc:0.759722\n",
            "[73]\tvalidation_0-auc:0.760526\n",
            "[74]\tvalidation_0-auc:0.760963\n",
            "[75]\tvalidation_0-auc:0.760875\n",
            "[76]\tvalidation_0-auc:0.760769\n",
            "[77]\tvalidation_0-auc:0.760536\n",
            "[78]\tvalidation_0-auc:0.762379\n",
            "[79]\tvalidation_0-auc:0.762204\n",
            "[80]\tvalidation_0-auc:0.762379\n",
            "[81]\tvalidation_0-auc:0.762611\n",
            "[82]\tvalidation_0-auc:0.761952\n",
            "[83]\tvalidation_0-auc:0.762165\n",
            "[84]\tvalidation_0-auc:0.762359\n",
            "[85]\tvalidation_0-auc:0.762417\n",
            "[86]\tvalidation_0-auc:0.762068\n",
            "[87]\tvalidation_0-auc:0.762204\n",
            "[88]\tvalidation_0-auc:0.761816\n",
            "[89]\tvalidation_0-auc:0.761932\n",
            "[90]\tvalidation_0-auc:0.761971\n",
            "[91]\tvalidation_0-auc:0.761661\n",
            "[92]\tvalidation_0-auc:0.761195\n",
            "[93]\tvalidation_0-auc:0.761409\n",
            "[94]\tvalidation_0-auc:0.761292\n",
            "[95]\tvalidation_0-auc:0.760284\n",
            "[96]\tvalidation_0-auc:0.760245\n",
            "[97]\tvalidation_0-auc:0.76009\n",
            "[98]\tvalidation_0-auc:0.760517\n",
            "[99]\tvalidation_0-auc:0.760206\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 15ms/step - loss: 0.6200 - accuracy: 0.6639 - val_loss: 0.5442 - val_accuracy: 0.7177\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.4394 - accuracy: 0.8069 - val_loss: 0.5184 - val_accuracy: 0.6915\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.4142 - accuracy: 0.8189 - val_loss: 0.5539 - val_accuracy: 0.7505\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.4068 - accuracy: 0.8177 - val_loss: 0.6588 - val_accuracy: 0.6849\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3897 - accuracy: 0.8232 - val_loss: 0.5553 - val_accuracy: 0.6805\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 13ms/step - loss: 0.5521 - accuracy: 0.7037 - val_loss: 0.6010 - val_accuracy: 0.6893\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3758 - accuracy: 0.8316 - val_loss: 0.6800 - val_accuracy: 0.7943\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3905 - accuracy: 0.8268 - val_loss: 0.5354 - val_accuracy: 0.7199\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3418 - accuracy: 0.8467 - val_loss: 0.7469 - val_accuracy: 0.6980\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3409 - accuracy: 0.8413 - val_loss: 0.6333 - val_accuracy: 0.6761\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.760893\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.755861\n",
            "[2]\tvalidation_0-auc:0.753895\n",
            "[3]\tvalidation_0-auc:0.753276\n",
            "[4]\tvalidation_0-auc:0.753202\n",
            "[5]\tvalidation_0-auc:0.752745\n",
            "[6]\tvalidation_0-auc:0.748578\n",
            "[7]\tvalidation_0-auc:0.745462\n",
            "[8]\tvalidation_0-auc:0.747169\n",
            "[9]\tvalidation_0-auc:0.749382\n",
            "[10]\tvalidation_0-auc:0.750915\n",
            "[11]\tvalidation_0-auc:0.752819\n",
            "[12]\tvalidation_0-auc:0.753734\n",
            "[13]\tvalidation_0-auc:0.754216\n",
            "[14]\tvalidation_0-auc:0.75544\n",
            "[15]\tvalidation_0-auc:0.75497\n",
            "[16]\tvalidation_0-auc:0.754525\n",
            "[17]\tvalidation_0-auc:0.752547\n",
            "[18]\tvalidation_0-auc:0.754031\n",
            "[19]\tvalidation_0-auc:0.754525\n",
            "[20]\tvalidation_0-auc:0.754748\n",
            "[21]\tvalidation_0-auc:0.754871\n",
            "[22]\tvalidation_0-auc:0.755218\n",
            "[23]\tvalidation_0-auc:0.757109\n",
            "[24]\tvalidation_0-auc:0.757653\n",
            "[25]\tvalidation_0-auc:0.759137\n",
            "[26]\tvalidation_0-auc:0.747577\n",
            "[27]\tvalidation_0-auc:0.7477\n",
            "[28]\tvalidation_0-auc:0.740455\n",
            "[29]\tvalidation_0-auc:0.741444\n",
            "[30]\tvalidation_0-auc:0.741865\n",
            "[31]\tvalidation_0-auc:0.762747\n",
            "[32]\tvalidation_0-auc:0.761758\n",
            "[33]\tvalidation_0-auc:0.751879\n",
            "[34]\tvalidation_0-auc:0.752572\n",
            "[35]\tvalidation_0-auc:0.760163\n",
            "[36]\tvalidation_0-auc:0.760262\n",
            "[37]\tvalidation_0-auc:0.758556\n",
            "[38]\tvalidation_0-auc:0.75727\n",
            "[39]\tvalidation_0-auc:0.757864\n",
            "[40]\tvalidation_0-auc:0.757443\n",
            "[41]\tvalidation_0-auc:0.759199\n",
            "[42]\tvalidation_0-auc:0.758408\n",
            "[43]\tvalidation_0-auc:0.757369\n",
            "[44]\tvalidation_0-auc:0.756751\n",
            "[45]\tvalidation_0-auc:0.757468\n",
            "[46]\tvalidation_0-auc:0.757641\n",
            "[47]\tvalidation_0-auc:0.759026\n",
            "[48]\tvalidation_0-auc:0.759471\n",
            "[49]\tvalidation_0-auc:0.759891\n",
            "[50]\tvalidation_0-auc:0.760831\n",
            "[51]\tvalidation_0-auc:0.758754\n",
            "[52]\tvalidation_0-auc:0.759125\n",
            "[53]\tvalidation_0-auc:0.759545\n",
            "[54]\tvalidation_0-auc:0.775087\n",
            "[55]\tvalidation_0-auc:0.774345\n",
            "[56]\tvalidation_0-auc:0.768855\n",
            "[57]\tvalidation_0-auc:0.768534\n",
            "[58]\tvalidation_0-auc:0.769844\n",
            "[59]\tvalidation_0-auc:0.770734\n",
            "[60]\tvalidation_0-auc:0.777225\n",
            "[61]\tvalidation_0-auc:0.775408\n",
            "[62]\tvalidation_0-auc:0.776298\n",
            "[63]\tvalidation_0-auc:0.776867\n",
            "[64]\tvalidation_0-auc:0.775915\n",
            "[65]\tvalidation_0-auc:0.77003\n",
            "[66]\tvalidation_0-auc:0.770821\n",
            "[67]\tvalidation_0-auc:0.771217\n",
            "[68]\tvalidation_0-auc:0.771118\n",
            "[69]\tvalidation_0-auc:0.773863\n",
            "[70]\tvalidation_0-auc:0.774011\n",
            "[71]\tvalidation_0-auc:0.77458\n",
            "[72]\tvalidation_0-auc:0.77406\n",
            "[73]\tvalidation_0-auc:0.773961\n",
            "[74]\tvalidation_0-auc:0.773195\n",
            "[75]\tvalidation_0-auc:0.773343\n",
            "[76]\tvalidation_0-auc:0.773541\n",
            "[77]\tvalidation_0-auc:0.773788\n",
            "[78]\tvalidation_0-auc:0.774617\n",
            "[79]\tvalidation_0-auc:0.773034\n",
            "[80]\tvalidation_0-auc:0.773133\n",
            "[81]\tvalidation_0-auc:0.773924\n",
            "[82]\tvalidation_0-auc:0.773207\n",
            "[83]\tvalidation_0-auc:0.773801\n",
            "[84]\tvalidation_0-auc:0.774295\n",
            "[85]\tvalidation_0-auc:0.774097\n",
            "[86]\tvalidation_0-auc:0.77432\n",
            "[87]\tvalidation_0-auc:0.774716\n",
            "[88]\tvalidation_0-auc:0.774518\n",
            "[89]\tvalidation_0-auc:0.774345\n",
            "[90]\tvalidation_0-auc:0.773924\n",
            "[91]\tvalidation_0-auc:0.773306\n",
            "[92]\tvalidation_0-auc:0.773356\n",
            "[93]\tvalidation_0-auc:0.77338\n",
            "[94]\tvalidation_0-auc:0.773529\n",
            "[95]\tvalidation_0-auc:0.773059\n",
            "[96]\tvalidation_0-auc:0.773232\n",
            "[97]\tvalidation_0-auc:0.77338\n",
            "[98]\tvalidation_0-auc:0.772713\n",
            "[99]\tvalidation_0-auc:0.773108\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.689795918367347  |  0.5035460992907801 | 0.46405228758169936 | 0.48299319727891155 |\n",
            "|      GRU 0.15     | 0.6836734693877551 | 0.49333333333333335 | 0.48366013071895425 |  0.4884488448844884 |\n",
            "|    XGBoost 0.15   | 0.6857142857142857 |  0.4967741935483871 |  0.5032679738562091 |         0.5         |\n",
            "|    Logreg 0.15    | 0.7163265306122449 |  0.5660377358490566 | 0.39215686274509803 |  0.4633204633204633 |\n",
            "|      SVM 0.15     | 0.6836734693877551 | 0.49333333333333335 | 0.48366013071895425 |  0.4884488448844884 |\n",
            "|   LSTM beta 0.15  | 0.6805251641137856 | 0.42168674698795183 |  0.5833333333333334 | 0.48951048951048953 |\n",
            "|   GRU beta 0.15   | 0.6761487964989059 |  0.4041095890410959 | 0.49166666666666664 | 0.44360902255639095 |\n",
            "| XGBoost beta 0.15 | 0.6849015317286652 | 0.43103448275862066 |        0.625        |  0.5102040816326531 |\n",
            "|  logreg beta 0.15 | 0.7024070021881839 | 0.43846153846153846 |        0.475        |        0.456        |\n",
            "|   svm beta 0.15   | 0.6783369803063457 |  0.4105960264900662 |  0.5166666666666667 |  0.4575645756457565 |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 15ms/step - loss: 0.6727 - accuracy: 0.6059 - val_loss: 0.6349 - val_accuracy: 0.6878\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.6656 - accuracy: 0.6095 - val_loss: 0.5962 - val_accuracy: 0.6878\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.4600 - accuracy: 0.7787 - val_loss: 0.6714 - val_accuracy: 0.6490\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.3620 - accuracy: 0.8426 - val_loss: 0.7474 - val_accuracy: 0.6653\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.3380 - accuracy: 0.8527 - val_loss: 0.6200 - val_accuracy: 0.6837\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 14ms/step - loss: 0.6653 - accuracy: 0.6166 - val_loss: 0.6312 - val_accuracy: 0.6878\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.5176 - accuracy: 0.7278 - val_loss: 0.6829 - val_accuracy: 0.6776\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.3358 - accuracy: 0.8586 - val_loss: 0.7295 - val_accuracy: 0.6796\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.3321 - accuracy: 0.8592 - val_loss: 0.8038 - val_accuracy: 0.6796\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.3402 - accuracy: 0.8521 - val_loss: 0.8322 - val_accuracy: 0.6898\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.726392\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.725219\n",
            "[2]\tvalidation_0-auc:0.724365\n",
            "[3]\tvalidation_0-auc:0.725888\n",
            "[4]\tvalidation_0-auc:0.726421\n",
            "[5]\tvalidation_0-auc:0.728157\n",
            "[6]\tvalidation_0-auc:0.733151\n",
            "[7]\tvalidation_0-auc:0.732841\n",
            "[8]\tvalidation_0-auc:0.732104\n",
            "[9]\tvalidation_0-auc:0.733229\n",
            "[10]\tvalidation_0-auc:0.733403\n",
            "[11]\tvalidation_0-auc:0.732336\n",
            "[12]\tvalidation_0-auc:0.732773\n",
            "[13]\tvalidation_0-auc:0.733859\n",
            "[14]\tvalidation_0-auc:0.734101\n",
            "[15]\tvalidation_0-auc:0.739648\n",
            "[16]\tvalidation_0-auc:0.73928\n",
            "[17]\tvalidation_0-auc:0.738727\n",
            "[18]\tvalidation_0-auc:0.738727\n",
            "[19]\tvalidation_0-auc:0.737777\n",
            "[20]\tvalidation_0-auc:0.736817\n",
            "[21]\tvalidation_0-auc:0.739522\n",
            "[22]\tvalidation_0-auc:0.739755\n",
            "[23]\tvalidation_0-auc:0.739532\n",
            "[24]\tvalidation_0-auc:0.741879\n",
            "[25]\tvalidation_0-auc:0.741171\n",
            "[26]\tvalidation_0-auc:0.74502\n",
            "[27]\tvalidation_0-auc:0.745137\n",
            "[28]\tvalidation_0-auc:0.746378\n",
            "[29]\tvalidation_0-auc:0.750703\n",
            "[30]\tvalidation_0-auc:0.751149\n",
            "[31]\tvalidation_0-auc:0.750286\n",
            "[32]\tvalidation_0-auc:0.750141\n",
            "[33]\tvalidation_0-auc:0.749966\n",
            "[34]\tvalidation_0-auc:0.74985\n",
            "[35]\tvalidation_0-auc:0.751159\n",
            "[36]\tvalidation_0-auc:0.750916\n",
            "[37]\tvalidation_0-auc:0.750325\n",
            "[38]\tvalidation_0-auc:0.749927\n",
            "[39]\tvalidation_0-auc:0.749888\n",
            "[40]\tvalidation_0-auc:0.749908\n",
            "[41]\tvalidation_0-auc:0.75176\n",
            "[42]\tvalidation_0-auc:0.752827\n",
            "[43]\tvalidation_0-auc:0.75688\n",
            "[44]\tvalidation_0-auc:0.756541\n",
            "[45]\tvalidation_0-auc:0.755823\n",
            "[46]\tvalidation_0-auc:0.755523\n",
            "[47]\tvalidation_0-auc:0.755581\n",
            "[48]\tvalidation_0-auc:0.755154\n",
            "[49]\tvalidation_0-auc:0.755629\n",
            "[50]\tvalidation_0-auc:0.755852\n",
            "[51]\tvalidation_0-auc:0.75594\n",
            "[52]\tvalidation_0-auc:0.755125\n",
            "[53]\tvalidation_0-auc:0.755523\n",
            "[54]\tvalidation_0-auc:0.755755\n",
            "[55]\tvalidation_0-auc:0.755697\n",
            "[56]\tvalidation_0-auc:0.756143\n",
            "[57]\tvalidation_0-auc:0.756521\n",
            "[58]\tvalidation_0-auc:0.756463\n",
            "[59]\tvalidation_0-auc:0.756318\n",
            "[60]\tvalidation_0-auc:0.75624\n",
            "[61]\tvalidation_0-auc:0.75561\n",
            "[62]\tvalidation_0-auc:0.75594\n",
            "[63]\tvalidation_0-auc:0.755183\n",
            "[64]\tvalidation_0-auc:0.755144\n",
            "[65]\tvalidation_0-auc:0.754611\n",
            "[66]\tvalidation_0-auc:0.754495\n",
            "[67]\tvalidation_0-auc:0.754863\n",
            "[68]\tvalidation_0-auc:0.758703\n",
            "[69]\tvalidation_0-auc:0.760284\n",
            "[70]\tvalidation_0-auc:0.760517\n",
            "[71]\tvalidation_0-auc:0.76073\n",
            "[72]\tvalidation_0-auc:0.760943\n",
            "[73]\tvalidation_0-auc:0.761932\n",
            "[74]\tvalidation_0-auc:0.761894\n",
            "[75]\tvalidation_0-auc:0.76201\n",
            "[76]\tvalidation_0-auc:0.761884\n",
            "[77]\tvalidation_0-auc:0.761962\n",
            "[78]\tvalidation_0-auc:0.76201\n",
            "[79]\tvalidation_0-auc:0.761991\n",
            "[80]\tvalidation_0-auc:0.761467\n",
            "[81]\tvalidation_0-auc:0.761545\n",
            "[82]\tvalidation_0-auc:0.761079\n",
            "[83]\tvalidation_0-auc:0.761079\n",
            "[84]\tvalidation_0-auc:0.761302\n",
            "[85]\tvalidation_0-auc:0.759392\n",
            "[86]\tvalidation_0-auc:0.759392\n",
            "[87]\tvalidation_0-auc:0.759838\n",
            "[88]\tvalidation_0-auc:0.760517\n",
            "[89]\tvalidation_0-auc:0.760711\n",
            "[90]\tvalidation_0-auc:0.762718\n",
            "[91]\tvalidation_0-auc:0.763106\n",
            "[92]\tvalidation_0-auc:0.763164\n",
            "[93]\tvalidation_0-auc:0.763174\n",
            "[94]\tvalidation_0-auc:0.763154\n",
            "[95]\tvalidation_0-auc:0.762825\n",
            "[96]\tvalidation_0-auc:0.762495\n",
            "[97]\tvalidation_0-auc:0.762223\n",
            "[98]\tvalidation_0-auc:0.761971\n",
            "[99]\tvalidation_0-auc:0.762301\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 15ms/step - loss: 0.6367 - accuracy: 0.6301 - val_loss: 0.5487 - val_accuracy: 0.6630\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.4378 - accuracy: 0.7936 - val_loss: 0.5412 - val_accuracy: 0.7593\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.4077 - accuracy: 0.8171 - val_loss: 0.5883 - val_accuracy: 0.6958\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3831 - accuracy: 0.8340 - val_loss: 0.5248 - val_accuracy: 0.7112\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3708 - accuracy: 0.8352 - val_loss: 0.5686 - val_accuracy: 0.6871\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 14ms/step - loss: 0.5276 - accuracy: 0.7296 - val_loss: 0.5926 - val_accuracy: 0.6740\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3698 - accuracy: 0.8226 - val_loss: 0.5354 - val_accuracy: 0.7133\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3687 - accuracy: 0.8485 - val_loss: 0.5638 - val_accuracy: 0.7287\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3723 - accuracy: 0.8485 - val_loss: 0.5617 - val_accuracy: 0.6958\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3663 - accuracy: 0.8346 - val_loss: 0.5070 - val_accuracy: 0.7243\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.760893\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.755861\n",
            "[2]\tvalidation_0-auc:0.753895\n",
            "[3]\tvalidation_0-auc:0.753276\n",
            "[4]\tvalidation_0-auc:0.753202\n",
            "[5]\tvalidation_0-auc:0.752745\n",
            "[6]\tvalidation_0-auc:0.748578\n",
            "[7]\tvalidation_0-auc:0.745462\n",
            "[8]\tvalidation_0-auc:0.747169\n",
            "[9]\tvalidation_0-auc:0.749382\n",
            "[10]\tvalidation_0-auc:0.750915\n",
            "[11]\tvalidation_0-auc:0.752819\n",
            "[12]\tvalidation_0-auc:0.753734\n",
            "[13]\tvalidation_0-auc:0.754216\n",
            "[14]\tvalidation_0-auc:0.75544\n",
            "[15]\tvalidation_0-auc:0.75497\n",
            "[16]\tvalidation_0-auc:0.754525\n",
            "[17]\tvalidation_0-auc:0.752547\n",
            "[18]\tvalidation_0-auc:0.754031\n",
            "[19]\tvalidation_0-auc:0.754525\n",
            "[20]\tvalidation_0-auc:0.754748\n",
            "[21]\tvalidation_0-auc:0.754871\n",
            "[22]\tvalidation_0-auc:0.755218\n",
            "[23]\tvalidation_0-auc:0.757109\n",
            "[24]\tvalidation_0-auc:0.757653\n",
            "[25]\tvalidation_0-auc:0.759137\n",
            "[26]\tvalidation_0-auc:0.747577\n",
            "[27]\tvalidation_0-auc:0.7477\n",
            "[28]\tvalidation_0-auc:0.740455\n",
            "[29]\tvalidation_0-auc:0.741444\n",
            "[30]\tvalidation_0-auc:0.741865\n",
            "[31]\tvalidation_0-auc:0.762747\n",
            "[32]\tvalidation_0-auc:0.761758\n",
            "[33]\tvalidation_0-auc:0.751879\n",
            "[34]\tvalidation_0-auc:0.752572\n",
            "[35]\tvalidation_0-auc:0.760163\n",
            "[36]\tvalidation_0-auc:0.760262\n",
            "[37]\tvalidation_0-auc:0.758556\n",
            "[38]\tvalidation_0-auc:0.75727\n",
            "[39]\tvalidation_0-auc:0.757864\n",
            "[40]\tvalidation_0-auc:0.757443\n",
            "[41]\tvalidation_0-auc:0.759199\n",
            "[42]\tvalidation_0-auc:0.758408\n",
            "[43]\tvalidation_0-auc:0.757369\n",
            "[44]\tvalidation_0-auc:0.756751\n",
            "[45]\tvalidation_0-auc:0.757468\n",
            "[46]\tvalidation_0-auc:0.757641\n",
            "[47]\tvalidation_0-auc:0.759026\n",
            "[48]\tvalidation_0-auc:0.759471\n",
            "[49]\tvalidation_0-auc:0.759891\n",
            "[50]\tvalidation_0-auc:0.760831\n",
            "[51]\tvalidation_0-auc:0.758754\n",
            "[52]\tvalidation_0-auc:0.759125\n",
            "[53]\tvalidation_0-auc:0.759545\n",
            "[54]\tvalidation_0-auc:0.775087\n",
            "[55]\tvalidation_0-auc:0.774345\n",
            "[56]\tvalidation_0-auc:0.768855\n",
            "[57]\tvalidation_0-auc:0.768534\n",
            "[58]\tvalidation_0-auc:0.769844\n",
            "[59]\tvalidation_0-auc:0.770734\n",
            "[60]\tvalidation_0-auc:0.777225\n",
            "[61]\tvalidation_0-auc:0.775408\n",
            "[62]\tvalidation_0-auc:0.776298\n",
            "[63]\tvalidation_0-auc:0.776867\n",
            "[64]\tvalidation_0-auc:0.775915\n",
            "[65]\tvalidation_0-auc:0.77003\n",
            "[66]\tvalidation_0-auc:0.770821\n",
            "[67]\tvalidation_0-auc:0.771217\n",
            "[68]\tvalidation_0-auc:0.771118\n",
            "[69]\tvalidation_0-auc:0.773863\n",
            "[70]\tvalidation_0-auc:0.774011\n",
            "[71]\tvalidation_0-auc:0.77458\n",
            "[72]\tvalidation_0-auc:0.77406\n",
            "[73]\tvalidation_0-auc:0.773961\n",
            "[74]\tvalidation_0-auc:0.773195\n",
            "[75]\tvalidation_0-auc:0.773343\n",
            "[76]\tvalidation_0-auc:0.773541\n",
            "[77]\tvalidation_0-auc:0.773788\n",
            "[78]\tvalidation_0-auc:0.774617\n",
            "[79]\tvalidation_0-auc:0.773034\n",
            "[80]\tvalidation_0-auc:0.773133\n",
            "[81]\tvalidation_0-auc:0.773924\n",
            "[82]\tvalidation_0-auc:0.773207\n",
            "[83]\tvalidation_0-auc:0.773801\n",
            "[84]\tvalidation_0-auc:0.774295\n",
            "[85]\tvalidation_0-auc:0.774097\n",
            "[86]\tvalidation_0-auc:0.77432\n",
            "[87]\tvalidation_0-auc:0.774716\n",
            "[88]\tvalidation_0-auc:0.774518\n",
            "[89]\tvalidation_0-auc:0.774345\n",
            "[90]\tvalidation_0-auc:0.773924\n",
            "[91]\tvalidation_0-auc:0.773306\n",
            "[92]\tvalidation_0-auc:0.773356\n",
            "[93]\tvalidation_0-auc:0.77338\n",
            "[94]\tvalidation_0-auc:0.773529\n",
            "[95]\tvalidation_0-auc:0.773059\n",
            "[96]\tvalidation_0-auc:0.773232\n",
            "[97]\tvalidation_0-auc:0.77338\n",
            "[98]\tvalidation_0-auc:0.772713\n",
            "[99]\tvalidation_0-auc:0.773108\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.6836734693877551 |  0.4930555555555556 | 0.46405228758169936 |  0.4781144781144781 |\n",
            "|     GRU 0.1      | 0.689795918367347  |  0.503448275862069  |  0.477124183006536  | 0.48993288590604034 |\n",
            "|   XGBoost 0.1    | 0.689795918367347  |  0.5032679738562091 |  0.5032679738562091 |  0.5032679738562091 |\n",
            "|    Logreg 0.1    | 0.7142857142857143 |  0.5585585585585585 | 0.40522875816993464 | 0.46969696969696967 |\n",
            "|     SVM 0.1      | 0.6857142857142857 | 0.49673202614379086 | 0.49673202614379086 | 0.49673202614379086 |\n",
            "|  LSTM beta 0.1   | 0.687089715536105  |  0.4148148148148148 |  0.4666666666666667 |  0.4392156862745098 |\n",
            "|   GRU beta 0.1   | 0.7242888402625821 |        0.475        |        0.475        | 0.47500000000000003 |\n",
            "| XGBoost beta 0.1 | 0.6849015317286652 | 0.43103448275862066 |        0.625        |  0.5102040816326531 |\n",
            "| logreg beta 0.1  | 0.7024070021881839 | 0.43846153846153846 |        0.475        |        0.456        |\n",
            "|   svm beta 0.1   | 0.6783369803063457 |  0.4105960264900662 |  0.5166666666666667 |  0.4575645756457565 |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "Threshhold =  0.05\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 15ms/step - loss: 0.6838 - accuracy: 0.5680 - val_loss: 0.6393 - val_accuracy: 0.7265\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.5430 - accuracy: 0.7195 - val_loss: 0.2724 - val_accuracy: 0.8673\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.4084 - accuracy: 0.8367 - val_loss: 0.2618 - val_accuracy: 0.8673\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.3297 - accuracy: 0.8675 - val_loss: 0.4614 - val_accuracy: 0.7510\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 12ms/step - loss: 0.2906 - accuracy: 0.8757 - val_loss: 0.2796 - val_accuracy: 0.8653\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 14ms/step - loss: 0.6755 - accuracy: 0.5828 - val_loss: 0.6240 - val_accuracy: 0.8082\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.3795 - accuracy: 0.8349 - val_loss: 0.2912 - val_accuracy: 0.8898\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.2716 - accuracy: 0.8935 - val_loss: 0.2842 - val_accuracy: 0.8776\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.2741 - accuracy: 0.8852 - val_loss: 0.3344 - val_accuracy: 0.8714\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.2587 - accuracy: 0.8929 - val_loss: 0.2343 - val_accuracy: 0.8796\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.938527\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.938118\n",
            "[2]\tvalidation_0-auc:0.956471\n",
            "[3]\tvalidation_0-auc:0.957477\n",
            "[4]\tvalidation_0-auc:0.95667\n",
            "[5]\tvalidation_0-auc:0.956555\n",
            "[6]\tvalidation_0-auc:0.956282\n",
            "[7]\tvalidation_0-auc:0.958169\n",
            "[8]\tvalidation_0-auc:0.958798\n",
            "[9]\tvalidation_0-auc:0.959186\n",
            "[10]\tvalidation_0-auc:0.958746\n",
            "[11]\tvalidation_0-auc:0.960328\n",
            "[12]\tvalidation_0-auc:0.960265\n",
            "[13]\tvalidation_0-auc:0.961785\n",
            "[14]\tvalidation_0-auc:0.961827\n",
            "[15]\tvalidation_0-auc:0.962058\n",
            "[16]\tvalidation_0-auc:0.962121\n",
            "[17]\tvalidation_0-auc:0.963714\n",
            "[18]\tvalidation_0-auc:0.963609\n",
            "[19]\tvalidation_0-auc:0.964343\n",
            "[20]\tvalidation_0-auc:0.963651\n",
            "[21]\tvalidation_0-auc:0.96384\n",
            "[22]\tvalidation_0-auc:0.963284\n",
            "[23]\tvalidation_0-auc:0.963536\n",
            "[24]\tvalidation_0-auc:0.964175\n",
            "[25]\tvalidation_0-auc:0.964468\n",
            "[26]\tvalidation_0-auc:0.964636\n",
            "[27]\tvalidation_0-auc:0.964825\n",
            "[28]\tvalidation_0-auc:0.964877\n",
            "[29]\tvalidation_0-auc:0.964982\n",
            "[30]\tvalidation_0-auc:0.965108\n",
            "[31]\tvalidation_0-auc:0.965506\n",
            "[32]\tvalidation_0-auc:0.96516\n",
            "[33]\tvalidation_0-auc:0.96473\n",
            "[34]\tvalidation_0-auc:0.964709\n",
            "[35]\tvalidation_0-auc:0.96473\n",
            "[36]\tvalidation_0-auc:0.964877\n",
            "[37]\tvalidation_0-auc:0.964206\n",
            "[38]\tvalidation_0-auc:0.964856\n",
            "[39]\tvalidation_0-auc:0.964594\n",
            "[40]\tvalidation_0-auc:0.964888\n",
            "[41]\tvalidation_0-auc:0.964867\n",
            "[42]\tvalidation_0-auc:0.965192\n",
            "[43]\tvalidation_0-auc:0.964856\n",
            "[44]\tvalidation_0-auc:0.964793\n",
            "[45]\tvalidation_0-auc:0.964709\n",
            "[46]\tvalidation_0-auc:0.964542\n",
            "[47]\tvalidation_0-auc:0.964573\n",
            "[48]\tvalidation_0-auc:0.964951\n",
            "[49]\tvalidation_0-auc:0.965286\n",
            "[50]\tvalidation_0-auc:0.965726\n",
            "[51]\tvalidation_0-auc:0.965726\n",
            "[52]\tvalidation_0-auc:0.965496\n",
            "[53]\tvalidation_0-auc:0.9656\n",
            "[54]\tvalidation_0-auc:0.965642\n",
            "[55]\tvalidation_0-auc:0.965621\n",
            "[56]\tvalidation_0-auc:0.965716\n",
            "[57]\tvalidation_0-auc:0.965779\n",
            "[58]\tvalidation_0-auc:0.965275\n",
            "[59]\tvalidation_0-auc:0.965192\n",
            "[60]\tvalidation_0-auc:0.965317\n",
            "[61]\tvalidation_0-auc:0.965317\n",
            "[62]\tvalidation_0-auc:0.965569\n",
            "[63]\tvalidation_0-auc:0.96538\n",
            "[64]\tvalidation_0-auc:0.965254\n",
            "[65]\tvalidation_0-auc:0.965234\n",
            "[66]\tvalidation_0-auc:0.965234\n",
            "[67]\tvalidation_0-auc:0.965296\n",
            "[68]\tvalidation_0-auc:0.965171\n",
            "[69]\tvalidation_0-auc:0.965171\n",
            "[70]\tvalidation_0-auc:0.965244\n",
            "[71]\tvalidation_0-auc:0.965223\n",
            "[72]\tvalidation_0-auc:0.96516\n",
            "[73]\tvalidation_0-auc:0.965055\n",
            "[74]\tvalidation_0-auc:0.96493\n",
            "[75]\tvalidation_0-auc:0.965244\n",
            "[76]\tvalidation_0-auc:0.96537\n",
            "[77]\tvalidation_0-auc:0.965265\n",
            "[78]\tvalidation_0-auc:0.965286\n",
            "[79]\tvalidation_0-auc:0.965118\n",
            "[80]\tvalidation_0-auc:0.965244\n",
            "[81]\tvalidation_0-auc:0.965275\n",
            "[82]\tvalidation_0-auc:0.965234\n",
            "[83]\tvalidation_0-auc:0.965139\n",
            "[84]\tvalidation_0-auc:0.96516\n",
            "[85]\tvalidation_0-auc:0.965192\n",
            "[86]\tvalidation_0-auc:0.965254\n",
            "[87]\tvalidation_0-auc:0.965317\n",
            "[88]\tvalidation_0-auc:0.965234\n",
            "[89]\tvalidation_0-auc:0.965254\n",
            "[90]\tvalidation_0-auc:0.965108\n",
            "[91]\tvalidation_0-auc:0.965108\n",
            "[92]\tvalidation_0-auc:0.964647\n",
            "[93]\tvalidation_0-auc:0.964751\n",
            "[94]\tvalidation_0-auc:0.96494\n",
            "[95]\tvalidation_0-auc:0.964961\n",
            "[96]\tvalidation_0-auc:0.96494\n",
            "[97]\tvalidation_0-auc:0.965024\n",
            "[98]\tvalidation_0-auc:0.965087\n",
            "[99]\tvalidation_0-auc:0.964605\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 4s 15ms/step - loss: 0.6530 - accuracy: 0.6005 - val_loss: 0.3557 - val_accuracy: 0.8621\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.4252 - accuracy: 0.8183 - val_loss: 0.3516 - val_accuracy: 0.8665\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3692 - accuracy: 0.8485 - val_loss: 0.3371 - val_accuracy: 0.8600\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 12ms/step - loss: 0.3633 - accuracy: 0.8491 - val_loss: 0.3082 - val_accuracy: 0.8709\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3344 - accuracy: 0.8618 - val_loss: 0.3154 - val_accuracy: 0.8687\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 13ms/step - loss: 0.5530 - accuracy: 0.6970 - val_loss: 0.3113 - val_accuracy: 0.8359\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3145 - accuracy: 0.8763 - val_loss: 0.2983 - val_accuracy: 0.8446\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3064 - accuracy: 0.8715 - val_loss: 0.4370 - val_accuracy: 0.8140\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3072 - accuracy: 0.8757 - val_loss: 0.3026 - val_accuracy: 0.8381\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3030 - accuracy: 0.8696 - val_loss: 0.3055 - val_accuracy: 0.8534\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.939132\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.939675\n",
            "[2]\tvalidation_0-auc:0.939675\n",
            "[3]\tvalidation_0-auc:0.954612\n",
            "[4]\tvalidation_0-auc:0.943325\n",
            "[5]\tvalidation_0-auc:0.954542\n",
            "[6]\tvalidation_0-auc:0.956622\n",
            "[7]\tvalidation_0-auc:0.954773\n",
            "[8]\tvalidation_0-auc:0.953422\n",
            "[9]\tvalidation_0-auc:0.955478\n",
            "[10]\tvalidation_0-auc:0.954669\n",
            "[11]\tvalidation_0-auc:0.951643\n",
            "[12]\tvalidation_0-auc:0.952093\n",
            "[13]\tvalidation_0-auc:0.952082\n",
            "[14]\tvalidation_0-auc:0.950314\n",
            "[15]\tvalidation_0-auc:0.950199\n",
            "[16]\tvalidation_0-auc:0.950522\n",
            "[17]\tvalidation_0-auc:0.950453\n",
            "[18]\tvalidation_0-auc:0.950753\n",
            "[19]\tvalidation_0-auc:0.951527\n",
            "[20]\tvalidation_0-auc:0.951989\n",
            "[21]\tvalidation_0-auc:0.951342\n",
            "[22]\tvalidation_0-auc:0.95021\n",
            "[23]\tvalidation_0-auc:0.949771\n",
            "[24]\tvalidation_0-auc:0.948963\n",
            "[25]\tvalidation_0-auc:0.948824\n",
            "[26]\tvalidation_0-auc:0.949309\n",
            "[27]\tvalidation_0-auc:0.949517\n",
            "[28]\tvalidation_0-auc:0.94991\n",
            "[29]\tvalidation_0-auc:0.949217\n",
            "[30]\tvalidation_0-auc:0.94887\n",
            "[31]\tvalidation_0-auc:0.948131\n",
            "[32]\tvalidation_0-auc:0.948293\n",
            "[33]\tvalidation_0-auc:0.948662\n",
            "[34]\tvalidation_0-auc:0.947392\n",
            "[35]\tvalidation_0-auc:0.947357\n",
            "[36]\tvalidation_0-auc:0.946964\n",
            "[37]\tvalidation_0-auc:0.945578\n",
            "[38]\tvalidation_0-auc:0.948466\n",
            "[39]\tvalidation_0-auc:0.949725\n",
            "[40]\tvalidation_0-auc:0.949448\n",
            "[41]\tvalidation_0-auc:0.949633\n",
            "[42]\tvalidation_0-auc:0.949563\n",
            "[43]\tvalidation_0-auc:0.949586\n",
            "[44]\tvalidation_0-auc:0.949702\n",
            "[45]\tvalidation_0-auc:0.94991\n",
            "[46]\tvalidation_0-auc:0.950049\n",
            "[47]\tvalidation_0-auc:0.950487\n",
            "[48]\tvalidation_0-auc:0.951365\n",
            "[49]\tvalidation_0-auc:0.951573\n",
            "[50]\tvalidation_0-auc:0.95125\n",
            "[51]\tvalidation_0-auc:0.950926\n",
            "[52]\tvalidation_0-auc:0.951262\n",
            "[53]\tvalidation_0-auc:0.950776\n",
            "[54]\tvalidation_0-auc:0.947819\n",
            "[55]\tvalidation_0-auc:0.946849\n",
            "[56]\tvalidation_0-auc:0.946733\n",
            "Stopping. Best iteration:\n",
            "[6]\tvalidation_0-auc:0.956622\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.05     | 0.8653061224489796 | 0.7236842105263158 | 0.8208955223880597 | 0.7692307692307693 |\n",
            "|      GRU 0.05     | 0.8795918367346939 | 0.7697841726618705 | 0.7985074626865671 | 0.7838827838827838 |\n",
            "|    XGBoost 0.05   | 0.889795918367347  | 0.7409638554216867 | 0.917910447761194  | 0.8200000000000001 |\n",
            "|    Logreg 0.05    | 0.8755102040816326 | 0.7517241379310344 | 0.8134328358208955 | 0.7813620071684588 |\n",
            "|      SVM 0.05     | 0.8816326530612245 |       0.7375       | 0.8805970149253731 | 0.8027210884353743 |\n",
            "|   LSTM beta 0.05  | 0.8687089715536105 | 0.8189655172413793 | 0.7089552238805971 |        0.76        |\n",
            "|   GRU beta 0.05   | 0.8533916849015317 | 0.7080745341614907 | 0.8507462686567164 | 0.7728813559322034 |\n",
            "| XGBoost beta 0.05 | 0.8555798687089715 | 0.7023809523809523 | 0.8805970149253731 | 0.7814569536423842 |\n",
            "|  logreg beta 0.05 | 0.8796498905908097 | 0.7687074829931972 | 0.8432835820895522 | 0.8042704626334519 |\n",
            "|   svm beta 0.05   | 0.8840262582056893 | 0.7547169811320755 | 0.8955223880597015 | 0.8191126279863482 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979
        },
        "id": "Hc5BQh76Ubow",
        "outputId": "058867ad-093e-4151-a39b-d2744db154ec"
      },
      "source": [
        "Result_purging.to_csv('DVN_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.503546</td>\n",
              "      <td>0.689796</td>\n",
              "      <td>0.482993</td>\n",
              "      <td>0.464052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.493333</td>\n",
              "      <td>0.683673</td>\n",
              "      <td>0.488449</td>\n",
              "      <td>0.483660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.496774</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.503268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.566038</td>\n",
              "      <td>0.716327</td>\n",
              "      <td>0.463320</td>\n",
              "      <td>0.392157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.493333</td>\n",
              "      <td>0.683673</td>\n",
              "      <td>0.488449</td>\n",
              "      <td>0.483660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.421687</td>\n",
              "      <td>0.680525</td>\n",
              "      <td>0.489510</td>\n",
              "      <td>0.583333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.404110</td>\n",
              "      <td>0.676149</td>\n",
              "      <td>0.443609</td>\n",
              "      <td>0.491667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.431034</td>\n",
              "      <td>0.684902</td>\n",
              "      <td>0.510204</td>\n",
              "      <td>0.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.438462</td>\n",
              "      <td>0.702407</td>\n",
              "      <td>0.456000</td>\n",
              "      <td>0.475000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.410596</td>\n",
              "      <td>0.678337</td>\n",
              "      <td>0.457565</td>\n",
              "      <td>0.516667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.493056</td>\n",
              "      <td>0.683673</td>\n",
              "      <td>0.478114</td>\n",
              "      <td>0.464052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.503448</td>\n",
              "      <td>0.689796</td>\n",
              "      <td>0.489933</td>\n",
              "      <td>0.477124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.503268</td>\n",
              "      <td>0.689796</td>\n",
              "      <td>0.503268</td>\n",
              "      <td>0.503268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.558559</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.469697</td>\n",
              "      <td>0.405229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.496732</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.496732</td>\n",
              "      <td>0.496732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.414815</td>\n",
              "      <td>0.687090</td>\n",
              "      <td>0.439216</td>\n",
              "      <td>0.466667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.724289</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.475000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.431034</td>\n",
              "      <td>0.684902</td>\n",
              "      <td>0.510204</td>\n",
              "      <td>0.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.438462</td>\n",
              "      <td>0.702407</td>\n",
              "      <td>0.456000</td>\n",
              "      <td>0.475000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.410596</td>\n",
              "      <td>0.678337</td>\n",
              "      <td>0.457565</td>\n",
              "      <td>0.516667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.05</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.723684</td>\n",
              "      <td>0.865306</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.820896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.05</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.769784</td>\n",
              "      <td>0.879592</td>\n",
              "      <td>0.783883</td>\n",
              "      <td>0.798507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.05</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.740964</td>\n",
              "      <td>0.889796</td>\n",
              "      <td>0.820000</td>\n",
              "      <td>0.917910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.05</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.751724</td>\n",
              "      <td>0.875510</td>\n",
              "      <td>0.781362</td>\n",
              "      <td>0.813433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.05</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.737500</td>\n",
              "      <td>0.881633</td>\n",
              "      <td>0.802721</td>\n",
              "      <td>0.880597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.05</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.818966</td>\n",
              "      <td>0.868709</td>\n",
              "      <td>0.760000</td>\n",
              "      <td>0.708955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.05</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.708075</td>\n",
              "      <td>0.853392</td>\n",
              "      <td>0.772881</td>\n",
              "      <td>0.850746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.05</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.702381</td>\n",
              "      <td>0.855580</td>\n",
              "      <td>0.781457</td>\n",
              "      <td>0.880597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.05</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.768707</td>\n",
              "      <td>0.879650</td>\n",
              "      <td>0.804270</td>\n",
              "      <td>0.843284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.05</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.754717</td>\n",
              "      <td>0.884026</td>\n",
              "      <td>0.819113</td>\n",
              "      <td>0.895522</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0          LSTM 0.15  DVN  0.503546  0.689796  0.482993  0.464052\n",
              "1           GRU 0.15  DVN  0.493333  0.683673  0.488449  0.483660\n",
              "2       XGBoost 0.15  DVN  0.496774  0.685714  0.500000  0.503268\n",
              "3        Logreg 0.15  DVN  0.566038  0.716327  0.463320  0.392157\n",
              "4           SVM 0.15  DVN  0.493333  0.683673  0.488449  0.483660\n",
              "5     LSTM beta 0.15  DVN  0.421687  0.680525  0.489510  0.583333\n",
              "6      GRU beta 0.15  DVN  0.404110  0.676149  0.443609  0.491667\n",
              "7  XGBoost beta 0.15  DVN  0.431034  0.684902  0.510204  0.625000\n",
              "8   logreg beta 0.15  DVN  0.438462  0.702407  0.456000  0.475000\n",
              "9      svm beta 0.15  DVN  0.410596  0.678337  0.457565  0.516667\n",
              "0           LSTM 0.1  DVN  0.493056  0.683673  0.478114  0.464052\n",
              "1            GRU 0.1  DVN  0.503448  0.689796  0.489933  0.477124\n",
              "2        XGBoost 0.1  DVN  0.503268  0.689796  0.503268  0.503268\n",
              "3         Logreg 0.1  DVN  0.558559  0.714286  0.469697  0.405229\n",
              "4            SVM 0.1  DVN  0.496732  0.685714  0.496732  0.496732\n",
              "5      LSTM beta 0.1  DVN  0.414815  0.687090  0.439216  0.466667\n",
              "6       GRU beta 0.1  DVN  0.475000  0.724289  0.475000  0.475000\n",
              "7   XGBoost beta 0.1  DVN  0.431034  0.684902  0.510204  0.625000\n",
              "8    logreg beta 0.1  DVN  0.438462  0.702407  0.456000  0.475000\n",
              "9       svm beta 0.1  DVN  0.410596  0.678337  0.457565  0.516667\n",
              "0          LSTM 0.05  DVN  0.723684  0.865306  0.769231  0.820896\n",
              "1           GRU 0.05  DVN  0.769784  0.879592  0.783883  0.798507\n",
              "2       XGBoost 0.05  DVN  0.740964  0.889796  0.820000  0.917910\n",
              "3        Logreg 0.05  DVN  0.751724  0.875510  0.781362  0.813433\n",
              "4           SVM 0.05  DVN  0.737500  0.881633  0.802721  0.880597\n",
              "5     LSTM beta 0.05  DVN  0.818966  0.868709  0.760000  0.708955\n",
              "6      GRU beta 0.05  DVN  0.708075  0.853392  0.772881  0.850746\n",
              "7  XGBoost beta 0.05  DVN  0.702381  0.855580  0.781457  0.880597\n",
              "8   logreg beta 0.05  DVN  0.768707  0.879650  0.804270  0.843284\n",
              "9      svm beta 0.05  DVN  0.754717  0.884026  0.819113  0.895522"
            ]
          },
          "metadata": {},
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh962R4yUbow"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvl0pxlkYiNp"
      },
      "source": [
        "## RMD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knMZBC-_YiNv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "outputId": "8fddc958-047e-4f6f-b893-876b8823186c"
      },
      "source": [
        "dfs = pd.read_csv(\"RMD.csv\")\n",
        "# dfs = dfs[::-1].reset_index()\n",
        "# Denoise prices\n",
        "dfs = denoise_data(dfs, '<CLOSE>', 50)\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>US1.RMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>32.830</td>\n",
              "      <td>32.870</td>\n",
              "      <td>32.28</td>\n",
              "      <td>32.468818</td>\n",
              "      <td>416669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>US1.RMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>32.650</td>\n",
              "      <td>33.205</td>\n",
              "      <td>32.65</td>\n",
              "      <td>32.468818</td>\n",
              "      <td>653616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>US1.RMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>33.240</td>\n",
              "      <td>33.460</td>\n",
              "      <td>33.16</td>\n",
              "      <td>32.468818</td>\n",
              "      <td>589273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>US1.RMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>33.330</td>\n",
              "      <td>33.330</td>\n",
              "      <td>32.59</td>\n",
              "      <td>32.468818</td>\n",
              "      <td>328524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US1.RMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>32.650</td>\n",
              "      <td>33.030</td>\n",
              "      <td>32.43</td>\n",
              "      <td>32.468818</td>\n",
              "      <td>689901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>US1.RMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>277.170</td>\n",
              "      <td>277.170</td>\n",
              "      <td>268.44</td>\n",
              "      <td>279.909800</td>\n",
              "      <td>7537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>US1.RMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>265.150</td>\n",
              "      <td>266.290</td>\n",
              "      <td>262.26</td>\n",
              "      <td>279.909800</td>\n",
              "      <td>15954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>US1.RMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>264.905</td>\n",
              "      <td>266.600</td>\n",
              "      <td>263.85</td>\n",
              "      <td>279.909800</td>\n",
              "      <td>15648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>US1.RMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>268.770</td>\n",
              "      <td>269.040</td>\n",
              "      <td>263.96</td>\n",
              "      <td>279.909800</td>\n",
              "      <td>7497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>US1.RMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>261.430</td>\n",
              "      <td>263.250</td>\n",
              "      <td>258.43</td>\n",
              "      <td>279.909800</td>\n",
              "      <td>12671</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2769 rows Ã— 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     <TICKER> <PER>    <DATE>  <TIME>  ...   <HIGH>   <LOW>     <CLOSE>   <VOL>\n",
              "0     US1.RMD     D  20101004       0  ...   32.870   32.28   32.468818  416669\n",
              "1     US1.RMD     D  20101005       0  ...   33.205   32.65   32.468818  653616\n",
              "2     US1.RMD     D  20101006       0  ...   33.460   33.16   32.468818  589273\n",
              "3     US1.RMD     D  20101007       0  ...   33.330   32.59   32.468818  328524\n",
              "4     US1.RMD     D  20101008       0  ...   33.030   32.43   32.468818  689901\n",
              "...       ...   ...       ...     ...  ...      ...     ...         ...     ...\n",
              "2764  US1.RMD     D  20210927       0  ...  277.170  268.44  279.909800    7537\n",
              "2765  US1.RMD     D  20210928       0  ...  266.290  262.26  279.909800   15954\n",
              "2766  US1.RMD     D  20210929       0  ...  266.600  263.85  279.909800   15648\n",
              "2767  US1.RMD     D  20210930       0  ...  269.040  263.96  279.909800    7497\n",
              "2768  US1.RMD     D  20211001       0  ...  263.250  258.43  279.909800   12671\n",
              "\n",
              "[2769 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAAHiCAYAAADrvQoIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXzU1b3/8ffJJCGEJIQECBA22SI7KKIItqhYrcst1Vat2trWn1u1tlpt3Vr1Wr3VUm1diqVVxKrV2mJVtLVaXKuCgGwBAyEsAZJA9j2Z5fz+yMw4IQnZZjKTzOv5eOTBzHc5cybeXt98/HzPMdZaAQAAANEuJtwTAAAAACIBwRgAAAAQwRgAAACQRDAGAAAAJBGMAQAAAEkEYwAAAEASwRgAws4Yc6kx5t8duO4JY8zPe2hOe4wxi3riswAgUhjWMQaApiAoKUOSS5Jb0jZJz0haZq31hHFqYeH9ffw/a+3bHbjWSpporc0N+cQAIISoGAPAF86z1iZLGiPpV5J+JunJ8E4JANBTCMYAcARrbYW19lVJF0m63BgzTZKMMf2MMUuMMfuMMUXe1ob+3nMLjTH7jTE/McYcMsYUGGO+5xvTGDPQGPOMMeawMWavMeZOY0yM99x3jTEfel8bY8zD3jEqjTFbAj7/aWPMLzv4eenGmNe8Y3xqjPml7zNaY4z5tndeJcaYO444N9cY87Exptz7OY8ZY+K95973XrbJGFNtjLnIGDPIGLPK+13LvK9HdvsfDACEGMEYANpgrV0rab+kU7yHfiVpkqRZkiZIypT0i4Bbhkka6D1+haTHjTGDvOce9Z4bJ+nLkr4j6Xtq6SuSvuT9nIGSLpRU0sYUj/Z5j0uq8V5zufenVcaYKZKWSvq2pBGS0iUFBlm3pBslDZY0T9Lpkn4gSdbaL3mvmWmtTbLWvqimf7csV1PlfbSkOkmPtfX5ABApCMYAcHQHJaUZY4ykqyTdaK0ttdZWSbpf0sUB1zol/a+11mmtfUNStaQsY4zDe91t1toqa+0eSb9RUxA9klNSsqRj1fQcyHZrbUEbczva510g6S5rba21dpukFUf5jt+QtMpa+761tkHSzyX5+6qtteuttZ9Ya13euf9BTeG+VdbaEmvt372fXSXpvqNdDwCRIjbcEwCACJcpqVTSEEmJktY3ZWRJkpHkCLi2xFrrCnhfKylJTZXWOEl7A87t9Y7djLV2tTHmMTVVfMcYY1ZKutlaW9nK3Nr6vCFq+v/v+QHnAl8faUTgeWttjTHGX6U2xkyS9JCkOWr6HcRKWt/WYMaYREkPSzpLkq+CnWyMcVhr3UeZBwCEFRVjAGiDMeYENYXXDyUVq6klYKq1NtX7M9Bam9SBoYrVVN0dE3BstKQDrV1srX3EWnu8pClqaqm4pZNTP6ym1TUC2yFGHeX6gsDz3mCbHnB+qaTP1bTyRIqk29X0l4K2/ERSlqQTvdf72i2Odg8AhB3BGACOYIxJMcacK+kFSc9aa7d4l2z7o6SHjTFDvddlGmPObG88b5X0r5LuM8YkG2PGSLpJ0rOtfPYJxpgTjTFxauoRrldAW0NHeD9vpaS7jTGJxphj1dTT3Ja/STrXGLPA+1Dd/6r5vx+SJVVKqvaOde0R9xepqXc68Po6SeXGmDRJd3Vm/gAQLgRjAPjCa8aYKjW1FdyhpvaBwAfkfiYpV9InxphKSW+rqTLaET9UU9DNU1MF+nlJT7VyXYqaAniZmtotSiT9utPfRLpeTQ/mFUr6s6S/SGpo7UJrbbak67xzKvB+9v6AS26WdImkKu/cXjxiiLslrfCuWnGhpN9K6q+mSvknkv7VhfkDQI9jgw8AiALGmAckDbPWtrk6BQBEOyrGANAHGWOONcbM8K6LPFdNy7m9HO55AUAkY1UKAOibktXUPjFCTT3Av5H0SlhnBAARjlYKAAAAQLRSAAAAAJIIxgAAAICkCOkxHjx4sB07dmy4pwEAAIA+bv369cXW2iGtnYuIYDx27FitW7cu3NMAAABAH2eM2dvWOVopAAAAABGMAQAAAEkEYwAAAEASwRgAAACQRDAGAAAAJBGMAQAAAEkEYwAAAEASwRgAAACQRDAGAAAAJBGMAQAAAEkdCMbGmARjzFpjzCZjTLYx5h7v8WOMMWuMMbnGmBeNMfHe4/2873O958eG9isAAAAA3deRinGDpNOstTMlzZJ0ljHmJEkPSHrYWjtBUpmkK7zXXyGpzHv8Ye91AAAAQERrNxjbJtXet3HeHyvpNEl/8x5fIWmx9/XXvO/lPX+6McYEbcYAAABACHSox9gY4zDGbJR0SNJbknZJKrfWuryX7JeU6X2dKSlfkrznKySlB3PSAAAAQLB1KBhba93W2lmSRkqaK+nY7n6wMeYqY8w6Y8y6w4cPd3c4AAAAoFs6tSqFtbZc0juS5klKNcbEek+NlHTA+/qApFGS5D0/UFJJK2Mts9bOsdbOGTJkSBenDwAAAARHR1alGGKMSfW+7i/pDEnb1RSQv+G97HJJr3hfv+p9L+/51dZaG8xJAwAAAMEW2/4lGi5phTHGoaYg/Vdr7SpjzDZJLxhjfinpM0lPeq9/UtKfjTG5kkolXRyCeQMAAKAXcDqdysnJ0ahRozRw4MBwT+eo2g3G1trNkma3cjxPTf3GRx6vl/TNoMwOAAAAvVp9fb0kqbS0NOKDMTvfAQAAIGQqKyslSQ0NDWGeSfsIxgAAAAgZj8cjSXK5XO1cGX4EYwAAAISMLxDHxnbk0bbwIhgDAAAgZNxutySpN2yETDAGAABAyPhW7e0Nq/cSjAEAABAyvkDscrn81eNIRTAGAABAyARWiouLi8M4k/YRjAEAABAyvlUpJFExBgAAQHSqra1tFoYjvc+YYAwAAICQqK6uDvcUOoVgDAAAgJA4skJMKwUAAACi0pHBuLKyUocOHQrTbNpHMAYAAEBI+ILxmDFj/MdKSkrCNZ12EYwBAAAQEi6XS/Hx8UpOTvYfi+Qd8AjGAAAACLrGxkZVVFTI4XCEeyodRjAGAABA0O3atUuSVFdXJ0mKjY2VRMUYAAAAUca3AkVcXFyzPwM3/Ig0BGMAAACETEZGhiQpKSlJkhQTE7nxM3JnBgAAgD5j6NChcjgcio+PD/dU2kQwBgAAQMgZY5SQkEArBQAAAKJH4MYegatSGGNabPoRSQjGAAAACCrfg3eJiYn+3mKpqb+YYAwAAICo0dDQIElKTU1ttjxbpFeMY8M9AQAAAPQt+fn5stbqjjvu0Ntvv+0Pwy6XS9ZaxcXF6YMPPtDw4cPDPNPmCMYAAAAIKmOMVq5cqSeeeEKnn366hg0bJkmqra2Vy+VSSkpKRK5OQTAGAABA0Hg8HsXHx+vdd99VVlaW3nrrLX87xcGDB1VRUaHJkyeHeZato8cYAAAAQZOTk6Oqqipt2LBBp5xySq/qMSYYAwAAIGjcbrfy8vJUWVmp+fPnNztnjGEdYwAAAESPzz77TJJaDcaSIrZqTDAGAABAUG3cuFHp6emaMGFCs+MxMU3Rk2AMAACAqLBx40bNnj27WX+xRMUYAAAAUaS4uFj79u3TvHnzWpwjGAMAACAqWGu1detWSSIYAwAAIHoVFhZq165dktTqWsUEYwAAAESFkpIS5eXlaejQoUpLS2tx3heMI3XJNoIxAAAAus3lckmSdu/erXHjxik5ObnFNaxKAQAAgD7P7XbLWqvdu3crKyurxYoUEq0UAAAAiALWWh0+fFjV1dWaOHFiq9cQjAEAANDneTwe5eXlSVKLjT18CMYAAADo89xut3bv3i1JGj9+fKvXEIwBAADQ5/kqxklJScrIyGj1GoIxAAAA+jyPx6Pdu3frmGOOUWxsbKvXsFwbAAAA+jxfK8XkyZOpGAMAACB6lZeX69ChQ5o9e7Z/veIjsY4xAAAA+rycnBxJrW8F7eOrGBcVFfXInDqLYAwAAIBu60wwdrvdPTKnziIYAwAAoNt27NihuLg4jRs3rs1rHA5Hsz8jDcEYAAAA3bZr1y6NHTu2zRUpfNLS0npoRp1HMAYAAEC37dq1q80d7wI5HA653e6IfACPYAwAAIBuaWxs1L59+zoUjH0VZZfLFeppdRrBGAAAAN2Sm5srt9vdoWA8YMAADR8+vM0l3cLp6E0gAAAAQDu2b98uSZo0aVK71yYkJCghISHUU+qSyIvqAAAA6FWys7MlqUMV40hGMAYAAEC35OTkKCMjQ6mpqeGeSrcQjAEAANAt+/btU2ZmphITE8M9lW4hGAMAAKBb9u3bF7EP1HVG7549AAAAwsrtduvgwYMaMWJEu5t7RDqCMQAAALrs4MGDcrlcVIwBAAAQ3fbt2ydJGjFiBMEYAAAA0Wvv3r2SpJEjR4Z5Jt1HMAYAAECX+SrGHdncI9IRjAEAANBl+fn5SklJUUpKSrin0m0EYwAAAHTZvn37NHTo0Ijd5rkz2g3GxphRxph3jDHbjDHZxpgfeY/fbYw5YIzZ6P05O+Ce24wxucaYHGPMmaH8AgAAAAiP/Px87dmzRxkZGYqPjw/3dLqtI4vNuST9xFq7wRiTLGm9MeYt77mHrbVLAi82xkyRdLGkqZJGSHrbGDPJWusO5sQBAAAQXhUVFSoqKtKkSZN6/YoUUgcqxtbaAmvtBu/rKknbJWUe5ZavSXrBWttgrd0tKVfS3GBMFgAAAJHD6XSquLhYQ4cOjY5gHMgYM1bSbElrvIeuN8ZsNsY8ZYwZ5D2WKSk/4Lb9OnqQBgAAQC9TUVGhkpISWWujLxgbY5Ik/V3Sj621lZKWShovaZakAkm/6cwHG2OuMsasM8asO3z4cGduBQAAQJg1NDSoqKhIkpSRkRE9wdgYE6emUPyctXalJFlri6y1bmutR9If9UW7xAFJowJuH+k91oy1dpm1do61ds6QIUO68x0AAADQw4wxOnTokCRp2LBhMsaEeUbd15FVKYykJyVtt9Y+FHB8eMBlX5e01fv6VUkXG2P6GWOOkTRR0trgTRkAAACRwFcx7gube0gdW5VivqRvS9pijNnoPXa7pG8ZY2ZJspL2SLpakqy12caYv0rapqYVLa5jRQoAAIC+pa6uTocOHVJ8fLwmTJgQ7ukERbvB2Fr7oaTWauNvHOWe+yTd1415AQAAIEI1NjaqsrJSRUVFyszM7BObe0jsfAcAAIBOqq6ulrVWb7zxhkaOHBnu6QQNwRgAAACdcvDgQZWVlUlqWpGiryAYAwAAoNMKCwslSZdcckmYZxI8BGMAAAB0SmxsrH9FilGjRrVzde9BMAYAAECnxMbGqry8XJLoMQYAAED0crlcKiwsVFxcnIYOHRru6QQNwRgAAACd4na7VVBQoJEjR/aJraB9+s43AQAAQMh5PB5Za1VQUNCn+oslgjEAAAA6wbcaha9i3JcQjAEAANBhpaWlkqSysjINGTIkzLMJLoIxAAAAOsXj8aiqqkopKSnhnkpQEYwBAADQKXV1dbLWEowBAAAQ3aqqqiRJycnJYZ5JcBGMAQAA0Cm+PuOMjIwwzyS4CMYAAADoEJfLJUlyOp2SpBEjRoRzOkFHMAYAAECHeDweSdKhQ4ckSZmZmeGcTtARjAEAANAhvmBcWFiomJgYWikAAAAQnay1kpo298jIyFBsbGyYZxRcBGMAAAB0iK9i3Bd3vZMIxgAAAOggX8X4wIEDBGMAAABEr8BWCoIxAAAAopbH41Ftba3Ky8v73IoUEsEYAAAAnVBUVCRJVIwBAAAQvay1BGMAAABA6rube0gEYwAAAHSQtVYFBQWSCMYAAACIcgcPHtSwYcPUv3//cE8l6AjGAAAA6BBrrQ4ePKjRo0eHeyohQTAGAABAhx08eFBjxowJ9zRCgmAMAACADnG73Tp48KDGjh0b7qmEBMEYAAAAHVJUVCSXy0UrBQAAAKLbvn37JImKMQAAAKJbfn6+JIIxAAAAohwVYwAAAPQ69fX1OnjwoKy1QRszPz9faWlpSkxMDNqYkYRgDAAA0Afl5uaqtLRUbrc7aGPm5+drxIgRMsYEbcxIQjAGAADoJUpLS1VdXd2pe4JdMR4xYkTQxos0BGMAAIBe4uDBg9qzZ89Rr6msrNThw4f974MVjN1ud5+vGMeGewIAAAAIHt8DcsGWm5urxsZGjR8/PiTjRwIqxgAAAH1YsCrGmzZtkiRlZWUFZbxIRDAGAADo5Wpra1VTU9PquWAGY4fDoXHjxgVlvEhEMAYAAOjl8vLytHv37lbPBSsYb968WePGjVNaWlpQxotEBGMAAIBeIJirS3TFpk2blJWVpdjYvvuIGsEYAACgj2gtPJeUlHR73NLSUuXn5ysrK6vPrkghEYwBAAB6hcDQe+DAgVav8Xg8LY6Vl5d3+7O3bNkiqW8/eCcRjAEAAHqFwGBcVlbW6jX19fUh+ezAFSmoGAMAACCsOtJj3NYDeN21adMmDR48WIMHDyYYAwAAILxcLlez9x19GC8lJaXbn71582bNnDlTkgjGAAAACC+n09nsva+f+PDhw7rpppt05ZVXqqCgoNk1MTExiouL69bnulwubd26VTNnzpS1lmAMAACA8HK73c3e19XV6bPPPtP06dP11ltv6ZNPPtFPfvITVVRUBPVzd+7cqfr6en8w7ssIxgAAAL2Ar0Lcr18/SdKePXv06quvqqioSLfddpseeugh5eTk6IorrvAv0WaM6XSYbWxsbLaL3ubNmyVJ06dPl9RUhe6r+u43AwAA6EN8wXj06NH+Yxs2bNCgQYP0rW99S2eccYYef/xx7d27V/fee6+OOeaYLn3Ozp07tXv3bv/nbdq0SbGxsf6l2milAAAAQFj5gmp8fLwSEhIkSevXr9dxxx3nD6snnXSSfvGLX+g///mP/vSnP3WpYuy7vrGxUVJTMJ48ebLi4+MlUTEGAABAmHk8HhljZIxRbGysCgsLdeDAAR1//PGSpMzMTEnSddddp/POO0+33HKLPvvss059RmCI9vU0b968WTNmzPAHcyrGAAAACKvAFSGMMdqwYYMk6bjjjpMkJSYmatq0aUpJSdGKFSs0ZMgQLVmypNXd8NpSW1vrf+12u1VaWqr9+/c3e/COijEAAADC6sil0jZs2KABAwb4e39jY2P95wYNGqRbbrlFGzZsUF5eXofGdzqd2rNnj/+9x+Px73g3c+ZMKsYAAACIDEdWjNevX69Zs2b5A/GRldzzzjtPkvTvf/+7Q33GOTk5za6z1vpXpJgxY4a/tSIwgPc1BGMAAIBeIDAYl5eXKzc3VyeccIL//JGV3HHjxikzM1MfffSRKisru/SZmzZt0tChQ1VcXOyvJnd3w5BIRjAGAADoAY2NjTp06FCLjTo6KrCa62tx+MpXvtLm9cYYLViwQGvWrPGvMNFZvhUpAhGMAQAA0C07duzQoUOHWmzb3FHWWn+7xMaNGxUfH6+FCxce9Z758+errq5OH3/8cYc/x+FwSGrqOc7OztaUKVOanafHGAAAAF0WWO0tLy/v9hgffvihTjzxRPXv3/+o95x44omKjY3VW2+91eHPGTp0qCQpNzdXDQ0Nmjp1apfm2xsRjAEAAELsyCXTutJO4esxrq6u1vr163XKKadIksaOHatx48a1ek9iYqKOP/54vfnmm+2O73uoLiUlRZKUnZ0tSS1aKfoygjEAAECIHRmMCwsLOz2GLxh/8skncrvd+tKXviRJSkpKUmJiYpv3LViwQDt37tTevXuPOr4xRqmpqf5WiXXr1ikxMVETJkyQMUYJCQkaM2ZMp+fdmxCMAQAAQuzIYFxWVtbpMXzB+MMPP1RMTIzmzZvXoftOO+00SdLKlSuPep3b7W625NvatWs1d+5cORwOORwOTZgwQcnJyZ2ed2/SbjA2xowyxrxjjNlmjMk2xvzIezzNGPOWMWan989B3uPGGPOIMSbXGLPZGHNcqL8EAABAJOvqShSBfMF43bp1mjJlir/loT2jR4/WpEmT9PLLL8vlcrU5tsfj8T94V1tbqy1btujkk0/2b0UdDTpSMXZJ+om1doqkkyRdZ4yZIulWSf+x1k6U9B/ve0n6qqSJ3p+rJC0N+qwBAAB6EV/F+JhjjtHAgQP9AbQzfKtSbNiwwb8NdHvS09MlSaeccoo++ugjffrppyopKWlzfg6HQ8YYffbZZ3K73Tr55JObrYbR17X7La21BdbaDd7XVZK2S8qU9DVJK7yXrZC02Pv6a5KesU0+kZRqjBke9JkDAAD0Er6KcUxMTJdCsdQUjA8fPqyCggLNnj27Q/cMHz5cI0aM0IIFC+R2u7VmzZpWg7Fvfr65/e1vf1NqaqpOP/10eTwegnFrjDFjJc2WtEZShrXWtxBfoaQM7+tMSfkBt+33HjtyrKuMMeuMMesOHz7cyWkDAAD0HoEV2e6MsW3bNknqcMVYkhISEjRz5kwlJSXpww8/bHWJt8BgnJ2drbffflvf+973lJCQQCtFa4wxSZL+LunH1tpm+wrapoX12t+Eu/k9y6y1c6y1c4YMGdKZWwEAAHoVXzDuTuXVWqutW7dKkmbNmtXh+xITEzVw4EDNmzdPH3zwgX9ZtkCBwfi3v/2t+vfvr6uvvtr/uVSMAxhj4tQUip+z1voeaSzytUh4/zzkPX5A0qiA20d6jwEAAESlwFYKqflmHR1lrVV2drYmTpzY4QfvfBwOh+bPn69Dhw7p888/b3N+ZWVlev7553XeeecpLS1NkqgYBzJNv4knJW231j4UcOpVSZd7X18u6ZWA49/xrk5xkqSKgJYLAACAqHPoUFP9MCYmpssh01qrLVu2dLi/+Ejz58+XJK1evbrFOd9qFStWrFBDQ4MuueSSZp9LxfgL8yV9W9JpxpiN3p+zJf1K0hnGmJ2SFnnfS9IbkvIk5Ur6o6QfBH/aAAAA0aW8vFz5+fmd6i8ONGzYME2cOLHVYFxTUyOn06knnnhCp556qsaPH++vakfTw3ctm0yOYK39UFJbf7U5vZXrraTrujkvAACAPqErbROtjbF9+3ZJnXvw7kgLFizQM888o6qqKiUnJ2vlypW65557tGTJEq1atUr79+/Xs88+67/e6XTK5XLRSgEAAIDua21zj86G5cBg3NVWCqlpPWO3263bb79deXl5uuyyy7R582ZdeOGFWrp0qS699FL/VtOVlZXKycmR1L2HBnuT6PiWAAAAYeJbkSIzs2n12q5UX33BODMzU4MHD+7yXI4//niNHTtWjz32mLKyshQfH6/nnntOxx13nC666CI99NBD/vnV19f774uWinG7rRQAAADoGmutf0ON7oRLXzCeMWNGl+73LdEWExOju+66S9dcc41OO+00PfLII4qPj9eMGTM0btw4JSYmtnp/MLa07g0IxgAAACFSVFQUlGBcWVmpPXv26MILL+zS/cOHD1diYqIOHDigOXPmqKioSAMHDpQk7d+/X5Ja3fjDp1+/fl363N6GVgoAAIAQKSsr87/uTjDetGmTrLWaOXNml+6PiYnRoEGD/AHX197h63WOjY1tc34Oh8O/pnFfR8UYAAAgBDweT7MWBF/wNMZ0+uG7jRs3SurcjnetOeaYY/T555/7t5f2BeS4uLg27xk5ciQ9xgAAAOi6xsbGZu+7Ey4/++wzpaWlacSIEd2ak8PhkNTUM+wLxVLTsmytmTZtWrc+r7ehlQIAACDCbdy4UZMnT+72smnGGMXExPh3ukNzBGMAAIAQCKzI+nz44YdavXp1p1opdu3apS1btmjy5MlBmZcxptW5gWAMAAAQEr7+Yl+V94UXXtApp5yiSy65RP/4xz86HI7//e9/S5JOPvnkoPT6EozbRjAGAAAIAV/4dDgcqqmp0W233aYTTjhBWVlZeuyxxzrczrBp0yalpqZqzpw5QZmXMabFusSjR48Oyti9HcEYAAAgBHzBODk5WatWrVJxcbEeeeQR3XrrrTp06JC/EtyeTZs2adq0aTLGBG11iCMrxq2tSpGQkBCUz+pNCMYAAAAh4KsIDx06VP/85z81Y8YMnXjiiVq0aJHS0tL01FNPtTuGx+PRli1bNH369KDNKyYmpkXF2Lcznk9WVpbGjRsXtM/sLQjGAAAAIVBUVCRJys7O1vr16/X9739fxhjFx8fr3HPP1WuvvabDhw8fdYxdu3appqbGv2xasCrGgUvJDR8+vEXFOC4urtsrYPRG0feNAQAAetADDzygxMREXXrppZKawu3Xv/51OZ1OPfvss0e917exRzDXEz4yXB9tK+hoQzAGAAAIkR07duiFF17Qj370Iw0ePNh/fMKECTrxxBP15JNPHnV1ik2bNsnhcPgfjgtGxbi+vr7Z+yPbKKIZwRgAACAE4uPj9cILLygpKUk333xzi/Pf/va3lZ2drZycnDbH2LRpk7KyslRdXR2SOQ4fPlzx8fEhGbs3IhgDAACEgMfj0fr163XaaacpLS3Nf9xX9T3ppJMkSe+9916bY2zatEkzZ85scW8wpKSkKD09PWjj9QUEYwAAgBCoqqpSXl5em+sPx8fHa/DgwXrnnXe0detW1dbWNjtfUlKi/Px8ZWVlhWR+0fhwXXv4jQAAAASZtVZbt26VJB1//PGtXmOM0Zw5c/T+++/LWquysrJm5z/99FNJ0vjx45vd012+6nUwq899BcEYAAAgyOrq6rRt2zZJbQdj37mCggIdOHCgRVBds2aNjDFBXcNYkr+nmIpxS/xGAAAAgsztdmvbtm3KzMzU0KFDm50LDMC+Not169a1GGPt2rU69thj5XA4Wr23q3y73lExbolgDAAAEGROp1NvvPGGZs+efdTrxo0bp9TU1BbB2FqrtWvXaurUqc2OByPM+paHo2LcEr8RAACAIPvggw8kSbNmzTrqdTExMZozZ46/n9hn9+7dKi4ubrYihaSjrnncUb4NPRITE7s9Vl9DMAYAAAgy3451119/fbvXzp07VwcPHtTevXv9x9auXStJ/opzcnKyhgwZ0mLr5q5ISUnRscceq6SkpG6P1dcQjAEAAIJs8+bNGj58eLPd7toyd+5cSdLHH3/sP/bJJ58oISFB48aN04ABAzRmzBhlZGQErS+Y3e5aRzAGAAAIsq1bt+rYY2/FmokAACAASURBVI9tNcgeeWzcuHFKT0/Xf//7X/+x9957TyeeeKLcbrcSEhJCPl80IRgDAABIKigo0NatW+V2u7s1jtvt1u7du3XMMcd0qMJrjNHcuXP10UcfyVqrkpISbdy4UQsWLJAkpaamdms+6DiCMQAAgJp2mpPUYqONztq/f7+cTqdGjRrV4daHk046SYWFhXriiSf8G3osXLhQkpot14bQosEEAAAggG+d367Kzc2VJI0ePbrV84FhOSsrS8YYffnLX5Yk/eAHP1B8fLyuvvpqzZw5U0VFRQTjHkQwBgAAUS8wDDudTm3dulWjRo3SwIEDOz1Wdna2pOZbOQcKXD/Yt8rE4MGDddlllyk2NlZLlixRenq6ioqKWlyP0CIYAwCAqJefn+9/XVtbK0kqLS3tUjBev3690tPTlZaW1ur51torrLX62c9+JumLLZvdbrccDgc71PUg/goCAACiXlVVlf+1bxONrm6msXbtWk2fPr3NQNte0HW5XKqvr/cHY/QcKsYAACDqGWNaBOKuBOPy8nJ9/vnnOuOMM9q8xrexRltrCR84cMD/2rdLHXoGwRgAAES9hIQENTQ0KD4+Xi6XS1LXgvG6deskSdOmTVN6enqr1xhjNG3atA6NR39xz+K3DQAAop7T6VRKSopiYmL8D+J1JRj7tnKeOnVqpzbmSExMbPU4rRQ9i2AMAACimtvtlsvlUr9+/WSM6XYwnjhxogYOHNipau/YsWOVlZXV4rjT6ez0HNB1BGMAABDVfK0TsbGxzcJsR4KxtbZZT/KaNWt0/PHHS2r/IbtAMTEx/qXbjhwfPYdgDAAAopqvQuxwODodjLdv3+7f0GP//v0qLCzUnDlzJHWtP3jUqFHN3tNj3LP4bQMAgKjmC8YxMTGdDqIej0cNDQ2SvugvPu644/zjddaRvcYE457FbxsAAEQ1t9stqSmEBrY/dLaNYe3atYqLi/OvONGVjTliY2P9y7lJUkpKSqfHQNexXBsAAIhqjY2Nkpp2nOtsK4XPoUOHtGbNGs2aNct/rCvB2BijsWPHqrKyUtXV1Ro0aFCnx0DXUTEGAABRzel0KiYmRrGxsc3CrK/FoiMOHDigdevW6bjjjlNRUZGk7rVBpKSkaMSIEWwH3cOoGAMAgKhVWVmpkpKSbo+zceNG1dTUaMGCBf5j9Af3PvwTAwAAUcla6w/FvupwcnKy/3xr1dq6ujpt3brV337h89FHH8nhcDQLxlR7ex+CMQAAiEr79+9XcXGxvv71r2vBggXKzs5uttNcfHx8i3sqKiokSWVlZc2Of/TRR5o3b16zh+UIxr0PwRgAAESliooKPf3008rNzVVVVZWmTZummpoa//nWHr4rLi5uca60tFTbt2/XmWee2ew4wbj3IRgDAICotH79ei1dulRnnnmmrrjiCknS+++/7z9/ZDAOfF9WVuZfv3j16tWy1urMM89sVkkmGPc+BGMAABB1rLW67777lJGRoTvuuENXX321YmJitGHDBv81Tqezzfvdbrfy8vIkSS+//LLGjx+vWbNmqb6+PuRzR+gQjAEAQNTZsmWLdu7cqauuukqDBg1S//79NXnyZP/udT7Z2dn+B+2OXL7N7XZr586d2rx5sxYvXqycnJwemz9Cg2AMAACizssvvyxjjE499VT/sTPPPFNvvvmmtm/f7j9mrVVBQYH/9ZGeeeYZ9e/fX4sXLw79pBFyBGMAABB1/vGPf2jGjBkaPHiw/9g111yjuLg43Xnnna22Ufi2jvbZv3+/Vq1ape9+97tKTU0N+ZwRegRjAAAQVfLz87Vx40adeuqpGjdunP/4xIkT9de//lU7duzQihUr/Md9leIDBw40G+f3v/+94uLidOutt2rgwIE9M3mEFMEYAABElVdffVWSdOqpp/rXLfatWbx48WItWrRIS5cu1csvv9ysr7i2ttb/uqqqSv/617+0ePFijR492n9dWlpaT30NhADBGAAARJVXXnlFY8eO1bhx4xQfH69jjjmmWeV4+fLlGjFihH7xi1/o4osvVk1NjVwuV7Mx3n33XTmdTp199tmSvqgqJyUlSVKzjULQexCMAQBA1CgvL9c777zjf+jOGKMBAwYoNjbWf83IkSP1zDPP6Morr9T27du1fPnyFv3Fa9euVVpammbMmNHsuDFGY8eO1YQJE0L/ZRB0BGMAANDn1dbWKjc3V8uWLZPL5dKpp57abPvmIw0aNEg33HCDFi5cqGXLljXbuGPSpEnauXOnsrKyFBPTFKUCV6xISkpSXFxc6L4MQoZgDAAA+ry8vDzV19dr1apVSk9P14wZMzRy5Mh277v++utVVVWl3/3ud/5jDQ0Nys7O1sSJE1tcz253vRvBGAAA9HmxsbHKy8vTBx98oIsuukgOh8Nf7T2arKwsnXHGGfrTn/7k3+jjggsukCTNnz8/pHNGz4tt/xIAAIDe76WXXlJ8fLwuvPBCDRky5KjXDh8+3L+xx6WXXqq33npL55xzjtLT05Wdna3/+Z//0bx58/wP27W2+Qd6HyrGAACgT7PWqri4WH/72990ySWX6IQTTlBGRsZR7xk0aJD/9QknnKCLL75YhYWFys7OVnp6uv70pz9p6tSpGjNmTLP7aKXo3QjGAACgT/N4PPr73/+u+vp63XLLLUpMTOz0/XfccYc2b96sv/71r9q9e7eGDBmimJgYfxDOzMxUampqp8dGZKGVAgAA9GkNDQ1auXKl5s2bpylTpnTontYqv8YYnX322RowYECLc/369evQw3yIbO1WjI0xTxljDhljtgYcu9sYc8AYs9H7c3bAuduMMbnGmBxjzJmhmjgAAEBH/PGPf9S+fft03XXXdXuswPWO0fd0pJXiaUlntXL8YWvtLO/PG5JkjJki6WJJU733/N4Yw9YvAACgx7lcLu3atUv33HOP5s6dq/PPP7/bY7KjXd/WbjC21r4vqbSD431N0gvW2gZr7W5JuZLmdmN+AAAAXZKfn6+f//znqqys1K233tqpTTfaeoiuI0u8offqzj/d640xm72tFr5HNzMl5Qdcs997rAVjzFXGmHXGmHWHDx/uxjQAAACaq6qqUnl5uVatWqVzzjlHEydODEq1l1Un+rauBuOlksZLmiWpQNJvOjuAtXaZtXaOtXZOe2sJAgAAdMbevXu1bt06VVVV6fTTT5cUnFBLMO7butRBbq0t8r02xvxR0irv2wOSRgVcOtJ7DAAAIKRcLpfcbrf69esnSVq9erUSEhL09a9/XcnJyWGeHXqDLlWMjTHDA95+XZJvxYpXJV1sjOlnjDlG0kRJa7s3RQAAgPbt3r1bO3fuVGNjo6y1evfddzV//nyNGjVKqamp4Z4eeoF2K8bGmL9IWihpsDFmv6S7JC00xsySZCXtkXS1JFlrs40xf5W0TZJL0nXWWndopg4AANDEWquGhgZJUmlpqT7//HMVFhbqhhtuCPPM0Ju0G4yttd9q5fCTR7n+Pkn3dWdSAAAAneF2f1GHa2xs1LvvvitjjE499dQwzgq9DWuOAACAXs/pdPpf19fX65133tHMmTOVnp4exlmhtyEYAwCAXq+xsdH/et++fdq+fbsWLlyooUOHhnFW6G0IxgAAoNcLrBivXr1aknTNNddo4MCB4ZoSeiGCMQAA6PU8Ho//9erVqzVx4kRlZWWFcUbojQjGAACg17PWSpIqKiq0fv16LV68OMwzQm9EMAYAAH3GmjVr5HK5dP7554d7KuiFCMYAAKDPeO+99zRs2DDNnTs36GNPnTo16GMishCMAQBAxPN4PKqsrPS3TBzJWqtdu3bptdde0/e//33FxAQ34vTv31/GmKCOichDMAYAABGvpKRE+/btU1VVVZvXPPHEExowYIBuuummHpwZ+hKCMQAAiGjWWhUVFUlqvsNdoG3btunNN9/UD3/4w5Bs6kG1ODoQjAEAQERraGho95qHHnpIiYmJuvnmm0MyB4JxdCAYAwCAiBa4eUdrPcYbN27U66+/rm9/+9tKS0vryamhjyEYAwCAiBa43XPgRh5SU2vFddddp5SUFF1++eUhmwMV4+hAMAYAABGrsLBQBQUF/ve+irEvIP/qV7/SRx99pKuuuiqk2z8TjKNDbLgnAAAA0BqPx6Pi4mJJUlxcnJxOp6y1KikpUUFBgd59913deeedOuuss/SjH/1I1dXVYZ4xejsqxgAAICIVFhb6X8fGxsoYI4/Ho4KCAn344Yf64Q9/qEWLFunvf/97SCq6kydP1oABAyRRMY4WBGMAABCRSktL/a8dDofi4uJUUVGhiooK3XXXXZowYYJee+01JSYmtrnxR3c4HA7/67i4uKCPj8hDKwUAAIg4gUG3oaFBf/7zn7Vy5Urt3bvXv0rFY489pn79+vmvC0VV1zcPgnF0IBgDAICI4wukbrdb1157rT799FOddNJJmj17tj799FN94xvf0OTJk+V2uxUbG/o4QytFdCAYAwCAiONbdWLFihX69NNP9X//93+66aabtGPHDknSkCFDdPjwYX3++efKysqStTakFWNEB4IxAACIKL4+4vXr1+vRRx/VGWecoeuuu65ZZTg+Pt7/Oicnp8WxYKNiHB0IxgAAIKLk5+crPz9fP/7xjzVy5Ei9+OKLSk5ObnZNTAzrByD4CMYAACBieDweOZ1O/fSnP5XH49Grr76qQYMG+c87HA4lJSW1WsF1uVwhmxcV4+hAMAYAABGjurpab7zxhrZu3apf//rXmj59erPzkydPliSVl5e3uPfI7aKBzuK/QwAAgIjhcrn03HPPafz48TrzzDPDPR3/w3dUjKMDwRgAAESMTz75RNu3b9cll1yipKSkNq8bOHCghg0bpilTpmj06NEhnxfBODrQSgEAACLGH/7wByUnJ+vcc89V//7927zOGKPBgwdLUo+sY4zoQMUYAABEhPz8fL3++uu64IILlJiY2OHA61uhIpRVXSrG0YG/YgEAgIjw+OOPy1qrSy+9VJmZmUpNTe3QfXFxcYqJidHw4cNDPEP0dQRjAAAQdjU1NVq2bJlOP/10DRs2rNkSbe1xOByaPHkyVV10G60UAAAgrBoaGvTb3/5WZWVluuyyy7o0RqhCMatSRBeCMQAACBuXy6UdO3boqaee0uTJkzV79uxwTwlRjGAMAAB6nLVWhYWF2rFjhz7++GPl5eXpsssukzFG6enp4Z5eC1SMowM9xgAAoMc5nU4VFxdLkp599lllZGTowgsvVEZGhpKTk8M8O0QrKsYAAKDH+Xp3d+3apQ8++EA/+MEPNGHChIgNxVSMowPBGAAA9DiPxyNJWrFihRISEvSDH/wgzDNqHQ/fRReCMQAA6HFut1uHDx/WqlWrdNFFF/l3sQPCiWAMAAB6lMfjUWlpqZ5//nm5XC5de+214Z5Sm6gYRxeCMQAA6FFlZWXauXOnnn/+eS1atEiTJ08O95TaRTCODgRjAADQo8rKynTvvffK5XLpsccei9gH7iQqxtGGYAwAAFrlcrmUm5urmpqaoI776KOP6r333tO1116rSZMmRXToJBhHF4IxAABoVWVlperr67V7925/QOyumpoaLV++XPPnz9eSJUuCMmZPIBhHB4IxAABoVW1tbdDHfP3111VVVaWbbrpJsbGRv88YFePoQjAGAKAPcjqd2rdvn5xOZ4eu93g8amhoaPa+urra/z5YFeOXXnpJAwcO1JlnnhmU8UKNYBxdCMYAAPRBBw8eVGVlpXJycjp0/b59+7Rz50653W7//S6XK6hzqq2t1T//+U8tWrRIiYmJQR071AjG0YFgDABAH+LxeFRQUNCsDaIj1V5fddjlcsnlcjWrFgfLP/7xD9XU1Ojss8/uNUEzKSlJEsE4WkR+cw8AAOiw6upqlZSUNDvmdrs73M/rdru1c+fOoM7J5XKppKRETz/9tNLT03X88ccHdfxQGjVqlFwuF8E4SlAxBgCgj/B4PP5WCEkaOHCgJDU71prc3Fz/68A+48AxutJj7PF4VFZWpoKCAuXm5urdd9/VWWedJYfD0emxwiUmJkbx8fHhngZ6CBVjAAD6gJKSEhUUFDQ7lpSUpIqKiqMGY7fbrfr6ev/7xsZG/+uhQ4cqJiZGFRUVXZpTdXW1Dhw4IKlpNQqn06nFixdr2LBhXRoPCDUqxgAA9AHl5eXN3k+ePFn9+vWTpKM+RHfkqhWHDx/2v+5uZdfj8fhfv/LKK5o8ebIWLFigwYMHd2tcIFQIxgAA9AEul0v79u1TUVGRUlJS5HA4/MH2aBXjwGDsdDqbvQ8Mxl1ppfDds2PHDm3btk3XXHMN1WJENFopAADo5Xbt2qXzzz9f27ZtkyQtWLBAt99+u77yla9IUrNWiSP5qskNDQ269NJL1a9fPz399NOKi4tTfHx8u/3JR+MLxv/6178UFxenSy65pMtjAT2BijEAAL1EbW2ttm7dqpycHH/YfeONNzR79mzt379f999/v+666y4dPHhQ55xzju677z6VlJQcdek1X4X4scceU05OjjZv3qwHHnhA1tpu70xnrVVNTY1eeuklLV68mBYKRDwqxgAA9ALWWuXl5UlqCrO5ubnasGGDrrjiCk2fPl0PPPCA5syZo0GDBumnP/2pLrjgAt11112KjY3VRRddpBUrVvhbI1wul+rq6pScnCyn06kPPvhATz/9tC6//HLFxMRo+fLlGjhwoB5//PFuz/lvf/ubysvLdfPNN3f7dwCEGsEYAIAIVlFR0axfWGoKnH/5y1/0wAMPaObMmfr973+vpKQk/7JiiYmJ+uc//6lt27bp/vvv13PPPaeMjAz95je/kSTt2bNH9fX1mjJlimpqanT//fdr2rRp+uUvf6mysjJVVlZq2bJlOvnkk3Xeeef5P7Ozqqur9dRTT+nUU0/V3Llzg/DbAEKLYAwAQATyeDzyeDzKz8+XJA0YMEB79uxRXV2dVqxYoddff11f/vKX9eCDD/q3V05ISGg2xpQpU7RkyRJZa/XQQw9p0qRJuvrqq/1tGPX19brtttu0f/9+LV++XElJSSovL9fPf/5z5eXl6cYbb9SJJ57Y5e+wbNkylZaW6t577+3yGEBPIhgDABBhamtr/W0TkpSXl6clS5bogw8+kCTFxsbqxz/+sb73ve8pJuaLx4VaW17N4XDolltuUWFhoa699lrl5OTo2GOPVV1dnW688Ua9/fbbuvHGG7Vo0SLV1NT477n77rv1zW9+U3feeafuvvvuTn+HsrIyPf7441q4cKFOPvnkTt8PhAPBGACACOMLxS6XS4888oj+/Oc/q3///rr99tv11a9+VSkpKf5A3K9fPzU0NLS5DFpsbKxiY2P1wAMP6M4779Sjjz7qX4miX79+uueee3T55ZdLkuLi4vz3TZ06VbfddpvuueceLVq0SJMmTerUd/j1r3+tqqoqXX/99WynjF7DdKVnKNjmzJlj161bF+5pAAAQEXJzc1VXV6fbb79dq1at0vnnn68f/ehHysrKUkZGhg4ePKjS0lJJ0oQJE/wbebQWQCsrK7Vv3z5JTSG5oqJCGzdu1IABA7Ro0SJVVFQoMTFRo0aNktvt1vbt2yVJ06ZNU0NDg6ZPn66amhqtWbNGI0eObHPO1lpVV1drwIAB2rVrl6ZPn65zzjlH9957r6ZMmRLsXxHQZcaY9dbaOa2dY7k2AAAiTGNjoz788EOtWrXKX7VNS0vzL58WGIAdDoeMMW1WZQOXXHO5XBowYIDmz5+vWbNm+Tf08AXrwLYMqami/Pjjj6u0tFTnnntui931AlVUVGjv3r0qLi7WlVdeqYSEBF1//fXd3j0P6EkEYwAAIojH41FZWZl+8YtfaM6cObrtttv853whd+jQoUpPT9fIkSObtT+0prVgOmTIEElNYVbSUSvOc+bM0W9/+1tt27ZNZ511VostpH18D/QtX75c7733nu6//37/5wC9BcEYAIAgaGho6NKSZoHq6+uVl5fnXzZt2bJlzYKvbzk2h8Oh4cOHKzU1td0xWwvGR1aGfeP6+IKy1BSW58+fr4ceekhr1qzRVVddJUk6fPiw6urq/P3KLpdLO3fu1N13362FCxfqlFNOkdS0mgbQW/DwHQAA3eRbRSIjI6PLVdKKigrl5ubqgQce0L/+9S/deOONmj17tiRpzJgxiouLa7EcW0e0FozT0tJUVFTkfx8YhI899tgWwVmSrrzySu3cuVOPPPKILr/8cg0ePNg/xoQJE3To0CHdeuutGjBggO666y5/9ZmqMXqTdivGxpinjDGHjDFbA46lGWPeMsbs9P45yHvcGGMeMcbkGmM2G2OOC+XkAQCIBA0NDZKaAnJXNDY2avfu3bruuuv00ksv6eKLL9avf/1r//nk5OQuhWKp9faII8NyYBCOjY1tNRhL0v3336+RI0fqhz/8odxut//4nj179OCDD2rHjh363//9X//Wz/369WsWuoFI15FWiqclnXXEsVsl/cdaO1HSf7zvJemrkiZ6f66StDQ40wQAIHL5+m4Dw2J78vPz/Q+z1dXV6ZFHHtGaNWt011136Y477gjqQ2sjRozQoEGDmh0bOHCgpPZbHXzB2lqrAQMG6MEHH9TWrVu1cuVKSdL777+vc845Ry+++KIuu+wyfelLX/Lfm5ycHLTvAPSEdlsprLXvG2PGHnH4a5IWel+vkPSupJ95jz9jm5qsPjHGpBpjhltrC4I1YQAAIs2hQ4ckdbxi7HK5VFFRoYqKCqWmpmrlypVavny5Lr/8cv34xz8O+rq/aWlpqq6uVllZmf/YqFGjNGrUqE6PNWfOHB1//PF69NFHlZOToxdffFFjxozRo48+qm9961sqKPjiX/n9+/cPyvyBntLVh+8yAsJuoaQM7+tMSfkB1+33HgMAICr4HkbzKSsrU2VlZbNjvhUcJOnzzz/XDTfcoBkzZuiJJ55Qamqqv5obTIHLtnWVy+VSQ0ODbr31VpWVlenFF1/UN77xDb388stauHCh0tPTNXXqVP/17a2YAUSabv+vxFprjTGdfgzXGHOVmtotNHr06O5OAwCAsPG1Pbjdbrnd7mYh9MCBA5KadpLzVYLr6uokSYWFhbrhhhvUr18/PfTQQ13uI+6I7gZja60/0M+ePVs33HCD6uvrddNNN8npdPp33gusdtNfjN6mq/8rKfK1SBhjhks65D1+QFLgf5cZ6T3WgrV2maRlUtPOd12cBwAAYWetlcPhkNvtlsfjaXbcJzs7W5KUmpqqqqoq/fnPf9bSpUvldru1fPlyZWaG9j+wdrVnOTDo+lpFxo4dq9/97neSpB07dkhqvTrM5h7obbraSvGqpMu9ry+X9ErA8e94V6c4SVIF/cUAgGgQWDX2CQzJPlu2bNE555yjBx98UFOmTNF///tfTZkyJeRtB76A252+3/r6esXHxzcLvI2NjZKoDqNvaLdibIz5i5oetBtsjNkv6S5Jv5L0V2PMFZL2SrrQe/kbks6WlCupVtL3QjBnAAAiiq9iLH0Rhq21/jaKIUOG6KWXXtIrr7yi999/XwkJCVqxYoWOO+6LVU07sllHd02ePLnLD/ZZa1VXV9ciWA8ePFjFxcXNgnF8fHyrfykAIl1HVqX4VhunTm/lWivpuu5OCgCA3sa39q/H45G1Vt/5znf0/PPP6+qrr1ZlZaWee+45DRkyRBdccIG++93vauLEiaqpqfHf3xPBuCutDb4g7XQ65XQ6W2zYMWzYMGVkZDQL3BMnTuzeRIEwYec7AAC66ciK8cMPP6xnn31WY8eO1dKlSxUTE6NrrrlGV111lb9lYsiQIbLWqra2Vv369Yv4flzfmsuttUwcWYUO9nJzQE8hGAMA0A2+B+x8wfa1117TzTffrDPOOENLlizR9u3bNWXKFGVkZKimpsZfJU5ISFBSUpJqa2tDuhpFsFRVVUmSEhMTwzwTIHQIxgAABEFMTIwaGhp09913a8SIEfrlL3+pmJgYzZs3r9mypFu3bpXUFKR97RdJSUlhmXNHBFZ/4+LiqAajTyMYAwDQDb6KsTFGf/zjH7Vnzx794Q9/8FdW2+odNsZo0KBBio+Pj+itk33hXaJajL6PYAwAQBDk5OToySef1LnnnquTTz5ZUtMqEEf2Do8bN86/pJvD4VBKSkqPz7UzAoPx8OHDwzgTIPQIxgAAdIO1VmVlZfr+97+vlJQU3XLLLZKkzMzMVh+o621V18BgHIxtpYFIxv+FAwDQDU6nU9dff71yc3P1zDPPKC0tTZI0aNCgMM8sOHzhnlCMaNDVne8AAICkpUuXavPmzfr973+vr371q5L61lbIDodDI0aM0Pjx48M9FSDk+OsfAABd9Oabb+rmm2/WySefrG9+85v+3d58VeO+oq99H6AtVIwBAOiCl156SYsXL9bUqVP18MMPKyYmxh+MWdIM6J0IxgAAdNLq1at10UUX6bjjjtMzzzyjxMREeTweDR48WCkpKUpPTw/3FAF0Aa0UAAB0grVWN954oyZMmKCHH37Y/1BafX29YmNjm23mAaB3IRgDANAJ7777rjZv3qzHH3+82dJrcXFxYZwVgGAgGAMA0AEej0fFxcX6zW9+o8GDB+v000+X0+lUTEyMRo8e3evWJwbQEj3GAAAcobi4WA0NDZKk2tpaOZ1OlZeX6+OPP9Ybb7yhCy644P+3d9/xUdX5/sdf35nJTHoPaYQQQq/SF0FQiuKuXgUVRVfKeuUnywr2goqCbS3LXgFFRGDxoqyLguIqSFME5IqgqASSEAIJCWmThMmkTD+/P5LMJhKQEkhCPs/HI4/MzJlz5nvy5Qzv+c634HQ68fHxoXv37gQGBtZbCEMI0TJJi7EQQghRh9vtJj8/H7PZTFJSEpmZmTidTpYtW8aqVasICAjg9ttvB8DX17eJSyuEaEzy8VYIIYSoo7S0FKgeZHf48GFcLhePcKWrsgAAIABJREFUP/44b775Jj179mT16tVERUUBLW95ZyHEmUmLsRBCCFFHfn4+UN2n2O12M3v2bDZv3syjjz7KpEmTAOjUqRN2u52goKCmLKoQopFJMBZCCCEa4PF4ePnll9mwYQMvvfQSt99+OzExMTgcDkwmEyaTqamLKIRoZBKMhRBCiAa8/fbbfPjhhzz88MM8+eST3sel+4QQly/pYyyEEKLVsdlsaJrW4DZ/f382btzIW2+9xYQJE3j11VcvcemEEE1FgrEQQohWxW63k5GRQUFBQYPbN2zYwBNPPMHIkSNZtWqVTMMmRCsiV7sQQohWxeFwANVzFddtNbZarUyfPp2//OUv9OnTh08++URWsxOilZFgLIQQolVxOp3e2zk5OQCkp6fTr18/lixZwuTJk1m9erXMOCFEKySD74QQQrRoZrMZk8lEYGAgSqnffH5tizGAxWKhrKyM4cOHo9frWbFiBf3790ev11/MIgshmikJxkIIIVq02nmHw8PDiYmJAaCkpITg4GCMRuMpz3c4HBiNRoxGI8ePH2fy5MkYjUY2bNiAwVD936L0KxaidZJgLIQQosXyeDze2yUlJVRWVuJ0Or3LOuv1erp06eINuhUVFZSVlWEymaiqquL+++/nxIkTfPzxx/j4+KCUIjo6mpCQkKY6JSFEE5JgLIQQosVyuVz17ttstnr33W43VVVV6HQ6iouLvd0o0tLSmD59OkVFRbz88sskJiaiaRqhoaFERERcsvILIZoXCcZCCCFaLLvdDkB8fDzbtm1j1apVFBYWEhwcTFxcHDExMaSlpbFx40YqKipQShEaGorFYiEwMJBVq1bRtWtX7/EkFAvRukkwFkII0WLVthC/+eabzJkzh4CAALp27UpeXh47duygoqKCiIgI7rrrLnr37s2BAwcwm820b9+eBx98kODgYAoLC/H390cpJcs8C9HKSTAWQly2HA4Hubm5xMbG4uvr29TFEReB1Wpl69atPPPMM0yYMIG3336bsLAwAMrLy9m+fTtxcXH06tWLkpISRowYAUDPnj29xwgMDGySsgshmh8ZdiuEuCxpmkZOTg4VFRUUFRU1dXHERbJ3714ef/xxhg4dynvvvecNxQAGg4HExER8fHywWCwUFhYCkJCQ0FTFFUI0cxKMhRCXpfLyciorK723s7OzcbvdTVwq8WuaptWbWaIhTqfzlEF2APv372f69OlER0ezbt26U7pB1J3TOC8vz3s7ODj4AksthLhcSTAWQlxWbDYbFRUV9YKU2+2mrKys3sIOounZ7XaOHTvGwYMHvUsza5rGyZMn6y3VnJaWRnp6er19f/jhB2644QbcbjerVq0iKirqlOM3tNhH586dz2oRECFE6yR9jIUQl5WMjIzTbqsbtkTTstvtHD582HvfbDYTHBxMeXk5eXl5lJSU0KFDB+/2uq3K3377Lddeey0BAQEsW7aMoUOHNvgadRfr8Hg8xMbGNrjghxBC1JJgLIS4bDQUfDt37uxtbZRg3HyUlpZ6b2uaxqFDh8jLy6Oqqork5GTvtl93odi4cSPjxo0jNjaWZcuWcdVVV512lTqdTldvkJ0QQvwWCcZCiMtGVVVVvfuRkZEYjUaSkpI4evSoBOPTsFqtZGVlER4eTlxc3EV/PU3TsFgs+Pr68uKLL7Jy5cp6dWMymbjxxhuZO3cusbGx3sePHDnCLbfcQlxcHMuXLychIcHbKiyEEI1B3lGEEJcNl8uFx+PhxIkTREdHExUVhaZp3j6lEoxPVVZWRnZ2NlC9pLLdbqd9+/YXpR9ucXExfn5+eDwe7HY7r7zyCh988AGTJk2iY8eOdOzYEaUUmzZt4v333+ejjz5i8ODBdOzYkaKiIvbv349Sik8//RRA5hwWQjQ6CcZCiMuGw+Hgz3/+M7t27UIphaZpXHfddXz44YeABOOG1IbiWhUVFZSWlhIeHt6or+N0OsnLy8NoNOLv78/cuXNZu3Ytc+fOZc6cOfWee/PNNzN58mT+9a9/8fXXX7N27VrCw8Pp3bs3jz76KEop9Hp9o5dRCCEkGAshLhuzZ89m165d/OlPfyI+Pp7S0lIWLVrEE088wYwZMyQYn0FISAgWiwWAEydOEBoaetq+u+ejdvCcw+Fg8eLFrF27lqeffvqUUAxgNBqJjIxk+vTpTJ8+/ZTtmqYRGxuLn59fo5VPCCFAgrEQ4jKxbds23n33XcaNG8c777yDXq8Hqr9u/9vf/kZiYmKDIas1q/2g4O/vT3x8POXl5d65nktKSggLC/P+HRvrtXbs2MEbb7zBbbfdxrx58xp8rk6no3v37qSkpJz2eLKSoRDiYlDNoQVlwIAB2t69e5u6GEKIFqB2jtu6K5ylpKQwbNgwwsPDWb16NQMHDvT2kXW5XAwbNoxjx46RkpJCREREUxW92fF4PHz77bcsXLiQr7/+GofDQWxsLH369OHKK69k3LhxtG3btlFeq6qqim3btjFx4kTi4uL4/vvvCQoKOuM+drsdpRQul4vy8nLvynUAPXr0kPmIhRDnRSm1T9O0AQ1tkwU+hBAtSk5ODrm5uQBUVlaSlZXF7bffjsFg4K233iI8PLxeYDIYDDz88MMUFBTw5ptvNlWxmx2Px8P333/PLbfcwrp16xg7dix33XUX0dHRfPrpp8ycOZMbbriB1NTURnk9q9XKAw88gF6v55133vnNUAzVrf21fZLbtGlDp06d6N69O126dJFQLIS4KKQrhRCixTp+/DivvvoqKSkprFixgvj4+HrTe9W66aab6N+/P88++yy9evVi9OjRZxXMLmfffPMNEydOBGDTpk1cffXV3m1lZWXMnz+f+fPnM3jwYD766CPGjBlz3q/ldrv505/+xJEjR/jkk08YPnz4eR2ndhaKxuz7LIQQdcm7ixCiRfJ4PKSlpbFy5UrGjRvHgAEDCAwMJCAg4JTnGgwG3njjDSIiInjsscfIyMiot5Jaa3Py5ElmzZqFxWJh4cKF9O/fv9724OBgpkyZwscff0x8fDzjx49nz5495ObmkpubS0lJyTm93vTp0/n888+ZMWMGo0aNasxTEUKIRiXBWAjRItlsNhYsWICvry8PPPAAAIGBgQ0+V6fTERISwpw5c8jIyGDVqlWtboYKj8dDVVUVDoeDhx56iJ9//plFixYxYsSIBv9ucXFxxMbGsmDBAoxGI7fffjvZ2dmUlpZy4sQJrFbrWb3ud999x7vvvsuNN97ItGnTpAuEEKJZk2AshGgx6obZDz/8kK1bt/LQQw+RkJAAUG9A3q8FBAQwcuRIRowYwdKlSykrK7vo5W1OcnNzOXLkCO+//z7/+Mc/uPXWW5k6dSrR0dENhlWj0UhsbCwxMTHMnz+fnJwcHnvsMe+sFVlZWb/54cLhcHDPPfcQFRXF7NmzUUpJMBZCNGsSjIUQLUZt9weLxcLs2bNJTEzkkUceISkpiZ49e55xarH27duTlJTE7NmzsVqtLF++/FIVu1mwWq2UlZXx1FNP0a5dO955553fDKm1HzT69+/PK6+8ws6dO3n33Xe92xsKxpqmUVZWhsfj4YknniAlJYVnnnnG2yptNBob8ayEEKJxyeA7IUSL4fF40DSN559/HrPZzJYtWwgODj6rfZVSBAQEMGDAAPr168eCBQuYNWsWBkPreBt0u9288MILFBYWsm3btjO2rteqnU+4srKShx56iIyMDBYtWkSbNm24+eab8Xg83oFwmqZx8uRJlFLk5OTw6aef8ve//50777yTq6++mtDQUHx8fBptXmQhhLgYpMVYCNFiuN1u1q9fz5dffsnTTz/NiBEjzvkYSimmTp1KdnY2a9asuQilvPQ0TfvNbg3vvfceGzZsYMaMGVx11VVnfWydTudt7X3jjTcYP348c+bMYfr06Wzbts37uhaLhdzcXHJycti3bx/z5s1j8ODBPProowDEx8cTHR19nmcohBCXhizwIYRoMX788UeGDRtGv379+Prrr8+r9fHkyZNkZ2czYcIEAgIC2Lt3b4vv95qTk0NVVRUdO3Y85Vw0TWPjxo3ccMMNjBo1ir/97W/06tXrvF/L4/Ewb948Fi1aRHFxMYmJiQwfPpwBAwbQsWNHfvrpJ+bOnUtCQgLLly8nOTmZ8PBwWalOCNFsnGmBDwnGQogWwWazMWrUKH788Ud+/PFHunTpcl7HOXnyJDk5OezYsYM///nPbN26lZEjRzZyaRtmtVpxOp2EhYWdcxi32WxkZmaSnJzsnc/XZrORl5dHRUUFAN26dav3YUHTNLZs2cKtt95KdHQ0q1atwt/fn549e17weaSnp7Nlyxb27NnDjh07KCoq8m4fOHAgX3zxBZGRkRf0OkIIcTGcKRi3js51QogWb8mSJXz77bfMnj2bTp06nfdxagPpxIkTee6555g9ezYff/wxcXFxF7Xl2O12k5WVBVQPQDvd1HKapnnLUTu9WkhICCdPnsTj8WCxWGjTpg0VFRUcPXq03r4ej6deMN6/fz9Tp071zuPs7+/fKOfi6+uLyWTiD3/4A+PGjcNut1NSUsLWrVsxGAzcd999EoqFEC2SBGMhRLOXm5vLnDlz6N+/P88///wFrXxWGzpNJhMzZszg2WefZfbs2cyePfu8W6FPx+l0YrfbCQgI4NChQ/Ueb0hZWRnZ2dl07twZvV7PkSNHvGWtLbfb7cbpdNYLxXq9HrfbTVpaGpmZmWRlZbFr1y42bNiA0+lkxYoV3intQkNDL/i8fHx8CAgIoLKyEofDgVKKiIgIJkyYgJ+fH1FRURf8GkII0RQkGAshmjWPx8PUqVNxOBy8+uqrjbYcsMPhYNy4cWRmZrJy5UqUUrz88svExMSglMLj8eB2uzEYDFgsFoKDg8/6tcvLy/Hz8yMnJ8fbzaGu0wXjwsJCoLqrQt3W6/z8fHbv3s327dv5+uuvyc/PZ/DgwbRt25bg4GCioqLYuXMnGzZs8O4TERHBVVddxZNPPklcXBwVFRUkJiaetqX6XAUGBp5ybiaTieTk5EY5vhBCNAUJxkI0Y3a7HR8fn0YLgy2Nx+PhhRdeYPPmzTzzzDP06dPngo9ZGziPHz+OUorFixfj5+fH22+/TXp6OsOGDWP06NHExsailCIxMZGcnBzCwsKIj4//zeO73W6OHTvmvV9WVsbu3bvx8/Nj1KhR6HQ6KisrG9y3dsxHXl4egHeVvnXr1nmnRhswYAB9+/Zl69atbN++HafTicfjISAggPvvv58rr7ySyMhI78IdPXv2xOPxYLVaCQwMbLTuIrX9nOuKjY1tlGMLIURTkWAsRDNlt9s5fPgwkZGRxMTENHVxmsSaNWt49tlnufrqq7ntttsapbXz18HQ19eXRYsWYbFY2Lx5M3v27OG1116jb9++PP74497nuVyuszp+3dbgL7/8kpdeeomSkhKgum/xnXfeybRp02jfvv1py2axWFiwYAFr1qzBx8eH8ePHM2zYMEaNGgVAUFAQlZWVdOvWDU3TyMnJoaCggMjISMrLy73H69ixI/CfJbEbU2BgICaTCbfbTVJSUqv+ACeEuHzIrBRCNEN5eXkUFxcD1cGtNuC0JkVFRXTq1InAwEDef/99IiIi6NGjxwW3eFqtVu8guC5duuDj4wNUB9r09HTsdjuffPIJixcvpqSkhCeffJI77rgDvV5PQkICRqPxjKu3mc1m8vPzWb9+PU899RSDBg3i9ddfB+B///d/Wb58OQEBAcyaNYvBgwfTtWtX2rdvj16vZ+vWrWzatMm7ZPWMGTN4+umn0el0hIWFkZ+f7w3ZQUFBJCYmesuelpYGVP97CQ8PR6/XN3oYFkKIy4HMSiFEC6JpmjcUA612pbBly5ZhsVhYt24dycnJ3r6/F8rtdgPVAbI2FEP1gLKIiAjMZjMTJkzgj3/8Iw888AAvvvgiP/74I08++aS3z3HXrl1Pe3ybzcY333zDs88+y4gRI9i0aZM3SF911VVMmzaN++67j+eff967T0hICJGRkd7BdkOGDGHx4sWndB0JCwvzBuO6M0zUPY/o6GiCgoLO988jhBCtmrQYC9EMeDwenE4nJpMJh8NBeno6UB14fHx86NChQxOX8NL6/PPPueWWW7jiiivYtGnTWS/7fDY8Hg/FxcVERESc8tW/x+Ph4MGD3kFkTqeThx9+mCVLlhASEsKcOXMYOXIk3bt3b7DbgKZpLF68mIceeohu3bqxffv2U8rudDpJTU0lLS0Nh8OB2WzmwIEDFBcX06VLF66//nqGDRt22g8Bbreb4uJiIiMj65XB4/FQWVnZaIPrhBDiciULfAjRzGVnZ1NWVka3bt3IysqisrKSxMREiouLcbvdzWKkv9Vqxc/PD4Ph4n7RtHbtWqZMmUJ4eDgrV65k2LBhTdpqrmkaP/74I5MmTSIlJYU77riDxYsXNzjt2Zw5c3jxxRfp2bMnW7ZsOe20ZS6XixMnTlBWVgZAVFQUVqsVm81GRESEDGITQoiL6EzBWEZKCNEM1Aak48ePU1lZSXl5Ofv378dqtdIcPry6XC6ysrJITU3FZrOd1T4FBQXeVtGz9dVXXzFx4kTi4+NZunQpERERTd6VRClFv379+OGHH3jwwQf55z//ybRp07xdMqB6oOSDDz7I888/z4gRI9i1a9cZ5/I1GAz1WoSrqqq8f9eAgICLdzJCCCHOSPoYC9HEaqfu2rdvH9988w379u3jwIEDuN1u9Ho9d999NwsWLGjSfqN156vNysoiJibmjAO7qqqqvEsEp6en065duwa7Q9hsNjRNw9fXl/T0dO666y4iIyNZt24dOp2OiIiIxj+Z82Q0Gpk/fz4ul4uFCxfyzTff0K9fPyoqKtizZw82m40777yT119//ay6M/j6+mKxWAC8M0kYDIZG7TYihBDi3FxQMFZKHQOsgBtwaZo2QCkVDnwItAeOARM0TSu9sGIKcfk6cuQIS5Ys4c0338RgMNC3b18efvhhBg4cyNq1a1m5ciW7du1i/fr1dOzY8aJ3ZWhI3Xl3nU4nx48frxeMrVYrHo+H4OBgsrOzsVqtQPWgMovFQnZ29in9cquqqryDzcxmM7NmzaKkpIQvvvjijIPbmtqMGTNISkpi69atHDlyhKCgIO69914GDx5M7969z3plucjISIKCgsjOzva2qjf2yntCCCHOTWP8D3uNpmnmOvefALZqmvZXpdQTNfcfb3hXIVous9mM0Wi8oBY+TdP4+9//zooVK7j77rt566236rU2/u53v2PMmDE88sgjDB48mKVLl9K9e3e6du16UQNyRkaGd9ovu93OV1995Z3twMfHh8rKSrKysjhx4gRVVVXk5+dz8uRJ9u3bx+7duykoKMBut9OmTRv69u3L9ddfT1hYGCEhIRw/fpzOnTtTWlpKeXk5//jHP1i5ciUul4slS5ZwzTXXXLTzagxOp5MxY8YwZsyYU7YppRpc+KIhSil8fX2JjY0lKyuLkJCQRlt8QwghxPm5GP+z3gRcXXN7JfA1EozFZaaiooL8/HyAc5pb1+12U1VV5Q2/zz33HCtWrGDq1Km8++67Dc6SMHDgQFavXs2UKVOYPn06q1atIjw8nLi4uMY9KaqDutPpxGazUVhYyMyZM/nss8/OenELvV5Pnz59uPHGGwkMDKS4uJjNmzezdu1a73Z/f3+Sk5MxmUykpKRQXl7O2LFjeeaZZ7jyyisb/ZwupdDQ0HNe5CIwMNC7tLMQQoimdUGzUiiljgKlgAYs0TTtHaXUSU3TQmu2K6C09v7pyKwUoqU5ceKEdz5ZqJ5TNikp6TcDcu3sE/Hx8axcuZIHH3yQcePGeQea/dqhQ4e8g7wyMzOZNGkSwcHBrFu3jv79+zfuSQFHjx6loqICi8XCtGnTSE9P59Zbb+XKK6/E39+foqIiKioqcDgc+Pn50bdvX8xmM3a7HT8/P3r06EGvXr3w8/PzHtNms/Hee++xf/9+XC4XFouFvLw83G43sbGxPProowwaNKjFtJYeOHAAgJiYGEpKStDr9VRVVQGQkJAgi2oIIUQzdzEX+BimaVquUqoNsFkplVp3o6ZpmlKqweStlJoGTANo167dBRZDiEvLZrOh0+nweDxAdR9cp9N5xhXRavcDWLx4MU899RTXXnstc+bMOe1+ISEh3gDeoUMHPvnkE8aOHcuUKVPYtWtXo7cy1obi6dOnk5GRwVtvvcWQIUMIDg4mISEBpZR3rt9aycnJBAUFefsV+/r61jumr68v9957LykpKd7HgoOD8fX1pbCwsFFWs7uUas81MjKSyMhIXC4XqanVb311F9oQQgjR8lxQMNY0Lbfmd6FSah0wCChQSsVqmpanlIoFCk+z7zvAO1DdYnwh5RDiXLlcLgoLC4mMjPzNMPtrHo8Hl8tFYGAg7dq1o6CggKKiIm/LrtlsxtfX95SZCYqKinA4HHz11VfMmTOHwYMH89JLL2EwGOqtYlZXbGwsMTExOBwOHA4HPXv25O233+aee+5h+PDhvPHGG4wYMeL8/gi/Urvi3syZM0lLS2PdunVcf/31APWCq06nIzk52TtwrracYWFh+Pn5NRhylVJ069aNkpISysvLqaio8HY5aEmhGE79IG8wGOjRowc2m61eS7kQQoiW57yDsVIqANBpmmatuX0tMA9YD0wG/lrz+9PGKKgQjamsrIySkhKUUue0mILL5WL16tUcPHgQq9XK4cOH2bFjBwaDgSFDhnDzzTczdOhQdDodPXv2rLdvQUEBH330Ec8//zz9+vVj+fLl3hbn083Vq5TyDtKqbYn94x//yMmTJ5k7dy5XX301U6ZMYf78+eh0OsxmM/Hx8ae02v4WTdPYtGkT99xzD8XFxaxbt47f//73p32+n58fPXv2pKKiAr1ej9Fo/M0PGHq9nqioKHx8fMjJyeHkyZPnVMbm4nTBX0KxEEK0fBfSYhwNrKv5T8IAfKBp2kal1PfAv5RS9wBZwIQLL6YQjau2b31t39AzcTgcKKUoKipi3Lhx7NmzB6gOej169GDKlCkUFxezdetWNm3aRK9evXj88cfp1KkTbrebzz77jN27d/P999/z7bffMmDAADZu3Ii/vz9Hjhw55wUdDAYD//Vf/8WgQYNYsWIFK1as4LvvvmPp0qXerhd+fn6EhoaeVWuspmk899xzvPLKKwQFBbFt2zaGDBlyVmU5n8UomnI+ZiGEEOJMZElo0SoVFRVRUFCA0Wikc+fOp31ebTeJn3/+mUceeQSLxcIjjzzC3XffTfv27b1Tph08eBC73c7nn3/OwoULKSwsJCQkBLvd7v2KPSoqimuuuYZ33nkHo9GIpmmUlpYSEhJyzqu71Z0DeOfOncyaNYvk5GSeffZZunfvjlKKTp06YTKZKCsrIycnh5iYGMLDw+sdR9M0nnrqKV5++WUGDhzISy+9xOjRo8/xr3nuagewdevWrclXthNCCNG6nGnwnQRj0Srl5+djNpvx8fE5ZVEFTdNQSuFyuThw4ACLFy9m+fLlREdHs2DBAjp37nzKgDGz2Ux+fj5+fn60adOG1157jWPHjhEREcHo0aNJSEhAp9M1ahDMzMz0Lrzx3XffMXPmTCorKwkJCaFbt26MHz+e6667ztuX12Aw1Fs4Izs7m0ceeYQ1a9Zwyy238MILL9C5c+dznm7sfJjNZpxO5zl1YxFCCCEagwRjIerQNI0jR45gs9nQ6/V069bNu612GrZu3bqxb98+Zs6cyXfffceoUaN47rnnCA0NxWQy0alTp3rHrKqq4vjx4yQkJODn50dqaioul8u78htUD1rr3r17o51HYWEhhYWFmEwmEhMTqays5IMPPmDbtm389NNPZGRkANCvXz9uv/12hg8fTvfu3Vm9ejVffvkl//73v9HpdNx7773ce++9xMfHN6slmIUQQoiLQYKxEDXcbjeZmZnY7XacTicpKSn88ssvpKam8vPPP5Odne1t0XU6nZhMJt58802GDBniHSh3Nq2+DQ0ua+xFOWoX4/j1oLfabhaFhYWsX7+ef//7395uF0opNE0jNDSUm2++mTvvvNPbahsfH09YWFijlU8IIYRojiQYC0F1kFyzZg1bt27lp59+4tChQ5SVlQGQlJREjx49iI2Nxel0emdQuOaaaxgzZgxms5mCgoJTuiOc6bUOHz6Mw+EgNjaWgIAATCbTJZmazOFwkJ6e7r0fGhrK3r172bFjBxUVFfTt25dBgwZ559zt0qULZWVlhIeHt7ip04QQQohzdTEX+BCiWfN4PFgsFnbu3Mm8efPYu3cver2e3r17M2HCBEaMGEH37t3rtbqaTCbsdjuANyxGRUWdUzcDpRTR0dEcP3683lRrl4LRaCQqKoqioiIAwsLCuOmmm+jTpw96vR6Px0N5eTlQHZp9fHykC4UQQgiBBGNxGXA6nUDDq44dOnSIL774gtmzZxMfH8/TTz/NAw88UC8Iut1uDh06BEBERASxsbF4PJ5TBqGd66C0kJAQ/P39m2Q1tOjoaG8wrp1SLSkpCfjPAiWlpaW0adPmkpdNCCGEaK4kGIsWqTa4appGWloaUL3McGxsLD4+Phw9epTt27ezcOFCfvjhB3r37s2SJUto167dKa2jer2erl27UlZW5u1j21gzMzTlEsG13Td+TafTYTQaiY6OboJSCSGEEM2XBGPR4litVrKysjCZTLjdbux2O4cOHeKnn37il19+4cCBA+Tm5gLQpk0bXnrpJa677jqMRuNpF6QwGAynzPHb0tW2EAshhBDi7EgwFi1C7df/SilSU1PZtm0bGzdu5PDhw+Tn5+N2uwGIi4ujV69eTJw4kT59+nDVVVeRmJiIzWajuLhYVl0TQgghxGlJMBbNXkVFBUePHqW8vJy3336bNWvWUFlZSUxMDH379uXGG2+ka9eu9O/fn9DQUKC6f29CQoJOUREUAAAO30lEQVT3GL6+vsTHxzfVKQghhBCiBZBgLJq9o0eP8t133/HUU09RWFjI+PHj+ctf/sLQoUMxGAzeuXk9Ho93EJ2EYCGEEEKcKwnGotnSNA1N09i7dy/33XcfycnJfPDBB4wcOfKU5yql0Ov1xMXF4ePjc0mWNRZCCCHE5UWCsWi2srKyOHjwIA888ADt2rVj9+7dv7ky2+U2gE4IIYQQl440q4lmqbi4mKysLO677z6UUqxfv16WKxZCCCHERSUtxmfJ4/Fgs9kwGo0YDPJnu1g0TcNsNnP06FHuv/9+ioqK2LJlCz169GjqogkhhBDiMicJ7yzl5eVRWlrqvd+hQwf8/f2bsETNm8vlwul04ufnd077Wa1WcnNzeeyxx0hJSWHt2rUMHTr0IpVSCCGEEOI/pCvFWfB4PPVCMcCJEyeaqDQtw5EjRzhy5Ih3ueaz5XK5eOGFF9i+fTuLFi3ipptuukglFEIIIYSoT1qMG2C1WjEajZhMJkpLS1m3bh0HDhygS5cu9O3bl6qqKnx9fUlOTkYpdc7H1zSN/Px8/P39CQkJuQhn0LTsdrs3EKelpaGUOquuEA6Hg8cee4yPP/6YJ554gunTp1/sogohhBBCeLXqYFxUVISmabRp08b7WEZGBjabjcOHD7N582bee+89KioqGty/TZs2DB48mClTpjBu3DiUUrhcLvR6/RkDc05ODhaLheLi4rMKxpqmUVBQQGBgIIGBged+opeY3W6vd1/TNNxuN3q9/rT75Obmctttt7F7924mT57MvHnzLnYxhRBCCCHqabXBuKCggDvuuAO3242vry8ATqeT8vJyzGYzx44dw2g0MnbsWG677TYGDRpEZWUlaWlpFBUVkZmZSVpaGv/3f//HZ599Rp8+fbjvvvsYOnQoUVFR2O12oqOjvceuVVZWhsVi8d5PTU3Fx8eHDh064HQ60ev1pwRIj8eD2WzGbDbTtWvXZj/4z+VyAZCYmMiJEydIT08nPz+foKAgEhMTCQoK4uTJk0RHR2MwGNi2bRt33HEHlZWVLF26lN/97ncyD7EQQgghLrnmnbAuIpvNRn5+PjqdDr1ej4+PD5qmodPp6NKlC5MmTWLMmDGEhoZ6wxzAFVdcAYDFYiEvL4+qqirWr1/P0qVLmT59OoMGDWLixIkMHz4cq9VKp06dsFqt3kCcmpqKUory8nIOHjxIdnY2x48fx9/fnwEDBjB06FD+8Ic/UFlZidVqpW3btvX66ebm5pKYmHhO5+pyubBYLISHh59X14/Tyc/Px2w2A9C2bVtWr17N8uXLSU1Nxd/fH6PRyIkTJ/B4PN59dDodAwcO5JprrmHkyJFs3LiR//mf/yExMZF3332XDh06eJ8nhBBCCHEpKU3TmroMDBgwQNu7d2+TvHZpaSm5ubn1HuvWrRtKKY4fP45eryc+Pr7BQOl0OklLS/Pe3rZtG6+88gpFRUXeldhqW09PJzw8nLZt21JWVsaxY8cA8PHxweVyER4eTteuXbn11lsZPXo0vr6+2Gw2kpOT6832YLPZKC0tJTo6usFAmZmZSWVlJZ07d8ZoNJ7rn6hBLpeL1NRUAPbt28df//pXUlNT6d+/P927d6eqqoqAgAASEhIICgoiICAAgJ9//pnNmzdz9OhRoDoA33zzzTz22GPe5/j7+3sDshBCCCFEY1JK7dM0bUCD21p7MAY4cOAAAO3atcNkMmEymc56X4fDQXp6uneAWUVFBR9//DEHDx6kuLgYPz8/QkJCiI+Px263YzKZaNeuHdHR0XTs2JGQkBAqKio4evQoxcXF7Ny5k8OHD2MymSguLuaHH37g6NGjdOrUiTvuuIOoqCjatWvHFVdcQbt27VBKcfjwYex2O+Hh4cTFxXnL5nQ6OX78OJWVlQB06tTpnM6t9hi1XTdqPxz88ssvfPHFF3z//fekpqaSkpJCTEwMDz/8MNdddx1KKaKiooiOjvYep7KykszMTAB69OjBZ599RkZGBh07dqRDhw6Eh4dTUlLi3d6YLdtCCCGEELUkGP+GsrIybwvt+XA4HOh0ulP6/prNZvLz8+s9lpCQ0OCAO7vdjsFgQKfTceLECaqqqrDZbLhcLr744gvWrFnD/v376+0THR3Nddddxy233OJtYa1t/fb398dkMtWbazk5ORmXy4Wfn99Z9VOu2yoM4Ha7WbRoEcuWLUPTNEJDQ7niiisYO3YskydP9narABpsnTabzeh0OsLDw7Hb7Rw+fJiQkBBiY2PR6XQUFRXh7+/v7bYihBBCCNHYJBg3EY/HQ0FBAeXl5bjdbgwGA0lJSWecnaGuqqoqPB6PN9yWlpZy6NAhfv75Z4qKitizZw9bt27FZrPRq1cvdDodP/30k3d/pRQdO3Zk7NixjB8/nsjISAAiIiKIjY39zde32WxkZGQAkJ2dzaxZs8jMzOTOO+9k8uTJ9O7d2zujh6ZpFBUVYTAYCAsLO6sWX03TpGVYCCGEEJeUBOPLTG3XD4CSkhI++eQTdu7cid1uZ8SIEXTp0gWn00lGRgY//PADu3fvxmg0cuONNzJp0iR69OhBhw4dfjOU1nZ/2LRpE/PmzUMpxWuvvebtLhEbG3vWIV8IIYQQojmQYHyZSU9Px+FweO+3bduW0NBQHA4H5eXl9Vbl69y5M6mpqcybN4/PP/8cl8vF1KlTmTVr1m8uunHkyBHuv/9+NmzYwMCBA/nwww9JSkq6aOclhBBCCHGxnSkYy5xYLVDdvtABAQEEBwcDYDQaCQsLqzedm4+PD127duW5555j69at3HbbbSxdupQhQ4awcOHCBmfNqKys5PXXX6d///5s2bKFZ555hp07d0ooFkIIIcRlTYJxC1R3XuCkpKR6U7QppQgKCqJ9+/YkJCSglMJoNNKxY0cGDx7MBx98wOeff06PHj2YOXMmvXv3ZtmyZRQUFGC1Wlm0aBFJSUk8+uij9OzZk48++oi5c+c22jRvQgghhBDNVatd4KMlq+3XW3dqtl/79dLRdVfgGzt2LAkJCWzZsoX58+fz3//930D1nMIej4cBAwbw2muv0a9fP5KSkmSAnBBCCCFaBQnGLVB4eDh6vb7Bad/Ohk6nw9/fnzFjxjB69GjS0tLYsWMHDoeDIUOG0LdvX3Q6HZ06dZKWYiGEEEK0GhKMWyClFKGhoRd0jMTEREpKSrxTq3Xt2hWA4OBgysvLadu2rYRiIYQQQrQqEoxbKYPB4J2DWCmFpmnExcWd9yInQgghhBAtnQy+E7Rv357Q0FDCwsKauihCCCGEEE1GWowFAQEBBAQENHUxhBBCCCGalLQYCyGEEEIIgQRjIYQQQgghAAnGQgghhBBCABKMhRBCCCGEACQYCyGEEEIIAUgwFkIIIYQQApBgLIQQQgghBCDBWAghhBBCCECCsRBCCCGEEIAEYyGEEEIIIQAJxkIIIYQQQgASjIUQQgghhAAkGAshhBBCCAFIMBZCCCGEEAKQYCyEEEIIIQQgwVgIIYQQQghAgrEQQgghhBCABGMhhBBCCCEAUJqmNXUZUEoVAVlN9PKRgLmJXlucP6m3lknqrWWSemt5pM5aJqm3SyNR07SohjY0i2DclJRSezVNG9DU5RDnRuqtZZJ6a5mk3loeqbOWSeqt6UlXCiGEEEIIIZBgLIQQQgghBCDBGOCdpi6AOC9Sby2T1FvLJPXW8kidtUxSb02s1fcxFkIIIYQQAqTFWAghhBBCCKAVB2Ol1FilVJpSKkMp9URTl0fUp5Q6ppT6RSm1Xym1t+axcKXUZqXU4ZrfYTWPK6XUgpq6/Fkp1a9pS996KKWWK6UKlVIH6jx2zvWklJpc8/zDSqnJTXEurclp6u05pVRuzTW3Xyn1+zrbnqyptzSl1HV1Hpf30UtIKZWglPpKKXVQKZWilJpV87hcc83YGepNrrnmSNO0VvcD6IEjQAfACPwEdG/qcslPvTo6BkT+6rFXgSdqbj8BvFJz+/fABkABvwO+a+ryt5YfYDjQDzhwvvUEhAOZNb/Dam6HNfW5Xc4/p6m354BHGnhu95r3SBOQVPPeqZf30Sapt1igX83tICC9pn7kmmvGP2eoN7nmmuFPa20xHgRkaJqWqWmaA/gncFMTl0n8tpuAlTW3VwI313n8Pa3a/wGhSqnYpihga6Np2jdAya8ePtd6ug7YrGlaiaZppcBmYOzFL33rdZp6O52bgH9qmmbXNO0okEH1e6i8j15imqblaZr2Q81tK3AIiEeuuWbtDPV2OnLNNaHWGozjgeN17udw5n+k4tLTgE1KqX1KqWk1j0VrmpZXczsfiK65LfXZvJxrPUn9NR9/qfnKfXnt1/FIvTVLSqn2QF/gO+SaazF+VW8g11yz01qDsWj+hmma1g+4HpihlBped6NW/X2TTKnSzEk9tSiLgWTgCiAP+FvTFkecjlIqEPgYeEDTtLK62+Saa74aqDe55pqh1hqMc4GEOvfb1jwmmglN03JrfhcC66j+CqmgtotEze/CmqdLfTYv51pPUn/NgKZpBZqmuTVN8wBLqb7mQOqtWVFK+VAdrt7XNG1tzcNyzTVzDdWbXHPNU2sNxt8DnZRSSUopI3AHsL6JyyRqKKUClFJBtbeBa4EDVNdR7ejpycCnNbfXA5NqRmD/DrDU+VpRXHrnWk9fAtcqpcJqvkq8tuYxcQn9ql/+OKqvOaiutzuUUialVBLQCdiDvI9eckopBSwDDmmaNr/OJrnmmrHT1Ztcc82ToakL0BQ0TXMppf5C9RuBHliuaVpKExdL/Ec0sK76vQQD8IGmaRuVUt8D/1JK3QNkARNqnv8F1aOvM4BKYOqlL3LrpJRaDVwNRCqlcoBngb9yDvWkaVqJUup5qt/0AeZpmna2A8PEeThNvV2tlLqC6q/hjwH/D0DTtBSl1L+Ag4ALmKFpmrvmOPI+emkNBe4GflFK7a95bDZyzTV3p6u3iXLNNT+y8p0QQgghhBC03q4UQgghhBBC1CPBWAghhBBCCCQYCyGEEEIIAUgwFkIIIYQQApBgLIQQQgghBCDBWAghhBBCCECCsRBCCCGEEIAEYyGEEEIIIQD4/1yBV9QMdFD7AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrF1s7fvYiNv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2b13d878-5eaa-447d-faa7-64f6f135084b"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.05, 100, 2300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"e0cc1a79-a6f0-4edd-9668-7c7e32b7d5e6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"e0cc1a79-a6f0-4edd-9668-7c7e32b7d5e6\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'e0cc1a79-a6f0-4edd-9668-7c7e32b7d5e6',\n",
              "                        [{\"type\": \"scatter\", \"x\": [20110225, 20110228, 20110301, 20110302, 20110303, 20110304, 20110307, 20110308, 20110309, 20110310, 20110311, 20110314, 20110315, 20110316, 20110317, 20110318, 20110321, 20110322, 20110323, 20110324, 20110325, 20110328, 20110329, 20110330, 20110331, 20110401, 20110404, 20110405, 20110406, 20110407, 20110408, 20110411, 20110412, 20110413, 20110414, 20110415, 20110418, 20110419, 20110420, 20110421, 20110425, 20110426, 20110427, 20110428, 20110429, 20110502, 20110503, 20110504, 20110505, 20110506, 20110509, 20110510, 20110511, 20110512, 20110513, 20110516, 20110517, 20110518, 20110519, 20110520, 20110523, 20110524, 20110525, 20110526, 20110527, 20110531, 20110601, 20110602, 20110603, 20110606, 20110607, 20110608, 20110609, 20110610, 20110613, 20110614, 20110615, 20110616, 20110617, 20110620, 20110621, 20110622, 20110623, 20110624, 20110627, 20110628, 20110629, 20110630, 20110701, 20110705, 20110706, 20110707, 20110708, 20110711, 20110712, 20110713, 20110714, 20110715, 20110718, 20110719, 20110720, 20110721, 20110722, 20110725, 20110726, 20110727, 20110728, 20110729, 20110801, 20110802, 20110803, 20110804, 20110805, 20110808, 20110809, 20110810, 20110811, 20110812, 20110815, 20110816, 20110817, 20110818, 20110819, 20110822, 20110823, 20110824, 20110825, 20110826, 20110829, 20110830, 20110831, 20110901, 20110902, 20110906, 20110907, 20110908, 20110909, 20110912, 20110913, 20110914, 20110915, 20110916, 20110919, 20110920, 20110921, 20110922, 20110923, 20110926, 20110927, 20110928, 20110929, 20110930, 20111003, 20111004, 20111005, 20111006, 20111007, 20111010, 20111011, 20111012, 20111013, 20111014, 20111017, 20111018, 20111019, 20111020, 20111021, 20111024, 20111025, 20111026, 20111027, 20111028, 20111031, 20111101, 20111102, 20111103, 20111104, 20111107, 20111108, 20111109, 20111110, 20111111, 20111114, 20111115, 20111116, 20111117, 20111118, 20111121, 20111122, 20111123, 20111125, 20111128, 20111129, 20111130, 20111201, 20111202, 20111205, 20111206, 20111207, 20111208, 20111209, 20111212, 20111213, 20111214, 20111215, 20111216, 20111219, 20111220, 20111221, 20111222, 20111223, 20111227, 20111228, 20111229, 20111230, 20120103, 20120104, 20120105, 20120106, 20120109, 20120110, 20120111, 20120112, 20120113, 20120117, 20120118, 20120119, 20120120, 20120123, 20120124, 20120125, 20120126, 20120127, 20120130, 20120131, 20120201, 20120202, 20120203, 20120206, 20120207, 20120208, 20120209, 20120210, 20120213, 20120214, 20120215, 20120216, 20120217, 20120221, 20120222, 20120223, 20120224, 20120227, 20120228, 20120229, 20120301, 20120302, 20120305, 20120306, 20120307, 20120308, 20120309, 20120312, 20120313, 20120314, 20120315, 20120316, 20120319, 20120320, 20120321, 20120322, 20120323, 20120326, 20120327, 20120328, 20120329, 20120330, 20120402, 20120403, 20120404, 20120405, 20120409, 20120410, 20120411, 20120412, 20120413, 20120416, 20120417, 20120418, 20120419, 20120420, 20120423, 20120424, 20120425, 20120426, 20120427, 20120430, 20120501, 20120502, 20120503, 20120504, 20120507, 20120508, 20120509, 20120510, 20120511, 20120514, 20120515, 20120516, 20120517, 20120518, 20120521, 20120522, 20120523, 20120524, 20120525, 20120529, 20120530, 20120531, 20120601, 20120604, 20120605, 20120606, 20120607, 20120608, 20120611, 20120612, 20120613, 20120614, 20120615, 20120618, 20120619, 20120620, 20120621, 20120622, 20120625, 20120626, 20120627, 20120628, 20120629, 20120702, 20120703, 20120705, 20120706, 20120709, 20120710, 20120711, 20120712, 20120713, 20120716, 20120717, 20120718, 20120719, 20120720, 20120723, 20120724, 20120725, 20120726, 20120727, 20120730, 20120731, 20120801, 20120802, 20120803, 20120806, 20120807, 20120808, 20120809, 20120810, 20120813, 20120814, 20120815, 20120816, 20120817, 20120820, 20120821, 20120822, 20120823, 20120824, 20120827, 20120828, 20120829, 20120830, 20120831, 20120904, 20120905, 20120906, 20120907, 20120910, 20120911, 20120912, 20120913, 20120914, 20120917, 20120918, 20120919, 20120920, 20120921, 20120924, 20120925, 20120926, 20120927, 20120928, 20121001, 20121002, 20121003, 20121004, 20121005, 20121008, 20121009, 20121010, 20121011, 20121012, 20121015, 20121016, 20121017, 20121018, 20121019, 20121022, 20121023, 20121024, 20121025, 20121026, 20121031, 20121101, 20121102, 20121105, 20121106, 20121107, 20121108, 20121109, 20121112, 20121113, 20121114, 20121115, 20121116, 20121119, 20121120, 20121121, 20121123, 20121126, 20121127, 20121128, 20121129, 20121130, 20121203, 20121204, 20121205, 20121206, 20121207, 20121210, 20121211, 20121212, 20121213, 20121214, 20121217, 20121218, 20121219, 20121220, 20121221, 20121224, 20121226, 20121227, 20121228, 20121231, 20130102, 20130103, 20130104, 20130107, 20130108, 20130109, 20130110, 20130111, 20130114, 20130115, 20130116, 20130117, 20130118, 20130122, 20130123, 20130124, 20130125, 20130128, 20130129, 20130130, 20130131, 20130201, 20130204, 20130205, 20130206, 20130207, 20130208, 20130211, 20130212, 20130213, 20130214, 20130215, 20130219, 20130220, 20130221, 20130222, 20130225, 20130226, 20130227, 20130228, 20130301, 20130304, 20130305, 20130306, 20130307, 20130308, 20130311, 20130312, 20130313, 20130314, 20130315, 20130318, 20130319, 20130320, 20130321, 20130322, 20130325, 20130326, 20130327, 20130328, 20130401, 20130402, 20130403, 20130404, 20130405, 20130408, 20130409, 20130410, 20130411, 20130412, 20130415, 20130416, 20130417, 20130418, 20130419, 20130422, 20130423, 20130424, 20130425, 20130426, 20130429, 20130430, 20130501, 20130502, 20130503, 20130506, 20130507, 20130508, 20130509, 20130510, 20130513, 20130514, 20130515, 20130516, 20130517, 20130520, 20130521, 20130522, 20130523, 20130524, 20130528, 20130529, 20130530, 20130531, 20130603, 20130604, 20130605, 20130606, 20130607, 20130610, 20130611, 20130612, 20130613, 20130614, 20130617, 20130618, 20130619, 20130620, 20130621, 20130624, 20130625, 20130626, 20130627, 20130628, 20130701, 20130702, 20130703, 20130705, 20130708, 20130709, 20130710, 20130711, 20130712, 20130715, 20130716, 20130717, 20130718, 20130719, 20130722, 20130723, 20130724, 20130725, 20130726, 20130729, 20130730, 20130731, 20130801, 20130802, 20130805, 20130806, 20130807, 20130808, 20130809, 20130812, 20130813, 20130814, 20130815, 20130816, 20130819, 20130820, 20130821, 20130822, 20130823, 20130826, 20130827, 20130828, 20130829, 20130830, 20130903, 20130904, 20130905, 20130906, 20130909, 20130910, 20130911, 20130912, 20130913, 20130916, 20130917, 20130918, 20130919, 20130920, 20130923, 20130924, 20130925, 20130926, 20130927, 20130930, 20131001, 20131002, 20131003, 20131004, 20131007, 20131008, 20131009, 20131010, 20131011, 20131014, 20131015, 20131016, 20131017, 20131018, 20131021, 20131022, 20131023, 20131024, 20131025, 20131028, 20131029, 20131030, 20131031, 20131101, 20131104, 20131105, 20131106, 20131107, 20131108, 20131111, 20131112, 20131113, 20131114, 20131115, 20131118, 20131119, 20131120, 20131121, 20131122, 20131125, 20131126, 20131127, 20131129, 20131202, 20131203, 20131204, 20131205, 20131206, 20131209, 20131210, 20131211, 20131212, 20131213, 20131216, 20131217, 20131218, 20131219, 20131220, 20131223, 20131224, 20131226, 20131227, 20131230, 20131231, 20140102, 20140103, 20140106, 20140107, 20140108, 20140109, 20140110, 20140113, 20140114, 20140115, 20140116, 20140117, 20140121, 20140122, 20140123, 20140124, 20140127, 20140128, 20140129, 20140130, 20140131, 20140203, 20140204, 20140205, 20140206, 20140207, 20140210, 20140211, 20140212, 20140213, 20140214, 20140218, 20140219, 20140220, 20140221, 20140224, 20140225, 20140226, 20140227, 20140228, 20140303, 20140304, 20140305, 20140306, 20140307, 20140310, 20140311, 20140312, 20140313, 20140314, 20140317, 20140318, 20140319, 20140320, 20140321, 20140324, 20140325, 20140326, 20140327, 20140328, 20140331, 20140401, 20140402, 20140403, 20140404, 20140407, 20140408, 20140409, 20140410, 20140411, 20140414, 20140415, 20140416, 20140417, 20140421, 20140422, 20140423, 20140424, 20140425, 20140428, 20140429, 20140430, 20140501, 20140502, 20140505, 20140506, 20140507, 20140508, 20140509, 20140512, 20140513, 20140514, 20140515, 20140516, 20140519, 20140520, 20140521, 20140522, 20140523, 20140527, 20140528, 20140529, 20140530, 20140602, 20140603, 20140604, 20140605, 20140606, 20140609, 20140610, 20140611, 20140612, 20140613, 20140616, 20140617, 20140618, 20140619, 20140620, 20140623, 20140624, 20140625, 20140626, 20140627, 20140630, 20140701, 20140702, 20140703, 20140707, 20140708, 20140709, 20140710, 20140711, 20140714, 20140715, 20140716, 20140717, 20140718, 20140721, 20140722, 20140723, 20140724, 20140725, 20140728, 20140729, 20140730, 20140731, 20140801, 20140804, 20140805, 20140806, 20140807, 20140808, 20140811, 20140812, 20140813, 20140814, 20140815, 20140818, 20140819, 20140820, 20140821, 20140822, 20140825, 20140826, 20140827, 20140828, 20140829, 20140902, 20140903, 20140904, 20140905, 20140908, 20140909, 20140910, 20140911, 20140912, 20140915, 20140916, 20140917, 20140918, 20140919, 20140922, 20140923, 20140924, 20140925, 20140926, 20140929, 20140930, 20141001, 20141002, 20141003, 20141006, 20141007, 20141008, 20141009, 20141010, 20141013, 20141014, 20141015, 20141016, 20141017, 20141020, 20141021, 20141022, 20141023, 20141024, 20141027, 20141028, 20141029, 20141030, 20141031, 20141103, 20141104, 20141105, 20141106, 20141107, 20141110, 20141111, 20141112, 20141113, 20141114, 20141117, 20141118, 20141119, 20141120, 20141121, 20141124, 20141125, 20141126, 20141128, 20141201, 20141202, 20141203, 20141204, 20141205, 20141208, 20141209, 20141210, 20141211, 20141212, 20141215, 20141216, 20141217, 20141218, 20141219, 20141222, 20141223, 20141224, 20141226, 20141229, 20141230, 20141231, 20150102, 20150105, 20150106, 20150107, 20150108, 20150109, 20150112, 20150113, 20150114, 20150115, 20150116, 20150120, 20150121, 20150122, 20150123, 20150126, 20150127, 20150128, 20150129, 20150130, 20150202, 20150203, 20150204, 20150205, 20150206, 20150209, 20150210, 20150211, 20150212, 20150213, 20150217, 20150218, 20150219, 20150220, 20150223, 20150224, 20150225, 20150226, 20150227, 20150302, 20150303, 20150304, 20150305, 20150306, 20150309, 20150310, 20150311, 20150312, 20150313, 20150316, 20150317, 20150318, 20150319, 20150320, 20150323, 20150324, 20150325, 20150326, 20150327, 20150330, 20150331, 20150401, 20150402, 20150406, 20150407, 20150408, 20150409, 20150410, 20150413, 20150414, 20150415, 20150416, 20150417, 20150420, 20150421, 20150422, 20150423, 20150424, 20150427, 20150428, 20150429, 20150430, 20150501, 20150504, 20150505, 20150506, 20150507, 20150508, 20150511, 20150512, 20150513, 20150514, 20150515, 20150518, 20150519, 20150520, 20150521, 20150522, 20150526, 20150527, 20150528, 20150529, 20150601, 20150602, 20150603, 20150604, 20150605, 20150608, 20150609, 20150610, 20150611, 20150612, 20150615, 20150616, 20150617, 20150618, 20150619, 20150622, 20150623, 20150624, 20150625, 20150626, 20150629, 20150630, 20150701, 20150702, 20150706, 20150707, 20150708, 20150709, 20150710, 20150713, 20150714, 20150715, 20150716, 20150717, 20150720, 20150721, 20150722, 20150723, 20150724, 20150727, 20150728, 20150729, 20150730, 20150731, 20150803, 20150804, 20150805, 20150806, 20150807, 20150810, 20150811, 20150812, 20150813, 20150814, 20150817, 20150818, 20150819, 20150820, 20150821, 20150824, 20150825, 20150826, 20150827, 20150828, 20150831, 20150901, 20150902, 20150903, 20150904, 20150908, 20150909, 20150910, 20150911, 20150914, 20150915, 20150916, 20150917, 20150918, 20150921, 20150922, 20150923, 20150924, 20150925, 20150928, 20150929, 20150930, 20151001, 20151002, 20151005, 20151006, 20151007, 20151008, 20151009, 20151012, 20151013, 20151014, 20151015, 20151016, 20151019, 20151020, 20151021, 20151022, 20151023, 20151026, 20151027, 20151028, 20151029, 20151030, 20151102, 20151103, 20151104, 20151105, 20151106, 20151109, 20151110, 20151111, 20151112, 20151113, 20151116, 20151117, 20151118, 20151119, 20151120, 20151123, 20151124, 20151125, 20151127, 20151130, 20151201, 20151202, 20151203, 20151204, 20151207, 20151208, 20151209, 20151210, 20151211, 20151214, 20151215, 20151216, 20151217, 20151218, 20151221, 20151222, 20151223, 20151224, 20151228, 20151229, 20151230, 20151231, 20160104, 20160105, 20160106, 20160107, 20160108, 20160111, 20160112, 20160113, 20160114, 20160115, 20160119, 20160120, 20160121, 20160122, 20160125, 20160126, 20160127, 20160128, 20160129, 20160201, 20160202, 20160203, 20160204, 20160205, 20160208, 20160209, 20160210, 20160211, 20160212, 20160216, 20160217, 20160218, 20160219, 20160222, 20160223, 20160224, 20160225, 20160226, 20160229, 20160301, 20160302, 20160303, 20160304, 20160307, 20160308, 20160309, 20160310, 20160311, 20160314, 20160315, 20160316, 20160317, 20160318, 20160321, 20160322, 20160323, 20160324, 20160328, 20160329, 20160330, 20160331, 20160401, 20160404, 20160405, 20160406, 20160407, 20160408, 20160411, 20160412, 20160413, 20160414, 20160415, 20160418, 20160419, 20160420, 20160421, 20160422, 20160425, 20160426, 20160427, 20160428, 20160429, 20160502, 20160503, 20160504, 20160505, 20160506, 20160509, 20160510, 20160511, 20160512, 20160513, 20160516, 20160517, 20160518, 20160519, 20160520, 20160523, 20160524, 20160525, 20160526, 20160527, 20160531, 20160601, 20160602, 20160603, 20160606, 20160607, 20160608, 20160609, 20160610, 20160613, 20160614, 20160615, 20160616, 20160617, 20160620, 20160621, 20160622, 20160623, 20160624, 20160627, 20160628, 20160629, 20160630, 20160701, 20160705, 20160706, 20160707, 20160708, 20160711, 20160712, 20160713, 20160714, 20160715, 20160718, 20160719, 20160720, 20160721, 20160722, 20160725, 20160726, 20160727, 20160728, 20160729, 20160801, 20160802, 20160803, 20160804, 20160805, 20160808, 20160809, 20160810, 20160811, 20160812, 20160815, 20160816, 20160817, 20160818, 20160819, 20160822, 20160823, 20160824, 20160825, 20160826, 20160829, 20160830, 20160831, 20160901, 20160902, 20160906, 20160907, 20160908, 20160909, 20160912, 20160913, 20160914, 20160915, 20160916, 20160919, 20160920, 20160921, 20160922, 20160923, 20160926, 20160927, 20160928, 20160929, 20160930, 20161003, 20161004, 20161005, 20161006, 20161007, 20161010, 20161011, 20161012, 20161013, 20161014, 20161017, 20161018, 20161019, 20161020, 20161021, 20161024, 20161025, 20161026, 20161027, 20161028, 20161031, 20161101, 20161102, 20161103, 20161104, 20161107, 20161108, 20161109, 20161110, 20161111, 20161114, 20161115, 20161116, 20161117, 20161118, 20161121, 20161122, 20161123, 20161125, 20161128, 20161129, 20161130, 20161201, 20161202, 20161205, 20161206, 20161207, 20161208, 20161209, 20161212, 20161213, 20161214, 20161215, 20161216, 20161219, 20161220, 20161221, 20161222, 20161223, 20161227, 20161228, 20161229, 20161230, 20170103, 20170104, 20170105, 20170106, 20170109, 20170110, 20170111, 20170112, 20170113, 20170117, 20170118, 20170119, 20170120, 20170123, 20170124, 20170125, 20170126, 20170127, 20170130, 20170131, 20170201, 20170202, 20170203, 20170206, 20170207, 20170208, 20170209, 20170210, 20170213, 20170214, 20170215, 20170216, 20170217, 20170221, 20170222, 20170223, 20170224, 20170227, 20170228, 20170301, 20170302, 20170303, 20170306, 20170307, 20170308, 20170309, 20170310, 20170313, 20170314, 20170315, 20170316, 20170317, 20170320, 20170321, 20170322, 20170323, 20170324, 20170327, 20170328, 20170329, 20170330, 20170331, 20170403, 20170404, 20170405, 20170406, 20170407, 20170410, 20170411, 20170412, 20170413, 20170417, 20170418, 20170419, 20170420, 20170421, 20170424, 20170425, 20170426, 20170427, 20170428, 20170501, 20170502, 20170503, 20170504, 20170505, 20170508, 20170509, 20170510, 20170511, 20170512, 20170515, 20170516, 20170517, 20170518, 20170519, 20170522, 20170523, 20170524, 20170525, 20170526, 20170530, 20170531, 20170601, 20170602, 20170605, 20170606, 20170607, 20170608, 20170609, 20170612, 20170613, 20170614, 20170615, 20170616, 20170619, 20170620, 20170621, 20170622, 20170623, 20170626, 20170627, 20170628, 20170629, 20170630, 20170703, 20170705, 20170706, 20170707, 20170710, 20170711, 20170712, 20170713, 20170714, 20170717, 20170718, 20170719, 20170720, 20170721, 20170724, 20170725, 20170726, 20170727, 20170728, 20170731, 20170801, 20170802, 20170803, 20170804, 20170807, 20170808, 20170809, 20170810, 20170811, 20170814, 20170815, 20170816, 20170817, 20170818, 20170821, 20170822, 20170823, 20170824, 20170825, 20170828, 20170829, 20170830, 20170831, 20170901, 20170905, 20170906, 20170907, 20170908, 20170911, 20170912, 20170913, 20170914, 20170915, 20170918, 20170919, 20170920, 20170921, 20170922, 20170925, 20170926, 20170927, 20170928, 20170929, 20171002, 20171003, 20171004, 20171005, 20171006, 20171009, 20171010, 20171011, 20171012, 20171013, 20171016, 20171017, 20171018, 20171019, 20171020, 20171023, 20171024, 20171025, 20171026, 20171027, 20171030, 20171031, 20171101, 20171102, 20171103, 20171106, 20171107, 20171108, 20171109, 20171110, 20171113, 20171114, 20171115, 20171116, 20171117, 20171120, 20171121, 20171122, 20171124, 20171127, 20171128, 20171129, 20171130, 20171201, 20171204, 20171205, 20171206, 20171207, 20171208, 20171211, 20171212, 20171213, 20171214, 20171215, 20171218, 20171219, 20171220, 20171221, 20171222, 20171226, 20171227, 20171228, 20171229, 20180102, 20180103, 20180104, 20180105, 20180108, 20180109, 20180110, 20180111, 20180112, 20180116, 20180117, 20180118, 20180119, 20180122, 20180123, 20180124, 20180125, 20180126, 20180129, 20180130, 20180131, 20180201, 20180202, 20180205, 20180206, 20180207, 20180208, 20180209, 20180212, 20180213, 20180214, 20180215, 20180216, 20180220, 20180221, 20180222, 20180223, 20180226, 20180227, 20180228, 20180301, 20180302, 20180305, 20180306, 20180307, 20180308, 20180309, 20180312, 20180313, 20180314, 20180315, 20180316, 20180319, 20180320, 20180321, 20180322, 20180323, 20180326, 20180327, 20180328, 20180329, 20180402, 20180403, 20180404, 20180405, 20180406, 20180409, 20180410, 20180411, 20180412, 20180413, 20180416, 20180417, 20180418, 20180419, 20180420, 20180423, 20180424, 20180425, 20180426, 20180427, 20180430, 20180501, 20180502, 20180503, 20180504, 20180507, 20180508, 20180509, 20180510, 20180511, 20180514, 20180515, 20180516, 20180517, 20180518, 20180521, 20180522, 20180523, 20180524, 20180525, 20180529, 20180530, 20180531, 20180601, 20180604, 20180605, 20180606, 20180607, 20180608, 20180611, 20180612, 20180613, 20180614, 20180615, 20180618, 20180619, 20180620, 20180621, 20180622, 20180625, 20180626, 20180627, 20180628, 20180629, 20180702, 20180703, 20180705, 20180706, 20180709, 20180710, 20180711, 20180712, 20180713, 20180716, 20180717, 20180718, 20180719, 20180720, 20180723, 20180724, 20180725, 20180726, 20180727, 20180730, 20180731, 20180801, 20180802, 20180803, 20180806, 20180807, 20180808, 20180809, 20180810, 20180813, 20180814, 20180815, 20180816, 20180817, 20180820, 20180821, 20180822, 20180823, 20180824, 20180827, 20180828, 20180829, 20180830, 20180831, 20180904, 20180905, 20180906, 20180907, 20180910, 20180911, 20180912, 20180913, 20180914, 20180917, 20180918, 20180919, 20180920, 20180921, 20180924, 20180925, 20180926, 20180927, 20180928, 20181001, 20181002, 20181003, 20181004, 20181005, 20181008, 20181009, 20181010, 20181011, 20181012, 20181015, 20181016, 20181017, 20181018, 20181019, 20181022, 20181023, 20181024, 20181025, 20181026, 20181029, 20181030, 20181031, 20181101, 20181102, 20181105, 20181106, 20181107, 20181108, 20181109, 20181112, 20181113, 20181114, 20181115, 20181116, 20181119, 20181120, 20181121, 20181123, 20181126, 20181127, 20181128, 20181129, 20181130, 20181203, 20181204, 20181206, 20181207, 20181210, 20181211, 20181212, 20181213, 20181214, 20181217, 20181218, 20181219, 20181220, 20181221, 20181224, 20181226, 20181227, 20181228, 20181231, 20190102, 20190103, 20190104, 20190107, 20190108, 20190109, 20190110, 20190111, 20190114, 20190115, 20190116, 20190117, 20190118, 20190122, 20190123, 20190124, 20190125, 20190128, 20190129, 20190130, 20190131, 20190201, 20190204, 20190205, 20190206, 20190207, 20190208, 20190211, 20190212, 20190213, 20190214, 20190215, 20190219, 20190220, 20190221, 20190222, 20190225, 20190226, 20190227, 20190228, 20190301, 20190304, 20190305, 20190306, 20190307, 20190308, 20190311, 20190312, 20190313, 20190314, 20190315, 20190318, 20190319, 20190320, 20190321, 20190322, 20190325, 20190326, 20190327, 20190328, 20190329, 20190401, 20190402, 20190403, 20190404, 20190405, 20190408, 20190409, 20190410, 20190411, 20190412, 20190415, 20190416, 20190417, 20190418, 20190422, 20190423, 20190424, 20190425, 20190426, 20190429, 20190430, 20190501, 20190502, 20190503, 20190506, 20190507, 20190508, 20190509, 20190510, 20190513, 20190514, 20190515, 20190516, 20190517, 20190520, 20190521, 20190522, 20190523, 20190524, 20190528, 20190529, 20190530, 20190531, 20190603, 20190604, 20190605, 20190606, 20190607, 20190610, 20190611, 20190612, 20190613, 20190614, 20190617, 20190618, 20190619, 20190620, 20190621, 20190624, 20190625, 20190626, 20190627, 20190628, 20190701, 20190702, 20190703, 20190705, 20190708, 20190709, 20190710, 20190711, 20190712, 20190715, 20190716, 20190717, 20190718, 20190719, 20190722, 20190723, 20190724, 20190725, 20190726, 20190729, 20190730, 20190731, 20190801, 20190802, 20190805, 20190806, 20190807, 20190808, 20190809, 20190812, 20190813, 20190814, 20190815, 20190816, 20190819, 20190820, 20190821, 20190822, 20190823, 20190826, 20190827, 20190828, 20190829, 20190830, 20190903, 20190904, 20190905, 20190906, 20190909, 20190910, 20190911, 20190912, 20190913, 20190916, 20190917, 20190918, 20190919, 20190920, 20190923, 20190924, 20190925, 20190926, 20190927, 20190930, 20191001, 20191002, 20191003, 20191004, 20191007, 20191008, 20191009, 20191010, 20191011, 20191014, 20191015, 20191016, 20191017, 20191018, 20191021, 20191022, 20191023, 20191024, 20191025, 20191028, 20191029, 20191030, 20191031, 20191101, 20191104, 20191105, 20191106, 20191107, 20191108, 20191111, 20191112, 20191113, 20191114, 20191115, 20191118, 20191119, 20191120], \"y\": [31.426800000000004, 31.382199999999997, 31.334, 31.2896, 31.2748, 31.254, 31.225, 31.2304, 31.2598, 31.2748, 31.2864, 31.286400000000004, 31.295400000000004, 31.303800000000003, 31.3148, 31.3386, 31.356000000000005, 31.3768, 31.412399999999998, 31.436799999999998, 31.4194, 31.404199999999996, 31.387000000000004, 31.3816, 31.385999999999996, 31.409800000000004, 31.425799999999995, 31.443599999999996, 31.4586, 31.4744, 31.4542, 31.4438, 31.436600000000002, 31.4236, 31.4296, 31.4364, 31.4284, 31.4388, 31.463800000000003, 31.506699999999995, 31.543899999999997, 31.5863, 31.6061, 31.625500000000002, 31.666299999999996, 31.6943, 31.71569999999999, 31.734899999999996, 31.7527, 31.765299999999996, 31.777299999999997, 31.804700000000004, 31.821699999999996, 31.8351, 31.832499999999996, 31.8375, 31.8495, 31.835680000000004, 31.80808, 31.76428, 31.717480000000002, 31.668879999999998, 31.62908, 31.592280000000006, 31.54848000000001, 31.499480000000002, 31.45608, 31.405880000000003, 31.34128, 31.27348000000001, 31.24268, 31.246280000000002, 31.270680000000002, 31.29688, 31.308479999999996, 31.309079999999998, 31.31068, 31.30188, 31.29708, 31.27808, 31.27508, 31.25868, 31.239280000000004, 31.21548, 31.184479999999997, 31.144680000000005, 31.11808, 31.05688, 30.99288, 30.867379999999997, 30.768780000000003, 30.65858, 30.59238, 30.488180000000003, 30.448780000000003, 30.407379999999993, 30.33838, 30.275179999999995, 30.27518, 30.22798, 30.20158, 30.17138, 30.14198, 30.119779999999995, 30.10278, 30.085579999999997, 30.06778, 30.0753, 30.069300000000002, 30.0563, 30.062900000000003, 30.054700000000004, 30.020300000000002, 29.974899999999998, 29.942299999999996, 29.9119, 29.8799, 29.8561, 29.824099999999998, 29.8009, 29.7663, 29.6813, 29.591300000000004, 29.5061, 29.451099999999997, 29.381099999999996, 29.3181, 29.2523, 29.1537, 29.078899999999997, 29.0211, 29.0071, 28.9905, 28.9881, 28.976700000000005, 28.995099999999997, 29.012500000000003, 29.058900000000005, 29.0891, 29.179299999999998, 29.2363, 29.3067, 29.364100000000004, 29.4237, 29.385499999999997, 29.348499999999998, 29.340100000000003, 29.371299999999998, 29.3813, 29.374699999999997, 29.3537, 29.3345, 29.3389, 29.335500000000007, 29.3221, 29.2773, 29.212900000000005, 29.151400000000002, 29.104400000000002, 29.0742, 29.0194, 28.9588, 28.9094, 28.853199999999998, 28.777600000000003, 28.692600000000002, 28.6046, 28.526799999999998, 28.453400000000002, 28.395800000000005, 28.340000000000003, 28.297400000000003, 28.243399999999998, 28.176399999999997, 28.094, 28.0172, 27.9358, 27.8534, 27.793400000000002, 27.623800000000003, 27.5198, 27.413399999999996, 27.317999999999998, 27.232000000000003, 27.232, 27.136200000000002, 27.0364, 26.9324, 26.839399999999998, 26.740399999999994, 26.6488, 26.557, 26.4352, 26.2976, 26.249000000000002, 26.2, 26.149999999999995, 26.096999999999994, 26.051399999999994, 26.009999999999994, 25.967799999999997, 25.913200000000003, 25.856800000000003, 25.789, 25.7326, 25.7064, 25.698800000000002, 25.683000000000003, 25.730199999999996, 25.758999999999997, 25.788799999999995, 25.837400000000002, 25.892000000000003, 25.9666, 26.0418, 26.130399999999998, 26.225200000000005, 26.305400000000006, 26.3922, 26.466399999999993, 26.5368, 26.609, 26.677400000000002, 26.751199999999997, 26.821200000000005, 26.9004, 26.97620000000001, 27.071399999999997, 27.179000000000002, 27.2984, 27.399399999999996, 27.502, 27.616799999999998, 27.704799999999995, 27.779, 27.855, 27.9458, 28.040599999999998, 28.143600000000003, 28.257, 28.378599999999995, 28.498399999999997, 28.629399999999997, 28.773000000000003, 28.9116, 29.0482, 29.1538, 29.2582, 29.377599999999997, 29.491799999999998, 29.589200000000005, 29.6888, 29.779600000000002, 29.8806, 29.976599999999998, 30.057000000000002, 30.123800000000003, 30.1864, 30.1818, 30.206400000000002, 30.242600000000003, 30.2648, 30.280599999999996, 30.297600000000003, 30.319000000000006, 30.329600000000006, 30.356000000000005, 30.386400000000002, 30.4228, 30.476800000000004, 30.571199999999997, 30.571199999999997, 30.663400000000003, 30.770000000000003, 30.891800000000003, 31.000800000000005, 31.0994, 31.187399999999997, 31.2672, 31.3492, 31.446999999999992, 31.524399999999996, 31.590799999999998, 31.674799999999994, 31.7582, 31.8152, 31.851800000000004, 31.896600000000003, 31.937, 31.959600000000002, 31.9716, 31.970999999999993, 31.968200000000003, 31.951399999999992, 31.933400000000006, 31.9118, 31.892400000000002, 31.875999999999998, 31.871000000000002, 31.88, 31.901, 31.914600000000004, 31.939599999999995, 31.9576, 31.963799999999996, 31.982400000000002, 32.0092, 32.0518, 32.102599999999995, 32.111799999999995, 32.12345, 32.12805, 32.13805, 32.13985, 32.13605, 32.15725, 32.17125, 32.18045, 32.17405, 32.15745, 32.13705, 32.072050000000004, 32.01745, 31.957050000000002, 31.894250000000003, 31.839649999999995, 31.810450000000007, 31.78065, 31.75085, 31.70585, 31.634249999999998, 31.57325, 31.522650000000002, 31.455649999999995, 31.428649999999998, 31.420650000000006, 31.419050000000002, 31.407450000000004, 31.400850000000002, 31.471050000000005, 31.53505, 31.61925, 31.703450000000007, 31.795650000000002, 31.894449999999996, 32.00825, 32.12825, 32.24645, 32.349650000000004, 32.44825, 32.55065, 32.669650000000004, 32.86395, 32.972150000000006, 33.08335, 33.19235, 33.29755, 33.297549999999994, 33.39874999999999, 33.53034999999999, 33.685300000000005, 33.8443, 34.025499999999994, 34.2087, 34.3919, 34.557300000000005, 34.723800000000004, 34.9008, 35.0744, 35.2376, 35.397200000000005, 35.5656, 35.727000000000004, 35.88420000000001, 36.0414, 36.208, 36.361200000000004, 36.521, 36.6922, 36.873799999999996, 37.0826, 37.285999999999994, 37.487, 37.692, 37.8694, 38.0334, 38.1992, 38.3692, 38.5344, 38.63, 38.743399999999994, 38.8488, 38.95, 39.037200000000006, 39.128, 39.216800000000006, 39.326, 39.4172, 39.4776, 39.5388, 39.6104, 39.6646, 39.739, 39.8339, 39.9117, 39.975699999999996, 40.0401, 40.0831, 40.121900000000004, 40.14660000000001, 40.142599999999995, 40.1554, 40.17920000000001, 40.18900000000001, 40.202600000000004, 40.2218, 40.2415, 40.268299999999996, 40.2991, 40.3443, 40.3869, 40.4151, 40.445299999999996, 40.4735, 40.50129999999999, 40.53189999999999, 40.5623, 40.5977, 40.6377, 40.6635, 40.6817, 40.7135, 40.7511, 40.7755, 40.805699999999995, 40.8373, 40.8701, 40.890299999999996, 40.9123, 40.93429999999999, 40.963699999999996, 41.0045, 41.0599, 41.11110000000001, 41.11110000000001, 41.1665, 41.2047, 41.2585, 41.3329, 41.408899999999996, 41.4729, 41.5571, 41.6287, 41.6893, 41.7671, 41.8583, 41.95269999999999, 42.1219, 42.301899999999996, 42.477, 42.63399999999999, 42.728199999999994, 42.802600000000005, 42.88660000000001, 42.971000000000004, 43.0476, 43.123000000000005, 43.1824, 43.230999999999995, 43.2782, 43.32599999999999, 43.375800000000005, 43.412, 43.4629, 43.4977, 43.514100000000006, 43.5463, 43.56129999999999, 43.5745, 43.6149, 43.664899999999996, 43.68909999999999, 43.7161, 43.7771, 43.8323, 43.876099999999994, 43.9208, 43.97459999999999, 44.0248, 44.0738, 44.117599999999996, 44.1346, 44.15200000000001, 44.173, 44.206199999999995, 44.224, 44.231100000000005, 44.2215, 44.255300000000005, 44.29769999999999, 44.34589999999999, 44.3815, 44.42210000000001, 44.451499999999996, 44.481700000000004, 44.51449999999999, 44.5335, 44.491899999999994, 44.44409999999999, 44.4163, 44.394299999999994, 44.3991, 44.4079, 44.409699999999994, 44.40469999999999, 44.398500000000006, 44.42229999999999, 44.4645, 44.508100000000006, 44.545500000000004, 44.598099999999995, 44.6779, 44.78490000000001, 44.867200000000004, 44.962399999999995, 45.1594, 45.2584, 45.391999999999996, 45.5212, 45.6474, 45.647400000000005, 45.79280000000001, 45.9336, 46.040400000000005, 46.135999999999996, 46.2644, 46.38849999999999, 46.51349999999999, 46.63430000000001, 46.7675, 46.8817, 47.01030000000001, 47.13409999999999, 47.2525, 47.3191, 47.3759, 47.443000000000005, 47.5066, 47.510600000000004, 47.5078, 47.499, 47.5088, 47.516400000000004, 47.5519, 47.5681, 47.5907, 47.61589999999999, 47.637299999999996, 47.6627, 47.707300000000004, 47.7547, 47.818299999999994, 47.863099999999996, 47.9079, 47.930099999999996, 47.9433, 47.930299999999995, 47.8979, 47.880500000000005, 47.86489999999999, 47.8179, 47.74010000000001, 47.6673, 47.6127, 47.56109999999999, 47.50629999999999, 47.4595, 47.399899999999995, 47.312900000000006, 47.219300000000004, 47.116099999999996, 46.99009999999999, 46.8759, 46.781499999999994, 46.7087, 46.6435, 46.59749999999999, 46.56590000000001, 46.5283, 46.471099999999986, 46.441500000000005, 46.4123, 46.414699999999996, 46.42329999999999, 46.45826, 46.49766, 46.521460000000005, 46.56166, 46.64126000000001, 46.71665999999999, 46.78486, 46.82666000000001, 46.87326000000001, 46.90036, 46.93976, 46.95796, 46.96896000000001, 46.986760000000004, 46.98596, 46.96816, 46.965360000000004, 47.00476, 47.123760000000004, 47.26596000000001, 47.41215999999999, 47.563359999999996, 47.72615999999999, 47.72615999999999, 47.89596, 48.07616000000001, 48.28736, 48.497960000000006, 48.66455999999998, 48.82896000000001, 48.97896, 49.13056, 49.286359999999995, 49.444759999999995, 49.59155999999999, 49.75035999999999, 49.90195999999999, 50.077160000000006, 50.248360000000005, 50.40596000000001, 50.541360000000005, 50.690960000000004, 50.85616, 51.00916, 51.17215999999999, 51.33236, 51.49116, 51.64156, 51.76875999999999, 51.88996, 52.033199999999994, 52.1924, 52.34479999999999, 52.361000000000004, 52.373200000000004, 52.397200000000005, 52.44040000000001, 52.4972, 52.533, 52.5648, 52.5872, 52.6082, 52.62960000000001, 52.67700000000001, 52.747, 52.7962, 52.861999999999995, 52.908, 52.94540000000001, 52.94700000000001, 52.919799999999995, 52.90520000000001, 52.874399999999994, 52.842000000000006, 52.788999999999994, 52.7054, 52.5916, 52.491800000000005, 52.415, 52.33260000000001, 52.22540000000001, 52.1154, 52.014799999999994, 51.92320000000001, 51.81539999999999, 51.657, 51.5, 51.3626, 51.2051, 51.04870000000001, 50.905699999999996, 50.7325, 50.5365, 50.34610000000001, 50.1645, 50.0067, 49.82730000000001, 49.6541, 49.4713, 49.2691, 49.0659, 48.8503, 48.6717, 48.6365, 48.5321, 48.4393, 48.32170000000001, 48.2369, 48.13789999999999, 48.13789999999999, 48.0421, 47.9671, 47.9121, 47.8377, 47.70129999999999, 47.578106000000005, 47.43530600000001, 47.27730600000001, 47.13550600000001, 46.992706000000005, 46.815706000000006, 46.65310600000001, 46.506306, 46.357906000000014, 46.216706000000016, 46.13710600000001, 46.07410599999999, 46.016506, 45.95830599999999, 45.899306, 45.878105999999995, 45.856306000000004, 45.829106, 45.791706000000005, 45.768505999999995, 45.75190599999999, 45.721506, 45.675706, 45.633005999999995, 45.585406000000006, 45.551806, 45.523206, 45.481006, 45.43260600000001, 45.38420600000001, 45.328405999999994, 45.291605999999994, 45.227006, 45.151205999999995, 45.10120599999999, 45.051206, 45.015206000000006, 44.94460599999999, 44.859406, 44.76280599999999, 44.679806000000006, 44.614605999999995, 44.571206000000004, 44.51580599999999, 44.49380599999999, 44.486405999999995, 44.471606, 44.436406, 44.395006, 44.41720599999999, 44.44740000000001, 44.474, 44.50599999999999, 44.515800000000006, 44.56320000000001, 44.659400000000005, 44.7268, 44.806000000000004, 44.87960000000001, 44.958400000000005, 44.9824, 45.01999999999999, 45.05799999999999, 45.101600000000005, 45.187000000000005, 45.26160000000001, 45.336200000000005, 45.40320000000001, 45.4804, 45.551399999999994, 45.657399999999996, 45.78059999999999, 45.9096, 46.0576, 46.208, 46.3382, 46.5876, 46.7162, 46.82719999999999, 46.942, 47.0586, 47.058599999999984, 47.17979999999999, 47.3062, 47.424, 47.5434, 47.654, 47.77400000000001, 47.8984, 48.0426, 48.19579999999999, 48.3508, 48.50919999999999, 48.6828, 48.849000000000004, 49.019400000000005, 49.175399999999996, 49.340599999999995, 49.499, 49.6606, 49.8182, 49.94040000000001, 50.074000000000005, 50.2096, 50.3226, 50.421800000000005, 50.5414, 50.605, 50.654799999999994, 50.7126, 50.778800000000004, 50.82280000000001, 50.845200000000006, 50.869400000000006, 50.860600000000005, 50.848800000000004, 50.838, 50.8232, 50.794900000000005, 50.7717, 50.7611, 50.740500000000004, 50.72250000000001, 50.6975, 50.6825, 50.6921, 50.710100000000004, 50.72689999999999, 50.7495, 50.7821, 50.77049999999999, 50.7393, 50.72670000000001, 50.7385, 50.74570000000001, 50.7495, 50.7757, 50.801700000000004, 50.83330000000001, 50.86070000000001, 50.8623, 50.869299999999996, 50.867700000000006, 50.8577, 50.83330000000001, 50.795500000000004, 50.761500000000005, 50.7433, 50.73849999999999, 50.735699999999994, 50.7329, 50.7593, 50.7879, 50.830900000000014, 50.8584, 50.878, 50.87820000000001, 50.9178, 50.976000000000006, 51.0158, 51.0626, 51.1126, 51.209799999999994, 51.24360000000001, 51.2662, 51.282799999999995, 51.3034, 51.30339999999999, 51.32589999999999, 51.347699999999996, 51.3588, 51.364799999999995, 51.35979999999999, 51.353199999999994, 51.352799999999995, 51.3578, 51.352199999999996, 51.340999999999994, 51.2846, 51.2208, 51.196999999999996, 51.1658, 51.115, 51.035999999999994, 50.9722, 50.90520000000001, 50.8382, 50.76480000000001, 50.6928, 50.702200000000005, 50.705799999999996, 50.7188, 50.7244, 50.7246, 50.733200000000004, 50.729699999999994, 50.7369, 50.723499999999994, 50.705, 50.669200000000004, 50.6282, 50.593999999999994, 50.572199999999995, 50.5448, 50.532700000000006, 50.522099999999995, 50.51870000000001, 50.5069, 50.5019, 50.508900000000004, 50.5171, 50.536300000000004, 50.576899999999995, 50.60929999999998, 50.6497, 50.7135, 50.790400000000005, 50.8718, 50.9646, 51.06440000000001, 51.15380000000001, 51.235400000000006, 51.3174, 51.38919999999999, 51.451499999999996, 51.51489999999999, 51.59969999999999, 51.7003, 51.8471, 52.012699999999995, 52.18290000000001, 52.3745, 52.5845, 52.79690000000001, 52.99610000000001, 53.17650000000001, 53.3596, 53.550399999999996, 53.72880000000001, 53.82899999999999, 53.9572, 54.07339999999999, 54.21099999999999, 54.366600000000005, 54.5286, 54.6997, 54.8777, 55.052699999999994, 55.223200000000006, 55.72159999999998, 56.04119999999999, 56.30059999999999, 56.5272, 56.7402, 56.7402, 56.9442, 57.14939999999999, 57.3798, 57.5912, 57.812200000000004, 58.0318, 58.23579999999999, 58.4394, 58.677600000000005, 58.921200000000006, 59.1748, 59.4233, 59.65789999999999, 59.8715, 60.0919, 60.319, 60.5594, 60.79259999999999, 61.008399999999995, 61.228100000000005, 61.4321, 61.62250000000001, 61.8019, 61.9909, 62.171699999999994, 62.3781, 62.5567, 62.7611, 62.99750000000001, 63.218500000000006, 63.51689999999999, 63.7904, 64.0984, 64.3992, 64.6934, 64.9806, 65.2442, 65.47800000000001, 65.686, 65.9104, 66.13859999999998, 66.35979999999999, 66.5836, 66.8322, 67.07319999999999, 67.21679999999999, 67.3328, 67.50580000000001, 67.736, 67.97059999999999, 68.1964, 68.4052, 68.57939999999999, 68.7552, 68.9358, 69.1216, 69.3216, 69.514, 69.52579999999998, 69.5068, 69.4748, 69.4276, 69.38740000000001, 69.38, 69.37819999999999, 69.35440000000001, 69.3438, 69.3424, 69.3704, 69.3958, 69.40939999999999, 69.2294, 69.06020000000001, 68.86880000000001, 68.68820000000001, 68.479, 68.29799999999999, 68.09240000000001, 67.86019999999999, 67.653, 67.152, 66.8796, 66.633, 66.3918, 66.1368, 66.13680000000001, 65.90180000000001, 65.68700000000001, 65.4598, 65.2026, 64.94460000000001, 64.69099999999999, 64.4342, 64.15520000000001, 63.8324, 63.5268, 63.2116, 62.886, 62.54880000000001, 62.18860000000001, 61.83959999999999, 61.503, 61.17579999999999, 60.847, 60.5162, 60.1596, 59.8, 59.43999999999999, 59.2588, 59.0896, 58.931599999999996, 58.790600000000005, 58.6596, 58.51559999999999, 58.34839999999999, 58.2118, 58.06179999999999, 57.899800000000006, 57.723, 57.541199999999996, 57.358999999999995, 57.3672, 57.342, 57.352999999999994, 57.334799999999994, 57.321, 57.3348, 57.2966, 57.2724, 57.234799999999986, 57.16839999999999, 57.0928, 57.025200000000005, 56.9416, 56.85139999999999, 56.76559999999999, 56.691399999999994, 56.6046, 56.52440000000001, 56.4398, 56.343, 56.21060000000001, 56.03099999999999, 55.87, 55.75079999999999, 55.641000000000005, 55.5466, 55.434000000000005, 55.288999999999994, 55.1918, 55.09859999999999, 54.998599999999996, 54.919000000000004, 54.81799999999999, 54.7042, 54.6284, 54.542, 54.45179999999999, 54.337199999999996, 54.24059999999999, 54.12479999999999, 53.989799999999995, 53.837599999999995, 53.6806, 53.5688, 53.423399999999994, 53.265600000000006, 53.02039999999999, 52.91539999999999, 52.82339999999999, 52.758799999999994, 52.7138, 52.71379999999999, 52.66139999999999, 52.62219999999999, 52.58520000000001, 52.516799999999996, 52.4992, 52.486000000000004, 52.501400000000004, 52.5354, 52.596000000000004, 52.6166, 52.637399999999985, 52.6438, 52.6632, 52.67100000000001, 52.712399999999995, 52.7686, 52.8254, 52.89900000000001, 53.014, 53.1944, 53.3546, 53.4888, 53.623200000000004, 53.733599999999996, 53.852999999999994, 54.0058, 54.115, 54.22879999999999, 54.3538, 54.46719999999999, 54.62219999999999, 54.773, 54.925599999999996, 55.07559999999999, 55.24, 55.412600000000005, 55.588, 55.7594, 55.952600000000004, 56.154399999999995, 56.3258, 56.4476, 56.5678, 56.708, 56.7966, 56.89019999999999, 56.971599999999995, 57.050999999999995, 57.111799999999995, 57.176399999999994, 57.1982, 57.199, 57.17139999999999, 57.1513, 57.143499999999996, 57.11729999999999, 57.0729, 57.0325, 56.961299999999994, 56.90549999999999, 56.825500000000005, 56.7641, 56.6672, 56.5792, 56.4742, 56.36640000000001, 56.2829, 56.1927, 56.1113, 56.005300000000005, 55.9075, 55.8125, 55.7263, 55.718700000000005, 55.7337, 55.747299999999996, 55.75170000000001, 55.729699999999994, 55.7149, 55.7079, 55.673100000000005, 55.647499999999994, 55.62670000000001, 55.58550000000001, 55.5383, 55.53830000000001, 55.4925, 55.443499999999986, 55.4025, 55.3853, 55.4039, 55.43150000000001, 55.4788, 55.53500000000001, 55.59369999999999, 55.63589999999999, 55.67709999999999, 55.71869999999999, 55.7429, 55.76689999999999, 55.807700000000004, 55.87709999999999, 55.975699999999996, 56.067, 56.143600000000006, 56.2252, 56.318999999999996, 56.43619999999999, 56.53059999999999, 56.632600000000004, 56.73420000000001, 56.8056, 56.90429999999999, 56.99089999999999, 57.0761, 57.15650000000001, 57.203399999999995, 57.260799999999996, 57.30299999999999, 57.37499999999999, 57.4587, 57.5635, 57.6579, 57.66929999999999, 57.68690000000001, 57.685900000000004, 57.706099999999985, 57.75549999999999, 57.8097, 57.868900000000004, 57.91710000000001, 57.9605, 58.0017, 58.046499999999995, 58.1061, 58.171499999999995, 58.23129999999999, 58.31250000000001, 58.37389999999999, 58.3263, 58.2717, 58.2041, 58.14860000000001, 58.07640000000001, 58.0369, 58.0097, 57.99149999999999, 57.9736, 57.9863, 57.9909, 58.0025, 58.002300000000005, 58.007299999999994, 57.9895, 57.968500000000006, 57.945100000000004, 57.92170000000001, 57.86290000000001, 57.84429800000001, 57.823297999999994, 57.841898, 57.891098, 57.945298, 58.005098, 58.068698, 58.132698, 58.22069799999999, 58.378498, 58.45009799999999, 58.489597999999994, 58.494198, 58.509997999999996, 58.509998, 58.526197999999994, 58.510798, 58.497997999999995, 58.523598, 58.585198, 58.643198000000005, 58.721198, 58.742198, 58.73159799999999, 58.740598, 58.788198, 58.85239799999999, 58.91439799999999, 58.97159799999999, 59.026798, 59.07459800000001, 59.231798, 59.387598, 59.55179799999999, 59.718398, 59.90739799999999, 60.083197999999996, 60.272397999999995, 60.452798, 60.639098000000004, 60.805798, 60.973397999999996, 61.127798000000006, 61.297798, 61.438998, 61.609198, 61.848598, 62.108998, 62.343998, 62.571798, 62.79080000000001, 63.032799999999995, 63.279799999999994, 63.52400000000001, 63.74580000000001, 63.97240000000001, 64.18960000000001, 64.4104, 64.59479999999999, 64.76720000000002, 64.9534, 65.14529999999999, 65.3449, 65.5069, 65.6757, 65.8649, 66.0707, 66.26190000000001, 66.4095, 66.5075, 66.60390000000001, 66.6635, 66.8027, 66.9705, 67.1049, 67.17309999999999, 67.2215, 67.2451, 67.2715, 67.29230000000001, 67.31450000000001, 67.29889999999999, 67.2821, 67.27770000000001, 67.2683, 67.2313, 67.2071, 67.1793, 67.16329999999999, 67.1253, 67.1037, 67.07130000000001, 67.0433, 67.00269999999999, 66.9897, 66.8793, 66.76410000000001, 66.65809999999999, 66.5593, 66.4706, 66.47059999999999, 66.3576, 66.2458, 66.1226, 65.9906, 65.8544, 65.721, 65.5922, 65.4216, 65.2484, 65.06400000000001, 64.85810000000001, 64.62590000000002, 64.43050000000001, 64.2169, 63.99650000000001, 63.774899999999995, 63.568299999999994, 63.39070000000001, 63.243300000000005, 63.0977, 62.927699999999994, 62.74870000000001, 62.551899999999996, 62.3867, 62.2483, 62.1057, 61.9723, 61.838699999999996, 61.7201, 61.64330000000001, 61.6109, 61.57150000000001, 61.4869, 61.3979, 61.3313, 61.2551, 61.1725, 61.1057, 61.0655, 61.0251, 60.99130000000001, 60.953099999999985, 60.921499999999995, 60.881899999999995, 60.84929999999999, 60.78710000000001, 60.7377, 60.6883, 60.651500000000006, 60.59579999999999, 60.54559999999999, 60.494200000000006, 60.445200000000014, 60.4132, 60.3862, 60.34800000000001, 60.308, 60.3622, 60.4504, 60.525999999999996, 60.61580000000001, 60.75900000000001, 60.891400000000004, 61.010600000000004, 61.11999999999999, 61.2334, 61.35380000000001, 61.58118, 61.79158, 61.962579999999996, 62.15397999999999, 62.33718, 62.53278, 62.69558000000001, 62.86498, 63.05857999999999, 63.27118, 63.48498000000001, 63.69138000000001, 63.87478000000001, 64.03678, 64.45378000000001, 64.69598, 64.93618, 65.18578, 65.42237999999999, 65.42238, 65.64258, 65.83058, 66.01397999999999, 66.20138, 66.39738, 66.60978, 66.80898, 67.01018, 67.20598000000001, 67.40418000000001, 67.58658, 67.76158, 67.95258, 68.15238000000001, 68.32358, 68.51217999999999, 68.69838, 68.88378, 69.07318000000001, 69.24318, 69.40338, 69.54478, 69.69398, 69.84237999999999, 69.97518, 70.14157999999999, 70.30618, 70.48777999999999, 70.64178, 70.78078, 70.7706, 70.7546, 70.76679999999999, 70.7838, 70.8184, 70.8354, 70.859, 70.9044, 70.9132, 70.92124999999999, 70.93555, 70.94515, 70.96435000000001, 70.97895, 70.99695, 71.03735, 70.96255, 70.90214999999999, 70.85275, 70.78815, 70.72055, 70.67535, 70.60695, 70.51655, 70.44014999999999, 70.34035, 70.25895, 70.18955, 70.12335, 70.04675, 69.98235000000001, 69.93495, 69.89654999999999, 69.85534999999999, 69.83494999999999, 69.80615, 69.78195000000001, 69.74954999999999, 69.74495, 69.77035, 69.78355, 69.81275000000001, 69.83215, 69.86534999999999, 69.89155, 69.91154999999999, 69.94215, 70.00615, 70.08675000000001, 70.19914999999999, 70.55395, 70.74295000000001, 70.93035, 71.09475, 71.29615, 71.29615, 71.48715, 71.64215, 71.81255, 71.9725, 72.1372, 72.29639999999999, 72.423, 72.5258, 72.6268, 72.70179999999999, 72.8726, 73.0364, 73.18260000000001, 73.3554, 73.52779999999998, 73.6904, 73.88119999999999, 74.092, 74.2678, 74.45179999999999, 74.6402, 74.816, 74.99520000000001, 75.18920000000001, 75.38119999999999, 75.56259999999999, 75.62280000000001, 75.6996, 75.80199999999999, 75.902, 75.98899999999999, 76.0712, 76.102, 76.11540000000001, 76.14720000000001, 76.14200000000001, 76.1446, 76.13380000000001, 76.12979999999999, 76.1288, 76.15899999999999, 76.1344, 76.105, 76.0662, 76.03500000000001, 75.9562, 75.8986, 75.8828, 75.8822, 75.8444, 75.84419999999999, 75.88040000000001, 75.93360000000001, 76.0194, 76.08, 76.11638400000001, 76.185984, 76.248884, 76.303384, 76.356184, 76.419384, 76.423984, 76.429184, 76.422584, 76.396384, 76.369584, 76.329984, 76.30838399999999, 76.316984, 76.304384, 76.292384, 76.284584, 76.279984, 76.255184, 76.23718400000001, 76.218584, 76.320184, 76.403784, 76.455384, 76.522984, 76.61238399999999, 76.83338400000001, 76.978584, 77.089184, 77.206584, 77.325984, 77.325984, 77.558584, 77.756784, 77.971384, 78.195784, 78.374584, 78.55878400000002, 78.738984, 78.877984, 79.022184, 79.16518400000001, 79.27498399999999, 79.403184, 79.556184, 79.687584, 79.810784, 79.906384, 79.97298400000001, 80.06758400000001, 80.179, 80.2702, 80.3793, 80.4924, 80.6084, 80.7246, 80.8652, 80.9916, 81.133, 81.31, 81.50160000000001, 81.70700000000001, 81.8972, 82.06559999999999, 82.2358, 82.37, 82.53, 82.6984, 82.8974, 83.08460000000001, 83.2526, 83.4034, 83.5624, 83.72420000000002, 83.88060000000002, 83.99719999999999, 84.1314, 84.27960000000002, 84.41340000000002, 84.58460000000001, 84.78020000000001, 84.9676, 85.021, 85.102, 85.1734, 85.2146, 85.33139999999999, 85.4454, 85.55419999999998, 85.65440000000002, 86.01800000000001, 86.3678, 86.73899999999999, 87.1146, 87.486, 87.81499999999998, 88.13099999999999, 88.4328, 88.6776, 88.83220000000001, 89.03020000000001, 89.17939999999999, 89.27159999999998, 89.38860000000001, 89.51799999999999, 89.6402, 89.8222, 90.03259999999999, 90.2224, 90.37579999999998, 90.5246, 90.67299999999999, 91.0632, 91.2726, 91.49859999999998, 91.67099999999999, 91.85679999999998, 91.85679999999998, 92.04259999999998, 92.25460000000001, 92.48819999999999, 92.7378, 93.0192, 93.29540000000001, 93.5478, 93.82580000000002, 94.07780000000001, 94.31380000000001, 94.5384, 94.74980000000001, 94.95340000000002, 95.1226, 95.2898, 95.5148, 95.70459999999999, 95.87280000000003, 96.0678, 96.2068, 96.37140000000001, 96.53840000000002, 96.4358, 96.3034, 96.16380000000001, 96.0412, 95.914, 95.87039999999999, 95.83160000000002, 95.83160000000002, 95.9068, 96.06360000000001, 96.16879999999999, 96.2934, 96.4658, 96.57600000000002, 96.67060000000001, 96.78400000000002, 96.80239999999999, 96.81020000000001, 96.83500000000002, 96.8952, 96.97139999999999, 97.0502, 97.0984, 97.1274, 97.18039999999999, 97.2968, 97.4588, 97.603, 97.71880000000002, 97.84299999999999, 97.9384, 98.0186, 98.0828, 98.1326, 98.20940000000002, 98.29960000000001, 98.4002, 98.459, 98.5556, 98.64779999999999, 98.74959999999997, 98.92259999999999, 99.12439999999998, 99.2916, 99.46499999999997, 99.6758, 99.83779999999999, 100.06159999999998, 100.2594, 100.4774, 100.68900000000002, 100.9426, 101.17179999999999, 101.37459999999999, 101.5862, 101.77500000000002, 101.9084, 102.0416, 102.096, 102.21920000000001, 102.31779999999999, 102.43000000000002, 102.60420000000002, 102.8114, 102.81139999999999, 103.0122, 103.28439999999999, 103.54400000000001, 103.823, 104.08839999999998, 104.32780000000001, 104.56119999999999, 104.77519999999998, 104.9946, 105.21780000000001, 105.3774, 105.51659999999998, 105.65979999999999, 105.8092, 105.891, 105.956, 106.03900000000002, 106.12180000000001, 106.2534, 106.29379999999999, 106.36740000000002, 106.4594, 106.59519999999999, 106.7052, 106.8048, 106.8704, 106.933, 106.965, 106.969, 106.98679999999999, 106.979, 106.9852, 106.9912, 107.02000000000002, 107.06280000000002, 107.16139999999997, 107.26700000000001, 107.39260000000002, 107.50479999999999, 107.5924, 107.6706, 107.79979999999998, 107.91100000000002, 108.08320000000002, 108.2416, 108.3964, 108.55460000000002, 108.76580000000001, 108.94480000000001, 109.0636, 109.18819999999998, 109.2806, 109.399, 109.4954, 109.59300000000002, 109.721, 109.8442, 109.96480000000001, 110.09559999999998, 110.2128, 110.3092, 110.3648, 110.37380000000002, 110.3968, 110.425, 110.458, 110.406, 110.30300000000001, 110.19499999999996, 110.1538, 110.13799999999999, 110.1168, 110.05939999999998, 109.95819999999999, 109.855, 109.73360000000001, 109.52859999999998, 109.35560000000001, 109.3014, 109.2292, 109.1848, 109.1596, 109.10880000000002, 109.031, 108.91619999999999, 108.9162, 108.8364, 108.75920000000002, 108.64939999999999, 108.49619999999999, 108.29279999999999, 108.0858, 107.932, 107.77500000000002, 107.5858, 107.39140000000002, 107.2156, 107.0078, 106.8184, 106.67559999999999, 106.58420000000002, 106.5422, 106.50800000000001, 106.49139999999997, 106.45419999999999, 106.39400000000002, 106.28300000000002, 106.24880000000002, 106.2208, 106.22039999999998, 106.24940000000001, 106.2732, 106.293, 106.32979999999999, 106.3628, 106.385, 106.44959999999999, 106.4832, 106.5962, 106.76559999999999, 106.8864, 107.03080000000001, 107.1652, 107.23519999999999, 107.38619999999999, 107.598, 107.90759999999999, 108.22860000000003, 108.45300000000002, 108.68539999999999, 108.8532, 109.04879999999997, 109.2666, 109.501, 109.75779999999999, 110.00740000000002, 110.20040000000003, 110.3752, 110.12799999999999, 109.88360000000003, 109.6908, 109.5336, 109.3528, 109.14039999999999, 108.97579999999999, 108.8468, 108.70179999999999, 108.54260000000001, 108.38459999999999, 108.2166, 108.00319999999999, 107.77180000000003, 107.52519999999998, 107.2744, 107.027, 106.7754, 106.57780000000001, 106.34559999999999, 106.0998, 105.81280000000001, 105.54299999999999, 105.33279999999999, 105.17979999999997, 104.98479999999998, 104.84219999999999, 104.7062, 104.58820000000001, 104.3882, 104.2208, 104.0324, 103.77459999999999, 103.5456, 103.54560000000001, 103.36319999999999, 103.1182, 102.87, 102.6198, 102.3316, 102.0528, 101.8034, 101.5618, 101.30239999999999, 101.04440000000001, 100.78880000000001, 100.53240000000001, 100.24340000000002, 99.92480000000002, 99.59800000000001, 99.73639999999999, 99.933, 100.12500000000001, 100.26679999999999, 100.40879999999999, 100.59999999999998, 100.75040000000001, 100.7948, 100.86739999999998, 100.95299999999999, 101.046, 101.1364, 101.21700000000001, 101.3167, 101.3889, 101.46970000000002, 101.56310000000002, 101.68269999999998, 101.9339, 102.1613, 102.37549999999999, 102.5879, 102.79250000000002, 102.98569999999998, 103.14029999999998, 103.34349999999999, 103.5517, 103.80869999999999, 104.0545, 104.2747, 104.49690000000002, 104.70389999999999, 104.9065, 105.16569999999997, 105.4267, 105.72169999999998, 106.0293, 106.3351, 106.57070000000002, 106.84990000000002, 107.12449999999998, 107.41590000000001, 107.73389999999999, 108.04549999999999, 108.32969999999999, 108.62790000000001, 108.9171, 109.2185, 109.5627, 109.93650000000001, 110.31770000000002, 110.75550000000001, 111.12429999999999, 111.5297, 111.88150000000002, 112.20890000000001, 112.5827, 113.07469999999999, 113.56450000000001, 114.03169999999999, 114.4795, 114.9319, 115.3633, 115.76520000000001, 116.18400000000001, 116.98720000000002, 117.3476, 117.5616, 117.785, 118.03300000000002, 118.03300000000002, 118.27340000000002, 118.50520000000002, 118.74, 119.03200000000001, 119.2692, 119.5502, 119.869, 120.2184, 120.5518, 120.933, 121.27619999999999, 121.5592, 121.86859999999999, 122.1806, 122.566, 122.92759999999998, 123.27720000000001, 123.72239999999998, 124.06199999999998, 124.40639999999999, 124.75599999999999, 125.12820000000002, 125.47039999999998, 125.8386, 126.16779999999999, 126.42039999999999, 126.7612, 127.11040000000001, 127.4588, 127.81559999999998, 128.13559999999998, 128.4898, 128.82399999999998, 129.2334, 129.64880000000002, 129.9406, 130.18779999999998, 130.3878, 130.5996, 130.7654, 130.892, 131.09320000000002, 131.264, 131.4536, 131.6492, 131.8718, 132.1098, 132.3622, 132.5788, 132.75560000000002, 132.9906, 133.18959999999998, 133.2918, 133.389, 133.55859999999998, 133.66459999999998, 133.6814, 133.7208, 133.7568, 133.718, 133.6968, 133.7672, 133.802, 133.8546, 133.85, 133.83300000000003, 133.7318, 133.5668, 133.50039999999998, 133.736, 133.9572, 134.1696, 134.40720000000002, 134.6336, 134.862, 135.1698, 135.355, 135.5006, 135.6042, 135.7134, 135.80640000000002, 136.0822, 136.184, 136.321, 136.57399999999998, 136.87439999999998, 136.8744, 137.1918, 137.43959999999998, 137.7012, 138.01460000000003, 138.3286, 138.694, 139.0162, 139.307, 139.5866, 139.87579999999997, 140.1596, 140.4792, 140.7936, 141.069, 141.40679999999998, 141.87400000000002, 142.3406, 142.812, 143.2974, 143.81219999999996, 144.2882, 144.7976, 145.3188, 145.8684, 146.3766]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e0cc1a79-a6f0-4edd-9668-7c7e32b7d5e6');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"09e64be3-0d23-412c-9faf-95d21e278799\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"09e64be3-0d23-412c-9faf-95d21e278799\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '09e64be3-0d23-412c-9faf-95d21e278799',\n",
              "                        [{\"type\": \"scatter\", \"x\": [20110225, 20110228, 20110301, 20110302, 20110303, 20110304, 20110307, 20110308, 20110309, 20110310, 20110311, 20110314, 20110315, 20110316, 20110317, 20110318, 20110321, 20110322, 20110323, 20110324, 20110325, 20110328, 20110329, 20110330, 20110331, 20110401, 20110404, 20110405, 20110406, 20110407, 20110408, 20110411, 20110412, 20110413, 20110414, 20110415, 20110418, 20110419, 20110420, 20110421, 20110425, 20110426, 20110427, 20110428, 20110429, 20110502, 20110503, 20110504, 20110505, 20110506, 20110509, 20110510, 20110511, 20110512, 20110513, 20110516, 20110517, 20110518, 20110519, 20110520, 20110523, 20110524, 20110525, 20110526, 20110527, 20110531, 20110601, 20110602, 20110603, 20110606, 20110607, 20110608, 20110609, 20110610, 20110613, 20110614, 20110615, 20110616, 20110617, 20110620, 20110621, 20110622, 20110623, 20110624, 20110627, 20110628, 20110629, 20110630, 20110701, 20110705, 20110706, 20110707, 20110708, 20110711, 20110712, 20110713, 20110714, 20110715, 20110718, 20110719, 20110720, 20110721, 20110722, 20110725, 20110726, 20110727, 20110728, 20110729, 20110801, 20110802, 20110803, 20110804, 20110805, 20110808, 20110809, 20110810, 20110811, 20110812, 20110815, 20110816, 20110817, 20110818, 20110819, 20110822, 20110823, 20110824, 20110825, 20110826, 20110829, 20110830, 20110831, 20110901, 20110902, 20110906, 20110907, 20110908, 20110909, 20110912, 20110913, 20110914, 20110915, 20110916, 20110919, 20110920, 20110921, 20110922, 20110923, 20110926, 20110927, 20110928, 20110929, 20110930, 20111003, 20111004, 20111005, 20111006, 20111007, 20111010, 20111011, 20111012, 20111013, 20111014, 20111017, 20111018, 20111019, 20111020, 20111021, 20111024, 20111025, 20111026, 20111027, 20111028, 20111031, 20111101, 20111102, 20111103, 20111104, 20111107, 20111108, 20111109, 20111110, 20111111, 20111114, 20111115, 20111116, 20111117, 20111118, 20111121, 20111122, 20111123, 20111125, 20111128, 20111129, 20111130, 20111201, 20111202, 20111205, 20111206, 20111207, 20111208, 20111209, 20111212, 20111213, 20111214, 20111215, 20111216, 20111219, 20111220, 20111221, 20111222, 20111223, 20111227, 20111228, 20111229, 20111230, 20120103, 20120104, 20120105, 20120106, 20120109, 20120110, 20120111, 20120112, 20120113, 20120117, 20120118, 20120119, 20120120, 20120123, 20120124, 20120125, 20120126, 20120127, 20120130, 20120131, 20120201, 20120202, 20120203, 20120206, 20120207, 20120208, 20120209, 20120210, 20120213, 20120214, 20120215, 20120216, 20120217, 20120221, 20120222, 20120223, 20120224, 20120227, 20120228, 20120229, 20120301, 20120302, 20120305, 20120306, 20120307, 20120308, 20120309, 20120312, 20120313, 20120314, 20120315, 20120316, 20120319, 20120320, 20120321, 20120322, 20120323, 20120326, 20120327, 20120328, 20120329, 20120330, 20120402, 20120403, 20120404, 20120405, 20120409, 20120410, 20120411, 20120412, 20120413, 20120416, 20120417, 20120418, 20120419, 20120420, 20120423, 20120424, 20120425, 20120426, 20120427, 20120430, 20120501, 20120502, 20120503, 20120504, 20120507, 20120508, 20120509, 20120510, 20120511, 20120514, 20120515, 20120516, 20120517, 20120518, 20120521, 20120522, 20120523, 20120524, 20120525, 20120529, 20120530, 20120531, 20120601, 20120604, 20120605, 20120606, 20120607, 20120608, 20120611, 20120612, 20120613, 20120614, 20120615, 20120618, 20120619, 20120620, 20120621, 20120622, 20120625, 20120626, 20120627, 20120628, 20120629, 20120702, 20120703, 20120705, 20120706, 20120709, 20120710, 20120711, 20120712, 20120713, 20120716, 20120717, 20120718, 20120719, 20120720, 20120723, 20120724, 20120725, 20120726, 20120727, 20120730, 20120731, 20120801, 20120802, 20120803, 20120806, 20120807, 20120808, 20120809, 20120810, 20120813, 20120814, 20120815, 20120816, 20120817, 20120820, 20120821, 20120822, 20120823, 20120824, 20120827, 20120828, 20120829, 20120830, 20120831, 20120904, 20120905, 20120906, 20120907, 20120910, 20120911, 20120912, 20120913, 20120914, 20120917, 20120918, 20120919, 20120920, 20120921, 20120924, 20120925, 20120926, 20120927, 20120928, 20121001, 20121002, 20121003, 20121004, 20121005, 20121008, 20121009, 20121010, 20121011, 20121012, 20121015, 20121016, 20121017, 20121018, 20121019, 20121022, 20121023, 20121024, 20121025, 20121026, 20121031, 20121101, 20121102, 20121105, 20121106, 20121107, 20121108, 20121109, 20121112, 20121113, 20121114, 20121115, 20121116, 20121119, 20121120, 20121121, 20121123, 20121126, 20121127, 20121128, 20121129, 20121130, 20121203, 20121204, 20121205, 20121206, 20121207, 20121210, 20121211, 20121212, 20121213, 20121214, 20121217, 20121218, 20121219, 20121220, 20121221, 20121224, 20121226, 20121227, 20121228, 20121231, 20130102, 20130103, 20130104, 20130107, 20130108, 20130109, 20130110, 20130111, 20130114, 20130115, 20130116, 20130117, 20130118, 20130122, 20130123, 20130124, 20130125, 20130128, 20130129, 20130130, 20130131, 20130201, 20130204, 20130205, 20130206, 20130207, 20130208, 20130211, 20130212, 20130213, 20130214, 20130215, 20130219, 20130220, 20130221, 20130222, 20130225, 20130226, 20130227, 20130228, 20130301, 20130304, 20130305, 20130306, 20130307, 20130308, 20130311, 20130312, 20130313, 20130314, 20130315, 20130318, 20130319, 20130320, 20130321, 20130322, 20130325, 20130326, 20130327, 20130328, 20130401, 20130402, 20130403, 20130404, 20130405, 20130408, 20130409, 20130410, 20130411, 20130412, 20130415, 20130416, 20130417, 20130418, 20130419, 20130422, 20130423, 20130424, 20130425, 20130426, 20130429, 20130430, 20130501, 20130502, 20130503, 20130506, 20130507, 20130508, 20130509, 20130510, 20130513, 20130514, 20130515, 20130516, 20130517, 20130520, 20130521, 20130522, 20130523, 20130524, 20130528, 20130529, 20130530, 20130531, 20130603, 20130604, 20130605, 20130606, 20130607, 20130610, 20130611, 20130612, 20130613, 20130614, 20130617, 20130618, 20130619, 20130620, 20130621, 20130624, 20130625, 20130626, 20130627, 20130628, 20130701, 20130702, 20130703, 20130705, 20130708, 20130709, 20130710, 20130711, 20130712, 20130715, 20130716, 20130717, 20130718, 20130719, 20130722, 20130723, 20130724, 20130725, 20130726, 20130729, 20130730, 20130731, 20130801, 20130802, 20130805, 20130806, 20130807, 20130808, 20130809, 20130812, 20130813, 20130814, 20130815, 20130816, 20130819, 20130820, 20130821, 20130822, 20130823, 20130826, 20130827, 20130828, 20130829, 20130830, 20130903, 20130904, 20130905, 20130906, 20130909, 20130910, 20130911, 20130912, 20130913, 20130916, 20130917, 20130918, 20130919, 20130920, 20130923, 20130924, 20130925, 20130926, 20130927, 20130930, 20131001, 20131002, 20131003, 20131004, 20131007, 20131008, 20131009, 20131010, 20131011, 20131014, 20131015, 20131016, 20131017, 20131018, 20131021, 20131022, 20131023, 20131024, 20131025, 20131028, 20131029, 20131030, 20131031, 20131101, 20131104, 20131105, 20131106, 20131107, 20131108, 20131111, 20131112, 20131113, 20131114, 20131115, 20131118, 20131119, 20131120, 20131121, 20131122, 20131125, 20131126, 20131127, 20131129, 20131202, 20131203, 20131204, 20131205, 20131206, 20131209, 20131210, 20131211, 20131212, 20131213, 20131216, 20131217, 20131218, 20131219, 20131220, 20131223, 20131224, 20131226, 20131227, 20131230, 20131231, 20140102, 20140103, 20140106, 20140107, 20140108, 20140109, 20140110, 20140113, 20140114, 20140115, 20140116, 20140117, 20140121, 20140122, 20140123, 20140124, 20140127, 20140128, 20140129, 20140130, 20140131, 20140203, 20140204, 20140205, 20140206, 20140207, 20140210, 20140211, 20140212, 20140213, 20140214, 20140218, 20140219, 20140220, 20140221, 20140224, 20140225, 20140226, 20140227, 20140228, 20140303, 20140304, 20140305, 20140306, 20140307, 20140310, 20140311, 20140312, 20140313, 20140314, 20140317, 20140318, 20140319, 20140320, 20140321, 20140324, 20140325, 20140326, 20140327, 20140328, 20140331, 20140401, 20140402, 20140403, 20140404, 20140407, 20140408, 20140409, 20140410, 20140411, 20140414, 20140415, 20140416, 20140417, 20140421, 20140422, 20140423, 20140424, 20140425, 20140428, 20140429, 20140430, 20140501, 20140502, 20140505, 20140506, 20140507, 20140508, 20140509, 20140512, 20140513, 20140514, 20140515, 20140516, 20140519, 20140520, 20140521, 20140522, 20140523, 20140527, 20140528, 20140529, 20140530, 20140602, 20140603, 20140604, 20140605, 20140606, 20140609, 20140610, 20140611, 20140612, 20140613, 20140616, 20140617, 20140618, 20140619, 20140620, 20140623, 20140624, 20140625, 20140626, 20140627, 20140630, 20140701, 20140702, 20140703, 20140707, 20140708, 20140709, 20140710, 20140711, 20140714, 20140715, 20140716, 20140717, 20140718, 20140721, 20140722, 20140723, 20140724, 20140725, 20140728, 20140729, 20140730, 20140731, 20140801, 20140804, 20140805, 20140806, 20140807, 20140808, 20140811, 20140812, 20140813, 20140814, 20140815, 20140818, 20140819, 20140820, 20140821, 20140822, 20140825, 20140826, 20140827, 20140828, 20140829, 20140902, 20140903, 20140904, 20140905, 20140908, 20140909, 20140910, 20140911, 20140912, 20140915, 20140916, 20140917, 20140918, 20140919, 20140922, 20140923, 20140924, 20140925, 20140926, 20140929, 20140930, 20141001, 20141002, 20141003, 20141006, 20141007, 20141008, 20141009, 20141010, 20141013, 20141014, 20141015, 20141016, 20141017, 20141020, 20141021, 20141022, 20141023, 20141024, 20141027, 20141028, 20141029, 20141030, 20141031, 20141103, 20141104, 20141105, 20141106, 20141107, 20141110, 20141111, 20141112, 20141113, 20141114, 20141117, 20141118, 20141119, 20141120, 20141121, 20141124, 20141125, 20141126, 20141128, 20141201, 20141202, 20141203, 20141204, 20141205, 20141208, 20141209, 20141210, 20141211, 20141212, 20141215, 20141216, 20141217, 20141218, 20141219, 20141222, 20141223, 20141224, 20141226, 20141229, 20141230, 20141231, 20150102, 20150105, 20150106, 20150107, 20150108, 20150109, 20150112, 20150113, 20150114, 20150115, 20150116, 20150120, 20150121, 20150122, 20150123, 20150126, 20150127, 20150128, 20150129, 20150130, 20150202, 20150203, 20150204, 20150205, 20150206, 20150209, 20150210, 20150211, 20150212, 20150213, 20150217, 20150218, 20150219, 20150220, 20150223, 20150224, 20150225, 20150226, 20150227, 20150302, 20150303, 20150304, 20150305, 20150306, 20150309, 20150310, 20150311, 20150312, 20150313, 20150316, 20150317, 20150318, 20150319, 20150320, 20150323, 20150324, 20150325, 20150326, 20150327, 20150330, 20150331, 20150401, 20150402, 20150406, 20150407, 20150408, 20150409, 20150410, 20150413, 20150414, 20150415, 20150416, 20150417, 20150420, 20150421, 20150422, 20150423, 20150424, 20150427, 20150428, 20150429, 20150430, 20150501, 20150504, 20150505, 20150506, 20150507, 20150508, 20150511, 20150512, 20150513, 20150514, 20150515, 20150518, 20150519, 20150520, 20150521, 20150522, 20150526, 20150527, 20150528, 20150529, 20150601, 20150602, 20150603, 20150604, 20150605, 20150608, 20150609, 20150610, 20150611, 20150612, 20150615, 20150616, 20150617, 20150618, 20150619, 20150622, 20150623, 20150624, 20150625, 20150626, 20150629, 20150630, 20150701, 20150702, 20150706, 20150707, 20150708, 20150709, 20150710, 20150713, 20150714, 20150715, 20150716, 20150717, 20150720, 20150721, 20150722, 20150723, 20150724, 20150727, 20150728, 20150729, 20150730, 20150731, 20150803, 20150804, 20150805, 20150806, 20150807, 20150810, 20150811, 20150812, 20150813, 20150814, 20150817, 20150818, 20150819, 20150820, 20150821, 20150824, 20150825, 20150826, 20150827, 20150828, 20150831, 20150901, 20150902, 20150903, 20150904, 20150908, 20150909, 20150910, 20150911, 20150914, 20150915, 20150916, 20150917, 20150918, 20150921, 20150922, 20150923, 20150924, 20150925, 20150928, 20150929, 20150930, 20151001, 20151002, 20151005, 20151006, 20151007, 20151008, 20151009, 20151012, 20151013, 20151014, 20151015, 20151016, 20151019, 20151020, 20151021, 20151022, 20151023, 20151026, 20151027, 20151028, 20151029, 20151030, 20151102, 20151103, 20151104, 20151105, 20151106, 20151109, 20151110, 20151111, 20151112, 20151113, 20151116, 20151117, 20151118, 20151119, 20151120, 20151123, 20151124, 20151125, 20151127, 20151130, 20151201, 20151202, 20151203, 20151204, 20151207, 20151208, 20151209, 20151210, 20151211, 20151214, 20151215, 20151216, 20151217, 20151218, 20151221, 20151222, 20151223, 20151224, 20151228, 20151229, 20151230, 20151231, 20160104, 20160105, 20160106, 20160107, 20160108, 20160111, 20160112, 20160113, 20160114, 20160115, 20160119, 20160120, 20160121, 20160122, 20160125, 20160126, 20160127, 20160128, 20160129, 20160201, 20160202, 20160203, 20160204, 20160205, 20160208, 20160209, 20160210, 20160211, 20160212, 20160216, 20160217, 20160218, 20160219, 20160222, 20160223, 20160224, 20160225, 20160226, 20160229, 20160301, 20160302, 20160303, 20160304, 20160307, 20160308, 20160309, 20160310, 20160311, 20160314, 20160315, 20160316, 20160317, 20160318, 20160321, 20160322, 20160323, 20160324, 20160328, 20160329, 20160330, 20160331, 20160401, 20160404, 20160405, 20160406, 20160407, 20160408, 20160411, 20160412, 20160413, 20160414, 20160415, 20160418, 20160419, 20160420, 20160421, 20160422, 20160425, 20160426, 20160427, 20160428, 20160429, 20160502, 20160503, 20160504, 20160505, 20160506, 20160509, 20160510, 20160511, 20160512, 20160513, 20160516, 20160517, 20160518, 20160519, 20160520, 20160523, 20160524, 20160525, 20160526, 20160527, 20160531, 20160601, 20160602, 20160603, 20160606, 20160607, 20160608, 20160609, 20160610, 20160613, 20160614, 20160615, 20160616, 20160617, 20160620, 20160621, 20160622, 20160623, 20160624, 20160627, 20160628, 20160629, 20160630, 20160701, 20160705, 20160706, 20160707, 20160708, 20160711, 20160712, 20160713, 20160714, 20160715, 20160718, 20160719, 20160720, 20160721, 20160722, 20160725, 20160726, 20160727, 20160728, 20160729, 20160801, 20160802, 20160803, 20160804, 20160805, 20160808, 20160809, 20160810, 20160811, 20160812, 20160815, 20160816, 20160817, 20160818, 20160819, 20160822, 20160823, 20160824, 20160825, 20160826, 20160829, 20160830, 20160831, 20160901, 20160902, 20160906, 20160907, 20160908, 20160909, 20160912, 20160913, 20160914, 20160915, 20160916, 20160919, 20160920, 20160921, 20160922, 20160923, 20160926, 20160927, 20160928, 20160929, 20160930, 20161003, 20161004, 20161005, 20161006, 20161007, 20161010, 20161011, 20161012, 20161013, 20161014, 20161017, 20161018, 20161019, 20161020, 20161021, 20161024, 20161025, 20161026, 20161027, 20161028, 20161031, 20161101, 20161102, 20161103, 20161104, 20161107, 20161108, 20161109, 20161110, 20161111, 20161114, 20161115, 20161116, 20161117, 20161118, 20161121, 20161122, 20161123, 20161125, 20161128, 20161129, 20161130, 20161201, 20161202, 20161205, 20161206, 20161207, 20161208, 20161209, 20161212, 20161213, 20161214, 20161215, 20161216, 20161219, 20161220, 20161221, 20161222, 20161223, 20161227, 20161228, 20161229, 20161230, 20170103, 20170104, 20170105, 20170106, 20170109, 20170110, 20170111, 20170112, 20170113, 20170117, 20170118, 20170119, 20170120, 20170123, 20170124, 20170125, 20170126, 20170127, 20170130, 20170131, 20170201, 20170202, 20170203, 20170206, 20170207, 20170208, 20170209, 20170210, 20170213, 20170214, 20170215, 20170216, 20170217, 20170221, 20170222, 20170223, 20170224, 20170227, 20170228, 20170301, 20170302, 20170303, 20170306, 20170307, 20170308, 20170309, 20170310, 20170313, 20170314, 20170315, 20170316, 20170317, 20170320, 20170321, 20170322, 20170323, 20170324, 20170327, 20170328, 20170329, 20170330, 20170331, 20170403, 20170404, 20170405, 20170406, 20170407, 20170410, 20170411, 20170412, 20170413, 20170417, 20170418, 20170419, 20170420, 20170421, 20170424, 20170425, 20170426, 20170427, 20170428, 20170501, 20170502, 20170503, 20170504, 20170505, 20170508, 20170509, 20170510, 20170511, 20170512, 20170515, 20170516, 20170517, 20170518, 20170519, 20170522, 20170523, 20170524, 20170525, 20170526, 20170530, 20170531, 20170601, 20170602, 20170605, 20170606, 20170607, 20170608, 20170609, 20170612, 20170613, 20170614, 20170615, 20170616, 20170619, 20170620, 20170621, 20170622, 20170623, 20170626, 20170627, 20170628, 20170629, 20170630, 20170703, 20170705, 20170706, 20170707, 20170710, 20170711, 20170712, 20170713, 20170714, 20170717, 20170718, 20170719, 20170720, 20170721, 20170724, 20170725, 20170726, 20170727, 20170728, 20170731, 20170801, 20170802, 20170803, 20170804, 20170807, 20170808, 20170809, 20170810, 20170811, 20170814, 20170815, 20170816, 20170817, 20170818, 20170821, 20170822, 20170823, 20170824, 20170825, 20170828, 20170829, 20170830, 20170831, 20170901, 20170905, 20170906, 20170907, 20170908, 20170911, 20170912, 20170913, 20170914, 20170915, 20170918, 20170919, 20170920, 20170921, 20170922, 20170925, 20170926, 20170927, 20170928, 20170929, 20171002, 20171003, 20171004, 20171005, 20171006, 20171009, 20171010, 20171011, 20171012, 20171013, 20171016, 20171017, 20171018, 20171019, 20171020, 20171023, 20171024, 20171025, 20171026, 20171027, 20171030, 20171031, 20171101, 20171102, 20171103, 20171106, 20171107, 20171108, 20171109, 20171110, 20171113, 20171114, 20171115, 20171116, 20171117, 20171120, 20171121, 20171122, 20171124, 20171127, 20171128, 20171129, 20171130, 20171201, 20171204, 20171205, 20171206, 20171207, 20171208, 20171211, 20171212, 20171213, 20171214, 20171215, 20171218, 20171219, 20171220, 20171221, 20171222, 20171226, 20171227, 20171228, 20171229, 20180102, 20180103, 20180104, 20180105, 20180108, 20180109, 20180110, 20180111, 20180112, 20180116, 20180117, 20180118, 20180119, 20180122, 20180123, 20180124, 20180125, 20180126, 20180129, 20180130, 20180131, 20180201, 20180202, 20180205, 20180206, 20180207, 20180208, 20180209, 20180212, 20180213, 20180214, 20180215, 20180216, 20180220, 20180221, 20180222, 20180223, 20180226, 20180227, 20180228, 20180301, 20180302, 20180305, 20180306, 20180307, 20180308, 20180309, 20180312, 20180313, 20180314, 20180315, 20180316, 20180319, 20180320, 20180321, 20180322, 20180323, 20180326, 20180327, 20180328, 20180329, 20180402, 20180403, 20180404, 20180405, 20180406, 20180409, 20180410, 20180411, 20180412, 20180413, 20180416, 20180417, 20180418, 20180419, 20180420, 20180423, 20180424, 20180425, 20180426, 20180427, 20180430, 20180501, 20180502, 20180503, 20180504, 20180507, 20180508, 20180509, 20180510, 20180511, 20180514, 20180515, 20180516, 20180517, 20180518, 20180521, 20180522, 20180523, 20180524, 20180525, 20180529, 20180530, 20180531, 20180601, 20180604, 20180605, 20180606, 20180607, 20180608, 20180611, 20180612, 20180613, 20180614, 20180615, 20180618, 20180619, 20180620, 20180621, 20180622, 20180625, 20180626, 20180627, 20180628, 20180629, 20180702, 20180703, 20180705, 20180706, 20180709, 20180710, 20180711, 20180712, 20180713, 20180716, 20180717, 20180718, 20180719, 20180720, 20180723, 20180724, 20180725, 20180726, 20180727, 20180730, 20180731, 20180801, 20180802, 20180803, 20180806, 20180807, 20180808, 20180809, 20180810, 20180813, 20180814, 20180815, 20180816, 20180817, 20180820, 20180821, 20180822, 20180823, 20180824, 20180827, 20180828, 20180829, 20180830, 20180831, 20180904, 20180905, 20180906, 20180907, 20180910, 20180911, 20180912, 20180913, 20180914, 20180917, 20180918, 20180919, 20180920, 20180921, 20180924, 20180925, 20180926, 20180927, 20180928, 20181001, 20181002, 20181003, 20181004, 20181005, 20181008, 20181009, 20181010, 20181011, 20181012, 20181015, 20181016, 20181017, 20181018, 20181019, 20181022, 20181023, 20181024, 20181025, 20181026, 20181029, 20181030, 20181031, 20181101, 20181102, 20181105, 20181106, 20181107, 20181108, 20181109, 20181112, 20181113, 20181114, 20181115, 20181116, 20181119, 20181120, 20181121, 20181123, 20181126, 20181127, 20181128, 20181129, 20181130, 20181203, 20181204, 20181206, 20181207, 20181210, 20181211, 20181212, 20181213, 20181214, 20181217, 20181218, 20181219, 20181220, 20181221, 20181224, 20181226, 20181227, 20181228, 20181231, 20190102, 20190103, 20190104, 20190107, 20190108, 20190109, 20190110, 20190111, 20190114, 20190115, 20190116, 20190117, 20190118, 20190122, 20190123, 20190124, 20190125, 20190128, 20190129, 20190130, 20190131, 20190201, 20190204, 20190205, 20190206, 20190207, 20190208, 20190211, 20190212, 20190213, 20190214, 20190215, 20190219, 20190220, 20190221, 20190222, 20190225, 20190226, 20190227, 20190228, 20190301, 20190304, 20190305, 20190306, 20190307, 20190308, 20190311, 20190312, 20190313, 20190314, 20190315, 20190318, 20190319, 20190320, 20190321, 20190322, 20190325, 20190326, 20190327, 20190328, 20190329, 20190401, 20190402, 20190403, 20190404, 20190405, 20190408, 20190409, 20190410, 20190411, 20190412, 20190415, 20190416, 20190417, 20190418, 20190422, 20190423, 20190424, 20190425, 20190426, 20190429, 20190430, 20190501, 20190502, 20190503, 20190506, 20190507, 20190508, 20190509, 20190510, 20190513, 20190514, 20190515, 20190516, 20190517, 20190520, 20190521, 20190522, 20190523, 20190524, 20190528, 20190529, 20190530, 20190531, 20190603, 20190604, 20190605, 20190606, 20190607, 20190610, 20190611, 20190612, 20190613, 20190614, 20190617, 20190618, 20190619, 20190620, 20190621, 20190624, 20190625, 20190626, 20190627, 20190628, 20190701, 20190702, 20190703, 20190705, 20190708, 20190709, 20190710, 20190711, 20190712, 20190715, 20190716, 20190717, 20190718, 20190719, 20190722, 20190723, 20190724, 20190725, 20190726, 20190729, 20190730, 20190731, 20190801, 20190802, 20190805, 20190806, 20190807, 20190808, 20190809, 20190812, 20190813, 20190814, 20190815, 20190816, 20190819, 20190820, 20190821, 20190822, 20190823, 20190826, 20190827, 20190828, 20190829, 20190830, 20190903, 20190904, 20190905, 20190906, 20190909, 20190910, 20190911, 20190912, 20190913, 20190916, 20190917, 20190918, 20190919, 20190920, 20190923, 20190924, 20190925, 20190926, 20190927, 20190930, 20191001, 20191002, 20191003, 20191004, 20191007, 20191008, 20191009, 20191010, 20191011, 20191014, 20191015, 20191016, 20191017, 20191018, 20191021, 20191022, 20191023, 20191024, 20191025, 20191028, 20191029, 20191030, 20191031, 20191101, 20191104, 20191105, 20191106, 20191107, 20191108, 20191111, 20191112, 20191113, 20191114, 20191115, 20191118, 20191119, 20191120], \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('09e64be3-0d23-412c-9faf-95d21e278799');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRkSCtJoYiNw"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqMU1Uh2YiNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6216b959-98ec-4d9c-bea8-552315504f5a"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.05]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "\n",
        "  historical = Train_data(dfs[col_name], train_start=100, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"RMD\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.05\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 8s 14ms/step - loss: 0.6128 - accuracy: 0.7077 - val_loss: 0.5634 - val_accuracy: 0.7510\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.6087 - accuracy: 0.7107 - val_loss: 0.5701 - val_accuracy: 0.7510\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.6048 - accuracy: 0.7107 - val_loss: 0.5566 - val_accuracy: 0.7510\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.5093 - accuracy: 0.7527 - val_loss: 0.3529 - val_accuracy: 0.8673\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.4168 - accuracy: 0.8207 - val_loss: 0.3320 - val_accuracy: 0.8653\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 12ms/step - loss: 0.6123 - accuracy: 0.7077 - val_loss: 0.5751 - val_accuracy: 0.7510\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 9ms/step - loss: 0.5490 - accuracy: 0.7331 - val_loss: 0.2956 - val_accuracy: 0.9000\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 9ms/step - loss: 0.3612 - accuracy: 0.8467 - val_loss: 0.3027 - val_accuracy: 0.8898\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 9ms/step - loss: 0.3421 - accuracy: 0.8491 - val_loss: 0.3405 - val_accuracy: 0.8612\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 9ms/step - loss: 0.3478 - accuracy: 0.8485 - val_loss: 0.3327 - val_accuracy: 0.8612\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.883397\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.884388\n",
            "[2]\tvalidation_0-auc:0.885702\n",
            "[3]\tvalidation_0-auc:0.887262\n",
            "[4]\tvalidation_0-auc:0.88734\n",
            "[5]\tvalidation_0-auc:0.886961\n",
            "[6]\tvalidation_0-auc:0.891182\n",
            "[7]\tvalidation_0-auc:0.891605\n",
            "[8]\tvalidation_0-auc:0.888353\n",
            "[9]\tvalidation_0-auc:0.890747\n",
            "[10]\tvalidation_0-auc:0.891872\n",
            "[11]\tvalidation_0-auc:0.893788\n",
            "[12]\tvalidation_0-auc:0.894478\n",
            "[13]\tvalidation_0-auc:0.89469\n",
            "[14]\tvalidation_0-auc:0.894423\n",
            "[15]\tvalidation_0-auc:0.894434\n",
            "[16]\tvalidation_0-auc:0.89165\n",
            "[17]\tvalidation_0-auc:0.891405\n",
            "[18]\tvalidation_0-auc:0.890892\n",
            "[19]\tvalidation_0-auc:0.891004\n",
            "[20]\tvalidation_0-auc:0.892184\n",
            "[21]\tvalidation_0-auc:0.892596\n",
            "[22]\tvalidation_0-auc:0.892641\n",
            "[23]\tvalidation_0-auc:0.893521\n",
            "[24]\tvalidation_0-auc:0.895191\n",
            "[25]\tvalidation_0-auc:0.895169\n",
            "[26]\tvalidation_0-auc:0.894991\n",
            "[27]\tvalidation_0-auc:0.895102\n",
            "[28]\tvalidation_0-auc:0.895013\n",
            "[29]\tvalidation_0-auc:0.894768\n",
            "[30]\tvalidation_0-auc:0.89381\n",
            "[31]\tvalidation_0-auc:0.894634\n",
            "[32]\tvalidation_0-auc:0.894701\n",
            "[33]\tvalidation_0-auc:0.894222\n",
            "[34]\tvalidation_0-auc:0.893754\n",
            "[35]\tvalidation_0-auc:0.893788\n",
            "[36]\tvalidation_0-auc:0.89391\n",
            "[37]\tvalidation_0-auc:0.893754\n",
            "[38]\tvalidation_0-auc:0.893732\n",
            "[39]\tvalidation_0-auc:0.893643\n",
            "[40]\tvalidation_0-auc:0.894389\n",
            "[41]\tvalidation_0-auc:0.894946\n",
            "[42]\tvalidation_0-auc:0.894935\n",
            "[43]\tvalidation_0-auc:0.894846\n",
            "[44]\tvalidation_0-auc:0.894835\n",
            "[45]\tvalidation_0-auc:0.895392\n",
            "[46]\tvalidation_0-auc:0.895726\n",
            "[47]\tvalidation_0-auc:0.895681\n",
            "[48]\tvalidation_0-auc:0.89479\n",
            "[49]\tvalidation_0-auc:0.894367\n",
            "[50]\tvalidation_0-auc:0.896338\n",
            "[51]\tvalidation_0-auc:0.896427\n",
            "[52]\tvalidation_0-auc:0.896316\n",
            "[53]\tvalidation_0-auc:0.89587\n",
            "[54]\tvalidation_0-auc:0.895225\n",
            "[55]\tvalidation_0-auc:0.893487\n",
            "[56]\tvalidation_0-auc:0.893621\n",
            "[57]\tvalidation_0-auc:0.893599\n",
            "[58]\tvalidation_0-auc:0.89391\n",
            "[59]\tvalidation_0-auc:0.895781\n",
            "[60]\tvalidation_0-auc:0.895715\n",
            "[61]\tvalidation_0-auc:0.895225\n",
            "[62]\tvalidation_0-auc:0.895013\n",
            "[63]\tvalidation_0-auc:0.894968\n",
            "[64]\tvalidation_0-auc:0.894812\n",
            "[65]\tvalidation_0-auc:0.894779\n",
            "[66]\tvalidation_0-auc:0.894244\n",
            "[67]\tvalidation_0-auc:0.894222\n",
            "[68]\tvalidation_0-auc:0.894512\n",
            "[69]\tvalidation_0-auc:0.893999\n",
            "[70]\tvalidation_0-auc:0.893977\n",
            "[71]\tvalidation_0-auc:0.893888\n",
            "[72]\tvalidation_0-auc:0.893198\n",
            "[73]\tvalidation_0-auc:0.892975\n",
            "[74]\tvalidation_0-auc:0.892797\n",
            "[75]\tvalidation_0-auc:0.892485\n",
            "[76]\tvalidation_0-auc:0.892307\n",
            "[77]\tvalidation_0-auc:0.892195\n",
            "[78]\tvalidation_0-auc:0.892374\n",
            "[79]\tvalidation_0-auc:0.893309\n",
            "[80]\tvalidation_0-auc:0.893788\n",
            "[81]\tvalidation_0-auc:0.893855\n",
            "[82]\tvalidation_0-auc:0.893966\n",
            "[83]\tvalidation_0-auc:0.893743\n",
            "[84]\tvalidation_0-auc:0.893766\n",
            "[85]\tvalidation_0-auc:0.893509\n",
            "[86]\tvalidation_0-auc:0.89342\n",
            "[87]\tvalidation_0-auc:0.893821\n",
            "[88]\tvalidation_0-auc:0.893855\n",
            "[89]\tvalidation_0-auc:0.893766\n",
            "[90]\tvalidation_0-auc:0.893832\n",
            "[91]\tvalidation_0-auc:0.893565\n",
            "[92]\tvalidation_0-auc:0.893387\n",
            "[93]\tvalidation_0-auc:0.893643\n",
            "[94]\tvalidation_0-auc:0.893621\n",
            "[95]\tvalidation_0-auc:0.893576\n",
            "[96]\tvalidation_0-auc:0.893732\n",
            "[97]\tvalidation_0-auc:0.893109\n",
            "[98]\tvalidation_0-auc:0.893086\n",
            "[99]\tvalidation_0-auc:0.892641\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 14ms/step - loss: 0.5725 - accuracy: 0.7333 - val_loss: 0.4137 - val_accuracy: 0.8074\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.4273 - accuracy: 0.8286 - val_loss: 0.5849 - val_accuracy: 0.7330\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3965 - accuracy: 0.8479 - val_loss: 0.4111 - val_accuracy: 0.8293\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3738 - accuracy: 0.8521 - val_loss: 0.4209 - val_accuracy: 0.8293\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3572 - accuracy: 0.8509 - val_loss: 0.4810 - val_accuracy: 0.8096\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 13ms/step - loss: 0.5102 - accuracy: 0.7683 - val_loss: 0.4324 - val_accuracy: 0.7834\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3435 - accuracy: 0.8696 - val_loss: 0.3374 - val_accuracy: 0.8796\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3284 - accuracy: 0.8612 - val_loss: 0.3951 - val_accuracy: 0.8403\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3238 - accuracy: 0.8715 - val_loss: 0.3558 - val_accuracy: 0.8665\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 0.3046 - accuracy: 0.8817 - val_loss: 0.4529 - val_accuracy: 0.8031\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.789234\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.789234\n",
            "[2]\tvalidation_0-auc:0.84803\n",
            "[3]\tvalidation_0-auc:0.84803\n",
            "[4]\tvalidation_0-auc:0.848507\n",
            "[5]\tvalidation_0-auc:0.851395\n",
            "[6]\tvalidation_0-auc:0.851395\n",
            "[7]\tvalidation_0-auc:0.851395\n",
            "[8]\tvalidation_0-auc:0.851395\n",
            "[9]\tvalidation_0-auc:0.851395\n",
            "[10]\tvalidation_0-auc:0.851395\n",
            "[11]\tvalidation_0-auc:0.888757\n",
            "[12]\tvalidation_0-auc:0.89184\n",
            "[13]\tvalidation_0-auc:0.906068\n",
            "[14]\tvalidation_0-auc:0.906643\n",
            "[15]\tvalidation_0-auc:0.901554\n",
            "[16]\tvalidation_0-auc:0.894654\n",
            "[17]\tvalidation_0-auc:0.895425\n",
            "[18]\tvalidation_0-auc:0.888109\n",
            "[19]\tvalidation_0-auc:0.890164\n",
            "[20]\tvalidation_0-auc:0.890347\n",
            "[21]\tvalidation_0-auc:0.892183\n",
            "[22]\tvalidation_0-auc:0.879594\n",
            "[23]\tvalidation_0-auc:0.880267\n",
            "[24]\tvalidation_0-auc:0.881245\n",
            "[25]\tvalidation_0-auc:0.881612\n",
            "[26]\tvalidation_0-auc:0.882542\n",
            "[27]\tvalidation_0-auc:0.882273\n",
            "[28]\tvalidation_0-auc:0.882591\n",
            "[29]\tvalidation_0-auc:0.87607\n",
            "[30]\tvalidation_0-auc:0.878909\n",
            "[31]\tvalidation_0-auc:0.880695\n",
            "[32]\tvalidation_0-auc:0.877661\n",
            "[33]\tvalidation_0-auc:0.875202\n",
            "[34]\tvalidation_0-auc:0.874468\n",
            "[35]\tvalidation_0-auc:0.878211\n",
            "[36]\tvalidation_0-auc:0.878945\n",
            "[37]\tvalidation_0-auc:0.879264\n",
            "[38]\tvalidation_0-auc:0.877795\n",
            "[39]\tvalidation_0-auc:0.882224\n",
            "[40]\tvalidation_0-auc:0.881857\n",
            "[41]\tvalidation_0-auc:0.884377\n",
            "[42]\tvalidation_0-auc:0.884133\n",
            "[43]\tvalidation_0-auc:0.887375\n",
            "[44]\tvalidation_0-auc:0.884989\n",
            "[45]\tvalidation_0-auc:0.884769\n",
            "[46]\tvalidation_0-auc:0.888879\n",
            "[47]\tvalidation_0-auc:0.893235\n",
            "[48]\tvalidation_0-auc:0.896685\n",
            "[49]\tvalidation_0-auc:0.897101\n",
            "[50]\tvalidation_0-auc:0.899401\n",
            "[51]\tvalidation_0-auc:0.905493\n",
            "[52]\tvalidation_0-auc:0.903413\n",
            "[53]\tvalidation_0-auc:0.903413\n",
            "[54]\tvalidation_0-auc:0.904698\n",
            "[55]\tvalidation_0-auc:0.906166\n",
            "[56]\tvalidation_0-auc:0.903793\n",
            "[57]\tvalidation_0-auc:0.904013\n",
            "[58]\tvalidation_0-auc:0.903793\n",
            "[59]\tvalidation_0-auc:0.905774\n",
            "[60]\tvalidation_0-auc:0.906215\n",
            "[61]\tvalidation_0-auc:0.906044\n",
            "[62]\tvalidation_0-auc:0.907071\n",
            "[63]\tvalidation_0-auc:0.907365\n",
            "[64]\tvalidation_0-auc:0.903426\n",
            "[65]\tvalidation_0-auc:0.900832\n",
            "[66]\tvalidation_0-auc:0.899535\n",
            "[67]\tvalidation_0-auc:0.901272\n",
            "[68]\tvalidation_0-auc:0.899755\n",
            "[69]\tvalidation_0-auc:0.899682\n",
            "[70]\tvalidation_0-auc:0.89792\n",
            "[71]\tvalidation_0-auc:0.897749\n",
            "[72]\tvalidation_0-auc:0.896844\n",
            "[73]\tvalidation_0-auc:0.897113\n",
            "[74]\tvalidation_0-auc:0.89885\n",
            "[75]\tvalidation_0-auc:0.89885\n",
            "[76]\tvalidation_0-auc:0.895645\n",
            "[77]\tvalidation_0-auc:0.895669\n",
            "[78]\tvalidation_0-auc:0.897186\n",
            "[79]\tvalidation_0-auc:0.897235\n",
            "[80]\tvalidation_0-auc:0.892415\n",
            "[81]\tvalidation_0-auc:0.894666\n",
            "[82]\tvalidation_0-auc:0.894666\n",
            "[83]\tvalidation_0-auc:0.896305\n",
            "[84]\tvalidation_0-auc:0.891583\n",
            "[85]\tvalidation_0-auc:0.89151\n",
            "[86]\tvalidation_0-auc:0.890922\n",
            "[87]\tvalidation_0-auc:0.890433\n",
            "[88]\tvalidation_0-auc:0.892048\n",
            "[89]\tvalidation_0-auc:0.89195\n",
            "[90]\tvalidation_0-auc:0.891975\n",
            "[91]\tvalidation_0-auc:0.890849\n",
            "[92]\tvalidation_0-auc:0.891265\n",
            "[93]\tvalidation_0-auc:0.890433\n",
            "[94]\tvalidation_0-auc:0.891705\n",
            "[95]\tvalidation_0-auc:0.890482\n",
            "[96]\tvalidation_0-auc:0.890409\n",
            "[97]\tvalidation_0-auc:0.890507\n",
            "[98]\tvalidation_0-auc:0.890653\n",
            "[99]\tvalidation_0-auc:0.889699\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.05     | 0.8653061224489796 | 0.8595238095238096 | 0.9809782608695652 | 0.9162436548223349 |\n",
            "|      GRU 0.05     | 0.8612244897959184 | 0.8554502369668247 | 0.9809782608695652 | 0.9139240506329114 |\n",
            "|    XGBoost 0.05   | 0.8877551020408163 | 0.8789346246973365 | 0.9864130434782609 | 0.9295774647887324 |\n",
            "|    Logreg 0.05    | 0.8183673469387756 | 0.8093126385809313 | 0.9918478260869565 | 0.8913308913308914 |\n",
            "|      SVM 0.05     | 0.8653061224489796 | 0.852803738317757  | 0.9918478260869565 | 0.9170854271356784 |\n",
            "|   LSTM beta 0.05  | 0.8096280087527352 | 0.8212435233160622 | 0.9462686567164179 | 0.8793342579750348 |\n",
            "|   GRU beta 0.05   | 0.8030634573304157 | 0.7966101694915254 | 0.982089552238806  | 0.8796791443850267 |\n",
            "| XGBoost beta 0.05 | 0.8161925601750547 | 0.8098765432098766 | 0.9791044776119403 | 0.8864864864864864 |\n",
            "|  logreg beta 0.05 | 0.8074398249452954 | 0.8079800498753117 | 0.9671641791044776 | 0.8804347826086957 |\n",
            "|   svm beta 0.05   | 0.8096280087527352 | 0.8054187192118226 | 0.9761194029850746 | 0.8825910931174088 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eErc1XoJYiNw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "10565119-1a36-4c39-b341-e5887c71b80f"
      },
      "source": [
        "Result_cross.to_csv('RMD_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.05</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.859524</td>\n",
              "      <td>0.865306</td>\n",
              "      <td>0.916244</td>\n",
              "      <td>0.980978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.05</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.855450</td>\n",
              "      <td>0.861224</td>\n",
              "      <td>0.913924</td>\n",
              "      <td>0.980978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.05</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.878935</td>\n",
              "      <td>0.887755</td>\n",
              "      <td>0.929577</td>\n",
              "      <td>0.986413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.05</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.809313</td>\n",
              "      <td>0.818367</td>\n",
              "      <td>0.891331</td>\n",
              "      <td>0.991848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.05</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.852804</td>\n",
              "      <td>0.865306</td>\n",
              "      <td>0.917085</td>\n",
              "      <td>0.991848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.05</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.821244</td>\n",
              "      <td>0.809628</td>\n",
              "      <td>0.879334</td>\n",
              "      <td>0.946269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.05</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.796610</td>\n",
              "      <td>0.803063</td>\n",
              "      <td>0.879679</td>\n",
              "      <td>0.982090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.05</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.809877</td>\n",
              "      <td>0.816193</td>\n",
              "      <td>0.886486</td>\n",
              "      <td>0.979104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.05</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.807980</td>\n",
              "      <td>0.807440</td>\n",
              "      <td>0.880435</td>\n",
              "      <td>0.967164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.05</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.805419</td>\n",
              "      <td>0.809628</td>\n",
              "      <td>0.882591</td>\n",
              "      <td>0.976119</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0          LSTM 0.05  RMD  0.859524  0.865306  0.916244  0.980978\n",
              "1           GRU 0.05  RMD  0.855450  0.861224  0.913924  0.980978\n",
              "2       XGBoost 0.05  RMD  0.878935  0.887755  0.929577  0.986413\n",
              "3        Logreg 0.05  RMD  0.809313  0.818367  0.891331  0.991848\n",
              "4           SVM 0.05  RMD  0.852804  0.865306  0.917085  0.991848\n",
              "5     LSTM beta 0.05  RMD  0.821244  0.809628  0.879334  0.946269\n",
              "6      GRU beta 0.05  RMD  0.796610  0.803063  0.879679  0.982090\n",
              "7  XGBoost beta 0.05  RMD  0.809877  0.816193  0.886486  0.979104\n",
              "8   logreg beta 0.05  RMD  0.807980  0.807440  0.880435  0.967164\n",
              "9      svm beta 0.05  RMD  0.805419  0.809628  0.882591  0.976119"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2kQniZqYiNw"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QRVzX6pYiNw"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUqD3VarYiNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6079baed-7e0a-4c59-e9ba-4b35ee086cdc"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"RMD\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.05\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.5265 - accuracy: 0.7987 - val_loss: 0.5781 - val_accuracy: 0.7510\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5066 - accuracy: 0.8047 - val_loss: 0.5644 - val_accuracy: 0.7510\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5007 - accuracy: 0.8047 - val_loss: 0.5628 - val_accuracy: 0.7510\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4998 - accuracy: 0.8047 - val_loss: 0.5739 - val_accuracy: 0.7510\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4975 - accuracy: 0.8047 - val_loss: 0.5655 - val_accuracy: 0.7510\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.5095 - accuracy: 0.8007 - val_loss: 0.5857 - val_accuracy: 0.7510\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4591 - accuracy: 0.8054 - val_loss: 0.4307 - val_accuracy: 0.7714\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.3040 - accuracy: 0.8691 - val_loss: 0.2907 - val_accuracy: 0.9122\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.2364 - accuracy: 0.9128 - val_loss: 0.4048 - val_accuracy: 0.8776\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.2199 - accuracy: 0.9054 - val_loss: 0.3935 - val_accuracy: 0.8592\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.871425\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.871358\n",
            "[2]\tvalidation_0-auc:0.881582\n",
            "[3]\tvalidation_0-auc:0.87892\n",
            "[4]\tvalidation_0-auc:0.877873\n",
            "[5]\tvalidation_0-auc:0.878163\n",
            "[6]\tvalidation_0-auc:0.878452\n",
            "[7]\tvalidation_0-auc:0.878642\n",
            "[8]\tvalidation_0-auc:0.878597\n",
            "[9]\tvalidation_0-auc:0.878586\n",
            "[10]\tvalidation_0-auc:0.878319\n",
            "[11]\tvalidation_0-auc:0.878363\n",
            "[12]\tvalidation_0-auc:0.8944\n",
            "[13]\tvalidation_0-auc:0.8943\n",
            "[14]\tvalidation_0-auc:0.894044\n",
            "[15]\tvalidation_0-auc:0.88891\n",
            "[16]\tvalidation_0-auc:0.88852\n",
            "[17]\tvalidation_0-auc:0.888386\n",
            "[18]\tvalidation_0-auc:0.889734\n",
            "[19]\tvalidation_0-auc:0.889979\n",
            "[20]\tvalidation_0-auc:0.887284\n",
            "[21]\tvalidation_0-auc:0.886616\n",
            "[22]\tvalidation_0-auc:0.88646\n",
            "[23]\tvalidation_0-auc:0.887718\n",
            "[24]\tvalidation_0-auc:0.886538\n",
            "[25]\tvalidation_0-auc:0.887807\n",
            "[26]\tvalidation_0-auc:0.887651\n",
            "[27]\tvalidation_0-auc:0.887674\n",
            "[28]\tvalidation_0-auc:0.887674\n",
            "[29]\tvalidation_0-auc:0.887707\n",
            "[30]\tvalidation_0-auc:0.887729\n",
            "[31]\tvalidation_0-auc:0.887351\n",
            "[32]\tvalidation_0-auc:0.887284\n",
            "[33]\tvalidation_0-auc:0.887373\n",
            "[34]\tvalidation_0-auc:0.887462\n",
            "[35]\tvalidation_0-auc:0.885034\n",
            "[36]\tvalidation_0-auc:0.884878\n",
            "[37]\tvalidation_0-auc:0.884968\n",
            "[38]\tvalidation_0-auc:0.87951\n",
            "[39]\tvalidation_0-auc:0.879488\n",
            "[40]\tvalidation_0-auc:0.87951\n",
            "[41]\tvalidation_0-auc:0.879377\n",
            "[42]\tvalidation_0-auc:0.87931\n",
            "[43]\tvalidation_0-auc:0.879265\n",
            "[44]\tvalidation_0-auc:0.88284\n",
            "[45]\tvalidation_0-auc:0.886092\n",
            "[46]\tvalidation_0-auc:0.886226\n",
            "[47]\tvalidation_0-auc:0.884199\n",
            "[48]\tvalidation_0-auc:0.884244\n",
            "[49]\tvalidation_0-auc:0.884667\n",
            "[50]\tvalidation_0-auc:0.884667\n",
            "[51]\tvalidation_0-auc:0.880646\n",
            "[52]\tvalidation_0-auc:0.881092\n",
            "[53]\tvalidation_0-auc:0.881181\n",
            "[54]\tvalidation_0-auc:0.881092\n",
            "[55]\tvalidation_0-auc:0.881181\n",
            "[56]\tvalidation_0-auc:0.881092\n",
            "[57]\tvalidation_0-auc:0.881159\n",
            "[58]\tvalidation_0-auc:0.881248\n",
            "[59]\tvalidation_0-auc:0.881292\n",
            "[60]\tvalidation_0-auc:0.88127\n",
            "[61]\tvalidation_0-auc:0.879355\n",
            "[62]\tvalidation_0-auc:0.879989\n",
            "Stopping. Best iteration:\n",
            "[12]\tvalidation_0-auc:0.8944\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.5287 - accuracy: 0.7948 - val_loss: 0.5869 - val_accuracy: 0.7330\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5140 - accuracy: 0.8003 - val_loss: 0.5855 - val_accuracy: 0.7330\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4236 - accuracy: 0.8428 - val_loss: 0.5860 - val_accuracy: 0.7309\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.3600 - accuracy: 0.8874 - val_loss: 0.4968 - val_accuracy: 0.7965\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4026 - accuracy: 0.8264 - val_loss: 0.7195 - val_accuracy: 0.7396\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5024 - accuracy: 0.7962 - val_loss: 0.5325 - val_accuracy: 0.7330\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.3257 - accuracy: 0.8813 - val_loss: 0.5339 - val_accuracy: 0.7396\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.2402 - accuracy: 0.9135 - val_loss: 0.4780 - val_accuracy: 0.7877\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.2126 - accuracy: 0.9231 - val_loss: 0.5674 - val_accuracy: 0.7856\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.1945 - accuracy: 0.9314 - val_loss: 0.6479 - val_accuracy: 0.7768\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.698618\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.698801\n",
            "[2]\tvalidation_0-auc:0.698801\n",
            "[3]\tvalidation_0-auc:0.699205\n",
            "[4]\tvalidation_0-auc:0.699205\n",
            "[5]\tvalidation_0-auc:0.70219\n",
            "[6]\tvalidation_0-auc:0.699107\n",
            "[7]\tvalidation_0-auc:0.699107\n",
            "[8]\tvalidation_0-auc:0.702728\n",
            "[9]\tvalidation_0-auc:0.701945\n",
            "[10]\tvalidation_0-auc:0.703364\n",
            "[11]\tvalidation_0-auc:0.702312\n",
            "[12]\tvalidation_0-auc:0.709946\n",
            "[13]\tvalidation_0-auc:0.709579\n",
            "[14]\tvalidation_0-auc:0.734316\n",
            "[15]\tvalidation_0-auc:0.734316\n",
            "[16]\tvalidation_0-auc:0.734438\n",
            "[17]\tvalidation_0-auc:0.741118\n",
            "[18]\tvalidation_0-auc:0.74058\n",
            "[19]\tvalidation_0-auc:0.740139\n",
            "[20]\tvalidation_0-auc:0.738574\n",
            "[21]\tvalidation_0-auc:0.739063\n",
            "[22]\tvalidation_0-auc:0.73735\n",
            "[23]\tvalidation_0-auc:0.738109\n",
            "[24]\tvalidation_0-auc:0.737644\n",
            "[25]\tvalidation_0-auc:0.737717\n",
            "[26]\tvalidation_0-auc:0.735613\n",
            "[27]\tvalidation_0-auc:0.74354\n",
            "[28]\tvalidation_0-auc:0.746782\n",
            "[29]\tvalidation_0-auc:0.74622\n",
            "[30]\tvalidation_0-auc:0.710472\n",
            "[31]\tvalidation_0-auc:0.719415\n",
            "[32]\tvalidation_0-auc:0.721373\n",
            "[33]\tvalidation_0-auc:0.720883\n",
            "[34]\tvalidation_0-auc:0.73505\n",
            "[35]\tvalidation_0-auc:0.734879\n",
            "[36]\tvalidation_0-auc:0.732114\n",
            "[37]\tvalidation_0-auc:0.737118\n",
            "[38]\tvalidation_0-auc:0.737803\n",
            "[39]\tvalidation_0-auc:0.737485\n",
            "[40]\tvalidation_0-auc:0.735919\n",
            "[41]\tvalidation_0-auc:0.733386\n",
            "[42]\tvalidation_0-auc:0.736567\n",
            "[43]\tvalidation_0-auc:0.736396\n",
            "[44]\tvalidation_0-auc:0.733607\n",
            "[45]\tvalidation_0-auc:0.736347\n",
            "[46]\tvalidation_0-auc:0.734524\n",
            "[47]\tvalidation_0-auc:0.735662\n",
            "[48]\tvalidation_0-auc:0.734218\n",
            "[49]\tvalidation_0-auc:0.75482\n",
            "[50]\tvalidation_0-auc:0.754465\n",
            "[51]\tvalidation_0-auc:0.753144\n",
            "[52]\tvalidation_0-auc:0.760668\n",
            "[53]\tvalidation_0-auc:0.777881\n",
            "[54]\tvalidation_0-auc:0.771275\n",
            "[55]\tvalidation_0-auc:0.773477\n",
            "[56]\tvalidation_0-auc:0.770688\n",
            "[57]\tvalidation_0-auc:0.776535\n",
            "[58]\tvalidation_0-auc:0.784353\n",
            "[59]\tvalidation_0-auc:0.782934\n",
            "[60]\tvalidation_0-auc:0.788414\n",
            "[61]\tvalidation_0-auc:0.786066\n",
            "[62]\tvalidation_0-auc:0.785478\n",
            "[63]\tvalidation_0-auc:0.792109\n",
            "[64]\tvalidation_0-auc:0.792011\n",
            "[65]\tvalidation_0-auc:0.788635\n",
            "[66]\tvalidation_0-auc:0.788586\n",
            "[67]\tvalidation_0-auc:0.793577\n",
            "[68]\tvalidation_0-auc:0.795657\n",
            "[69]\tvalidation_0-auc:0.795657\n",
            "[70]\tvalidation_0-auc:0.793846\n",
            "[71]\tvalidation_0-auc:0.798813\n",
            "[72]\tvalidation_0-auc:0.797419\n",
            "[73]\tvalidation_0-auc:0.792806\n",
            "[74]\tvalidation_0-auc:0.79354\n",
            "[75]\tvalidation_0-auc:0.793443\n",
            "[76]\tvalidation_0-auc:0.788953\n",
            "[77]\tvalidation_0-auc:0.792036\n",
            "[78]\tvalidation_0-auc:0.792415\n",
            "[79]\tvalidation_0-auc:0.794654\n",
            "[80]\tvalidation_0-auc:0.793944\n",
            "[81]\tvalidation_0-auc:0.797125\n",
            "[82]\tvalidation_0-auc:0.795853\n",
            "[83]\tvalidation_0-auc:0.795926\n",
            "[84]\tvalidation_0-auc:0.795314\n",
            "[85]\tvalidation_0-auc:0.796024\n",
            "[86]\tvalidation_0-auc:0.796293\n",
            "[87]\tvalidation_0-auc:0.800232\n",
            "[88]\tvalidation_0-auc:0.802324\n",
            "[89]\tvalidation_0-auc:0.802275\n",
            "[90]\tvalidation_0-auc:0.798214\n",
            "[91]\tvalidation_0-auc:0.79354\n",
            "[92]\tvalidation_0-auc:0.792293\n",
            "[93]\tvalidation_0-auc:0.794054\n",
            "[94]\tvalidation_0-auc:0.789895\n",
            "[95]\tvalidation_0-auc:0.789136\n",
            "[96]\tvalidation_0-auc:0.784218\n",
            "[97]\tvalidation_0-auc:0.784169\n",
            "[98]\tvalidation_0-auc:0.780279\n",
            "[99]\tvalidation_0-auc:0.781135\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.05     | 0.7510204081632653 | 0.7510204081632653 |        1.0         | 0.8578088578088577 |\n",
            "|      GRU 0.05     | 0.8591836734693877 | 0.8468677494199536 | 0.9918478260869565 | 0.9136420525657071 |\n",
            "|    XGBoost 0.05   | 0.8653061224489796 | 0.8511627906976744 | 0.9945652173913043 | 0.9172932330827068 |\n",
            "|    Logreg 0.05    | 0.7510204081632653 | 0.7510204081632653 |        1.0         | 0.8578088578088577 |\n",
            "|      SVM 0.05     | 0.8571428571428571 | 0.8449074074074074 | 0.9918478260869565 |       0.9125       |\n",
            "|   LSTM beta 0.05  | 0.7396061269146609 | 0.7465753424657534 | 0.9761194029850746 | 0.8460543337645537 |\n",
            "|   GRU beta 0.05   | 0.7768052516411379 | 0.7754137115839244 | 0.9791044776119403 | 0.8654353562005277 |\n",
            "| XGBoost beta 0.05 | 0.8140043763676149 | 0.8078817733990148 | 0.9791044776119403 | 0.8852901484480432 |\n",
            "|  logreg beta 0.05 | 0.7680525164113785 | 0.7694117647058824 | 0.9761194029850746 | 0.8605263157894738 |\n",
            "|   svm beta 0.05   | 0.7768052516411379 | 0.7767220902612827 | 0.9761194029850746 | 0.8650793650793651 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZeyIeftYiNw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "665921f4-6c2f-4b5e-e168-73e55a41fef8"
      },
      "source": [
        "Result_purging.to_csv('RMD_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.05</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.751020</td>\n",
              "      <td>0.751020</td>\n",
              "      <td>0.857809</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.05</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.846868</td>\n",
              "      <td>0.859184</td>\n",
              "      <td>0.913642</td>\n",
              "      <td>0.991848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.05</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.851163</td>\n",
              "      <td>0.865306</td>\n",
              "      <td>0.917293</td>\n",
              "      <td>0.994565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.05</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.751020</td>\n",
              "      <td>0.751020</td>\n",
              "      <td>0.857809</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.05</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.844907</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.912500</td>\n",
              "      <td>0.991848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.05</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.746575</td>\n",
              "      <td>0.739606</td>\n",
              "      <td>0.846054</td>\n",
              "      <td>0.976119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.05</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.775414</td>\n",
              "      <td>0.776805</td>\n",
              "      <td>0.865435</td>\n",
              "      <td>0.979104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.05</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.807882</td>\n",
              "      <td>0.814004</td>\n",
              "      <td>0.885290</td>\n",
              "      <td>0.979104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.05</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.769412</td>\n",
              "      <td>0.768053</td>\n",
              "      <td>0.860526</td>\n",
              "      <td>0.976119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.05</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.776722</td>\n",
              "      <td>0.776805</td>\n",
              "      <td>0.865079</td>\n",
              "      <td>0.976119</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0          LSTM 0.05  RMD  0.751020  0.751020  0.857809  1.000000\n",
              "1           GRU 0.05  RMD  0.846868  0.859184  0.913642  0.991848\n",
              "2       XGBoost 0.05  RMD  0.851163  0.865306  0.917293  0.994565\n",
              "3        Logreg 0.05  RMD  0.751020  0.751020  0.857809  1.000000\n",
              "4           SVM 0.05  RMD  0.844907  0.857143  0.912500  0.991848\n",
              "5     LSTM beta 0.05  RMD  0.746575  0.739606  0.846054  0.976119\n",
              "6      GRU beta 0.05  RMD  0.775414  0.776805  0.865435  0.979104\n",
              "7  XGBoost beta 0.05  RMD  0.807882  0.814004  0.885290  0.979104\n",
              "8   logreg beta 0.05  RMD  0.769412  0.768053  0.860526  0.976119\n",
              "9      svm beta 0.05  RMD  0.776722  0.776805  0.865079  0.976119"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH2Iq8EcYiNw"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3kQ1guHaT_S"
      },
      "source": [
        "## WU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZVmjjLlaT_S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "outputId": "cf17cf62-fdd3-4c4b-fc30-eab420a36aaa"
      },
      "source": [
        "dfs = pd.read_csv(\"WU.csv\")\n",
        "# dfs = dfs[::-1].reset_index()\n",
        "# Denoise prices\n",
        "dfs = denoise_data(dfs, '<CLOSE>')\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>US1.WU</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>17.62</td>\n",
              "      <td>17.790</td>\n",
              "      <td>17.360</td>\n",
              "      <td>18.879175</td>\n",
              "      <td>4081478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>US1.WU</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>17.56</td>\n",
              "      <td>17.990</td>\n",
              "      <td>17.400</td>\n",
              "      <td>18.879175</td>\n",
              "      <td>5231417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>US1.WU</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>17.99</td>\n",
              "      <td>18.000</td>\n",
              "      <td>17.570</td>\n",
              "      <td>18.879175</td>\n",
              "      <td>3513358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>US1.WU</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>17.77</td>\n",
              "      <td>17.820</td>\n",
              "      <td>17.540</td>\n",
              "      <td>18.879175</td>\n",
              "      <td>3231681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US1.WU</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>17.62</td>\n",
              "      <td>17.780</td>\n",
              "      <td>17.500</td>\n",
              "      <td>18.879175</td>\n",
              "      <td>2250995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>US1.WU</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>20.94</td>\n",
              "      <td>20.940</td>\n",
              "      <td>20.450</td>\n",
              "      <td>22.843950</td>\n",
              "      <td>157910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>US1.WU</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>20.82</td>\n",
              "      <td>21.130</td>\n",
              "      <td>20.755</td>\n",
              "      <td>22.843950</td>\n",
              "      <td>160902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>US1.WU</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>20.86</td>\n",
              "      <td>20.950</td>\n",
              "      <td>20.660</td>\n",
              "      <td>22.843950</td>\n",
              "      <td>65787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>US1.WU</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>20.83</td>\n",
              "      <td>20.870</td>\n",
              "      <td>20.215</td>\n",
              "      <td>22.843950</td>\n",
              "      <td>155988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>US1.WU</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>20.50</td>\n",
              "      <td>20.675</td>\n",
              "      <td>20.160</td>\n",
              "      <td>22.843950</td>\n",
              "      <td>111302</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2769 rows Ã— 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     <TICKER> <PER>    <DATE>  <TIME>  ...  <HIGH>   <LOW>    <CLOSE>    <VOL>\n",
              "0      US1.WU     D  20101004       0  ...  17.790  17.360  18.879175  4081478\n",
              "1      US1.WU     D  20101005       0  ...  17.990  17.400  18.879175  5231417\n",
              "2      US1.WU     D  20101006       0  ...  18.000  17.570  18.879175  3513358\n",
              "3      US1.WU     D  20101007       0  ...  17.820  17.540  18.879175  3231681\n",
              "4      US1.WU     D  20101008       0  ...  17.780  17.500  18.879175  2250995\n",
              "...       ...   ...       ...     ...  ...     ...     ...        ...      ...\n",
              "2764   US1.WU     D  20210927       0  ...  20.940  20.450  22.843950   157910\n",
              "2765   US1.WU     D  20210928       0  ...  21.130  20.755  22.843950   160902\n",
              "2766   US1.WU     D  20210929       0  ...  20.950  20.660  22.843950    65787\n",
              "2767   US1.WU     D  20210930       0  ...  20.870  20.215  22.843950   155988\n",
              "2768   US1.WU     D  20211001       0  ...  20.675  20.160  22.843950   111302\n",
              "\n",
              "[2769 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9P6ZUYWaT_S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf811995-d287-4f5f-c06e-e44c1ef9d678"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.05, 100, 2300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"8a494b0f-5a27-40a3-a6fb-f40d61011bba\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"8a494b0f-5a27-40a3-a6fb-f40d61011bba\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '8a494b0f-5a27-40a3-a6fb-f40d61011bba',\n",
              "                        [{\"type\": \"scatter\", \"x\": [20110225, 20110228, 20110301, 20110302, 20110303, 20110304, 20110307, 20110308, 20110309, 20110310, 20110311, 20110314, 20110315, 20110316, 20110317, 20110318, 20110321, 20110322, 20110323, 20110324, 20110325, 20110328, 20110329, 20110330, 20110331, 20110401, 20110404, 20110405, 20110406, 20110407, 20110408, 20110411, 20110412, 20110413, 20110414, 20110415, 20110418, 20110419, 20110420, 20110421, 20110425, 20110426, 20110427, 20110428, 20110429, 20110502, 20110503, 20110504, 20110505, 20110506, 20110509, 20110510, 20110511, 20110512, 20110513, 20110516, 20110517, 20110518, 20110519, 20110520, 20110523, 20110524, 20110525, 20110526, 20110527, 20110531, 20110601, 20110602, 20110603, 20110606, 20110607, 20110608, 20110609, 20110610, 20110613, 20110614, 20110615, 20110616, 20110617, 20110620, 20110621, 20110622, 20110623, 20110624, 20110627, 20110628, 20110629, 20110630, 20110701, 20110705, 20110706, 20110707, 20110708, 20110711, 20110712, 20110713, 20110714, 20110715, 20110718, 20110719, 20110720, 20110721, 20110722, 20110725, 20110726, 20110727, 20110728, 20110729, 20110801, 20110802, 20110803, 20110804, 20110805, 20110808, 20110809, 20110810, 20110811, 20110812, 20110815, 20110816, 20110817, 20110818, 20110819, 20110822, 20110823, 20110824, 20110825, 20110826, 20110829, 20110830, 20110831, 20110901, 20110902, 20110906, 20110907, 20110908, 20110909, 20110912, 20110913, 20110914, 20110915, 20110916, 20110919, 20110920, 20110921, 20110922, 20110923, 20110926, 20110927, 20110928, 20110929, 20110930, 20111003, 20111004, 20111005, 20111006, 20111007, 20111010, 20111011, 20111012, 20111013, 20111014, 20111017, 20111018, 20111019, 20111020, 20111021, 20111024, 20111025, 20111026, 20111027, 20111028, 20111031, 20111101, 20111102, 20111103, 20111104, 20111107, 20111108, 20111109, 20111110, 20111111, 20111114, 20111115, 20111116, 20111117, 20111118, 20111121, 20111122, 20111123, 20111125, 20111128, 20111129, 20111130, 20111201, 20111202, 20111205, 20111206, 20111207, 20111208, 20111209, 20111212, 20111213, 20111214, 20111215, 20111216, 20111219, 20111220, 20111221, 20111222, 20111223, 20111227, 20111228, 20111229, 20111230, 20120103, 20120104, 20120105, 20120106, 20120109, 20120110, 20120111, 20120112, 20120113, 20120117, 20120118, 20120119, 20120120, 20120123, 20120124, 20120125, 20120126, 20120127, 20120130, 20120131, 20120201, 20120202, 20120203, 20120206, 20120207, 20120208, 20120209, 20120210, 20120213, 20120214, 20120215, 20120216, 20120217, 20120221, 20120222, 20120223, 20120224, 20120227, 20120228, 20120229, 20120301, 20120302, 20120305, 20120306, 20120307, 20120308, 20120309, 20120312, 20120313, 20120314, 20120315, 20120316, 20120319, 20120320, 20120321, 20120322, 20120323, 20120326, 20120327, 20120328, 20120329, 20120330, 20120402, 20120403, 20120404, 20120405, 20120409, 20120410, 20120411, 20120412, 20120413, 20120416, 20120417, 20120418, 20120419, 20120420, 20120423, 20120424, 20120425, 20120426, 20120427, 20120430, 20120501, 20120502, 20120503, 20120504, 20120507, 20120508, 20120509, 20120510, 20120511, 20120514, 20120515, 20120516, 20120517, 20120518, 20120521, 20120522, 20120523, 20120524, 20120525, 20120529, 20120530, 20120531, 20120601, 20120604, 20120605, 20120606, 20120607, 20120608, 20120611, 20120612, 20120613, 20120614, 20120615, 20120618, 20120619, 20120620, 20120621, 20120622, 20120625, 20120626, 20120627, 20120628, 20120629, 20120702, 20120703, 20120705, 20120706, 20120709, 20120710, 20120711, 20120712, 20120713, 20120716, 20120717, 20120718, 20120719, 20120720, 20120723, 20120724, 20120725, 20120726, 20120727, 20120730, 20120731, 20120801, 20120802, 20120803, 20120806, 20120807, 20120808, 20120809, 20120810, 20120813, 20120814, 20120815, 20120816, 20120817, 20120820, 20120821, 20120822, 20120823, 20120824, 20120827, 20120828, 20120829, 20120830, 20120831, 20120904, 20120905, 20120906, 20120907, 20120910, 20120911, 20120912, 20120913, 20120914, 20120917, 20120918, 20120919, 20120920, 20120921, 20120924, 20120925, 20120926, 20120927, 20120928, 20121001, 20121002, 20121003, 20121004, 20121005, 20121008, 20121009, 20121010, 20121011, 20121012, 20121015, 20121016, 20121017, 20121018, 20121019, 20121022, 20121023, 20121024, 20121025, 20121026, 20121031, 20121101, 20121102, 20121105, 20121106, 20121107, 20121108, 20121109, 20121112, 20121113, 20121114, 20121115, 20121116, 20121119, 20121120, 20121121, 20121123, 20121126, 20121127, 20121128, 20121129, 20121130, 20121203, 20121204, 20121205, 20121206, 20121207, 20121210, 20121211, 20121212, 20121213, 20121214, 20121217, 20121218, 20121219, 20121220, 20121221, 20121224, 20121226, 20121227, 20121228, 20121231, 20130102, 20130103, 20130104, 20130107, 20130108, 20130109, 20130110, 20130111, 20130114, 20130115, 20130116, 20130117, 20130118, 20130122, 20130123, 20130124, 20130125, 20130128, 20130129, 20130130, 20130131, 20130201, 20130204, 20130205, 20130206, 20130207, 20130208, 20130211, 20130212, 20130213, 20130214, 20130215, 20130219, 20130220, 20130221, 20130222, 20130225, 20130226, 20130227, 20130228, 20130301, 20130304, 20130305, 20130306, 20130307, 20130308, 20130311, 20130312, 20130313, 20130314, 20130315, 20130318, 20130319, 20130320, 20130321, 20130322, 20130325, 20130326, 20130327, 20130328, 20130401, 20130402, 20130403, 20130404, 20130405, 20130408, 20130409, 20130410, 20130411, 20130412, 20130415, 20130416, 20130417, 20130418, 20130419, 20130422, 20130423, 20130424, 20130425, 20130426, 20130429, 20130430, 20130501, 20130502, 20130503, 20130506, 20130507, 20130508, 20130509, 20130510, 20130513, 20130514, 20130515, 20130516, 20130517, 20130520, 20130521, 20130522, 20130523, 20130524, 20130528, 20130529, 20130530, 20130531, 20130603, 20130604, 20130605, 20130606, 20130607, 20130610, 20130611, 20130612, 20130613, 20130614, 20130617, 20130618, 20130619, 20130620, 20130621, 20130624, 20130625, 20130626, 20130627, 20130628, 20130701, 20130702, 20130703, 20130705, 20130708, 20130709, 20130710, 20130711, 20130712, 20130715, 20130716, 20130717, 20130718, 20130719, 20130722, 20130723, 20130724, 20130725, 20130726, 20130729, 20130730, 20130731, 20130801, 20130802, 20130805, 20130806, 20130807, 20130808, 20130809, 20130812, 20130813, 20130814, 20130815, 20130816, 20130819, 20130820, 20130821, 20130822, 20130823, 20130826, 20130827, 20130828, 20130829, 20130830, 20130903, 20130904, 20130905, 20130906, 20130909, 20130910, 20130911, 20130912, 20130913, 20130916, 20130917, 20130918, 20130919, 20130920, 20130923, 20130924, 20130925, 20130926, 20130927, 20130930, 20131001, 20131002, 20131003, 20131004, 20131007, 20131008, 20131009, 20131010, 20131011, 20131014, 20131015, 20131016, 20131017, 20131018, 20131021, 20131022, 20131023, 20131024, 20131025, 20131028, 20131029, 20131030, 20131031, 20131101, 20131104, 20131105, 20131106, 20131107, 20131108, 20131111, 20131112, 20131113, 20131114, 20131115, 20131118, 20131119, 20131120, 20131121, 20131122, 20131125, 20131126, 20131127, 20131129, 20131202, 20131203, 20131204, 20131205, 20131206, 20131209, 20131210, 20131211, 20131212, 20131213, 20131216, 20131217, 20131218, 20131219, 20131220, 20131223, 20131224, 20131226, 20131227, 20131230, 20131231, 20140102, 20140103, 20140106, 20140107, 20140108, 20140109, 20140110, 20140113, 20140114, 20140115, 20140116, 20140117, 20140121, 20140122, 20140123, 20140124, 20140127, 20140128, 20140129, 20140130, 20140131, 20140203, 20140204, 20140205, 20140206, 20140207, 20140210, 20140211, 20140212, 20140213, 20140214, 20140218, 20140219, 20140220, 20140221, 20140224, 20140225, 20140226, 20140227, 20140228, 20140303, 20140304, 20140305, 20140306, 20140307, 20140310, 20140311, 20140312, 20140313, 20140314, 20140317, 20140318, 20140319, 20140320, 20140321, 20140324, 20140325, 20140326, 20140327, 20140328, 20140331, 20140401, 20140402, 20140403, 20140404, 20140407, 20140408, 20140409, 20140410, 20140411, 20140414, 20140415, 20140416, 20140417, 20140421, 20140422, 20140423, 20140424, 20140425, 20140428, 20140429, 20140430, 20140501, 20140502, 20140505, 20140506, 20140507, 20140508, 20140509, 20140512, 20140513, 20140514, 20140515, 20140516, 20140519, 20140520, 20140521, 20140522, 20140523, 20140527, 20140528, 20140529, 20140530, 20140602, 20140603, 20140604, 20140605, 20140606, 20140609, 20140610, 20140611, 20140612, 20140613, 20140616, 20140617, 20140618, 20140619, 20140620, 20140623, 20140624, 20140625, 20140626, 20140627, 20140630, 20140701, 20140702, 20140703, 20140707, 20140708, 20140709, 20140710, 20140711, 20140714, 20140715, 20140716, 20140717, 20140718, 20140721, 20140722, 20140723, 20140724, 20140725, 20140728, 20140729, 20140730, 20140731, 20140801, 20140804, 20140805, 20140806, 20140807, 20140808, 20140811, 20140812, 20140813, 20140814, 20140815, 20140818, 20140819, 20140820, 20140821, 20140822, 20140825, 20140826, 20140827, 20140828, 20140829, 20140902, 20140903, 20140904, 20140905, 20140908, 20140909, 20140910, 20140911, 20140912, 20140915, 20140916, 20140917, 20140918, 20140919, 20140922, 20140923, 20140924, 20140925, 20140926, 20140929, 20140930, 20141001, 20141002, 20141003, 20141006, 20141007, 20141008, 20141009, 20141010, 20141013, 20141014, 20141015, 20141016, 20141017, 20141020, 20141021, 20141022, 20141023, 20141024, 20141027, 20141028, 20141029, 20141030, 20141031, 20141103, 20141104, 20141105, 20141106, 20141107, 20141110, 20141111, 20141112, 20141113, 20141114, 20141117, 20141118, 20141119, 20141120, 20141121, 20141124, 20141125, 20141126, 20141128, 20141201, 20141202, 20141203, 20141204, 20141205, 20141208, 20141209, 20141210, 20141211, 20141212, 20141215, 20141216, 20141217, 20141218, 20141219, 20141222, 20141223, 20141224, 20141226, 20141229, 20141230, 20141231, 20150102, 20150105, 20150106, 20150107, 20150108, 20150109, 20150112, 20150113, 20150114, 20150115, 20150116, 20150120, 20150121, 20150122, 20150123, 20150126, 20150127, 20150128, 20150129, 20150130, 20150202, 20150203, 20150204, 20150205, 20150206, 20150209, 20150210, 20150211, 20150212, 20150213, 20150217, 20150218, 20150219, 20150220, 20150223, 20150224, 20150225, 20150226, 20150227, 20150302, 20150303, 20150304, 20150305, 20150306, 20150309, 20150310, 20150311, 20150312, 20150313, 20150316, 20150317, 20150318, 20150319, 20150320, 20150323, 20150324, 20150325, 20150326, 20150327, 20150330, 20150331, 20150401, 20150402, 20150406, 20150407, 20150408, 20150409, 20150410, 20150413, 20150414, 20150415, 20150416, 20150417, 20150420, 20150421, 20150422, 20150423, 20150424, 20150427, 20150428, 20150429, 20150430, 20150501, 20150504, 20150505, 20150506, 20150507, 20150508, 20150511, 20150512, 20150513, 20150514, 20150515, 20150518, 20150519, 20150520, 20150521, 20150522, 20150526, 20150527, 20150528, 20150529, 20150601, 20150602, 20150603, 20150604, 20150605, 20150608, 20150609, 20150610, 20150611, 20150612, 20150615, 20150616, 20150617, 20150618, 20150619, 20150622, 20150623, 20150624, 20150625, 20150626, 20150629, 20150630, 20150701, 20150702, 20150706, 20150707, 20150708, 20150709, 20150710, 20150713, 20150714, 20150715, 20150716, 20150717, 20150720, 20150721, 20150722, 20150723, 20150724, 20150727, 20150728, 20150729, 20150730, 20150731, 20150803, 20150804, 20150805, 20150806, 20150807, 20150810, 20150811, 20150812, 20150813, 20150814, 20150817, 20150818, 20150819, 20150820, 20150821, 20150824, 20150825, 20150826, 20150827, 20150828, 20150831, 20150901, 20150902, 20150903, 20150904, 20150908, 20150909, 20150910, 20150911, 20150914, 20150915, 20150916, 20150917, 20150918, 20150921, 20150922, 20150923, 20150924, 20150925, 20150928, 20150929, 20150930, 20151001, 20151002, 20151005, 20151006, 20151007, 20151008, 20151009, 20151012, 20151013, 20151014, 20151015, 20151016, 20151019, 20151020, 20151021, 20151022, 20151023, 20151026, 20151027, 20151028, 20151029, 20151030, 20151102, 20151103, 20151104, 20151105, 20151106, 20151109, 20151110, 20151111, 20151112, 20151113, 20151116, 20151117, 20151118, 20151119, 20151120, 20151123, 20151124, 20151125, 20151127, 20151130, 20151201, 20151202, 20151203, 20151204, 20151207, 20151208, 20151209, 20151210, 20151211, 20151214, 20151215, 20151216, 20151217, 20151218, 20151221, 20151222, 20151223, 20151224, 20151228, 20151229, 20151230, 20151231, 20160104, 20160105, 20160106, 20160107, 20160108, 20160111, 20160112, 20160113, 20160114, 20160115, 20160119, 20160120, 20160121, 20160122, 20160125, 20160126, 20160127, 20160128, 20160129, 20160201, 20160202, 20160203, 20160204, 20160205, 20160208, 20160209, 20160210, 20160211, 20160212, 20160216, 20160217, 20160218, 20160219, 20160222, 20160223, 20160224, 20160225, 20160226, 20160229, 20160301, 20160302, 20160303, 20160304, 20160307, 20160308, 20160309, 20160310, 20160311, 20160314, 20160315, 20160316, 20160317, 20160318, 20160321, 20160322, 20160323, 20160324, 20160328, 20160329, 20160330, 20160331, 20160401, 20160404, 20160405, 20160406, 20160407, 20160408, 20160411, 20160412, 20160413, 20160414, 20160415, 20160418, 20160419, 20160420, 20160421, 20160422, 20160425, 20160426, 20160427, 20160428, 20160429, 20160502, 20160503, 20160504, 20160505, 20160506, 20160509, 20160510, 20160511, 20160512, 20160513, 20160516, 20160517, 20160518, 20160519, 20160520, 20160523, 20160524, 20160525, 20160526, 20160527, 20160531, 20160601, 20160602, 20160603, 20160606, 20160607, 20160608, 20160609, 20160610, 20160613, 20160614, 20160615, 20160616, 20160617, 20160620, 20160621, 20160622, 20160623, 20160624, 20160627, 20160628, 20160629, 20160630, 20160701, 20160705, 20160706, 20160707, 20160708, 20160711, 20160712, 20160713, 20160714, 20160715, 20160718, 20160719, 20160720, 20160721, 20160722, 20160725, 20160726, 20160727, 20160728, 20160729, 20160801, 20160802, 20160803, 20160804, 20160805, 20160808, 20160809, 20160810, 20160811, 20160812, 20160815, 20160816, 20160817, 20160818, 20160819, 20160822, 20160823, 20160824, 20160825, 20160826, 20160829, 20160830, 20160831, 20160901, 20160902, 20160906, 20160907, 20160908, 20160909, 20160912, 20160913, 20160914, 20160915, 20160916, 20160919, 20160920, 20160921, 20160922, 20160923, 20160926, 20160927, 20160928, 20160929, 20160930, 20161003, 20161004, 20161005, 20161006, 20161007, 20161010, 20161011, 20161012, 20161013, 20161014, 20161017, 20161018, 20161019, 20161020, 20161021, 20161024, 20161025, 20161026, 20161027, 20161028, 20161031, 20161101, 20161102, 20161103, 20161104, 20161107, 20161108, 20161109, 20161110, 20161111, 20161114, 20161115, 20161116, 20161117, 20161118, 20161121, 20161122, 20161123, 20161125, 20161128, 20161129, 20161130, 20161201, 20161202, 20161205, 20161206, 20161207, 20161208, 20161209, 20161212, 20161213, 20161214, 20161215, 20161216, 20161219, 20161220, 20161221, 20161222, 20161223, 20161227, 20161228, 20161229, 20161230, 20170103, 20170104, 20170105, 20170106, 20170109, 20170110, 20170111, 20170112, 20170113, 20170117, 20170118, 20170119, 20170120, 20170123, 20170124, 20170125, 20170126, 20170127, 20170130, 20170131, 20170201, 20170202, 20170203, 20170206, 20170207, 20170208, 20170209, 20170210, 20170213, 20170214, 20170215, 20170216, 20170217, 20170221, 20170222, 20170223, 20170224, 20170227, 20170228, 20170301, 20170302, 20170303, 20170306, 20170307, 20170308, 20170309, 20170310, 20170313, 20170314, 20170315, 20170316, 20170317, 20170320, 20170321, 20170322, 20170323, 20170324, 20170327, 20170328, 20170329, 20170330, 20170331, 20170403, 20170404, 20170405, 20170406, 20170407, 20170410, 20170411, 20170412, 20170413, 20170417, 20170418, 20170419, 20170420, 20170421, 20170424, 20170425, 20170426, 20170427, 20170428, 20170501, 20170502, 20170503, 20170504, 20170505, 20170508, 20170509, 20170510, 20170511, 20170512, 20170515, 20170516, 20170517, 20170518, 20170519, 20170522, 20170523, 20170524, 20170525, 20170526, 20170530, 20170531, 20170601, 20170602, 20170605, 20170606, 20170607, 20170608, 20170609, 20170612, 20170613, 20170614, 20170615, 20170616, 20170619, 20170620, 20170621, 20170622, 20170623, 20170626, 20170627, 20170628, 20170629, 20170630, 20170703, 20170705, 20170706, 20170707, 20170710, 20170711, 20170712, 20170713, 20170714, 20170717, 20170718, 20170719, 20170720, 20170721, 20170724, 20170725, 20170726, 20170727, 20170728, 20170731, 20170801, 20170802, 20170803, 20170804, 20170807, 20170808, 20170809, 20170810, 20170811, 20170814, 20170815, 20170816, 20170817, 20170818, 20170821, 20170822, 20170823, 20170824, 20170825, 20170828, 20170829, 20170830, 20170831, 20170901, 20170905, 20170906, 20170907, 20170908, 20170911, 20170912, 20170913, 20170914, 20170915, 20170918, 20170919, 20170920, 20170921, 20170922, 20170925, 20170926, 20170927, 20170928, 20170929, 20171002, 20171003, 20171004, 20171005, 20171006, 20171009, 20171010, 20171011, 20171012, 20171013, 20171016, 20171017, 20171018, 20171019, 20171020, 20171023, 20171024, 20171025, 20171026, 20171027, 20171030, 20171031, 20171101, 20171102, 20171103, 20171106, 20171107, 20171108, 20171109, 20171110, 20171113, 20171114, 20171115, 20171116, 20171117, 20171120, 20171121, 20171122, 20171124, 20171127, 20171128, 20171129, 20171130, 20171201, 20171204, 20171205, 20171206, 20171207, 20171208, 20171211, 20171212, 20171213, 20171214, 20171215, 20171218, 20171219, 20171220, 20171221, 20171222, 20171226, 20171227, 20171228, 20171229, 20180102, 20180103, 20180104, 20180105, 20180108, 20180109, 20180110, 20180111, 20180112, 20180116, 20180117, 20180118, 20180119, 20180122, 20180123, 20180124, 20180125, 20180126, 20180129, 20180130, 20180131, 20180201, 20180202, 20180205, 20180206, 20180207, 20180208, 20180209, 20180212, 20180213, 20180214, 20180215, 20180216, 20180220, 20180221, 20180222, 20180223, 20180226, 20180227, 20180228, 20180301, 20180302, 20180305, 20180306, 20180307, 20180308, 20180309, 20180312, 20180313, 20180314, 20180315, 20180316, 20180319, 20180320, 20180321, 20180322, 20180323, 20180326, 20180327, 20180328, 20180329, 20180402, 20180403, 20180404, 20180405, 20180406, 20180409, 20180410, 20180411, 20180412, 20180413, 20180416, 20180417, 20180418, 20180419, 20180420, 20180423, 20180424, 20180425, 20180426, 20180427, 20180430, 20180501, 20180502, 20180503, 20180504, 20180507, 20180508, 20180509, 20180510, 20180511, 20180514, 20180515, 20180516, 20180517, 20180518, 20180521, 20180522, 20180523, 20180524, 20180525, 20180529, 20180530, 20180531, 20180601, 20180604, 20180605, 20180606, 20180607, 20180608, 20180611, 20180612, 20180613, 20180614, 20180615, 20180618, 20180619, 20180620, 20180621, 20180622, 20180625, 20180626, 20180627, 20180628, 20180629, 20180702, 20180703, 20180705, 20180706, 20180709, 20180710, 20180711, 20180712, 20180713, 20180716, 20180717, 20180718, 20180719, 20180720, 20180723, 20180724, 20180725, 20180726, 20180727, 20180730, 20180731, 20180801, 20180802, 20180803, 20180806, 20180807, 20180808, 20180809, 20180810, 20180813, 20180814, 20180815, 20180816, 20180817, 20180820, 20180821, 20180822, 20180823, 20180824, 20180827, 20180828, 20180829, 20180830, 20180831, 20180904, 20180905, 20180906, 20180907, 20180910, 20180911, 20180912, 20180913, 20180914, 20180917, 20180918, 20180919, 20180920, 20180921, 20180924, 20180925, 20180926, 20180927, 20180928, 20181001, 20181002, 20181003, 20181004, 20181005, 20181008, 20181009, 20181010, 20181011, 20181012, 20181015, 20181016, 20181017, 20181018, 20181019, 20181022, 20181023, 20181024, 20181025, 20181026, 20181029, 20181030, 20181031, 20181101, 20181102, 20181105, 20181106, 20181107, 20181108, 20181109, 20181112, 20181113, 20181114, 20181115, 20181116, 20181119, 20181120, 20181121, 20181123, 20181126, 20181127, 20181128, 20181129, 20181130, 20181203, 20181204, 20181206, 20181207, 20181210, 20181211, 20181212, 20181213, 20181214, 20181217, 20181218, 20181219, 20181220, 20181221, 20181224, 20181226, 20181227, 20181228, 20181231, 20190102, 20190103, 20190104, 20190107, 20190108, 20190109, 20190110, 20190111, 20190114, 20190115, 20190116, 20190117, 20190118, 20190122, 20190123, 20190124, 20190125, 20190128, 20190129, 20190130, 20190131, 20190201, 20190204, 20190205, 20190206, 20190207, 20190208, 20190211, 20190212, 20190213, 20190214, 20190215, 20190219, 20190220, 20190221, 20190222, 20190225, 20190226, 20190227, 20190228, 20190301, 20190304, 20190305, 20190306, 20190307, 20190308, 20190311, 20190312, 20190313, 20190314, 20190315, 20190318, 20190319, 20190320, 20190321, 20190322, 20190325, 20190326, 20190327, 20190328, 20190329, 20190401, 20190402, 20190403, 20190404, 20190405, 20190408, 20190409, 20190410, 20190411, 20190412, 20190415, 20190416, 20190417, 20190418, 20190422, 20190423, 20190424, 20190425, 20190426, 20190429, 20190430, 20190501, 20190502, 20190503, 20190506, 20190507, 20190508, 20190509, 20190510, 20190513, 20190514, 20190515, 20190516, 20190517, 20190520, 20190521, 20190522, 20190523, 20190524, 20190528, 20190529, 20190530, 20190531, 20190603, 20190604, 20190605, 20190606, 20190607, 20190610, 20190611, 20190612, 20190613, 20190614, 20190617, 20190618, 20190619, 20190620, 20190621, 20190624, 20190625, 20190626, 20190627, 20190628, 20190701, 20190702, 20190703, 20190705, 20190708, 20190709, 20190710, 20190711, 20190712, 20190715, 20190716, 20190717, 20190718, 20190719, 20190722, 20190723, 20190724, 20190725, 20190726, 20190729, 20190730, 20190731, 20190801, 20190802, 20190805, 20190806, 20190807, 20190808, 20190809, 20190812, 20190813, 20190814, 20190815, 20190816, 20190819, 20190820, 20190821, 20190822, 20190823, 20190826, 20190827, 20190828, 20190829, 20190830, 20190903, 20190904, 20190905, 20190906, 20190909, 20190910, 20190911, 20190912, 20190913, 20190916, 20190917, 20190918, 20190919, 20190920, 20190923, 20190924, 20190925, 20190926, 20190927, 20190930, 20191001, 20191002, 20191003, 20191004, 20191007, 20191008, 20191009, 20191010, 20191011, 20191014, 20191015, 20191016, 20191017, 20191018, 20191021, 20191022, 20191023, 20191024, 20191025, 20191028, 20191029, 20191030, 20191031, 20191101, 20191104, 20191105, 20191106, 20191107, 20191108, 20191111, 20191112, 20191113, 20191114, 20191115, 20191118, 20191119, 20191120], \"y\": [20.461458999999998, 20.482059, 20.506759000000002, 20.532759000000006, 20.559659, 20.583059000000002, 20.605159, 20.626059, 20.650159, 20.672359, 20.695459, 20.713859000000003, 20.732259, 20.752259, 20.768959000000006, 20.786459, 20.802759000000002, 20.811258999999996, 20.822559000000002, 20.830558999999997, 20.837859, 20.847759, 20.854859, 20.862259000000005, 20.865559000000005, 20.872159, 20.881608999999997, 20.886008999999994, 20.885409, 20.885809, 20.882309, 20.878609, 20.876909, 20.868509, 20.858109, 20.843808999999997, 20.832909, 20.822509, 20.811009, 20.799809000000003, 20.783208999999996, 20.766209000000003, 20.748708999999998, 20.731709000000002, 20.712959, 20.690159000000005, 20.666559000000003, 20.640259, 20.620059, 20.601658999999998, 20.585859, 20.561159000000004, 20.535059, 20.513659000000004, 20.489459, 20.473509, 20.452108999999997, 20.429709, 20.406709, 20.381309, 20.354509, 20.331709, 20.300708999999998, 20.271109, 20.231809, 20.193509, 20.146799, 20.105199000000002, 20.060599000000003, 20.016899000000002, 19.969599000000002, 19.9207, 19.862699999999997, 19.756500000000003, 19.7136, 19.6709, 19.623499999999996, 19.5791, 19.533999999999995, 19.534000000000002, 19.4889, 19.4465, 19.4035, 19.3567, 19.303900000000002, 19.2559, 19.2058, 19.1512, 19.097299999999997, 19.0447, 18.9943, 18.9456, 18.895300000000002, 18.844900000000003, 18.7929, 18.7406, 18.6806, 18.6222, 18.570199999999996, 18.5235, 18.4744, 18.4241, 18.3669, 18.3069, 18.248099999999997, 18.193999999999996, 18.1448, 18.097599999999996, 18.0548, 18.012, 17.9724, 17.9346, 17.898, 17.858, 17.8239, 17.7858, 17.747, 17.6889, 17.6615, 17.6342, 17.6125, 17.5942, 17.568700000000003, 17.5687, 17.5422, 17.5174, 17.491600000000002, 17.4679, 17.4451, 17.4244, 17.3967, 17.3648, 17.3418, 17.317, 17.295099999999998, 17.266199999999998, 17.2319, 17.1947, 17.1578, 17.119899999999998, 17.0885, 17.0561, 17.026500000000002, 16.996000000000002, 16.976000000000003, 16.9583, 16.9423, 16.9302, 16.917699999999996, 16.9051, 16.887700000000002, 16.877100000000002, 16.859399999999997, 16.841099999999997, 16.8227, 16.7933, 16.7755, 16.7572, 16.745800000000003, 16.7347, 16.730399999999996, 16.7355, 16.73895, 16.75865, 16.77365, 16.79505, 16.79505, 16.806449999999998, 16.820850000000004, 16.832549999999998, 16.84615, 16.86335, 16.89065, 16.92245, 16.9518, 16.971549999999997, 16.99445, 17.02505, 17.05295, 17.07725, 17.10445, 17.13435, 17.16045, 17.192300000000003, 17.2265, 17.255999999999997, 17.287499999999998, 17.323600000000003, 17.362999999999996, 17.3975, 17.4314, 17.443900000000003, 17.4565, 17.4697, 17.483499999999996, 17.500099999999996, 17.524, 17.551000000000002, 17.575499999999998, 17.5928, 17.6147, 17.635, 17.660800000000002, 17.6916, 17.718400000000003, 17.7401, 17.7579, 17.772400000000005, 17.7799, 17.786250000000003, 17.792450000000002, 17.80145, 17.81065, 17.82275, 17.833849999999998, 17.84895, 17.862000000000002, 17.870699999999996, 17.874899999999997, 17.8823, 17.88595, 17.88355, 17.883050000000004, 17.89065, 17.90205, 17.90785, 17.911049999999996, 17.913249999999998, 17.92085, 17.924750000000003, 17.932750000000002, 17.94405, 17.947650000000003, 17.95125, 17.95645, 17.96985, 17.98375, 17.99895, 18.015950000000004, 18.02885, 18.04735, 18.06435, 18.076850000000004, 18.090450000000004, 18.09985, 18.108449999999998, 18.12445, 18.13035, 18.132550000000002, 18.13475, 18.129949999999997, 18.12975, 18.129749999999998, 18.129849999999998, 18.128149999999998, 18.12735, 18.12065, 18.11865, 18.107750000000003, 18.09745, 18.082349999999998, 18.06325, 18.04875, 18.0364, 18.0225, 18.01, 17.9937, 17.980900000000005, 17.963399999999996, 17.9445, 17.9206, 17.8939, 17.8667, 17.8406, 17.81795, 17.7951, 17.767500000000002, 17.7388, 17.7076, 17.679199999999998, 17.6497, 17.618299999999998, 17.592899999999997, 17.570449999999997, 17.546850000000003, 17.52325, 17.495250000000002, 17.465850000000003, 17.43355, 17.40305, 17.36675, 17.36045, 17.35435, 17.34645, 17.33855, 17.329150000000002, 17.329150000000002, 17.31905, 17.30395, 17.29365, 17.28115, 17.27135, 17.26265, 17.25255, 17.242150000000002, 17.23685, 17.23505, 17.23225, 17.23355, 17.2392, 17.2421, 17.242700000000003, 17.237, 17.2291, 17.220599999999997, 17.2134, 17.209349999999997, 17.20555, 17.20185, 17.200050000000005, 17.1991, 17.1982, 17.194599999999994, 17.1897, 17.185299999999998, 17.1849, 17.183899999999998, 17.182899999999997, 17.1753, 17.1689, 17.164550000000006, 17.15885, 17.15865, 17.159149999999997, 17.15775, 17.15895, 17.16255, 17.163750000000004, 17.16865, 17.16865, 17.17135, 17.176244999999998, 17.183794999999996, 17.193994999999997, 17.199794999999998, 17.205995, 17.206995000000003, 17.210195000000002, 17.212595, 17.218045000000004, 17.222744, 17.228094, 17.236494, 17.243494, 17.252594, 17.261794, 17.273194, 17.284094, 17.296893999999998, 17.309294, 17.324294, 17.338594, 17.348794, 17.358294, 17.366294000000003, 17.377793999999998, 17.392694000000002, 17.405494, 17.418394, 17.435793999999998, 17.456194000000004, 17.476693999999995, 17.494193999999997, 17.509794000000003, 17.471294, 17.431894, 17.390994, 17.350493999999998, 17.315594, 17.280493999999997, 17.241994, 17.201794000000003, 17.159194, 17.115794, 17.075494000000003, 17.034894, 16.997894000000002, 16.962894000000002, 16.922794, 16.883993999999998, 16.843093999999997, 16.800894, 16.755793999999998, 16.712994, 16.671294, 16.628293999999997, 16.587495, 16.547595, 16.511694999999996, 16.474194999999998, 16.436605, 16.397605000000002, 16.359705, 16.322505, 16.287605, 16.249404999999996, 16.208904999999998, 16.172905, 16.137405000000005, 16.098605, 16.059005, 16.019105, 15.982405000000002, 15.949304999999999, 15.910505, 15.873304999999998, 15.835005, 15.793505000000003, 15.757405000000004, 15.679105000000002, 15.636304999999995, 15.599105, 15.559955, 15.521255000000002, 15.479755, 15.479754999999997, 15.437455, 15.396904999999999, 15.357405000000002, 15.321204999999999, 15.283354999999998, 15.244954999999997, 15.204854999999998, 15.170955000000001, 15.135354999999999, 15.100155000000003, 15.065555, 15.028705, 14.993004999999998, 14.959004999999996, 14.923604999999995, 14.884609999999999, 14.844159999999999, 14.795660000000003, 14.749760000000002, 14.70236, 14.656810000000002, 14.61381, 14.56941, 14.523059999999997, 14.478361, 14.436411, 14.390311000000002, 14.348111000000001, 14.308660999999999, 14.267061000000002, 14.227161, 14.183760999999999, 14.142060999999998, 14.103061000000002, 14.064261000000002, 14.030761, 13.997011, 13.929910999999999, 13.893611, 13.856211, 13.819911000000001, 13.786411000000001, 13.749910999999999, 13.749910999999999, 13.714711, 13.681411, 13.650011, 13.618911, 13.640511000000002, 13.668111000000001, 13.695311000000002, 13.721411, 13.744511, 13.765811, 13.785110999999999, 13.806110999999996, 13.830511, 13.854711, 13.879311000000003, 13.902811000000002, 13.924010999999998, 13.946611, 13.968711, 13.988411, 14.005610999999998, 14.022811, 14.042761000000002, 14.061661, 14.081961000000002, 14.101661, 14.12186, 14.14186, 14.169660000000004, 14.19556, 14.221950000000001, 14.249450000000001, 14.274550000000001, 14.300049999999999, 14.32845, 14.387150000000002, 14.416749999999997, 14.444849999999997, 14.47135, 14.501850000000005, 14.533750000000003, 14.565450000000004, 14.56545, 14.595749999999999, 14.62675, 14.655049999999996, 14.68155, 14.71015, 14.734649999999998, 14.7618, 14.789549999999998, 14.819649999999998, 14.842350000000001, 14.868699999999999, 14.897, 14.927999999999999, 14.959299999999999, 14.990450000000001, 15.025399999999998, 15.0576, 15.090900000000001, 15.1273, 15.1603, 15.185, 15.21015, 15.232549999999998, 15.255449999999998, 15.280099999999997, 15.303799999999999, 15.326599999999996, 15.350049999999996, 15.37205, 15.393951, 15.421750999999999, 15.449950999999999, 15.480351, 15.512301000000003, 15.542401, 15.574601, 15.609400999999998, 15.644500999999998, 15.678500999999999, 15.714301, 15.747500999999998, 15.780150999999998, 15.813651, 15.845451, 15.876301000000002, 15.903150999999998, 15.928450999999997, 15.961751, 15.995250999999998, 16.031101, 16.069301, 16.107201, 16.145601, 16.182001, 16.218901, 16.257901, 16.297301, 16.338901000000003, 16.377101, 16.412401000000003, 16.446701000000004, 16.475701, 16.505801, 16.537601, 16.571001000000003, 16.606601, 16.641901, 16.675301, 16.708801, 16.739601, 16.764701, 16.788701, 16.816301, 16.848501, 16.879101000000002, 16.915701, 16.956001, 17.000401, 17.040601000000002, 17.120451, 17.161851, 17.204850999999998, 17.246851, 17.285351000000002, 17.315251, 17.315251000000004, 17.347451, 17.376051, 17.404750999999997, 17.433851, 17.462751, 17.488751, 17.515751, 17.540151, 17.564251, 17.584151000000002, 17.602351000000002, 17.615351, 17.632851000000002, 17.651051, 17.670751, 17.691151, 17.712550999999998, 17.731451, 17.754251000000004, 17.778751000000003, 17.806451, 17.834641, 17.864741000000002, 17.896441000000003, 17.925041, 17.951041, 17.951691, 17.955291, 17.964491, 17.967741, 17.970341, 17.973440999999998, 17.976141, 17.979841, 17.987491000000002, 17.996541, 18.007741, 18.015441, 18.013241, 18.007941000000002, 18.000591, 17.999191, 17.999090000000002, 17.999090000000002, 17.995590000000004, 17.99179, 17.98799, 17.981890000000003, 17.97469, 17.968690000000002, 17.960890000000003, 17.95129, 17.94459, 17.93759, 17.93249, 17.92369, 17.917489999999997, 17.906789999999997, 17.899739999999998, 17.89489, 17.89519, 17.88669, 17.87719, 17.86649, 17.85479, 17.844189999999998, 17.832489999999996, 17.821839999999998, 17.81054, 17.79624, 17.781040000000004, 17.764940000000003, 17.75454, 17.746640000000003, 17.73474, 17.72444, 17.707940000000004, 17.694540000000003, 17.679840000000002, 17.646340000000002, 17.63144, 17.61664, 17.599239999999998, 17.58144, 17.56124, 17.54234, 17.542340000000003, 17.51734, 17.49344, 17.46384, 17.429840000000002, 17.39304, 17.36034, 17.329139999999995, 17.29804, 17.26609, 17.23619, 17.21119, 17.18589, 17.16219, 17.13699, 17.11249, 17.08639, 17.060290000000002, 17.03709, 17.011390000000002, 16.988490000000002, 16.96799, 16.947690000000005, 16.92859, 16.913890000000002, 16.903090000000002, 16.88729, 16.87029, 16.85089, 16.83409, 16.814690000000002, 16.792290000000005, 16.76489, 16.737890000000004, 16.710690000000003, 16.6813, 16.6525, 16.6214, 16.5929, 16.5641, 16.55575, 16.54885, 16.53625, 16.527649999999998, 16.5226, 16.518, 16.5092, 16.5012, 16.48875, 16.4751, 16.464000000000002, 16.4501, 16.4412, 16.4322, 16.4237, 16.4201, 16.4062, 16.392500000000002, 16.3822, 16.371600000000004, 16.3616, 16.35005, 16.33935, 16.327350000000003, 16.31915, 16.313149999999997, 16.30855, 16.30425, 16.29725, 16.294150000000002, 16.289050000000003, 16.28875, 16.28914, 16.288140000000002, 16.27814, 16.26964, 16.258340000000004, 16.247940000000003, 16.233739999999997, 16.217339999999997, 16.18794, 16.17324, 16.16164, 16.15274, 16.14404, 16.12989, 16.12989, 16.11539, 16.10739, 16.102189999999997, 16.100790000000003, 16.10019, 16.098440000000004, 16.094739999999998, 16.090139999999998, 16.08934, 16.086140000000004, 16.08754, 16.094140000000003, 16.102539999999998, 16.108539999999998, 16.120140000000003, 16.130640000000003, 16.144540000000003, 16.163140000000002, 16.18334, 16.204439999999998, 16.225340000000003, 16.24204, 16.25959, 16.27534, 16.285390000000003, 16.29799, 16.30819, 16.32339, 16.33749, 16.35029, 16.36569, 16.375490000000003, 16.389290000000003, 16.40059, 16.410389999999996, 16.419089999999997, 16.430189999999996, 16.444989999999997, 16.452090000000002, 16.461889999999997, 16.468989999999998, 16.468490000000003, 16.47229, 16.47229, 16.480790000000002, 16.491690000000002, 16.49849, 16.50409, 16.51299, 16.519040000000004, 16.53114, 16.540139999999997, 16.54984, 16.565139999999996, 16.577339999999996, 16.59204, 16.60574, 16.61299, 16.61999, 16.630040000000005, 16.64134, 16.65619, 16.66974, 16.68044, 16.69474, 16.71154, 16.72864, 16.74434, 16.75464, 16.774340000000002, 16.78834, 16.79954, 16.81044, 16.81954, 16.833489999999998, 16.845689999999998, 16.856389999999998, 16.862589999999997, 16.866889999999998, 16.86789, 16.87018, 16.86928, 16.867179999999998, 16.86288, 16.858790000000003, 16.85729, 16.85729, 16.85969, 16.859090000000002, 16.86349, 16.86109, 16.86064, 16.859640000000002, 16.859240000000003, 16.860040000000005, 16.86064, 16.860840000000003, 16.86204, 16.865740000000002, 16.86779, 16.87229, 16.87449, 16.87554, 16.877740000000003, 16.87744, 16.87919, 16.88539, 16.896489999999996, 16.907590000000003, 16.92309, 16.93984, 16.954639999999998, 16.973940000000002, 16.99199, 17.00889, 17.022989999999997, 17.036389999999997, 17.048389999999998, 17.057290000000002, 17.066190000000002, 17.07509, 17.08439, 17.09279, 17.101240000000004, 17.11059, 17.12259, 17.131190999999998, 17.136491, 17.146590999999997, 17.158690999999997, 17.167991, 17.177490999999996, 17.185740999999997, 17.189040999999996, 17.190941, 17.185141, 17.178741, 17.173140999999998, 17.172141, 17.174841, 17.177841, 17.183741, 17.196941, 17.207341, 17.219141, 17.229491, 17.240691, 17.249891, 17.257091, 17.263741, 17.266941, 17.269091, 17.274791, 17.278691000000002, 17.281491, 17.282791, 17.278091000000003, 17.273291, 17.270591000000003, 17.265441, 17.261841, 17.262891000000003, 17.264291, 17.266890999999998, 17.265791, 17.260690999999998, 17.253391, 17.256791000000003, 17.261241, 17.269241, 17.280441, 17.289441, 17.289441, 17.298240999999997, 17.312790999999997, 17.327291, 17.342990999999998, 17.368191, 17.398491, 17.427190999999997, 17.457391, 17.488401000000003, 17.519301, 17.552051000000002, 17.588851, 17.623950999999998, 17.656951, 17.695051, 17.732350999999998, 17.765551000000002, 17.802400999999996, 17.839951, 17.880501, 17.917301, 17.951651, 17.989251000000003, 18.025450999999997, 18.060001, 18.087501, 18.118700999999998, 18.147101, 18.174501, 18.208351000000004, 18.240751000000003, 18.270451, 18.297351000000003, 18.324751000000003, 18.357951000000003, 18.393951, 18.424151, 18.477401, 18.495300999999998, 18.517801, 18.540100999999996, 18.565201000000002, 18.589101000000003, 18.589101000000003, 18.613601000000003, 18.640101, 18.665401, 18.688800999999998, 18.713701, 18.738401, 18.763551, 18.790451, 18.810450999999997, 18.832549999999998, 18.85445, 18.87355, 18.89165, 18.91915, 18.948150000000002, 18.9744, 19.014499999999998, 19.0538, 19.102800000000002, 19.1496, 19.1975, 19.2414, 19.2832, 19.320449999999997, 19.35975, 19.40295, 19.44715, 19.49135, 19.5366, 19.5788, 19.621600000000004, 19.661800000000003, 19.703600000000005, 19.745900000000002, 19.79147, 19.87452, 19.91372, 19.950770000000002, 19.99377, 20.04157, 20.088119999999996, 20.13262, 20.132620000000003, 20.173019999999998, 20.210019999999997, 20.245320000000003, 20.280920000000002, 20.31992, 20.36492, 20.407870000000003, 20.449320000000004, 20.48392, 20.51327, 20.54002, 20.56432, 20.59092, 20.60462, 20.610120000000002, 20.618119999999998, 20.623820000000002, 20.62282, 20.62017, 20.61802, 20.61712, 20.61222, 20.61232, 20.611069999999998, 20.60712, 20.602019999999996, 20.59772, 20.58632, 20.57582, 20.56277, 20.55422, 20.55027, 20.54482, 20.55352, 20.562820000000002, 20.56872, 20.576819999999998, 20.58457, 20.59547, 20.60757, 20.61917, 20.63127, 20.63767, 20.644069999999996, 20.654370000000004, 20.66417, 20.66667, 20.657470000000004, 20.637069999999998, 20.609269999999995, 20.578269999999996, 20.558870000000002, 20.544169999999998, 20.52402, 20.50222, 20.47312, 20.44852, 20.42652, 20.40122, 20.37972, 20.355769999999996, 20.332770000000004, 20.30877, 20.281819999999996, 20.25882, 20.24242, 20.227720000000005, 20.20872, 20.192619999999998, 20.17472, 20.146720000000002, 20.11752, 20.090120000000002, 20.050120000000003, 20.01402, 19.978219999999997, 19.94277, 19.910719999999998, 19.87952, 19.817020000000003, 19.78917, 19.75627, 19.72267, 19.686970000000002, 19.648370000000003, 19.648370000000003, 19.61542, 19.58442, 19.555719999999997, 19.526619999999998, 19.49787, 19.47315, 19.448600000000003, 19.4261, 19.4011, 19.381849999999996, 19.35615, 19.330650000000002, 19.3059, 19.2855, 19.266199999999998, 19.249000000000002, 19.236299999999996, 19.2178, 19.1992, 19.1773, 19.150850000000002, 19.1249, 19.103499999999997, 19.087699999999998, 19.07915, 19.066650000000003, 19.053449999999998, 19.053149999999995, 19.0518, 19.0489, 19.048000000000002, 19.0477, 19.04945, 19.048000000000002, 19.042900000000003, 19.0423, 19.0399, 19.03385, 19.031950000000002, 19.023750000000003, 19.016150000000003, 19.010450000000002, 19.0121, 19.012100000000004, 19.0071, 18.9949, 18.9848, 18.964900000000004, 18.9479, 18.9311, 18.91025, 18.89075, 18.86925, 18.842950000000002, 18.81505, 18.786449999999995, 18.754299999999997, 18.7213, 18.68615, 18.65125, 18.62215, 18.593149999999998, 18.57695, 18.56545, 18.551799999999997, 18.5339, 18.5171, 18.5057, 18.4906, 18.4863, 18.4744, 18.4609, 18.456899999999997, 18.447200000000002, 18.43575, 18.42435, 18.41685, 18.41135, 18.398699999999998, 18.360599999999998, 18.3438, 18.331599999999998, 18.33055, 18.33085, 18.3303, 18.330199999999998, 18.330199999999998, 18.335299999999997, 18.3338, 18.3316, 18.33305, 18.3322, 18.3273, 18.327450000000002, 18.323999999999998, 18.323050000000002, 18.324250000000003, 18.328000000000003, 18.329649999999997, 18.334749999999996, 18.3329, 18.333, 18.333099999999998, 18.3275, 18.323150000000002, 18.31665, 18.31035, 18.30485, 18.301249999999996, 18.293349999999997, 18.29125, 18.28735, 18.284799999999997, 18.283899999999996, 18.283999999999995, 18.283, 18.2782, 18.276850000000003, 18.2797, 18.281399999999998, 18.289599999999997, 18.301399999999997, 18.3129, 18.323199999999996, 18.327799999999996, 18.337, 18.347399999999997, 18.359099999999998, 18.372749999999996, 18.385550000000002, 18.39845, 18.40945, 18.41965, 18.435550000000003, 18.45015, 18.46115, 18.471950000000003, 18.481050000000003, 18.4823, 18.4818, 18.489099999999997, 18.4981, 18.5108, 18.5172, 18.528499999999998, 18.542050000000003, 18.554250000000003, 18.56215, 18.56775, 18.571649999999998, 18.58165, 18.58735, 18.59755, 18.61235, 18.628800000000002, 18.6479, 18.67085, 18.694950000000002, 18.7207, 18.7462, 18.7717, 18.8018, 18.828199999999995, 18.858400000000003, 18.88755, 18.91315, 18.94775, 18.9665, 18.9838, 19.005049999999997, 19.02335, 19.035749999999997, 19.03575, 19.053149999999995, 19.068549999999995, 19.076849999999997, 19.08845, 19.10085, 19.11855, 19.14195, 19.16335, 19.18635, 19.20085, 19.2125, 19.223599999999998, 19.24125, 19.25795, 19.274550000000005, 19.29635, 19.31625, 19.3337, 19.350900000000003, 19.3701, 19.3864, 19.40375, 19.41675, 19.428050000000002, 19.4349, 19.4438, 19.451149999999995, 19.45985, 19.46805, 19.4841, 19.5093, 19.531399999999998, 19.551, 19.5689, 19.5888, 19.609599999999997, 19.631800000000002, 19.674, 19.69335, 19.711949999999998, 19.73325, 19.75205, 19.77265, 19.77265, 19.7945, 19.813750000000002, 19.833849999999998, 19.85345, 19.87135, 19.88675, 19.902249999999995, 19.92025, 19.9381, 19.9501, 19.9565, 19.964249999999996, 19.96925, 19.97155, 19.97495, 19.975949999999997, 19.97065, 19.967749999999995, 19.969749999999998, 19.971749999999997, 19.97635, 19.988999999999997, 20.002999999999997, 20.017899999999997, 20.032249999999998, 20.04695, 20.06165, 20.07235, 20.08365, 20.095399999999998, 20.1091, 20.121299999999998, 20.1315, 20.13965, 20.148950000000003, 20.162, 20.167050000000003, 20.17245, 20.177950000000003, 20.183400000000002, 20.191900000000004, 20.20115, 20.20115, 20.20325, 20.2061, 20.207800000000002, 20.212, 20.215799999999998, 20.2241, 20.2325, 20.2403, 20.25585, 20.26995, 20.2788, 20.288899999999998, 20.3023, 20.3144, 20.334, 20.3585, 20.377599999999997, 20.3944, 20.41315, 20.43225, 20.4554, 20.4784, 20.500400000000003, 20.516400000000004, 20.5308, 20.538400000000003, 20.5444, 20.552999999999997, 20.568050000000003, 20.58825, 20.6087, 20.6269, 20.6431, 20.658399999999997, 20.6717, 20.681099999999997, 20.68975, 20.70405, 20.7211, 20.738249999999997, 20.75985, 20.779125, 20.792575000000003, 20.799974999999996, 20.809874999999998, 20.817574999999998, 20.827774999999995, 20.841074999999996, 20.854424999999996, 20.869824999999995, 20.886975000000003, 20.898975, 20.911424999999998, 20.920725, 20.928325, 20.934825, 20.938825, 20.935325, 20.928825000000003, 20.921425, 20.909825, 20.895425, 20.877125, 20.856925, 20.835325, 20.813364999999997, 20.795665, 20.785265, 20.775365, 20.767865, 20.763165, 20.758965, 20.759465, 20.756964999999994, 20.754064999999997, 20.748965, 20.741765000000004, 20.734465, 20.729915000000002, 20.720615000000002, 20.715065, 20.708565, 20.703665, 20.696865000000003, 20.692965, 20.692965000000004, 20.688115000000003, 20.683564999999998, 20.679764999999996, 20.679765000000003, 20.677364999999995, 20.674865, 20.670415000000002, 20.667765000000003, 20.672765000000005, 20.674465, 20.676565, 20.680615000000003, 20.679415, 20.676765, 20.680115, 20.682565000000004, 20.685665000000004, 20.685265, 20.689365000000002, 20.692854999999998, 20.700155, 20.707305, 20.701504999999997, 20.694404999999996, 20.692404999999997, 20.688805, 20.678104999999995, 20.666605, 20.656005, 20.644605000000002, 20.631904999999996, 20.621955, 20.608205, 20.593305, 20.578454999999998, 20.561305, 20.550155, 20.530555, 20.523105, 20.516904999999998, 20.507754999999996, 20.492804999999997, 20.464105000000004, 20.464105000000004, 20.436605, 20.410405000000004, 20.387904999999996, 20.364904999999997, 20.346105, 20.327305000000003, 20.307655, 20.285555000000002, 20.260055, 20.232405, 20.203104999999997, 20.174729999999997, 20.145629999999997, 20.11653, 20.09083, 20.06663, 20.037655, 20.005155, 19.972305000000002, 19.941105000000004, 19.908405000000002, 19.880105, 19.849105, 19.819205, 19.786905, 19.756805, 19.730955000000005, 19.714255, 19.696855, 19.679555000000004, 19.666354999999996, 19.656654999999997, 19.650354999999998, 19.642855, 19.634755, 19.625265, 19.616865, 19.606865000000003, 19.597165, 19.589365, 19.581864999999997, 19.564965, 19.564965, 19.552315, 19.543464999999998, 19.533665, 19.525764999999996, 19.520364999999998, 19.511615000000003, 19.503665, 19.493615000000002, 19.483815000000003, 19.473715000000006, 19.463265000000003, 19.457665, 19.450265, 19.445865, 19.441115000000003, 19.439115000000005, 19.436564999999998, 19.435765, 19.436265000000002, 19.434465000000003, 19.434515, 19.428915000000003, 19.420815, 19.411715, 19.400315, 19.387316000000002, 19.375416, 19.365466000000005, 19.356416, 19.347816, 19.337666000000002, 19.324866, 19.311076, 19.296326, 19.282476000000003, 19.274376000000004, 19.267276, 19.260876, 19.255776, 19.251576000000004, 19.246476, 19.239076, 19.231726, 19.223325999999997, 19.213776, 19.204026000000002, 19.195226, 19.187676, 19.181176, 19.169726, 19.157425999999997, 19.147525999999996, 19.138175999999998, 19.129176, 19.118576000000004, 19.106976000000003, 19.104476000000002, 19.097876, 19.094076, 19.089275999999998, 19.086276, 19.083876000000004, 19.083176, 19.084476000000006, 19.086976000000003, 19.090575999999995, 19.096726, 19.104025999999998, 19.111126, 19.117326, 19.126825999999998, 19.131625999999997, 19.135626000000002, 19.142951000000004, 19.149851, 19.162701000000002, 19.170401000000005, 19.178651000000002, 19.184901000000004, 19.206451, 19.214151, 19.222751000000002, 19.229900999999998, 19.236100999999998, 19.238701, 19.238701, 19.243201000000003, 19.244001, 19.250601, 19.261801000000002, 19.274701000000004, 19.282101, 19.286951000000002, 19.292951000000002, 19.301951, 19.308951, 19.317251000000002, 19.323851, 19.327751, 19.337350999999998, 19.347301, 19.357651000000004, 19.367551, 19.376351, 19.383251, 19.390951, 19.396200999999998, 19.400251000000004, 19.405951000000005, 19.408151000000004, 19.412601, 19.416601, 19.418401000000003, 19.417101000000002, 19.416901000000003, 19.415501000000003, 19.414951000000002, 19.412351, 19.407650999999998, 19.403451, 19.396451, 19.387351, 19.384250999999995, 19.379500999999998, 19.38115, 19.38675, 19.40095, 19.4235, 19.44005, 19.44005, 19.462249999999997, 19.485349999999997, 19.507749999999998, 19.523601, 19.536201000000002, 19.548601, 19.561001, 19.576200999999998, 19.592751, 19.608601, 19.627301, 19.651201, 19.672651, 19.693651, 19.716151000000004, 19.736051, 19.749951, 19.753301, 19.760151, 19.765651000000002, 19.764850999999997, 19.767250999999998, 19.774451, 19.782451000000002, 19.796301, 19.808501, 19.820901000000003, 19.836001, 19.846901, 19.857301, 19.867901000000003, 19.876851000000002, 19.882751000000003, 19.886751, 19.886701000000002, 19.894001, 19.897101000000003, 19.899401, 19.898551, 19.902301, 19.906151, 19.912900999999998, 19.912900999999998, 19.918251, 19.919750999999998, 19.917301000000002, 19.911601, 19.908101, 19.904550999999998, 19.895650999999997, 19.883051, 19.878000999999998, 19.868201, 19.860701, 19.852201, 19.846401, 19.838901, 19.838300999999998, 19.831401, 19.817101, 19.802901, 19.797851, 19.790501000000003, 19.784401, 19.775501000000002, 19.770251, 19.767451000000005, 19.764151000000005, 19.762851, 19.757151, 19.753701, 19.747001, 19.741551, 19.740601, 19.736901, 19.737250999999997, 19.740851, 19.739401, 19.738201, 19.740301, 19.742800999999996, 19.745101, 19.750701, 19.757701, 19.761801000000002, 19.763601, 19.767901, 19.773301, 19.781800999999998, 19.791901, 19.804601, 19.818601, 19.829101, 19.839551, 19.848551, 19.854801, 19.859601, 19.852901000000003, 19.838651, 19.831801, 19.825700999999995, 19.823351, 19.820051, 19.822049999999997, 19.82885, 19.837849999999996, 19.84505, 19.84625, 19.845900000000004, 19.84815, 19.85025, 19.85045, 19.85295, 19.85835, 19.85835, 19.857699999999998, 19.857999999999997, 19.865499999999997, 19.870899999999995, 19.8803, 19.893199999999997, 19.9028, 19.913, 19.9145, 19.915499999999998, 19.9169, 19.9181, 19.9199, 19.9199, 19.92295, 19.922050000000002, 19.9218, 19.924799999999998, 19.9288, 19.934450000000002, 19.939650000000004, 19.944750000000003, 19.94835, 19.949949999999998, 19.9548, 19.954750000000004, 19.9592, 19.94935, 19.93685, 19.92845, 19.91925, 19.91205, 19.902849999999997, 19.893599999999996, 19.888199999999998, 19.887300000000003, 19.88385, 19.884249999999998, 19.881600000000002, 19.878200000000003, 19.878649999999997, 19.88065, 19.87795, 19.875650000000004, 19.87735, 19.879150000000003, 19.8771, 19.877299999999998, 19.875899999999998, 19.8741, 19.87225, 19.864650000000005, 19.862150000000003, 19.86425, 19.8604, 19.8576, 19.85125, 19.85125, 19.842, 19.834600000000002, 19.824350000000003, 19.81555, 19.8108, 19.804399999999998, 19.7991, 19.7907, 19.784200000000002, 19.775199999999998, 19.76375, 19.7558, 19.7443, 19.725749999999998, 19.705749999999995, 19.68635, 19.66315, 19.63835, 19.61555, 19.594849999999997, 19.57815, 19.56255, 19.54895, 19.53135, 19.512600000000003, 19.491249999999997, 19.46425, 19.4373, 19.404049999999998, 19.372049999999998, 19.342650000000003, 19.31395, 19.283450000000002, 19.252950000000002, 19.230750000000004, 19.185950000000002, 19.16565, 19.141949999999998, 19.11685, 19.094250000000002, 19.075, 19.05635, 19.05635, 19.04215, 19.02615, 19.00895, 18.98895, 18.972450000000002, 18.9546, 18.9385, 18.924, 18.906299999999998, 18.890399999999996, 18.8736, 18.8536, 18.8357, 18.813149999999997, 18.794449999999998, 18.77535, 18.75785, 18.742550000000005, 18.7221, 18.6966, 18.669, 18.6413, 18.6104, 18.577, 18.541, 18.505300000000002, 18.4821, 18.46165, 18.440600000000003, 18.4206, 18.4001, 18.382699999999996, 18.36545, 18.34805, 18.33225, 18.317750000000004, 18.30375, 18.2891, 18.2772, 18.265050000000002, 18.25535, 18.24965, 18.242250000000002, 18.23475, 18.2276, 18.2216, 18.214000000000002, 18.2067, 18.2027, 18.195600000000002, 18.1914, 18.1841, 18.177599999999998, 18.1707, 18.1664, 18.15915, 18.152549999999998, 18.14595, 18.13955, 18.129849999999998, 18.11755, 18.10825, 18.099149999999998, 18.08475, 18.073649999999997, 18.06005, 18.047250000000002, 18.036199999999997, 18.02405, 18.02025, 18.020799999999998, 18.0213, 18.021, 18.020400000000002, 18.02295, 18.02535, 18.026850000000003, 18.026950000000003, 18.02465, 18.025050000000004, 18.0256, 18.026300000000003, 18.032899999999998, 18.036099999999998, 18.041999999999998, 18.041999999999998, 18.0519, 18.05895, 18.06475, 18.06875, 18.0761, 18.0809, 18.0868, 18.0921, 18.096400000000003, 18.1029, 18.111, 18.1158, 18.118750000000002, 18.12235, 18.125149999999998, 18.13045, 18.137300000000003, 18.145900000000005, 18.154000000000003, 18.162750000000003, 18.17075, 18.17415, 18.184250000000002, 18.191150000000004, 18.198150000000002, 18.207750000000004, 18.215700000000002, 18.227099999999997, 18.23645, 18.24475, 18.251549999999998, 18.25665, 18.268900000000002, 18.281200000000002, 18.2979, 18.3174, 18.3432, 18.394999999999996, 18.418999999999997, 18.4429, 18.46735, 18.4902, 18.512099999999997, 18.512099999999997, 18.536999999999995, 18.559299999999997, 18.583, 18.6062, 18.6367, 18.661099999999998, 18.6855, 18.7121, 18.7352, 18.756600000000002, 18.7788, 18.796799999999998, 18.816699999999994, 18.836499999999997, 18.85045, 18.86535, 18.882450000000002, 18.90205, 18.91835, 18.93525, 18.946649999999998, 18.95935, 18.973049999999997, 18.98685, 19.00535, 19.0282, 19.0531, 19.0767, 19.1035, 19.1314, 19.1576, 19.18405, 19.2131, 19.2421, 19.26775, 19.32245, 19.34845, 19.376350000000002, 19.40295, 19.42915, 19.45915, 19.489050000000002, 19.48905, 19.520950000000003, 19.551600000000004, 19.581000000000003, 19.617400000000004, 19.644000000000002, 19.675900000000002, 19.706100000000003, 19.74035, 19.77555, 19.80845, 19.83935, 19.86435, 19.89105, 19.91635, 19.9444, 19.9721, 20.005100000000002, 20.03755, 20.062250000000002, 20.088350000000002, 20.116650000000003, 20.14225, 20.16875, 20.19615, 20.22175, 20.252750000000002, 20.28745, 20.324249999999996, 20.364949999999997, 20.4041, 20.448400000000003, 20.4932, 20.5327, 20.565899999999996, 20.6026, 20.633699999999997, 20.666700000000002, 20.697500000000005, 20.7283, 20.75465, 20.78505, 20.817249999999998, 20.855749999999997, 20.895449999999997, 20.93565, 20.96935, 21.007649999999998, 21.047450000000005, 21.08625, 21.121849999999995, 21.16245, 21.205799999999996, 21.249000000000002, 21.29185, 21.33675, 21.382849999999998, 21.429050000000004, 21.475749999999998, 21.5261, 21.5746, 21.62595, 21.669849999999997, 21.719550000000005, 21.770149999999997, 21.81875, 21.870649999999998, 21.92254999999999, 21.98605, 22.051450000000003, 22.115750000000002, 22.18985, 22.272750000000006, 22.35435, 22.429050000000004, 22.494500000000002, 22.559, 22.624499999999994, 22.69645, 22.770349999999997, 22.914949999999997, 22.983649999999997, 23.0506, 23.120700000000003, 23.1908, 23.2573, 23.257299999999997, 23.320700000000002, 23.385700000000007, 23.448150000000002, 23.508100000000002, 23.571100000000005, 23.63575, 23.699850000000005, 23.764, 23.8258, 23.8897, 23.953200000000002, 24.016000000000005, 24.0739, 24.131400000000003, 24.194099999999995, 24.258499999999998, 24.3189, 24.3713, 24.4337, 24.4936, 24.547400000000007, 24.599149999999998, 24.645150000000005, 24.69315, 24.737550000000002, 24.789849999999998, 24.842949999999995, 24.898650000000004, 24.947400000000005, 24.998450000000002, 25.047549999999998, 25.099149999999998, 25.160999999999994, 25.22185, 25.276549999999997, 25.33275, 25.388999999999992, 25.491100000000007, 25.543400000000002, 25.592699999999994, 25.64015, 25.676149999999993, 25.71485, 25.714850000000002]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8a494b0f-5a27-40a3-a6fb-f40d61011bba');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"114b9c65-60a7-4d7c-8d93-30b96fa0f624\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"114b9c65-60a7-4d7c-8d93-30b96fa0f624\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '114b9c65-60a7-4d7c-8d93-30b96fa0f624',\n",
              "                        [{\"type\": \"scatter\", \"x\": [20110225, 20110228, 20110301, 20110302, 20110303, 20110304, 20110307, 20110308, 20110309, 20110310, 20110311, 20110314, 20110315, 20110316, 20110317, 20110318, 20110321, 20110322, 20110323, 20110324, 20110325, 20110328, 20110329, 20110330, 20110331, 20110401, 20110404, 20110405, 20110406, 20110407, 20110408, 20110411, 20110412, 20110413, 20110414, 20110415, 20110418, 20110419, 20110420, 20110421, 20110425, 20110426, 20110427, 20110428, 20110429, 20110502, 20110503, 20110504, 20110505, 20110506, 20110509, 20110510, 20110511, 20110512, 20110513, 20110516, 20110517, 20110518, 20110519, 20110520, 20110523, 20110524, 20110525, 20110526, 20110527, 20110531, 20110601, 20110602, 20110603, 20110606, 20110607, 20110608, 20110609, 20110610, 20110613, 20110614, 20110615, 20110616, 20110617, 20110620, 20110621, 20110622, 20110623, 20110624, 20110627, 20110628, 20110629, 20110630, 20110701, 20110705, 20110706, 20110707, 20110708, 20110711, 20110712, 20110713, 20110714, 20110715, 20110718, 20110719, 20110720, 20110721, 20110722, 20110725, 20110726, 20110727, 20110728, 20110729, 20110801, 20110802, 20110803, 20110804, 20110805, 20110808, 20110809, 20110810, 20110811, 20110812, 20110815, 20110816, 20110817, 20110818, 20110819, 20110822, 20110823, 20110824, 20110825, 20110826, 20110829, 20110830, 20110831, 20110901, 20110902, 20110906, 20110907, 20110908, 20110909, 20110912, 20110913, 20110914, 20110915, 20110916, 20110919, 20110920, 20110921, 20110922, 20110923, 20110926, 20110927, 20110928, 20110929, 20110930, 20111003, 20111004, 20111005, 20111006, 20111007, 20111010, 20111011, 20111012, 20111013, 20111014, 20111017, 20111018, 20111019, 20111020, 20111021, 20111024, 20111025, 20111026, 20111027, 20111028, 20111031, 20111101, 20111102, 20111103, 20111104, 20111107, 20111108, 20111109, 20111110, 20111111, 20111114, 20111115, 20111116, 20111117, 20111118, 20111121, 20111122, 20111123, 20111125, 20111128, 20111129, 20111130, 20111201, 20111202, 20111205, 20111206, 20111207, 20111208, 20111209, 20111212, 20111213, 20111214, 20111215, 20111216, 20111219, 20111220, 20111221, 20111222, 20111223, 20111227, 20111228, 20111229, 20111230, 20120103, 20120104, 20120105, 20120106, 20120109, 20120110, 20120111, 20120112, 20120113, 20120117, 20120118, 20120119, 20120120, 20120123, 20120124, 20120125, 20120126, 20120127, 20120130, 20120131, 20120201, 20120202, 20120203, 20120206, 20120207, 20120208, 20120209, 20120210, 20120213, 20120214, 20120215, 20120216, 20120217, 20120221, 20120222, 20120223, 20120224, 20120227, 20120228, 20120229, 20120301, 20120302, 20120305, 20120306, 20120307, 20120308, 20120309, 20120312, 20120313, 20120314, 20120315, 20120316, 20120319, 20120320, 20120321, 20120322, 20120323, 20120326, 20120327, 20120328, 20120329, 20120330, 20120402, 20120403, 20120404, 20120405, 20120409, 20120410, 20120411, 20120412, 20120413, 20120416, 20120417, 20120418, 20120419, 20120420, 20120423, 20120424, 20120425, 20120426, 20120427, 20120430, 20120501, 20120502, 20120503, 20120504, 20120507, 20120508, 20120509, 20120510, 20120511, 20120514, 20120515, 20120516, 20120517, 20120518, 20120521, 20120522, 20120523, 20120524, 20120525, 20120529, 20120530, 20120531, 20120601, 20120604, 20120605, 20120606, 20120607, 20120608, 20120611, 20120612, 20120613, 20120614, 20120615, 20120618, 20120619, 20120620, 20120621, 20120622, 20120625, 20120626, 20120627, 20120628, 20120629, 20120702, 20120703, 20120705, 20120706, 20120709, 20120710, 20120711, 20120712, 20120713, 20120716, 20120717, 20120718, 20120719, 20120720, 20120723, 20120724, 20120725, 20120726, 20120727, 20120730, 20120731, 20120801, 20120802, 20120803, 20120806, 20120807, 20120808, 20120809, 20120810, 20120813, 20120814, 20120815, 20120816, 20120817, 20120820, 20120821, 20120822, 20120823, 20120824, 20120827, 20120828, 20120829, 20120830, 20120831, 20120904, 20120905, 20120906, 20120907, 20120910, 20120911, 20120912, 20120913, 20120914, 20120917, 20120918, 20120919, 20120920, 20120921, 20120924, 20120925, 20120926, 20120927, 20120928, 20121001, 20121002, 20121003, 20121004, 20121005, 20121008, 20121009, 20121010, 20121011, 20121012, 20121015, 20121016, 20121017, 20121018, 20121019, 20121022, 20121023, 20121024, 20121025, 20121026, 20121031, 20121101, 20121102, 20121105, 20121106, 20121107, 20121108, 20121109, 20121112, 20121113, 20121114, 20121115, 20121116, 20121119, 20121120, 20121121, 20121123, 20121126, 20121127, 20121128, 20121129, 20121130, 20121203, 20121204, 20121205, 20121206, 20121207, 20121210, 20121211, 20121212, 20121213, 20121214, 20121217, 20121218, 20121219, 20121220, 20121221, 20121224, 20121226, 20121227, 20121228, 20121231, 20130102, 20130103, 20130104, 20130107, 20130108, 20130109, 20130110, 20130111, 20130114, 20130115, 20130116, 20130117, 20130118, 20130122, 20130123, 20130124, 20130125, 20130128, 20130129, 20130130, 20130131, 20130201, 20130204, 20130205, 20130206, 20130207, 20130208, 20130211, 20130212, 20130213, 20130214, 20130215, 20130219, 20130220, 20130221, 20130222, 20130225, 20130226, 20130227, 20130228, 20130301, 20130304, 20130305, 20130306, 20130307, 20130308, 20130311, 20130312, 20130313, 20130314, 20130315, 20130318, 20130319, 20130320, 20130321, 20130322, 20130325, 20130326, 20130327, 20130328, 20130401, 20130402, 20130403, 20130404, 20130405, 20130408, 20130409, 20130410, 20130411, 20130412, 20130415, 20130416, 20130417, 20130418, 20130419, 20130422, 20130423, 20130424, 20130425, 20130426, 20130429, 20130430, 20130501, 20130502, 20130503, 20130506, 20130507, 20130508, 20130509, 20130510, 20130513, 20130514, 20130515, 20130516, 20130517, 20130520, 20130521, 20130522, 20130523, 20130524, 20130528, 20130529, 20130530, 20130531, 20130603, 20130604, 20130605, 20130606, 20130607, 20130610, 20130611, 20130612, 20130613, 20130614, 20130617, 20130618, 20130619, 20130620, 20130621, 20130624, 20130625, 20130626, 20130627, 20130628, 20130701, 20130702, 20130703, 20130705, 20130708, 20130709, 20130710, 20130711, 20130712, 20130715, 20130716, 20130717, 20130718, 20130719, 20130722, 20130723, 20130724, 20130725, 20130726, 20130729, 20130730, 20130731, 20130801, 20130802, 20130805, 20130806, 20130807, 20130808, 20130809, 20130812, 20130813, 20130814, 20130815, 20130816, 20130819, 20130820, 20130821, 20130822, 20130823, 20130826, 20130827, 20130828, 20130829, 20130830, 20130903, 20130904, 20130905, 20130906, 20130909, 20130910, 20130911, 20130912, 20130913, 20130916, 20130917, 20130918, 20130919, 20130920, 20130923, 20130924, 20130925, 20130926, 20130927, 20130930, 20131001, 20131002, 20131003, 20131004, 20131007, 20131008, 20131009, 20131010, 20131011, 20131014, 20131015, 20131016, 20131017, 20131018, 20131021, 20131022, 20131023, 20131024, 20131025, 20131028, 20131029, 20131030, 20131031, 20131101, 20131104, 20131105, 20131106, 20131107, 20131108, 20131111, 20131112, 20131113, 20131114, 20131115, 20131118, 20131119, 20131120, 20131121, 20131122, 20131125, 20131126, 20131127, 20131129, 20131202, 20131203, 20131204, 20131205, 20131206, 20131209, 20131210, 20131211, 20131212, 20131213, 20131216, 20131217, 20131218, 20131219, 20131220, 20131223, 20131224, 20131226, 20131227, 20131230, 20131231, 20140102, 20140103, 20140106, 20140107, 20140108, 20140109, 20140110, 20140113, 20140114, 20140115, 20140116, 20140117, 20140121, 20140122, 20140123, 20140124, 20140127, 20140128, 20140129, 20140130, 20140131, 20140203, 20140204, 20140205, 20140206, 20140207, 20140210, 20140211, 20140212, 20140213, 20140214, 20140218, 20140219, 20140220, 20140221, 20140224, 20140225, 20140226, 20140227, 20140228, 20140303, 20140304, 20140305, 20140306, 20140307, 20140310, 20140311, 20140312, 20140313, 20140314, 20140317, 20140318, 20140319, 20140320, 20140321, 20140324, 20140325, 20140326, 20140327, 20140328, 20140331, 20140401, 20140402, 20140403, 20140404, 20140407, 20140408, 20140409, 20140410, 20140411, 20140414, 20140415, 20140416, 20140417, 20140421, 20140422, 20140423, 20140424, 20140425, 20140428, 20140429, 20140430, 20140501, 20140502, 20140505, 20140506, 20140507, 20140508, 20140509, 20140512, 20140513, 20140514, 20140515, 20140516, 20140519, 20140520, 20140521, 20140522, 20140523, 20140527, 20140528, 20140529, 20140530, 20140602, 20140603, 20140604, 20140605, 20140606, 20140609, 20140610, 20140611, 20140612, 20140613, 20140616, 20140617, 20140618, 20140619, 20140620, 20140623, 20140624, 20140625, 20140626, 20140627, 20140630, 20140701, 20140702, 20140703, 20140707, 20140708, 20140709, 20140710, 20140711, 20140714, 20140715, 20140716, 20140717, 20140718, 20140721, 20140722, 20140723, 20140724, 20140725, 20140728, 20140729, 20140730, 20140731, 20140801, 20140804, 20140805, 20140806, 20140807, 20140808, 20140811, 20140812, 20140813, 20140814, 20140815, 20140818, 20140819, 20140820, 20140821, 20140822, 20140825, 20140826, 20140827, 20140828, 20140829, 20140902, 20140903, 20140904, 20140905, 20140908, 20140909, 20140910, 20140911, 20140912, 20140915, 20140916, 20140917, 20140918, 20140919, 20140922, 20140923, 20140924, 20140925, 20140926, 20140929, 20140930, 20141001, 20141002, 20141003, 20141006, 20141007, 20141008, 20141009, 20141010, 20141013, 20141014, 20141015, 20141016, 20141017, 20141020, 20141021, 20141022, 20141023, 20141024, 20141027, 20141028, 20141029, 20141030, 20141031, 20141103, 20141104, 20141105, 20141106, 20141107, 20141110, 20141111, 20141112, 20141113, 20141114, 20141117, 20141118, 20141119, 20141120, 20141121, 20141124, 20141125, 20141126, 20141128, 20141201, 20141202, 20141203, 20141204, 20141205, 20141208, 20141209, 20141210, 20141211, 20141212, 20141215, 20141216, 20141217, 20141218, 20141219, 20141222, 20141223, 20141224, 20141226, 20141229, 20141230, 20141231, 20150102, 20150105, 20150106, 20150107, 20150108, 20150109, 20150112, 20150113, 20150114, 20150115, 20150116, 20150120, 20150121, 20150122, 20150123, 20150126, 20150127, 20150128, 20150129, 20150130, 20150202, 20150203, 20150204, 20150205, 20150206, 20150209, 20150210, 20150211, 20150212, 20150213, 20150217, 20150218, 20150219, 20150220, 20150223, 20150224, 20150225, 20150226, 20150227, 20150302, 20150303, 20150304, 20150305, 20150306, 20150309, 20150310, 20150311, 20150312, 20150313, 20150316, 20150317, 20150318, 20150319, 20150320, 20150323, 20150324, 20150325, 20150326, 20150327, 20150330, 20150331, 20150401, 20150402, 20150406, 20150407, 20150408, 20150409, 20150410, 20150413, 20150414, 20150415, 20150416, 20150417, 20150420, 20150421, 20150422, 20150423, 20150424, 20150427, 20150428, 20150429, 20150430, 20150501, 20150504, 20150505, 20150506, 20150507, 20150508, 20150511, 20150512, 20150513, 20150514, 20150515, 20150518, 20150519, 20150520, 20150521, 20150522, 20150526, 20150527, 20150528, 20150529, 20150601, 20150602, 20150603, 20150604, 20150605, 20150608, 20150609, 20150610, 20150611, 20150612, 20150615, 20150616, 20150617, 20150618, 20150619, 20150622, 20150623, 20150624, 20150625, 20150626, 20150629, 20150630, 20150701, 20150702, 20150706, 20150707, 20150708, 20150709, 20150710, 20150713, 20150714, 20150715, 20150716, 20150717, 20150720, 20150721, 20150722, 20150723, 20150724, 20150727, 20150728, 20150729, 20150730, 20150731, 20150803, 20150804, 20150805, 20150806, 20150807, 20150810, 20150811, 20150812, 20150813, 20150814, 20150817, 20150818, 20150819, 20150820, 20150821, 20150824, 20150825, 20150826, 20150827, 20150828, 20150831, 20150901, 20150902, 20150903, 20150904, 20150908, 20150909, 20150910, 20150911, 20150914, 20150915, 20150916, 20150917, 20150918, 20150921, 20150922, 20150923, 20150924, 20150925, 20150928, 20150929, 20150930, 20151001, 20151002, 20151005, 20151006, 20151007, 20151008, 20151009, 20151012, 20151013, 20151014, 20151015, 20151016, 20151019, 20151020, 20151021, 20151022, 20151023, 20151026, 20151027, 20151028, 20151029, 20151030, 20151102, 20151103, 20151104, 20151105, 20151106, 20151109, 20151110, 20151111, 20151112, 20151113, 20151116, 20151117, 20151118, 20151119, 20151120, 20151123, 20151124, 20151125, 20151127, 20151130, 20151201, 20151202, 20151203, 20151204, 20151207, 20151208, 20151209, 20151210, 20151211, 20151214, 20151215, 20151216, 20151217, 20151218, 20151221, 20151222, 20151223, 20151224, 20151228, 20151229, 20151230, 20151231, 20160104, 20160105, 20160106, 20160107, 20160108, 20160111, 20160112, 20160113, 20160114, 20160115, 20160119, 20160120, 20160121, 20160122, 20160125, 20160126, 20160127, 20160128, 20160129, 20160201, 20160202, 20160203, 20160204, 20160205, 20160208, 20160209, 20160210, 20160211, 20160212, 20160216, 20160217, 20160218, 20160219, 20160222, 20160223, 20160224, 20160225, 20160226, 20160229, 20160301, 20160302, 20160303, 20160304, 20160307, 20160308, 20160309, 20160310, 20160311, 20160314, 20160315, 20160316, 20160317, 20160318, 20160321, 20160322, 20160323, 20160324, 20160328, 20160329, 20160330, 20160331, 20160401, 20160404, 20160405, 20160406, 20160407, 20160408, 20160411, 20160412, 20160413, 20160414, 20160415, 20160418, 20160419, 20160420, 20160421, 20160422, 20160425, 20160426, 20160427, 20160428, 20160429, 20160502, 20160503, 20160504, 20160505, 20160506, 20160509, 20160510, 20160511, 20160512, 20160513, 20160516, 20160517, 20160518, 20160519, 20160520, 20160523, 20160524, 20160525, 20160526, 20160527, 20160531, 20160601, 20160602, 20160603, 20160606, 20160607, 20160608, 20160609, 20160610, 20160613, 20160614, 20160615, 20160616, 20160617, 20160620, 20160621, 20160622, 20160623, 20160624, 20160627, 20160628, 20160629, 20160630, 20160701, 20160705, 20160706, 20160707, 20160708, 20160711, 20160712, 20160713, 20160714, 20160715, 20160718, 20160719, 20160720, 20160721, 20160722, 20160725, 20160726, 20160727, 20160728, 20160729, 20160801, 20160802, 20160803, 20160804, 20160805, 20160808, 20160809, 20160810, 20160811, 20160812, 20160815, 20160816, 20160817, 20160818, 20160819, 20160822, 20160823, 20160824, 20160825, 20160826, 20160829, 20160830, 20160831, 20160901, 20160902, 20160906, 20160907, 20160908, 20160909, 20160912, 20160913, 20160914, 20160915, 20160916, 20160919, 20160920, 20160921, 20160922, 20160923, 20160926, 20160927, 20160928, 20160929, 20160930, 20161003, 20161004, 20161005, 20161006, 20161007, 20161010, 20161011, 20161012, 20161013, 20161014, 20161017, 20161018, 20161019, 20161020, 20161021, 20161024, 20161025, 20161026, 20161027, 20161028, 20161031, 20161101, 20161102, 20161103, 20161104, 20161107, 20161108, 20161109, 20161110, 20161111, 20161114, 20161115, 20161116, 20161117, 20161118, 20161121, 20161122, 20161123, 20161125, 20161128, 20161129, 20161130, 20161201, 20161202, 20161205, 20161206, 20161207, 20161208, 20161209, 20161212, 20161213, 20161214, 20161215, 20161216, 20161219, 20161220, 20161221, 20161222, 20161223, 20161227, 20161228, 20161229, 20161230, 20170103, 20170104, 20170105, 20170106, 20170109, 20170110, 20170111, 20170112, 20170113, 20170117, 20170118, 20170119, 20170120, 20170123, 20170124, 20170125, 20170126, 20170127, 20170130, 20170131, 20170201, 20170202, 20170203, 20170206, 20170207, 20170208, 20170209, 20170210, 20170213, 20170214, 20170215, 20170216, 20170217, 20170221, 20170222, 20170223, 20170224, 20170227, 20170228, 20170301, 20170302, 20170303, 20170306, 20170307, 20170308, 20170309, 20170310, 20170313, 20170314, 20170315, 20170316, 20170317, 20170320, 20170321, 20170322, 20170323, 20170324, 20170327, 20170328, 20170329, 20170330, 20170331, 20170403, 20170404, 20170405, 20170406, 20170407, 20170410, 20170411, 20170412, 20170413, 20170417, 20170418, 20170419, 20170420, 20170421, 20170424, 20170425, 20170426, 20170427, 20170428, 20170501, 20170502, 20170503, 20170504, 20170505, 20170508, 20170509, 20170510, 20170511, 20170512, 20170515, 20170516, 20170517, 20170518, 20170519, 20170522, 20170523, 20170524, 20170525, 20170526, 20170530, 20170531, 20170601, 20170602, 20170605, 20170606, 20170607, 20170608, 20170609, 20170612, 20170613, 20170614, 20170615, 20170616, 20170619, 20170620, 20170621, 20170622, 20170623, 20170626, 20170627, 20170628, 20170629, 20170630, 20170703, 20170705, 20170706, 20170707, 20170710, 20170711, 20170712, 20170713, 20170714, 20170717, 20170718, 20170719, 20170720, 20170721, 20170724, 20170725, 20170726, 20170727, 20170728, 20170731, 20170801, 20170802, 20170803, 20170804, 20170807, 20170808, 20170809, 20170810, 20170811, 20170814, 20170815, 20170816, 20170817, 20170818, 20170821, 20170822, 20170823, 20170824, 20170825, 20170828, 20170829, 20170830, 20170831, 20170901, 20170905, 20170906, 20170907, 20170908, 20170911, 20170912, 20170913, 20170914, 20170915, 20170918, 20170919, 20170920, 20170921, 20170922, 20170925, 20170926, 20170927, 20170928, 20170929, 20171002, 20171003, 20171004, 20171005, 20171006, 20171009, 20171010, 20171011, 20171012, 20171013, 20171016, 20171017, 20171018, 20171019, 20171020, 20171023, 20171024, 20171025, 20171026, 20171027, 20171030, 20171031, 20171101, 20171102, 20171103, 20171106, 20171107, 20171108, 20171109, 20171110, 20171113, 20171114, 20171115, 20171116, 20171117, 20171120, 20171121, 20171122, 20171124, 20171127, 20171128, 20171129, 20171130, 20171201, 20171204, 20171205, 20171206, 20171207, 20171208, 20171211, 20171212, 20171213, 20171214, 20171215, 20171218, 20171219, 20171220, 20171221, 20171222, 20171226, 20171227, 20171228, 20171229, 20180102, 20180103, 20180104, 20180105, 20180108, 20180109, 20180110, 20180111, 20180112, 20180116, 20180117, 20180118, 20180119, 20180122, 20180123, 20180124, 20180125, 20180126, 20180129, 20180130, 20180131, 20180201, 20180202, 20180205, 20180206, 20180207, 20180208, 20180209, 20180212, 20180213, 20180214, 20180215, 20180216, 20180220, 20180221, 20180222, 20180223, 20180226, 20180227, 20180228, 20180301, 20180302, 20180305, 20180306, 20180307, 20180308, 20180309, 20180312, 20180313, 20180314, 20180315, 20180316, 20180319, 20180320, 20180321, 20180322, 20180323, 20180326, 20180327, 20180328, 20180329, 20180402, 20180403, 20180404, 20180405, 20180406, 20180409, 20180410, 20180411, 20180412, 20180413, 20180416, 20180417, 20180418, 20180419, 20180420, 20180423, 20180424, 20180425, 20180426, 20180427, 20180430, 20180501, 20180502, 20180503, 20180504, 20180507, 20180508, 20180509, 20180510, 20180511, 20180514, 20180515, 20180516, 20180517, 20180518, 20180521, 20180522, 20180523, 20180524, 20180525, 20180529, 20180530, 20180531, 20180601, 20180604, 20180605, 20180606, 20180607, 20180608, 20180611, 20180612, 20180613, 20180614, 20180615, 20180618, 20180619, 20180620, 20180621, 20180622, 20180625, 20180626, 20180627, 20180628, 20180629, 20180702, 20180703, 20180705, 20180706, 20180709, 20180710, 20180711, 20180712, 20180713, 20180716, 20180717, 20180718, 20180719, 20180720, 20180723, 20180724, 20180725, 20180726, 20180727, 20180730, 20180731, 20180801, 20180802, 20180803, 20180806, 20180807, 20180808, 20180809, 20180810, 20180813, 20180814, 20180815, 20180816, 20180817, 20180820, 20180821, 20180822, 20180823, 20180824, 20180827, 20180828, 20180829, 20180830, 20180831, 20180904, 20180905, 20180906, 20180907, 20180910, 20180911, 20180912, 20180913, 20180914, 20180917, 20180918, 20180919, 20180920, 20180921, 20180924, 20180925, 20180926, 20180927, 20180928, 20181001, 20181002, 20181003, 20181004, 20181005, 20181008, 20181009, 20181010, 20181011, 20181012, 20181015, 20181016, 20181017, 20181018, 20181019, 20181022, 20181023, 20181024, 20181025, 20181026, 20181029, 20181030, 20181031, 20181101, 20181102, 20181105, 20181106, 20181107, 20181108, 20181109, 20181112, 20181113, 20181114, 20181115, 20181116, 20181119, 20181120, 20181121, 20181123, 20181126, 20181127, 20181128, 20181129, 20181130, 20181203, 20181204, 20181206, 20181207, 20181210, 20181211, 20181212, 20181213, 20181214, 20181217, 20181218, 20181219, 20181220, 20181221, 20181224, 20181226, 20181227, 20181228, 20181231, 20190102, 20190103, 20190104, 20190107, 20190108, 20190109, 20190110, 20190111, 20190114, 20190115, 20190116, 20190117, 20190118, 20190122, 20190123, 20190124, 20190125, 20190128, 20190129, 20190130, 20190131, 20190201, 20190204, 20190205, 20190206, 20190207, 20190208, 20190211, 20190212, 20190213, 20190214, 20190215, 20190219, 20190220, 20190221, 20190222, 20190225, 20190226, 20190227, 20190228, 20190301, 20190304, 20190305, 20190306, 20190307, 20190308, 20190311, 20190312, 20190313, 20190314, 20190315, 20190318, 20190319, 20190320, 20190321, 20190322, 20190325, 20190326, 20190327, 20190328, 20190329, 20190401, 20190402, 20190403, 20190404, 20190405, 20190408, 20190409, 20190410, 20190411, 20190412, 20190415, 20190416, 20190417, 20190418, 20190422, 20190423, 20190424, 20190425, 20190426, 20190429, 20190430, 20190501, 20190502, 20190503, 20190506, 20190507, 20190508, 20190509, 20190510, 20190513, 20190514, 20190515, 20190516, 20190517, 20190520, 20190521, 20190522, 20190523, 20190524, 20190528, 20190529, 20190530, 20190531, 20190603, 20190604, 20190605, 20190606, 20190607, 20190610, 20190611, 20190612, 20190613, 20190614, 20190617, 20190618, 20190619, 20190620, 20190621, 20190624, 20190625, 20190626, 20190627, 20190628, 20190701, 20190702, 20190703, 20190705, 20190708, 20190709, 20190710, 20190711, 20190712, 20190715, 20190716, 20190717, 20190718, 20190719, 20190722, 20190723, 20190724, 20190725, 20190726, 20190729, 20190730, 20190731, 20190801, 20190802, 20190805, 20190806, 20190807, 20190808, 20190809, 20190812, 20190813, 20190814, 20190815, 20190816, 20190819, 20190820, 20190821, 20190822, 20190823, 20190826, 20190827, 20190828, 20190829, 20190830, 20190903, 20190904, 20190905, 20190906, 20190909, 20190910, 20190911, 20190912, 20190913, 20190916, 20190917, 20190918, 20190919, 20190920, 20190923, 20190924, 20190925, 20190926, 20190927, 20190930, 20191001, 20191002, 20191003, 20191004, 20191007, 20191008, 20191009, 20191010, 20191011, 20191014, 20191015, 20191016, 20191017, 20191018, 20191021, 20191022, 20191023, 20191024, 20191025, 20191028, 20191029, 20191030, 20191031, 20191101, 20191104, 20191105, 20191106, 20191107, 20191108, 20191111, 20191112, 20191113, 20191114, 20191115, 20191118, 20191119, 20191120], \"y\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('114b9c65-60a7-4d7c-8d93-30b96fa0f624');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Myc10MEbaT_S"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJpJDHsGaT_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd77fd15-7b49-47dc-8d34-3a39fe3eb709"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .05]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "\n",
        "  historical = Train_data(dfs[col_name], train_start=100, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"WU\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 14ms/step - loss: 0.6788 - accuracy: 0.5953 - val_loss: 0.6952 - val_accuracy: 0.5245\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.6768 - accuracy: 0.5994 - val_loss: 0.6976 - val_accuracy: 0.5245\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.6757 - accuracy: 0.5994 - val_loss: 0.6980 - val_accuracy: 0.5245\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.6729 - accuracy: 0.5994 - val_loss: 0.6523 - val_accuracy: 0.6490\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.6222 - accuracy: 0.6609 - val_loss: 0.4903 - val_accuracy: 0.8224\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 12ms/step - loss: 0.6759 - accuracy: 0.5775 - val_loss: 0.7144 - val_accuracy: 0.5245\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 9ms/step - loss: 0.6714 - accuracy: 0.6006 - val_loss: 0.6712 - val_accuracy: 0.5184\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 9ms/step - loss: 0.5516 - accuracy: 0.7107 - val_loss: 0.3672 - val_accuracy: 0.8061\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.3837 - accuracy: 0.8296 - val_loss: 0.2773 - val_accuracy: 0.8816\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 9ms/step - loss: 0.3789 - accuracy: 0.8237 - val_loss: 0.3264 - val_accuracy: 0.8531\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.938879\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.939305\n",
            "[2]\tvalidation_0-auc:0.943379\n",
            "[3]\tvalidation_0-auc:0.941643\n",
            "[4]\tvalidation_0-auc:0.940006\n",
            "[5]\tvalidation_0-auc:0.944239\n",
            "[6]\tvalidation_0-auc:0.943772\n",
            "[7]\tvalidation_0-auc:0.941517\n",
            "[8]\tvalidation_0-auc:0.941317\n",
            "[9]\tvalidation_0-auc:0.943989\n",
            "[10]\tvalidation_0-auc:0.943747\n",
            "[11]\tvalidation_0-auc:0.944064\n",
            "[12]\tvalidation_0-auc:0.946853\n",
            "[13]\tvalidation_0-auc:0.944782\n",
            "[14]\tvalidation_0-auc:0.94646\n",
            "[15]\tvalidation_0-auc:0.946895\n",
            "[16]\tvalidation_0-auc:0.946936\n",
            "[17]\tvalidation_0-auc:0.947529\n",
            "[18]\tvalidation_0-auc:0.947379\n",
            "[19]\tvalidation_0-auc:0.946444\n",
            "[20]\tvalidation_0-auc:0.946544\n",
            "[21]\tvalidation_0-auc:0.946143\n",
            "[22]\tvalidation_0-auc:0.946227\n",
            "[23]\tvalidation_0-auc:0.94636\n",
            "[24]\tvalidation_0-auc:0.946293\n",
            "[25]\tvalidation_0-auc:0.945843\n",
            "[26]\tvalidation_0-auc:0.945943\n",
            "[27]\tvalidation_0-auc:0.944156\n",
            "[28]\tvalidation_0-auc:0.943638\n",
            "[29]\tvalidation_0-auc:0.943571\n",
            "[30]\tvalidation_0-auc:0.943254\n",
            "[31]\tvalidation_0-auc:0.942962\n",
            "[32]\tvalidation_0-auc:0.943029\n",
            "[33]\tvalidation_0-auc:0.942319\n",
            "[34]\tvalidation_0-auc:0.942185\n",
            "[35]\tvalidation_0-auc:0.942185\n",
            "[36]\tvalidation_0-auc:0.942169\n",
            "[37]\tvalidation_0-auc:0.942035\n",
            "[38]\tvalidation_0-auc:0.942202\n",
            "[39]\tvalidation_0-auc:0.942269\n",
            "[40]\tvalidation_0-auc:0.942269\n",
            "[41]\tvalidation_0-auc:0.942736\n",
            "[42]\tvalidation_0-auc:0.942787\n",
            "[43]\tvalidation_0-auc:0.942853\n",
            "[44]\tvalidation_0-auc:0.940707\n",
            "[45]\tvalidation_0-auc:0.940549\n",
            "[46]\tvalidation_0-auc:0.940549\n",
            "[47]\tvalidation_0-auc:0.940499\n",
            "[48]\tvalidation_0-auc:0.940565\n",
            "[49]\tvalidation_0-auc:0.940073\n",
            "[50]\tvalidation_0-auc:0.940173\n",
            "[51]\tvalidation_0-auc:0.940073\n",
            "[52]\tvalidation_0-auc:0.939956\n",
            "[53]\tvalidation_0-auc:0.93943\n",
            "[54]\tvalidation_0-auc:0.939463\n",
            "[55]\tvalidation_0-auc:0.939229\n",
            "[56]\tvalidation_0-auc:0.939229\n",
            "[57]\tvalidation_0-auc:0.93943\n",
            "[58]\tvalidation_0-auc:0.93953\n",
            "[59]\tvalidation_0-auc:0.939213\n",
            "[60]\tvalidation_0-auc:0.939263\n",
            "[61]\tvalidation_0-auc:0.939422\n",
            "[62]\tvalidation_0-auc:0.939422\n",
            "[63]\tvalidation_0-auc:0.939305\n",
            "[64]\tvalidation_0-auc:0.939138\n",
            "[65]\tvalidation_0-auc:0.939138\n",
            "[66]\tvalidation_0-auc:0.939271\n",
            "[67]\tvalidation_0-auc:0.939472\n",
            "Stopping. Best iteration:\n",
            "[17]\tvalidation_0-auc:0.947529\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 14ms/step - loss: 0.6783 - accuracy: 0.5975 - val_loss: 0.7082 - val_accuracy: 0.4902\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.6058 - accuracy: 0.6765 - val_loss: 0.6570 - val_accuracy: 0.6411\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.4606 - accuracy: 0.8033 - val_loss: 0.3820 - val_accuracy: 0.9037\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.4334 - accuracy: 0.8135 - val_loss: 0.4176 - val_accuracy: 0.8643\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.4280 - accuracy: 0.8183 - val_loss: 0.3675 - val_accuracy: 0.9081\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 12ms/step - loss: 0.6141 - accuracy: 0.6439 - val_loss: 0.3529 - val_accuracy: 0.8578\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.4306 - accuracy: 0.8117 - val_loss: 0.3226 - val_accuracy: 0.9059\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 0.3825 - accuracy: 0.8316 - val_loss: 0.2960 - val_accuracy: 0.9256\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 0.3830 - accuracy: 0.8365 - val_loss: 0.2906 - val_accuracy: 0.9125\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3585 - accuracy: 0.8377 - val_loss: 0.2994 - val_accuracy: 0.9168\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.915217\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.923283\n",
            "[2]\tvalidation_0-auc:0.925582\n",
            "[3]\tvalidation_0-auc:0.925582\n",
            "[4]\tvalidation_0-auc:0.925582\n",
            "[5]\tvalidation_0-auc:0.925851\n",
            "[6]\tvalidation_0-auc:0.917411\n",
            "[7]\tvalidation_0-auc:0.917411\n",
            "[8]\tvalidation_0-auc:0.917296\n",
            "[9]\tvalidation_0-auc:0.917842\n",
            "[10]\tvalidation_0-auc:0.917842\n",
            "[11]\tvalidation_0-auc:0.92905\n",
            "[12]\tvalidation_0-auc:0.929031\n",
            "[13]\tvalidation_0-auc:0.929031\n",
            "[14]\tvalidation_0-auc:0.928935\n",
            "[15]\tvalidation_0-auc:0.928859\n",
            "[16]\tvalidation_0-auc:0.928859\n",
            "[17]\tvalidation_0-auc:0.928859\n",
            "[18]\tvalidation_0-auc:0.928993\n",
            "[19]\tvalidation_0-auc:0.928878\n",
            "[20]\tvalidation_0-auc:0.933946\n",
            "[21]\tvalidation_0-auc:0.934022\n",
            "[22]\tvalidation_0-auc:0.934022\n",
            "[23]\tvalidation_0-auc:0.934022\n",
            "[24]\tvalidation_0-auc:0.934022\n",
            "[25]\tvalidation_0-auc:0.928418\n",
            "[26]\tvalidation_0-auc:0.928418\n",
            "[27]\tvalidation_0-auc:0.928418\n",
            "[28]\tvalidation_0-auc:0.928342\n",
            "[29]\tvalidation_0-auc:0.928418\n",
            "[30]\tvalidation_0-auc:0.92838\n",
            "[31]\tvalidation_0-auc:0.927757\n",
            "[32]\tvalidation_0-auc:0.927412\n",
            "[33]\tvalidation_0-auc:0.927355\n",
            "[34]\tvalidation_0-auc:0.928169\n",
            "[35]\tvalidation_0-auc:0.927872\n",
            "[36]\tvalidation_0-auc:0.927968\n",
            "[37]\tvalidation_0-auc:0.92837\n",
            "[38]\tvalidation_0-auc:0.92837\n",
            "[39]\tvalidation_0-auc:0.928409\n",
            "[40]\tvalidation_0-auc:0.927173\n",
            "[41]\tvalidation_0-auc:0.927192\n",
            "[42]\tvalidation_0-auc:0.927115\n",
            "[43]\tvalidation_0-auc:0.927173\n",
            "[44]\tvalidation_0-auc:0.927259\n",
            "[45]\tvalidation_0-auc:0.927278\n",
            "[46]\tvalidation_0-auc:0.926665\n",
            "[47]\tvalidation_0-auc:0.926703\n",
            "[48]\tvalidation_0-auc:0.926665\n",
            "[49]\tvalidation_0-auc:0.92768\n",
            "[50]\tvalidation_0-auc:0.927623\n",
            "[51]\tvalidation_0-auc:0.9277\n",
            "[52]\tvalidation_0-auc:0.927757\n",
            "[53]\tvalidation_0-auc:0.927738\n",
            "[54]\tvalidation_0-auc:0.927738\n",
            "[55]\tvalidation_0-auc:0.927393\n",
            "[56]\tvalidation_0-auc:0.927278\n",
            "[57]\tvalidation_0-auc:0.927479\n",
            "[58]\tvalidation_0-auc:0.927479\n",
            "[59]\tvalidation_0-auc:0.92746\n",
            "[60]\tvalidation_0-auc:0.92723\n",
            "[61]\tvalidation_0-auc:0.926924\n",
            "[62]\tvalidation_0-auc:0.926885\n",
            "[63]\tvalidation_0-auc:0.927173\n",
            "[64]\tvalidation_0-auc:0.927192\n",
            "[65]\tvalidation_0-auc:0.925161\n",
            "[66]\tvalidation_0-auc:0.925257\n",
            "[67]\tvalidation_0-auc:0.925257\n",
            "[68]\tvalidation_0-auc:0.924529\n",
            "[69]\tvalidation_0-auc:0.924414\n",
            "[70]\tvalidation_0-auc:0.92428\n",
            "[71]\tvalidation_0-auc:0.924433\n",
            "Stopping. Best iteration:\n",
            "[21]\tvalidation_0-auc:0.934022\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.8224489795918367 | 0.7534722222222222 | 0.9313304721030042 | 0.8330134357005757 |\n",
            "|     GRU 0.1      | 0.8530612244897959 | 0.7927272727272727 | 0.9356223175965666 | 0.858267716535433  |\n",
            "|   XGBoost 0.1    | 0.8612244897959184 | 0.880184331797235  | 0.8197424892703863 | 0.8488888888888888 |\n",
            "|    Logreg 0.1    | 0.8734693877551021 |        1.0         | 0.7339055793991416 | 0.8465346534653465 |\n",
            "|     SVM 0.1      | 0.889795918367347  | 0.8537549407114624 | 0.927038626609442  | 0.8888888888888887 |\n",
            "|  LSTM beta 0.1   | 0.9080962800875274 |        1.0         | 0.8197424892703863 | 0.9009433962264152 |\n",
            "|   GRU beta 0.1   | 0.9168490153172867 |        1.0         | 0.8369098712446352 | 0.9112149532710281 |\n",
            "| XGBoost beta 0.1 | 0.8161925601750547 | 0.8281938325991189 | 0.8068669527896996 | 0.817391304347826  |\n",
            "| logreg beta 0.1  | 0.8862144420131292 |        1.0         | 0.776824034334764  | 0.8743961352657006 |\n",
            "|   svm beta 0.1   | 0.9212253829321663 |        1.0         | 0.8454935622317596 | 0.9162790697674419 |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.05\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 14ms/step - loss: 0.6927 - accuracy: 0.5314 - val_loss: 0.6941 - val_accuracy: 0.5245\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.6892 - accuracy: 0.5467 - val_loss: 0.6946 - val_accuracy: 0.5245\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.6792 - accuracy: 0.5680 - val_loss: 0.5174 - val_accuracy: 0.8918\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.6776 - accuracy: 0.5834 - val_loss: 0.6945 - val_accuracy: 0.5245\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 11ms/step - loss: 0.6912 - accuracy: 0.5414 - val_loss: 0.6916 - val_accuracy: 0.5245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 3s 13ms/step - loss: 0.6912 - accuracy: 0.5278 - val_loss: 0.6924 - val_accuracy: 0.5245\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 1s 9ms/step - loss: 0.6144 - accuracy: 0.6260 - val_loss: 0.4489 - val_accuracy: 0.7612\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.3280 - accuracy: 0.8734 - val_loss: 0.3164 - val_accuracy: 0.8265\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.3135 - accuracy: 0.8675 - val_loss: 0.2735 - val_accuracy: 0.8551\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 1s 10ms/step - loss: 0.3050 - accuracy: 0.8775 - val_loss: 0.2427 - val_accuracy: 0.8939\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.938069\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.93928\n",
            "[2]\tvalidation_0-auc:0.941843\n",
            "[3]\tvalidation_0-auc:0.943404\n",
            "[4]\tvalidation_0-auc:0.944966\n",
            "[5]\tvalidation_0-auc:0.945834\n",
            "[6]\tvalidation_0-auc:0.945851\n",
            "[7]\tvalidation_0-auc:0.946653\n",
            "[8]\tvalidation_0-auc:0.946736\n",
            "[9]\tvalidation_0-auc:0.946861\n",
            "[10]\tvalidation_0-auc:0.946444\n",
            "[11]\tvalidation_0-auc:0.944757\n",
            "[12]\tvalidation_0-auc:0.94606\n",
            "[13]\tvalidation_0-auc:0.947554\n",
            "[14]\tvalidation_0-auc:0.947638\n",
            "[15]\tvalidation_0-auc:0.947496\n",
            "[16]\tvalidation_0-auc:0.947763\n",
            "[17]\tvalidation_0-auc:0.948924\n",
            "[18]\tvalidation_0-auc:0.949224\n",
            "[19]\tvalidation_0-auc:0.949208\n",
            "[20]\tvalidation_0-auc:0.9495\n",
            "[21]\tvalidation_0-auc:0.9495\n",
            "[22]\tvalidation_0-auc:0.949408\n",
            "[23]\tvalidation_0-auc:0.950009\n",
            "[24]\tvalidation_0-auc:0.950143\n",
            "[25]\tvalidation_0-auc:0.950159\n",
            "[26]\tvalidation_0-auc:0.950894\n",
            "[27]\tvalidation_0-auc:0.950944\n",
            "[28]\tvalidation_0-auc:0.951003\n",
            "[29]\tvalidation_0-auc:0.950928\n",
            "[30]\tvalidation_0-auc:0.950944\n",
            "[31]\tvalidation_0-auc:0.949825\n",
            "[32]\tvalidation_0-auc:0.949825\n",
            "[33]\tvalidation_0-auc:0.949926\n",
            "[34]\tvalidation_0-auc:0.949792\n",
            "[35]\tvalidation_0-auc:0.949659\n",
            "[36]\tvalidation_0-auc:0.949074\n",
            "[37]\tvalidation_0-auc:0.9494\n",
            "[38]\tvalidation_0-auc:0.949416\n",
            "[39]\tvalidation_0-auc:0.949157\n",
            "[40]\tvalidation_0-auc:0.949024\n",
            "[41]\tvalidation_0-auc:0.949149\n",
            "[42]\tvalidation_0-auc:0.949032\n",
            "[43]\tvalidation_0-auc:0.948231\n",
            "[44]\tvalidation_0-auc:0.948164\n",
            "[45]\tvalidation_0-auc:0.948164\n",
            "[46]\tvalidation_0-auc:0.948164\n",
            "[47]\tvalidation_0-auc:0.947663\n",
            "[48]\tvalidation_0-auc:0.948481\n",
            "[49]\tvalidation_0-auc:0.948481\n",
            "[50]\tvalidation_0-auc:0.948348\n",
            "[51]\tvalidation_0-auc:0.948398\n",
            "[52]\tvalidation_0-auc:0.948348\n",
            "[53]\tvalidation_0-auc:0.948323\n",
            "[54]\tvalidation_0-auc:0.948256\n",
            "[55]\tvalidation_0-auc:0.948222\n",
            "[56]\tvalidation_0-auc:0.948757\n",
            "[57]\tvalidation_0-auc:0.948723\n",
            "[58]\tvalidation_0-auc:0.948907\n",
            "[59]\tvalidation_0-auc:0.948924\n",
            "[60]\tvalidation_0-auc:0.948965\n",
            "[61]\tvalidation_0-auc:0.948865\n",
            "[62]\tvalidation_0-auc:0.948882\n",
            "[63]\tvalidation_0-auc:0.948815\n",
            "[64]\tvalidation_0-auc:0.948782\n",
            "[65]\tvalidation_0-auc:0.948556\n",
            "[66]\tvalidation_0-auc:0.948381\n",
            "[67]\tvalidation_0-auc:0.948172\n",
            "[68]\tvalidation_0-auc:0.947771\n",
            "[69]\tvalidation_0-auc:0.947755\n",
            "[70]\tvalidation_0-auc:0.947738\n",
            "[71]\tvalidation_0-auc:0.947755\n",
            "[72]\tvalidation_0-auc:0.947771\n",
            "[73]\tvalidation_0-auc:0.947454\n",
            "[74]\tvalidation_0-auc:0.947421\n",
            "[75]\tvalidation_0-auc:0.947012\n",
            "[76]\tvalidation_0-auc:0.946978\n",
            "[77]\tvalidation_0-auc:0.947128\n",
            "[78]\tvalidation_0-auc:0.947112\n",
            "Stopping. Best iteration:\n",
            "[28]\tvalidation_0-auc:0.951003\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 14ms/step - loss: 0.6852 - accuracy: 0.5468 - val_loss: 0.5547 - val_accuracy: 0.7002\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.4826 - accuracy: 0.7779 - val_loss: 0.3135 - val_accuracy: 0.8731\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3909 - accuracy: 0.8473 - val_loss: 0.3061 - val_accuracy: 0.8600\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3730 - accuracy: 0.8437 - val_loss: 0.2825 - val_accuracy: 0.9234\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 11ms/step - loss: 0.3402 - accuracy: 0.8570 - val_loss: 0.2845 - val_accuracy: 0.8928\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "104/104 [==============================] - 3s 13ms/step - loss: 0.6413 - accuracy: 0.6119 - val_loss: 0.3701 - val_accuracy: 0.7965\n",
            "Epoch 2/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3592 - accuracy: 0.8449 - val_loss: 0.2742 - val_accuracy: 0.8556\n",
            "Epoch 3/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.3135 - accuracy: 0.8666 - val_loss: 0.2548 - val_accuracy: 0.9190\n",
            "Epoch 4/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2981 - accuracy: 0.8793 - val_loss: 0.2518 - val_accuracy: 0.9037\n",
            "Epoch 5/5\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 0.2964 - accuracy: 0.8727 - val_loss: 0.2613 - val_accuracy: 0.8687\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.919739\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.918532\n",
            "[2]\tvalidation_0-auc:0.926464\n",
            "[3]\tvalidation_0-auc:0.926157\n",
            "[4]\tvalidation_0-auc:0.91972\n",
            "[5]\tvalidation_0-auc:0.919834\n",
            "[6]\tvalidation_0-auc:0.919672\n",
            "[7]\tvalidation_0-auc:0.919672\n",
            "[8]\tvalidation_0-auc:0.919672\n",
            "[9]\tvalidation_0-auc:0.919815\n",
            "[10]\tvalidation_0-auc:0.919921\n",
            "[11]\tvalidation_0-auc:0.919921\n",
            "[12]\tvalidation_0-auc:0.919873\n",
            "[13]\tvalidation_0-auc:0.928878\n",
            "[14]\tvalidation_0-auc:0.928878\n",
            "[15]\tvalidation_0-auc:0.929721\n",
            "[16]\tvalidation_0-auc:0.931024\n",
            "[17]\tvalidation_0-auc:0.931407\n",
            "[18]\tvalidation_0-auc:0.931752\n",
            "[19]\tvalidation_0-auc:0.93179\n",
            "[20]\tvalidation_0-auc:0.931407\n",
            "[21]\tvalidation_0-auc:0.931081\n",
            "[22]\tvalidation_0-auc:0.930229\n",
            "[23]\tvalidation_0-auc:0.930669\n",
            "[24]\tvalidation_0-auc:0.930669\n",
            "[25]\tvalidation_0-auc:0.930056\n",
            "[26]\tvalidation_0-auc:0.930056\n",
            "[27]\tvalidation_0-auc:0.930056\n",
            "[28]\tvalidation_0-auc:0.929635\n",
            "[29]\tvalidation_0-auc:0.929654\n",
            "[30]\tvalidation_0-auc:0.927316\n",
            "[31]\tvalidation_0-auc:0.923369\n",
            "[32]\tvalidation_0-auc:0.923446\n",
            "[33]\tvalidation_0-auc:0.924327\n",
            "[34]\tvalidation_0-auc:0.925745\n",
            "[35]\tvalidation_0-auc:0.924749\n",
            "[36]\tvalidation_0-auc:0.924327\n",
            "[37]\tvalidation_0-auc:0.924327\n",
            "[38]\tvalidation_0-auc:0.924347\n",
            "[39]\tvalidation_0-auc:0.924519\n",
            "[40]\tvalidation_0-auc:0.924519\n",
            "[41]\tvalidation_0-auc:0.923255\n",
            "[42]\tvalidation_0-auc:0.923255\n",
            "[43]\tvalidation_0-auc:0.925075\n",
            "[44]\tvalidation_0-auc:0.925075\n",
            "[45]\tvalidation_0-auc:0.925075\n",
            "[46]\tvalidation_0-auc:0.925535\n",
            "[47]\tvalidation_0-auc:0.925535\n",
            "[48]\tvalidation_0-auc:0.925535\n",
            "[49]\tvalidation_0-auc:0.925535\n",
            "[50]\tvalidation_0-auc:0.925381\n",
            "[51]\tvalidation_0-auc:0.925381\n",
            "[52]\tvalidation_0-auc:0.925381\n",
            "[53]\tvalidation_0-auc:0.926263\n",
            "[54]\tvalidation_0-auc:0.926263\n",
            "[55]\tvalidation_0-auc:0.926263\n",
            "[56]\tvalidation_0-auc:0.923849\n",
            "[57]\tvalidation_0-auc:0.92473\n",
            "[58]\tvalidation_0-auc:0.923714\n",
            "[59]\tvalidation_0-auc:0.923408\n",
            "[60]\tvalidation_0-auc:0.923408\n",
            "[61]\tvalidation_0-auc:0.923408\n",
            "[62]\tvalidation_0-auc:0.923408\n",
            "[63]\tvalidation_0-auc:0.923839\n",
            "[64]\tvalidation_0-auc:0.923801\n",
            "[65]\tvalidation_0-auc:0.922268\n",
            "[66]\tvalidation_0-auc:0.922555\n",
            "[67]\tvalidation_0-auc:0.922555\n",
            "[68]\tvalidation_0-auc:0.922555\n",
            "[69]\tvalidation_0-auc:0.921932\n",
            "Stopping. Best iteration:\n",
            "[19]\tvalidation_0-auc:0.93179\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.05     | 0.5244897959183673 |        0.0         |        0.0         |        0.0         |\n",
            "|      GRU 0.05     | 0.8938775510204081 | 0.9330143540669856 | 0.8369098712446352 | 0.8823529411764706 |\n",
            "|    XGBoost 0.05   | 0.8530612244897959 | 0.8454935622317596 | 0.8454935622317596 | 0.8454935622317596 |\n",
            "|    Logreg 0.05    | 0.8959183673469387 | 0.941747572815534  | 0.8326180257510729 | 0.8838268792710706 |\n",
            "|      SVM 0.05     | 0.8571428571428571 | 0.7985347985347986 | 0.9356223175965666 | 0.8616600790513834 |\n",
            "|   LSTM beta 0.05  | 0.8927789934354485 |        1.0         | 0.7896995708154506 | 0.882494004796163  |\n",
            "|   GRU beta 0.05   | 0.8687089715536105 | 0.8680851063829788 | 0.8755364806866953 | 0.8717948717948718 |\n",
            "| XGBoost beta 0.05 | 0.8052516411378556 | 0.8272727272727273 | 0.7811158798283262 | 0.8035320088300221 |\n",
            "|  logreg beta 0.05 | 0.9234135667396062 |        1.0         | 0.8497854077253219 | 0.9187935034802784 |\n",
            "|   svm beta 0.05   | 0.9146608315098468 | 0.9801980198019802 | 0.8497854077253219 | 0.9103448275862069 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9mZjIPEaT_T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "6602c28e-d842-4821-d723-025297f6c5e1"
      },
      "source": [
        "Result_cross.to_csv('WU_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.753472</td>\n",
              "      <td>0.822449</td>\n",
              "      <td>0.833013</td>\n",
              "      <td>0.931330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.792727</td>\n",
              "      <td>0.853061</td>\n",
              "      <td>0.858268</td>\n",
              "      <td>0.935622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.880184</td>\n",
              "      <td>0.861224</td>\n",
              "      <td>0.848889</td>\n",
              "      <td>0.819742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.846535</td>\n",
              "      <td>0.733906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.853755</td>\n",
              "      <td>0.889796</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.927039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.908096</td>\n",
              "      <td>0.900943</td>\n",
              "      <td>0.819742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916849</td>\n",
              "      <td>0.911215</td>\n",
              "      <td>0.836910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.828194</td>\n",
              "      <td>0.816193</td>\n",
              "      <td>0.817391</td>\n",
              "      <td>0.806867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.886214</td>\n",
              "      <td>0.874396</td>\n",
              "      <td>0.776824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.921225</td>\n",
              "      <td>0.916279</td>\n",
              "      <td>0.845494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.05</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.524490</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.05</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.933014</td>\n",
              "      <td>0.893878</td>\n",
              "      <td>0.882353</td>\n",
              "      <td>0.836910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.05</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.845494</td>\n",
              "      <td>0.853061</td>\n",
              "      <td>0.845494</td>\n",
              "      <td>0.845494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.05</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.941748</td>\n",
              "      <td>0.895918</td>\n",
              "      <td>0.883827</td>\n",
              "      <td>0.832618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.05</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.798535</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.861660</td>\n",
              "      <td>0.935622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.05</td>\n",
              "      <td>WU</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.892779</td>\n",
              "      <td>0.882494</td>\n",
              "      <td>0.789700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.05</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.868085</td>\n",
              "      <td>0.868709</td>\n",
              "      <td>0.871795</td>\n",
              "      <td>0.875536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.05</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.827273</td>\n",
              "      <td>0.805252</td>\n",
              "      <td>0.803532</td>\n",
              "      <td>0.781116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.05</td>\n",
              "      <td>WU</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.923414</td>\n",
              "      <td>0.918794</td>\n",
              "      <td>0.849785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.05</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.980198</td>\n",
              "      <td>0.914661</td>\n",
              "      <td>0.910345</td>\n",
              "      <td>0.849785</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1   WU  0.753472  0.822449  0.833013  0.931330\n",
              "1            GRU 0.1   WU  0.792727  0.853061  0.858268  0.935622\n",
              "2        XGBoost 0.1   WU  0.880184  0.861224  0.848889  0.819742\n",
              "3         Logreg 0.1   WU  1.000000  0.873469  0.846535  0.733906\n",
              "4            SVM 0.1   WU  0.853755  0.889796  0.888889  0.927039\n",
              "5      LSTM beta 0.1   WU  1.000000  0.908096  0.900943  0.819742\n",
              "6       GRU beta 0.1   WU  1.000000  0.916849  0.911215  0.836910\n",
              "7   XGBoost beta 0.1   WU  0.828194  0.816193  0.817391  0.806867\n",
              "8    logreg beta 0.1   WU  1.000000  0.886214  0.874396  0.776824\n",
              "9       svm beta 0.1   WU  1.000000  0.921225  0.916279  0.845494\n",
              "0          LSTM 0.05   WU  0.000000  0.524490  0.000000  0.000000\n",
              "1           GRU 0.05   WU  0.933014  0.893878  0.882353  0.836910\n",
              "2       XGBoost 0.05   WU  0.845494  0.853061  0.845494  0.845494\n",
              "3        Logreg 0.05   WU  0.941748  0.895918  0.883827  0.832618\n",
              "4           SVM 0.05   WU  0.798535  0.857143  0.861660  0.935622\n",
              "5     LSTM beta 0.05   WU  1.000000  0.892779  0.882494  0.789700\n",
              "6      GRU beta 0.05   WU  0.868085  0.868709  0.871795  0.875536\n",
              "7  XGBoost beta 0.05   WU  0.827273  0.805252  0.803532  0.781116\n",
              "8   logreg beta 0.05   WU  1.000000  0.923414  0.918794  0.849785\n",
              "9      svm beta 0.05   WU  0.980198  0.914661  0.910345  0.849785"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43TxeJH1aT_T"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5OyCM2YaT_T"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgTYdmwYaT_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6fcf43b-032b-4497-d2d5-226a4df5dfc5"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"WU\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6920 - accuracy: 0.5450 - val_loss: 0.6935 - val_accuracy: 0.5245\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6886 - accuracy: 0.5564 - val_loss: 0.6918 - val_accuracy: 0.5245\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6881 - accuracy: 0.5564 - val_loss: 0.6932 - val_accuracy: 0.5245\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6871 - accuracy: 0.5584 - val_loss: 0.6911 - val_accuracy: 0.5245\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6876 - accuracy: 0.5570 - val_loss: 0.6898 - val_accuracy: 0.5245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6907 - accuracy: 0.5537 - val_loss: 0.6877 - val_accuracy: 0.5245\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6787 - accuracy: 0.5644 - val_loss: 0.6458 - val_accuracy: 0.8918\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4674 - accuracy: 0.7906 - val_loss: 0.2620 - val_accuracy: 0.8878\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.3603 - accuracy: 0.8329 - val_loss: 0.2496 - val_accuracy: 0.9082\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.3513 - accuracy: 0.8383 - val_loss: 0.2325 - val_accuracy: 0.8939\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.940691\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.94682\n",
            "[2]\tvalidation_0-auc:0.94641\n",
            "[3]\tvalidation_0-auc:0.946277\n",
            "[4]\tvalidation_0-auc:0.9456\n",
            "[5]\tvalidation_0-auc:0.946895\n",
            "[6]\tvalidation_0-auc:0.947504\n",
            "[7]\tvalidation_0-auc:0.942728\n",
            "[8]\tvalidation_0-auc:0.942728\n",
            "[9]\tvalidation_0-auc:0.945926\n",
            "[10]\tvalidation_0-auc:0.948656\n",
            "[11]\tvalidation_0-auc:0.948807\n",
            "[12]\tvalidation_0-auc:0.948915\n",
            "[13]\tvalidation_0-auc:0.950235\n",
            "[14]\tvalidation_0-auc:0.950335\n",
            "[15]\tvalidation_0-auc:0.950569\n",
            "[16]\tvalidation_0-auc:0.950652\n",
            "[17]\tvalidation_0-auc:0.950109\n",
            "[18]\tvalidation_0-auc:0.950093\n",
            "[19]\tvalidation_0-auc:0.950418\n",
            "[20]\tvalidation_0-auc:0.9498\n",
            "[21]\tvalidation_0-auc:0.950168\n",
            "[22]\tvalidation_0-auc:0.950335\n",
            "[23]\tvalidation_0-auc:0.950711\n",
            "[24]\tvalidation_0-auc:0.950443\n",
            "[25]\tvalidation_0-auc:0.947262\n",
            "[26]\tvalidation_0-auc:0.947437\n",
            "[27]\tvalidation_0-auc:0.947103\n",
            "[28]\tvalidation_0-auc:0.944156\n",
            "[29]\tvalidation_0-auc:0.94444\n",
            "[30]\tvalidation_0-auc:0.943747\n",
            "[31]\tvalidation_0-auc:0.943463\n",
            "[32]\tvalidation_0-auc:0.942803\n",
            "[33]\tvalidation_0-auc:0.942803\n",
            "[34]\tvalidation_0-auc:0.941876\n",
            "[35]\tvalidation_0-auc:0.941701\n",
            "[36]\tvalidation_0-auc:0.941225\n",
            "[37]\tvalidation_0-auc:0.941225\n",
            "[38]\tvalidation_0-auc:0.941634\n",
            "[39]\tvalidation_0-auc:0.941634\n",
            "[40]\tvalidation_0-auc:0.941117\n",
            "[41]\tvalidation_0-auc:0.940732\n",
            "[42]\tvalidation_0-auc:0.940565\n",
            "[43]\tvalidation_0-auc:0.939856\n",
            "[44]\tvalidation_0-auc:0.939839\n",
            "[45]\tvalidation_0-auc:0.939163\n",
            "[46]\tvalidation_0-auc:0.938912\n",
            "[47]\tvalidation_0-auc:0.938879\n",
            "[48]\tvalidation_0-auc:0.938478\n",
            "[49]\tvalidation_0-auc:0.938044\n",
            "[50]\tvalidation_0-auc:0.938311\n",
            "[51]\tvalidation_0-auc:0.938294\n",
            "[52]\tvalidation_0-auc:0.938628\n",
            "[53]\tvalidation_0-auc:0.937977\n",
            "[54]\tvalidation_0-auc:0.938027\n",
            "[55]\tvalidation_0-auc:0.937944\n",
            "[56]\tvalidation_0-auc:0.937626\n",
            "[57]\tvalidation_0-auc:0.937626\n",
            "[58]\tvalidation_0-auc:0.937601\n",
            "[59]\tvalidation_0-auc:0.937518\n",
            "[60]\tvalidation_0-auc:0.937284\n",
            "[61]\tvalidation_0-auc:0.937384\n",
            "[62]\tvalidation_0-auc:0.937267\n",
            "[63]\tvalidation_0-auc:0.937217\n",
            "[64]\tvalidation_0-auc:0.937317\n",
            "[65]\tvalidation_0-auc:0.937217\n",
            "[66]\tvalidation_0-auc:0.937284\n",
            "[67]\tvalidation_0-auc:0.937184\n",
            "[68]\tvalidation_0-auc:0.93675\n",
            "[69]\tvalidation_0-auc:0.9368\n",
            "[70]\tvalidation_0-auc:0.93705\n",
            "[71]\tvalidation_0-auc:0.937\n",
            "[72]\tvalidation_0-auc:0.937017\n",
            "[73]\tvalidation_0-auc:0.93695\n",
            "Stopping. Best iteration:\n",
            "[23]\tvalidation_0-auc:0.950711\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6950 - accuracy: 0.5264 - val_loss: 0.6935 - val_accuracy: 0.4902\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6588 - accuracy: 0.5854 - val_loss: 0.3520 - val_accuracy: 0.9081\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4462 - accuracy: 0.8188 - val_loss: 0.2874 - val_accuracy: 0.8928\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.3976 - accuracy: 0.8367 - val_loss: 0.2956 - val_accuracy: 0.8775\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.3719 - accuracy: 0.8469 - val_loss: 0.2744 - val_accuracy: 0.8906\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 2s 13ms/step - loss: 0.5829 - accuracy: 0.6719 - val_loss: 0.3050 - val_accuracy: 0.9103\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.3814 - accuracy: 0.8408 - val_loss: 0.2683 - val_accuracy: 0.9103\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.3625 - accuracy: 0.8421 - val_loss: 0.2815 - val_accuracy: 0.8512\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.3432 - accuracy: 0.8421 - val_loss: 0.2640 - val_accuracy: 0.9212\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.3378 - accuracy: 0.8449 - val_loss: 0.2834 - val_accuracy: 0.8271\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.923858\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.926129\n",
            "[2]\tvalidation_0-auc:0.926129\n",
            "[3]\tvalidation_0-auc:0.925027\n",
            "[4]\tvalidation_0-auc:0.924826\n",
            "[5]\tvalidation_0-auc:0.924826\n",
            "[6]\tvalidation_0-auc:0.925247\n",
            "[7]\tvalidation_0-auc:0.924653\n",
            "[8]\tvalidation_0-auc:0.924653\n",
            "[9]\tvalidation_0-auc:0.934262\n",
            "[10]\tvalidation_0-auc:0.927029\n",
            "[11]\tvalidation_0-auc:0.926837\n",
            "[12]\tvalidation_0-auc:0.927087\n",
            "[13]\tvalidation_0-auc:0.927154\n",
            "[14]\tvalidation_0-auc:0.926215\n",
            "[15]\tvalidation_0-auc:0.928322\n",
            "[16]\tvalidation_0-auc:0.927786\n",
            "[17]\tvalidation_0-auc:0.927863\n",
            "[18]\tvalidation_0-auc:0.927853\n",
            "[19]\tvalidation_0-auc:0.928428\n",
            "[20]\tvalidation_0-auc:0.928428\n",
            "[21]\tvalidation_0-auc:0.928428\n",
            "[22]\tvalidation_0-auc:0.92792\n",
            "[23]\tvalidation_0-auc:0.92792\n",
            "[24]\tvalidation_0-auc:0.927977\n",
            "[25]\tvalidation_0-auc:0.929252\n",
            "[26]\tvalidation_0-auc:0.925918\n",
            "[27]\tvalidation_0-auc:0.9254\n",
            "[28]\tvalidation_0-auc:0.926502\n",
            "[29]\tvalidation_0-auc:0.926502\n",
            "[30]\tvalidation_0-auc:0.92587\n",
            "[31]\tvalidation_0-auc:0.925525\n",
            "[32]\tvalidation_0-auc:0.925525\n",
            "[33]\tvalidation_0-auc:0.925276\n",
            "[34]\tvalidation_0-auc:0.925276\n",
            "[35]\tvalidation_0-auc:0.92495\n",
            "[36]\tvalidation_0-auc:0.924452\n",
            "[37]\tvalidation_0-auc:0.925525\n",
            "[38]\tvalidation_0-auc:0.925525\n",
            "[39]\tvalidation_0-auc:0.925525\n",
            "[40]\tvalidation_0-auc:0.925314\n",
            "[41]\tvalidation_0-auc:0.925314\n",
            "[42]\tvalidation_0-auc:0.925659\n",
            "[43]\tvalidation_0-auc:0.925659\n",
            "[44]\tvalidation_0-auc:0.925659\n",
            "[45]\tvalidation_0-auc:0.926138\n",
            "[46]\tvalidation_0-auc:0.926138\n",
            "[47]\tvalidation_0-auc:0.925784\n",
            "[48]\tvalidation_0-auc:0.925784\n",
            "[49]\tvalidation_0-auc:0.925333\n",
            "[50]\tvalidation_0-auc:0.925027\n",
            "[51]\tvalidation_0-auc:0.924557\n",
            "[52]\tvalidation_0-auc:0.924557\n",
            "[53]\tvalidation_0-auc:0.924768\n",
            "[54]\tvalidation_0-auc:0.924232\n",
            "[55]\tvalidation_0-auc:0.924289\n",
            "[56]\tvalidation_0-auc:0.923772\n",
            "[57]\tvalidation_0-auc:0.924327\n",
            "[58]\tvalidation_0-auc:0.923925\n",
            "[59]\tvalidation_0-auc:0.923925\n",
            "Stopping. Best iteration:\n",
            "[9]\tvalidation_0-auc:0.934262\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.5244897959183673 |        0.0         |        0.0         |        0.0         |\n",
            "|     GRU 0.1      | 0.8938775510204081 | 0.9330143540669856 | 0.8369098712446352 | 0.8823529411764706 |\n",
            "|   XGBoost 0.1    | 0.8795918367346939 | 0.9065420560747663 | 0.8326180257510729 | 0.8680089485458613 |\n",
            "|    Logreg 0.1    | 0.8918367346938776 |        1.0         | 0.7725321888412017 | 0.8716707021791766 |\n",
            "|     SVM 0.1      | 0.8857142857142857 | 0.8470588235294118 | 0.927038626609442  | 0.8852459016393442 |\n",
            "|  LSTM beta 0.1   | 0.8905908096280087 |        1.0         | 0.7854077253218884 | 0.8798076923076923 |\n",
            "|   GRU beta 0.1   | 0.8271334792122538 | 0.7961538461538461 | 0.8884120171673819 | 0.8397565922920892 |\n",
            "| XGBoost beta 0.1 | 0.8555798687089715 | 0.9326424870466321 | 0.7725321888412017 | 0.8450704225352113 |\n",
            "| logreg beta 0.1  | 0.9146608315098468 |        1.0         | 0.8326180257510729 | 0.9086651053864169 |\n",
            "|   svm beta 0.1   | 0.9190371991247265 | 0.9949494949494949 | 0.8454935622317596 |  0.91415313225058  |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.05\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6933 - accuracy: 0.5443 - val_loss: 0.6938 - val_accuracy: 0.5245\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6929 - accuracy: 0.5322 - val_loss: 0.6921 - val_accuracy: 0.5245\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6905 - accuracy: 0.5409 - val_loss: 0.6877 - val_accuracy: 0.5245\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6200 - accuracy: 0.6423 - val_loss: 0.5183 - val_accuracy: 0.7245\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.3974 - accuracy: 0.8174 - val_loss: 0.2925 - val_accuracy: 0.9061\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6929 - accuracy: 0.5101 - val_loss: 0.6933 - val_accuracy: 0.5245\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6803 - accuracy: 0.5658 - val_loss: 0.6267 - val_accuracy: 0.5571\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4416 - accuracy: 0.7812 - val_loss: 0.2395 - val_accuracy: 0.8878\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.3336 - accuracy: 0.8597 - val_loss: 0.2987 - val_accuracy: 0.8367\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.3267 - accuracy: 0.8577 - val_loss: 0.2654 - val_accuracy: 0.8918\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.937685\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.942294\n",
            "[2]\tvalidation_0-auc:0.944423\n",
            "[3]\tvalidation_0-auc:0.942469\n",
            "[4]\tvalidation_0-auc:0.943455\n",
            "[5]\tvalidation_0-auc:0.945642\n",
            "[6]\tvalidation_0-auc:0.944932\n",
            "[7]\tvalidation_0-auc:0.946202\n",
            "[8]\tvalidation_0-auc:0.945233\n",
            "[9]\tvalidation_0-auc:0.944122\n",
            "[10]\tvalidation_0-auc:0.944173\n",
            "[11]\tvalidation_0-auc:0.944214\n",
            "[12]\tvalidation_0-auc:0.945191\n",
            "[13]\tvalidation_0-auc:0.945567\n",
            "[14]\tvalidation_0-auc:0.945442\n",
            "[15]\tvalidation_0-auc:0.945609\n",
            "[16]\tvalidation_0-auc:0.945408\n",
            "[17]\tvalidation_0-auc:0.946093\n",
            "[18]\tvalidation_0-auc:0.946126\n",
            "[19]\tvalidation_0-auc:0.946327\n",
            "[20]\tvalidation_0-auc:0.946519\n",
            "[21]\tvalidation_0-auc:0.946435\n",
            "[22]\tvalidation_0-auc:0.946185\n",
            "[23]\tvalidation_0-auc:0.946035\n",
            "[24]\tvalidation_0-auc:0.945901\n",
            "[25]\tvalidation_0-auc:0.945484\n",
            "[26]\tvalidation_0-auc:0.9455\n",
            "[27]\tvalidation_0-auc:0.945266\n",
            "[28]\tvalidation_0-auc:0.945951\n",
            "[29]\tvalidation_0-auc:0.945951\n",
            "[30]\tvalidation_0-auc:0.946644\n",
            "[31]\tvalidation_0-auc:0.946627\n",
            "[32]\tvalidation_0-auc:0.946661\n",
            "[33]\tvalidation_0-auc:0.946886\n",
            "[34]\tvalidation_0-auc:0.946218\n",
            "[35]\tvalidation_0-auc:0.94682\n",
            "[36]\tvalidation_0-auc:0.946377\n",
            "[37]\tvalidation_0-auc:0.946377\n",
            "[38]\tvalidation_0-auc:0.946018\n",
            "[39]\tvalidation_0-auc:0.945976\n",
            "[40]\tvalidation_0-auc:0.946193\n",
            "[41]\tvalidation_0-auc:0.946243\n",
            "[42]\tvalidation_0-auc:0.94646\n",
            "[43]\tvalidation_0-auc:0.945776\n",
            "[44]\tvalidation_0-auc:0.945726\n",
            "[45]\tvalidation_0-auc:0.946277\n",
            "[46]\tvalidation_0-auc:0.946293\n",
            "[47]\tvalidation_0-auc:0.946494\n",
            "[48]\tvalidation_0-auc:0.946761\n",
            "[49]\tvalidation_0-auc:0.946728\n",
            "[50]\tvalidation_0-auc:0.946961\n",
            "[51]\tvalidation_0-auc:0.946928\n",
            "[52]\tvalidation_0-auc:0.947053\n",
            "[53]\tvalidation_0-auc:0.947137\n",
            "[54]\tvalidation_0-auc:0.947204\n",
            "[55]\tvalidation_0-auc:0.94717\n",
            "[56]\tvalidation_0-auc:0.947204\n",
            "[57]\tvalidation_0-auc:0.94707\n",
            "[58]\tvalidation_0-auc:0.947204\n",
            "[59]\tvalidation_0-auc:0.947321\n",
            "[60]\tvalidation_0-auc:0.947471\n",
            "[61]\tvalidation_0-auc:0.947304\n",
            "[62]\tvalidation_0-auc:0.946853\n",
            "[63]\tvalidation_0-auc:0.946803\n",
            "[64]\tvalidation_0-auc:0.946719\n",
            "[65]\tvalidation_0-auc:0.946653\n",
            "[66]\tvalidation_0-auc:0.946377\n",
            "[67]\tvalidation_0-auc:0.946377\n",
            "[68]\tvalidation_0-auc:0.946327\n",
            "[69]\tvalidation_0-auc:0.945091\n",
            "[70]\tvalidation_0-auc:0.945091\n",
            "[71]\tvalidation_0-auc:0.945091\n",
            "[72]\tvalidation_0-auc:0.945108\n",
            "[73]\tvalidation_0-auc:0.944406\n",
            "[74]\tvalidation_0-auc:0.94439\n",
            "[75]\tvalidation_0-auc:0.944882\n",
            "[76]\tvalidation_0-auc:0.944882\n",
            "[77]\tvalidation_0-auc:0.945868\n",
            "[78]\tvalidation_0-auc:0.945717\n",
            "[79]\tvalidation_0-auc:0.945734\n",
            "[80]\tvalidation_0-auc:0.945467\n",
            "[81]\tvalidation_0-auc:0.945517\n",
            "[82]\tvalidation_0-auc:0.945141\n",
            "[83]\tvalidation_0-auc:0.945108\n",
            "[84]\tvalidation_0-auc:0.945074\n",
            "[85]\tvalidation_0-auc:0.945074\n",
            "[86]\tvalidation_0-auc:0.944824\n",
            "[87]\tvalidation_0-auc:0.944874\n",
            "[88]\tvalidation_0-auc:0.94459\n",
            "[89]\tvalidation_0-auc:0.944573\n",
            "[90]\tvalidation_0-auc:0.944173\n",
            "[91]\tvalidation_0-auc:0.944173\n",
            "[92]\tvalidation_0-auc:0.944122\n",
            "[93]\tvalidation_0-auc:0.94429\n",
            "[94]\tvalidation_0-auc:0.944256\n",
            "[95]\tvalidation_0-auc:0.944423\n",
            "[96]\tvalidation_0-auc:0.94449\n",
            "[97]\tvalidation_0-auc:0.94464\n",
            "[98]\tvalidation_0-auc:0.94449\n",
            "[99]\tvalidation_0-auc:0.944206\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6927 - accuracy: 0.5237 - val_loss: 0.6745 - val_accuracy: 0.4902\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5164 - accuracy: 0.7646 - val_loss: 0.4038 - val_accuracy: 0.8643\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4152 - accuracy: 0.8305 - val_loss: 0.2916 - val_accuracy: 0.9059\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.3891 - accuracy: 0.8463 - val_loss: 0.3107 - val_accuracy: 0.8271\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.3842 - accuracy: 0.8421 - val_loss: 0.3088 - val_accuracy: 0.8993\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6666 - accuracy: 0.5923 - val_loss: 0.4823 - val_accuracy: 0.8993\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.3819 - accuracy: 0.8373 - val_loss: 0.3708 - val_accuracy: 0.7899\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.3546 - accuracy: 0.8483 - val_loss: 0.2959 - val_accuracy: 0.9278\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.3239 - accuracy: 0.8717 - val_loss: 0.2686 - val_accuracy: 0.8928\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.3146 - accuracy: 0.8655 - val_loss: 0.2874 - val_accuracy: 0.8271\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.925582\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.927853\n",
            "[2]\tvalidation_0-auc:0.927853\n",
            "[3]\tvalidation_0-auc:0.927853\n",
            "[4]\tvalidation_0-auc:0.921616\n",
            "[5]\tvalidation_0-auc:0.921616\n",
            "[6]\tvalidation_0-auc:0.921616\n",
            "[7]\tvalidation_0-auc:0.921616\n",
            "[8]\tvalidation_0-auc:0.923523\n",
            "[9]\tvalidation_0-auc:0.933132\n",
            "[10]\tvalidation_0-auc:0.926282\n",
            "[11]\tvalidation_0-auc:0.92609\n",
            "[12]\tvalidation_0-auc:0.925879\n",
            "[13]\tvalidation_0-auc:0.926358\n",
            "[14]\tvalidation_0-auc:0.92609\n",
            "[15]\tvalidation_0-auc:0.92791\n",
            "[16]\tvalidation_0-auc:0.927566\n",
            "[17]\tvalidation_0-auc:0.927556\n",
            "[18]\tvalidation_0-auc:0.928092\n",
            "[19]\tvalidation_0-auc:0.928092\n",
            "[20]\tvalidation_0-auc:0.928524\n",
            "[21]\tvalidation_0-auc:0.927977\n",
            "[22]\tvalidation_0-auc:0.927977\n",
            "[23]\tvalidation_0-auc:0.927776\n",
            "[24]\tvalidation_0-auc:0.927776\n",
            "[25]\tvalidation_0-auc:0.928428\n",
            "[26]\tvalidation_0-auc:0.928035\n",
            "[27]\tvalidation_0-auc:0.928303\n",
            "[28]\tvalidation_0-auc:0.929587\n",
            "[29]\tvalidation_0-auc:0.926378\n",
            "[30]\tvalidation_0-auc:0.927795\n",
            "[31]\tvalidation_0-auc:0.92747\n",
            "[32]\tvalidation_0-auc:0.927451\n",
            "[33]\tvalidation_0-auc:0.926914\n",
            "[34]\tvalidation_0-auc:0.926914\n",
            "[35]\tvalidation_0-auc:0.924845\n",
            "[36]\tvalidation_0-auc:0.924845\n",
            "[37]\tvalidation_0-auc:0.924845\n",
            "[38]\tvalidation_0-auc:0.925669\n",
            "[39]\tvalidation_0-auc:0.92586\n",
            "[40]\tvalidation_0-auc:0.92586\n",
            "[41]\tvalidation_0-auc:0.92586\n",
            "[42]\tvalidation_0-auc:0.92701\n",
            "[43]\tvalidation_0-auc:0.92701\n",
            "[44]\tvalidation_0-auc:0.927278\n",
            "[45]\tvalidation_0-auc:0.927278\n",
            "[46]\tvalidation_0-auc:0.927278\n",
            "[47]\tvalidation_0-auc:0.927163\n",
            "[48]\tvalidation_0-auc:0.927163\n",
            "[49]\tvalidation_0-auc:0.927029\n",
            "[50]\tvalidation_0-auc:0.927029\n",
            "[51]\tvalidation_0-auc:0.927029\n",
            "[52]\tvalidation_0-auc:0.926263\n",
            "[53]\tvalidation_0-auc:0.926186\n",
            "[54]\tvalidation_0-auc:0.926186\n",
            "[55]\tvalidation_0-auc:0.92563\n",
            "[56]\tvalidation_0-auc:0.92563\n",
            "[57]\tvalidation_0-auc:0.924241\n",
            "[58]\tvalidation_0-auc:0.923379\n",
            "[59]\tvalidation_0-auc:0.923877\n",
            "Stopping. Best iteration:\n",
            "[9]\tvalidation_0-auc:0.933132\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.05     | 0.9061224489795918 | 0.9012875536480687 | 0.9012875536480687 | 0.9012875536480687 |\n",
            "|      GRU 0.05     | 0.8918367346938776 | 0.8571428571428571 | 0.927038626609442  | 0.890721649484536  |\n",
            "|    XGBoost 0.05   | 0.8306122448979592 | 0.8099173553719008 | 0.8412017167381974 | 0.8252631578947367 |\n",
            "|    Logreg 0.05    | 0.8959183673469387 |       0.9375       | 0.8369098712446352 | 0.8843537414965986 |\n",
            "|      SVM 0.05     | 0.8489795918367347 | 0.7849462365591398 | 0.9399141630901288 | 0.8554687500000001 |\n",
            "|   LSTM beta 0.05  | 0.899343544857768  |        1.0         | 0.8025751072961373 | 0.8904761904761905 |\n",
            "|   GRU beta 0.05   | 0.8271334792122538 | 0.7916666666666666 | 0.8969957081545065 | 0.8410462776659959 |\n",
            "| XGBoost beta 0.05 | 0.8730853391684902 | 0.9629629629629629 | 0.7811158798283262 | 0.862559241706161  |\n",
            "|  logreg beta 0.05 | 0.9168490153172867 |        1.0         | 0.8369098712446352 | 0.9112149532710281 |\n",
            "|   svm beta 0.05   | 0.912472647702407  | 0.9800995024875622 | 0.8454935622317596 | 0.9078341013824884 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dWGPyBdaT_T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "e32706c9-24ab-476b-cecd-205c519ccfcb"
      },
      "source": [
        "Result_purging.to_csv('WU_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.524490</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.933014</td>\n",
              "      <td>0.893878</td>\n",
              "      <td>0.882353</td>\n",
              "      <td>0.836910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.906542</td>\n",
              "      <td>0.879592</td>\n",
              "      <td>0.868009</td>\n",
              "      <td>0.832618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.891837</td>\n",
              "      <td>0.871671</td>\n",
              "      <td>0.772532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.847059</td>\n",
              "      <td>0.885714</td>\n",
              "      <td>0.885246</td>\n",
              "      <td>0.927039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.890591</td>\n",
              "      <td>0.879808</td>\n",
              "      <td>0.785408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.796154</td>\n",
              "      <td>0.827133</td>\n",
              "      <td>0.839757</td>\n",
              "      <td>0.888412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.932642</td>\n",
              "      <td>0.855580</td>\n",
              "      <td>0.845070</td>\n",
              "      <td>0.772532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.914661</td>\n",
              "      <td>0.908665</td>\n",
              "      <td>0.832618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.994949</td>\n",
              "      <td>0.919037</td>\n",
              "      <td>0.914153</td>\n",
              "      <td>0.845494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.05</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.901288</td>\n",
              "      <td>0.906122</td>\n",
              "      <td>0.901288</td>\n",
              "      <td>0.901288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.05</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.891837</td>\n",
              "      <td>0.890722</td>\n",
              "      <td>0.927039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.05</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.809917</td>\n",
              "      <td>0.830612</td>\n",
              "      <td>0.825263</td>\n",
              "      <td>0.841202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.05</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.895918</td>\n",
              "      <td>0.884354</td>\n",
              "      <td>0.836910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.05</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.784946</td>\n",
              "      <td>0.848980</td>\n",
              "      <td>0.855469</td>\n",
              "      <td>0.939914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.05</td>\n",
              "      <td>WU</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.899344</td>\n",
              "      <td>0.890476</td>\n",
              "      <td>0.802575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.05</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.827133</td>\n",
              "      <td>0.841046</td>\n",
              "      <td>0.896996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.05</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.962963</td>\n",
              "      <td>0.873085</td>\n",
              "      <td>0.862559</td>\n",
              "      <td>0.781116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.05</td>\n",
              "      <td>WU</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916849</td>\n",
              "      <td>0.911215</td>\n",
              "      <td>0.836910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.05</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.980100</td>\n",
              "      <td>0.912473</td>\n",
              "      <td>0.907834</td>\n",
              "      <td>0.845494</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1   WU  0.000000  0.524490  0.000000  0.000000\n",
              "1            GRU 0.1   WU  0.933014  0.893878  0.882353  0.836910\n",
              "2        XGBoost 0.1   WU  0.906542  0.879592  0.868009  0.832618\n",
              "3         Logreg 0.1   WU  1.000000  0.891837  0.871671  0.772532\n",
              "4            SVM 0.1   WU  0.847059  0.885714  0.885246  0.927039\n",
              "5      LSTM beta 0.1   WU  1.000000  0.890591  0.879808  0.785408\n",
              "6       GRU beta 0.1   WU  0.796154  0.827133  0.839757  0.888412\n",
              "7   XGBoost beta 0.1   WU  0.932642  0.855580  0.845070  0.772532\n",
              "8    logreg beta 0.1   WU  1.000000  0.914661  0.908665  0.832618\n",
              "9       svm beta 0.1   WU  0.994949  0.919037  0.914153  0.845494\n",
              "0          LSTM 0.05   WU  0.901288  0.906122  0.901288  0.901288\n",
              "1           GRU 0.05   WU  0.857143  0.891837  0.890722  0.927039\n",
              "2       XGBoost 0.05   WU  0.809917  0.830612  0.825263  0.841202\n",
              "3        Logreg 0.05   WU  0.937500  0.895918  0.884354  0.836910\n",
              "4           SVM 0.05   WU  0.784946  0.848980  0.855469  0.939914\n",
              "5     LSTM beta 0.05   WU  1.000000  0.899344  0.890476  0.802575\n",
              "6      GRU beta 0.05   WU  0.791667  0.827133  0.841046  0.896996\n",
              "7  XGBoost beta 0.05   WU  0.962963  0.873085  0.862559  0.781116\n",
              "8   logreg beta 0.05   WU  1.000000  0.916849  0.911215  0.836910\n",
              "9      svm beta 0.05   WU  0.980100  0.912473  0.907834  0.845494"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3CUgwa9aT_T"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK9sWU9XFn7o"
      },
      "source": [
        "## AMT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "o-uELrKBFn7y",
        "outputId": "c131d22f-3b2d-42fc-e50e-1f78cbbfb002"
      },
      "source": [
        "dfs = pd.read_csv(\"AMT.csv\")\n",
        "# dfs = dfs[::-1].reset_index()\n",
        "# Denoise prices\n",
        "dfs = denoise_data(dfs, '<CLOSE>', k=25)\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>US1.AMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>51.59</td>\n",
              "      <td>51.84</td>\n",
              "      <td>50.855</td>\n",
              "      <td>50.9896</td>\n",
              "      <td>2776426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>US1.AMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>51.43</td>\n",
              "      <td>52.34</td>\n",
              "      <td>51.390</td>\n",
              "      <td>50.9896</td>\n",
              "      <td>2281804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>US1.AMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>51.94</td>\n",
              "      <td>52.05</td>\n",
              "      <td>50.310</td>\n",
              "      <td>50.9896</td>\n",
              "      <td>3568106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>US1.AMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>51.03</td>\n",
              "      <td>51.63</td>\n",
              "      <td>50.310</td>\n",
              "      <td>50.9896</td>\n",
              "      <td>3668859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US1.AMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>50.53</td>\n",
              "      <td>50.56</td>\n",
              "      <td>50.130</td>\n",
              "      <td>50.9896</td>\n",
              "      <td>2086997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2762</th>\n",
              "      <td>US1.AMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>280.62</td>\n",
              "      <td>280.71</td>\n",
              "      <td>273.670</td>\n",
              "      <td>289.3756</td>\n",
              "      <td>59047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2763</th>\n",
              "      <td>US1.AMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>271.08</td>\n",
              "      <td>271.81</td>\n",
              "      <td>266.860</td>\n",
              "      <td>289.3756</td>\n",
              "      <td>76212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>US1.AMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>269.10</td>\n",
              "      <td>271.24</td>\n",
              "      <td>268.440</td>\n",
              "      <td>289.3756</td>\n",
              "      <td>43260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>US1.AMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>271.80</td>\n",
              "      <td>272.14</td>\n",
              "      <td>265.300</td>\n",
              "      <td>289.3756</td>\n",
              "      <td>50333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>US1.AMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>265.44</td>\n",
              "      <td>268.98</td>\n",
              "      <td>265.440</td>\n",
              "      <td>289.3756</td>\n",
              "      <td>33744</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2767 rows Ã— 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     <TICKER> <PER>    <DATE>  <TIME>  ...  <HIGH>    <LOW>   <CLOSE>    <VOL>\n",
              "0     US1.AMT     D  20101004       0  ...   51.84   50.855   50.9896  2776426\n",
              "1     US1.AMT     D  20101005       0  ...   52.34   51.390   50.9896  2281804\n",
              "2     US1.AMT     D  20101006       0  ...   52.05   50.310   50.9896  3568106\n",
              "3     US1.AMT     D  20101007       0  ...   51.63   50.310   50.9896  3668859\n",
              "4     US1.AMT     D  20101008       0  ...   50.56   50.130   50.9896  2086997\n",
              "...       ...   ...       ...     ...  ...     ...      ...       ...      ...\n",
              "2762  US1.AMT     D  20210927       0  ...  280.71  273.670  289.3756    59047\n",
              "2763  US1.AMT     D  20210928       0  ...  271.81  266.860  289.3756    76212\n",
              "2764  US1.AMT     D  20210929       0  ...  271.24  268.440  289.3756    43260\n",
              "2765  US1.AMT     D  20210930       0  ...  272.14  265.300  289.3756    50333\n",
              "2766  US1.AMT     D  20211001       0  ...  268.98  265.440  289.3756    33744\n",
              "\n",
              "[2767 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFsUev3JFn7z",
        "outputId": "3d629208-2fd4-4057-903c-2864fe16a2aa"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.05, 100, 2300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"3adcbd0f-31fc-4544-9297-5417c01c1f84\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"3adcbd0f-31fc-4544-9297-5417c01c1f84\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '3adcbd0f-31fc-4544-9297-5417c01c1f84',\n",
              "                        [{\"type\": \"scatter\", \"x\": [20110225, 20110228, 20110301, 20110302, 20110303, 20110304, 20110307, 20110308, 20110309, 20110310, 20110311, 20110314, 20110315, 20110316, 20110317, 20110318, 20110321, 20110322, 20110323, 20110324, 20110325, 20110328, 20110329, 20110330, 20110331, 20110401, 20110404, 20110405, 20110406, 20110407, 20110408, 20110411, 20110412, 20110413, 20110414, 20110415, 20110418, 20110419, 20110420, 20110421, 20110425, 20110426, 20110427, 20110428, 20110429, 20110502, 20110503, 20110504, 20110505, 20110506, 20110509, 20110510, 20110511, 20110512, 20110513, 20110516, 20110517, 20110518, 20110519, 20110520, 20110523, 20110524, 20110525, 20110526, 20110527, 20110531, 20110601, 20110602, 20110603, 20110606, 20110607, 20110608, 20110609, 20110610, 20110613, 20110614, 20110615, 20110616, 20110617, 20110620, 20110621, 20110622, 20110623, 20110624, 20110627, 20110628, 20110629, 20110630, 20110701, 20110705, 20110706, 20110707, 20110708, 20110711, 20110712, 20110713, 20110714, 20110715, 20110718, 20110719, 20110720, 20110721, 20110722, 20110725, 20110726, 20110727, 20110728, 20110729, 20110801, 20110802, 20110803, 20110804, 20110805, 20110808, 20110809, 20110810, 20110811, 20110812, 20110815, 20110816, 20110817, 20110818, 20110819, 20110822, 20110823, 20110824, 20110825, 20110826, 20110829, 20110830, 20110831, 20110901, 20110902, 20110906, 20110907, 20110908, 20110909, 20110912, 20110913, 20110914, 20110915, 20110916, 20110919, 20110920, 20110921, 20110922, 20110923, 20110926, 20110927, 20110928, 20110929, 20110930, 20111003, 20111004, 20111005, 20111006, 20111007, 20111010, 20111011, 20111012, 20111013, 20111014, 20111017, 20111018, 20111019, 20111020, 20111021, 20111024, 20111025, 20111026, 20111027, 20111028, 20111031, 20111101, 20111102, 20111103, 20111104, 20111107, 20111108, 20111109, 20111110, 20111111, 20111114, 20111115, 20111116, 20111117, 20111118, 20111121, 20111122, 20111123, 20111125, 20111128, 20111129, 20111130, 20111201, 20111202, 20111205, 20111206, 20111207, 20111208, 20111209, 20111212, 20111213, 20111214, 20111215, 20111216, 20111219, 20111220, 20111221, 20111222, 20111223, 20111227, 20111228, 20111229, 20111230, 20120103, 20120104, 20120105, 20120106, 20120109, 20120110, 20120111, 20120112, 20120113, 20120117, 20120118, 20120119, 20120120, 20120123, 20120124, 20120125, 20120126, 20120127, 20120130, 20120131, 20120201, 20120202, 20120203, 20120206, 20120207, 20120208, 20120209, 20120210, 20120213, 20120214, 20120215, 20120216, 20120217, 20120221, 20120222, 20120223, 20120224, 20120227, 20120228, 20120229, 20120301, 20120302, 20120305, 20120306, 20120307, 20120308, 20120309, 20120312, 20120313, 20120314, 20120315, 20120316, 20120319, 20120320, 20120321, 20120322, 20120323, 20120326, 20120327, 20120328, 20120329, 20120330, 20120402, 20120403, 20120404, 20120405, 20120409, 20120410, 20120411, 20120412, 20120413, 20120416, 20120417, 20120418, 20120419, 20120420, 20120423, 20120424, 20120425, 20120426, 20120427, 20120430, 20120501, 20120502, 20120503, 20120504, 20120507, 20120508, 20120509, 20120510, 20120511, 20120514, 20120515, 20120516, 20120517, 20120518, 20120521, 20120522, 20120523, 20120524, 20120525, 20120529, 20120530, 20120531, 20120601, 20120604, 20120605, 20120606, 20120607, 20120608, 20120611, 20120612, 20120613, 20120614, 20120615, 20120618, 20120619, 20120620, 20120621, 20120622, 20120625, 20120626, 20120627, 20120628, 20120629, 20120702, 20120703, 20120705, 20120706, 20120709, 20120710, 20120711, 20120712, 20120713, 20120716, 20120717, 20120718, 20120719, 20120720, 20120723, 20120724, 20120725, 20120726, 20120727, 20120730, 20120731, 20120801, 20120802, 20120803, 20120806, 20120807, 20120808, 20120809, 20120810, 20120813, 20120814, 20120815, 20120816, 20120817, 20120820, 20120821, 20120822, 20120823, 20120824, 20120827, 20120828, 20120829, 20120830, 20120831, 20120904, 20120905, 20120906, 20120907, 20120910, 20120911, 20120912, 20120913, 20120914, 20120917, 20120918, 20120919, 20120920, 20120921, 20120924, 20120925, 20120926, 20120927, 20120928, 20121001, 20121002, 20121003, 20121004, 20121005, 20121008, 20121009, 20121010, 20121011, 20121012, 20121015, 20121016, 20121017, 20121018, 20121019, 20121022, 20121023, 20121024, 20121025, 20121026, 20121031, 20121101, 20121102, 20121105, 20121106, 20121107, 20121108, 20121109, 20121112, 20121113, 20121114, 20121115, 20121116, 20121119, 20121120, 20121121, 20121123, 20121126, 20121127, 20121128, 20121129, 20121130, 20121203, 20121204, 20121205, 20121206, 20121207, 20121210, 20121211, 20121212, 20121213, 20121214, 20121217, 20121218, 20121219, 20121220, 20121221, 20121224, 20121226, 20121227, 20121228, 20121231, 20130102, 20130103, 20130104, 20130107, 20130108, 20130109, 20130110, 20130111, 20130114, 20130115, 20130116, 20130117, 20130118, 20130122, 20130123, 20130124, 20130125, 20130128, 20130129, 20130130, 20130131, 20130201, 20130204, 20130205, 20130206, 20130207, 20130208, 20130211, 20130212, 20130213, 20130214, 20130215, 20130219, 20130220, 20130221, 20130222, 20130225, 20130226, 20130227, 20130228, 20130301, 20130304, 20130305, 20130306, 20130307, 20130308, 20130311, 20130312, 20130313, 20130314, 20130315, 20130318, 20130319, 20130320, 20130321, 20130322, 20130325, 20130326, 20130327, 20130328, 20130401, 20130402, 20130403, 20130404, 20130405, 20130408, 20130409, 20130410, 20130411, 20130412, 20130415, 20130416, 20130417, 20130418, 20130419, 20130422, 20130423, 20130424, 20130425, 20130426, 20130429, 20130430, 20130501, 20130502, 20130503, 20130506, 20130507, 20130508, 20130509, 20130510, 20130513, 20130514, 20130515, 20130516, 20130517, 20130520, 20130521, 20130522, 20130523, 20130524, 20130528, 20130529, 20130530, 20130531, 20130603, 20130604, 20130605, 20130606, 20130607, 20130610, 20130611, 20130612, 20130613, 20130614, 20130617, 20130618, 20130619, 20130620, 20130621, 20130624, 20130625, 20130626, 20130627, 20130628, 20130701, 20130702, 20130703, 20130705, 20130708, 20130709, 20130710, 20130711, 20130712, 20130715, 20130716, 20130717, 20130718, 20130719, 20130722, 20130723, 20130724, 20130725, 20130726, 20130729, 20130730, 20130731, 20130801, 20130802, 20130805, 20130806, 20130807, 20130808, 20130809, 20130812, 20130813, 20130814, 20130815, 20130816, 20130819, 20130820, 20130821, 20130822, 20130823, 20130826, 20130827, 20130828, 20130829, 20130830, 20130903, 20130904, 20130905, 20130906, 20130909, 20130910, 20130911, 20130912, 20130913, 20130916, 20130917, 20130918, 20130919, 20130920, 20130923, 20130924, 20130925, 20130926, 20130927, 20130930, 20131001, 20131002, 20131003, 20131004, 20131007, 20131008, 20131009, 20131010, 20131011, 20131014, 20131015, 20131016, 20131017, 20131018, 20131021, 20131022, 20131023, 20131024, 20131025, 20131028, 20131029, 20131030, 20131031, 20131101, 20131104, 20131105, 20131106, 20131107, 20131108, 20131111, 20131112, 20131113, 20131114, 20131115, 20131118, 20131119, 20131120, 20131121, 20131122, 20131125, 20131126, 20131127, 20131129, 20131202, 20131203, 20131204, 20131205, 20131206, 20131209, 20131210, 20131211, 20131212, 20131213, 20131216, 20131217, 20131218, 20131219, 20131220, 20131223, 20131224, 20131226, 20131227, 20131230, 20131231, 20140102, 20140103, 20140106, 20140107, 20140108, 20140109, 20140110, 20140113, 20140114, 20140115, 20140116, 20140117, 20140121, 20140122, 20140123, 20140124, 20140127, 20140128, 20140129, 20140130, 20140131, 20140203, 20140204, 20140205, 20140206, 20140207, 20140210, 20140211, 20140212, 20140213, 20140214, 20140218, 20140219, 20140220, 20140221, 20140224, 20140225, 20140226, 20140227, 20140228, 20140303, 20140304, 20140305, 20140306, 20140307, 20140310, 20140311, 20140312, 20140313, 20140314, 20140317, 20140318, 20140319, 20140320, 20140321, 20140324, 20140325, 20140326, 20140327, 20140328, 20140331, 20140401, 20140402, 20140403, 20140404, 20140407, 20140408, 20140409, 20140410, 20140411, 20140414, 20140415, 20140416, 20140417, 20140421, 20140422, 20140423, 20140424, 20140425, 20140428, 20140429, 20140430, 20140501, 20140502, 20140505, 20140506, 20140507, 20140508, 20140509, 20140512, 20140513, 20140514, 20140515, 20140516, 20140519, 20140520, 20140521, 20140522, 20140523, 20140527, 20140528, 20140529, 20140530, 20140602, 20140603, 20140604, 20140605, 20140606, 20140609, 20140610, 20140611, 20140612, 20140613, 20140616, 20140617, 20140618, 20140619, 20140620, 20140623, 20140624, 20140625, 20140626, 20140627, 20140630, 20140701, 20140702, 20140703, 20140707, 20140708, 20140709, 20140710, 20140711, 20140714, 20140715, 20140716, 20140717, 20140718, 20140721, 20140722, 20140723, 20140724, 20140725, 20140728, 20140729, 20140730, 20140731, 20140801, 20140804, 20140805, 20140806, 20140807, 20140808, 20140811, 20140812, 20140813, 20140814, 20140815, 20140818, 20140819, 20140820, 20140821, 20140822, 20140825, 20140826, 20140827, 20140828, 20140829, 20140902, 20140903, 20140904, 20140905, 20140908, 20140909, 20140910, 20140911, 20140912, 20140915, 20140916, 20140917, 20140918, 20140919, 20140922, 20140923, 20140924, 20140925, 20140926, 20140929, 20140930, 20141001, 20141002, 20141003, 20141006, 20141007, 20141008, 20141009, 20141010, 20141013, 20141014, 20141015, 20141016, 20141017, 20141020, 20141021, 20141022, 20141023, 20141024, 20141027, 20141028, 20141029, 20141030, 20141031, 20141103, 20141104, 20141105, 20141106, 20141107, 20141110, 20141111, 20141112, 20141113, 20141114, 20141117, 20141118, 20141119, 20141120, 20141121, 20141124, 20141125, 20141126, 20141128, 20141201, 20141202, 20141203, 20141204, 20141205, 20141208, 20141209, 20141210, 20141211, 20141212, 20141215, 20141216, 20141217, 20141218, 20141219, 20141222, 20141223, 20141224, 20141226, 20141229, 20141230, 20141231, 20150102, 20150105, 20150106, 20150107, 20150108, 20150109, 20150112, 20150113, 20150114, 20150115, 20150116, 20150120, 20150121, 20150122, 20150123, 20150126, 20150127, 20150128, 20150129, 20150130, 20150202, 20150203, 20150204, 20150205, 20150206, 20150209, 20150210, 20150211, 20150212, 20150213, 20150217, 20150218, 20150219, 20150220, 20150223, 20150224, 20150225, 20150226, 20150227, 20150302, 20150303, 20150304, 20150305, 20150306, 20150309, 20150310, 20150311, 20150312, 20150313, 20150316, 20150317, 20150318, 20150319, 20150320, 20150323, 20150324, 20150325, 20150326, 20150327, 20150330, 20150331, 20150401, 20150402, 20150406, 20150407, 20150408, 20150409, 20150410, 20150413, 20150414, 20150415, 20150416, 20150417, 20150420, 20150421, 20150422, 20150423, 20150424, 20150427, 20150428, 20150429, 20150430, 20150501, 20150504, 20150505, 20150506, 20150507, 20150508, 20150511, 20150512, 20150513, 20150514, 20150515, 20150518, 20150519, 20150520, 20150521, 20150522, 20150526, 20150527, 20150528, 20150529, 20150601, 20150602, 20150603, 20150604, 20150605, 20150608, 20150609, 20150610, 20150611, 20150612, 20150615, 20150616, 20150617, 20150618, 20150619, 20150622, 20150623, 20150624, 20150625, 20150626, 20150629, 20150630, 20150701, 20150702, 20150706, 20150707, 20150708, 20150709, 20150710, 20150713, 20150714, 20150715, 20150716, 20150717, 20150720, 20150721, 20150722, 20150723, 20150724, 20150727, 20150728, 20150729, 20150730, 20150731, 20150803, 20150804, 20150805, 20150806, 20150807, 20150810, 20150811, 20150812, 20150813, 20150814, 20150817, 20150818, 20150819, 20150820, 20150821, 20150824, 20150825, 20150826, 20150827, 20150828, 20150831, 20150901, 20150902, 20150903, 20150904, 20150908, 20150909, 20150910, 20150911, 20150914, 20150915, 20150916, 20150917, 20150918, 20150921, 20150922, 20150923, 20150924, 20150925, 20150928, 20150929, 20150930, 20151001, 20151002, 20151005, 20151006, 20151007, 20151008, 20151009, 20151012, 20151013, 20151014, 20151015, 20151016, 20151019, 20151020, 20151021, 20151022, 20151023, 20151026, 20151027, 20151028, 20151029, 20151030, 20151102, 20151103, 20151104, 20151105, 20151106, 20151109, 20151110, 20151111, 20151112, 20151113, 20151116, 20151117, 20151118, 20151119, 20151120, 20151123, 20151124, 20151125, 20151127, 20151130, 20151201, 20151202, 20151203, 20151204, 20151207, 20151208, 20151209, 20151210, 20151211, 20151214, 20151215, 20151216, 20151217, 20151218, 20151221, 20151222, 20151223, 20151224, 20151228, 20151229, 20151230, 20151231, 20160104, 20160105, 20160106, 20160107, 20160108, 20160111, 20160112, 20160113, 20160114, 20160115, 20160119, 20160120, 20160121, 20160122, 20160125, 20160126, 20160127, 20160128, 20160129, 20160201, 20160202, 20160203, 20160204, 20160205, 20160208, 20160209, 20160210, 20160211, 20160212, 20160216, 20160217, 20160218, 20160219, 20160222, 20160223, 20160224, 20160225, 20160226, 20160229, 20160301, 20160302, 20160303, 20160304, 20160307, 20160308, 20160309, 20160310, 20160311, 20160314, 20160315, 20160316, 20160317, 20160318, 20160321, 20160322, 20160323, 20160324, 20160328, 20160329, 20160330, 20160331, 20160401, 20160404, 20160405, 20160406, 20160407, 20160408, 20160411, 20160412, 20160413, 20160414, 20160415, 20160418, 20160419, 20160420, 20160421, 20160422, 20160425, 20160426, 20160427, 20160428, 20160429, 20160502, 20160503, 20160504, 20160505, 20160506, 20160509, 20160510, 20160511, 20160512, 20160513, 20160516, 20160517, 20160518, 20160519, 20160520, 20160523, 20160524, 20160525, 20160526, 20160527, 20160531, 20160601, 20160602, 20160603, 20160606, 20160607, 20160608, 20160609, 20160610, 20160613, 20160614, 20160615, 20160616, 20160617, 20160620, 20160621, 20160622, 20160623, 20160624, 20160627, 20160628, 20160629, 20160630, 20160701, 20160705, 20160706, 20160707, 20160708, 20160711, 20160712, 20160713, 20160714, 20160715, 20160718, 20160719, 20160720, 20160721, 20160722, 20160725, 20160726, 20160727, 20160728, 20160729, 20160801, 20160802, 20160803, 20160804, 20160805, 20160808, 20160809, 20160810, 20160811, 20160812, 20160815, 20160816, 20160817, 20160818, 20160819, 20160822, 20160823, 20160824, 20160825, 20160826, 20160829, 20160830, 20160831, 20160901, 20160902, 20160906, 20160907, 20160908, 20160909, 20160912, 20160913, 20160914, 20160915, 20160916, 20160919, 20160920, 20160921, 20160922, 20160923, 20160926, 20160927, 20160928, 20160929, 20160930, 20161003, 20161004, 20161005, 20161006, 20161007, 20161010, 20161011, 20161012, 20161013, 20161014, 20161017, 20161018, 20161019, 20161020, 20161021, 20161024, 20161025, 20161026, 20161027, 20161028, 20161031, 20161101, 20161102, 20161103, 20161104, 20161107, 20161108, 20161109, 20161110, 20161111, 20161114, 20161115, 20161116, 20161117, 20161118, 20161121, 20161122, 20161123, 20161125, 20161128, 20161129, 20161130, 20161201, 20161202, 20161205, 20161206, 20161207, 20161208, 20161209, 20161212, 20161213, 20161214, 20161215, 20161216, 20161219, 20161220, 20161221, 20161222, 20161223, 20161227, 20161228, 20161229, 20161230, 20170103, 20170104, 20170105, 20170106, 20170109, 20170110, 20170111, 20170112, 20170113, 20170117, 20170118, 20170119, 20170120, 20170123, 20170124, 20170125, 20170126, 20170127, 20170130, 20170131, 20170201, 20170202, 20170203, 20170206, 20170207, 20170208, 20170209, 20170210, 20170213, 20170214, 20170215, 20170216, 20170217, 20170221, 20170222, 20170223, 20170224, 20170227, 20170228, 20170301, 20170302, 20170303, 20170306, 20170307, 20170308, 20170309, 20170310, 20170313, 20170314, 20170315, 20170316, 20170317, 20170320, 20170321, 20170322, 20170323, 20170324, 20170327, 20170328, 20170329, 20170330, 20170331, 20170403, 20170404, 20170405, 20170406, 20170407, 20170410, 20170411, 20170412, 20170413, 20170417, 20170418, 20170419, 20170420, 20170421, 20170424, 20170425, 20170426, 20170427, 20170428, 20170501, 20170502, 20170503, 20170504, 20170505, 20170508, 20170509, 20170510, 20170511, 20170512, 20170515, 20170516, 20170517, 20170518, 20170519, 20170522, 20170523, 20170524, 20170525, 20170526, 20170530, 20170531, 20170601, 20170602, 20170605, 20170606, 20170607, 20170608, 20170609, 20170612, 20170613, 20170614, 20170615, 20170616, 20170619, 20170620, 20170621, 20170622, 20170623, 20170626, 20170627, 20170628, 20170629, 20170630, 20170703, 20170705, 20170706, 20170707, 20170710, 20170711, 20170712, 20170713, 20170714, 20170717, 20170718, 20170719, 20170720, 20170721, 20170724, 20170725, 20170726, 20170727, 20170728, 20170731, 20170801, 20170802, 20170803, 20170804, 20170807, 20170808, 20170809, 20170810, 20170811, 20170814, 20170815, 20170816, 20170817, 20170818, 20170821, 20170822, 20170823, 20170824, 20170825, 20170828, 20170829, 20170830, 20170831, 20170901, 20170905, 20170906, 20170907, 20170908, 20170911, 20170912, 20170913, 20170914, 20170915, 20170918, 20170919, 20170920, 20170921, 20170922, 20170925, 20170926, 20170927, 20170928, 20170929, 20171002, 20171003, 20171004, 20171005, 20171006, 20171009, 20171010, 20171011, 20171012, 20171013, 20171016, 20171017, 20171018, 20171019, 20171020, 20171023, 20171024, 20171025, 20171026, 20171027, 20171030, 20171031, 20171101, 20171102, 20171103, 20171106, 20171107, 20171108, 20171109, 20171110, 20171113, 20171114, 20171115, 20171116, 20171117, 20171120, 20171121, 20171122, 20171124, 20171127, 20171128, 20171129, 20171130, 20171201, 20171204, 20171205, 20171206, 20171207, 20171208, 20171211, 20171212, 20171213, 20171214, 20171215, 20171218, 20171219, 20171220, 20171221, 20171222, 20171226, 20171227, 20171228, 20171229, 20180102, 20180103, 20180104, 20180105, 20180108, 20180109, 20180110, 20180111, 20180112, 20180116, 20180117, 20180118, 20180119, 20180122, 20180123, 20180124, 20180125, 20180126, 20180129, 20180130, 20180131, 20180201, 20180202, 20180205, 20180206, 20180207, 20180208, 20180209, 20180212, 20180213, 20180214, 20180215, 20180216, 20180220, 20180221, 20180222, 20180223, 20180226, 20180227, 20180228, 20180301, 20180302, 20180305, 20180306, 20180307, 20180308, 20180309, 20180312, 20180313, 20180314, 20180315, 20180316, 20180319, 20180320, 20180321, 20180322, 20180323, 20180326, 20180327, 20180328, 20180329, 20180402, 20180403, 20180404, 20180405, 20180406, 20180409, 20180410, 20180411, 20180412, 20180413, 20180416, 20180417, 20180418, 20180419, 20180420, 20180423, 20180424, 20180425, 20180426, 20180427, 20180430, 20180501, 20180502, 20180503, 20180504, 20180507, 20180508, 20180509, 20180510, 20180511, 20180514, 20180515, 20180516, 20180517, 20180518, 20180521, 20180522, 20180523, 20180524, 20180525, 20180529, 20180530, 20180531, 20180601, 20180604, 20180605, 20180606, 20180607, 20180608, 20180611, 20180612, 20180613, 20180614, 20180615, 20180618, 20180619, 20180620, 20180621, 20180622, 20180625, 20180626, 20180627, 20180628, 20180629, 20180702, 20180703, 20180705, 20180706, 20180709, 20180710, 20180711, 20180712, 20180713, 20180716, 20180717, 20180718, 20180719, 20180720, 20180723, 20180724, 20180725, 20180726, 20180727, 20180730, 20180731, 20180801, 20180802, 20180803, 20180806, 20180807, 20180808, 20180809, 20180810, 20180813, 20180814, 20180815, 20180816, 20180817, 20180820, 20180821, 20180822, 20180823, 20180824, 20180827, 20180828, 20180829, 20180830, 20180831, 20180904, 20180905, 20180906, 20180907, 20180910, 20180911, 20180912, 20180913, 20180914, 20180917, 20180918, 20180919, 20180920, 20180921, 20180924, 20180925, 20180926, 20180927, 20180928, 20181001, 20181002, 20181003, 20181004, 20181005, 20181008, 20181009, 20181010, 20181011, 20181012, 20181015, 20181016, 20181017, 20181018, 20181019, 20181022, 20181023, 20181024, 20181025, 20181026, 20181029, 20181030, 20181031, 20181101, 20181102, 20181105, 20181106, 20181107, 20181108, 20181109, 20181112, 20181113, 20181114, 20181115, 20181116, 20181119, 20181120, 20181121, 20181123, 20181126, 20181127, 20181128, 20181129, 20181130, 20181203, 20181204, 20181206, 20181207, 20181210, 20181211, 20181212, 20181213, 20181214, 20181217, 20181218, 20181219, 20181220, 20181221, 20181224, 20181226, 20181227, 20181228, 20181231, 20190102, 20190103, 20190104, 20190107, 20190108, 20190109, 20190110, 20190111, 20190114, 20190115, 20190116, 20190117, 20190118, 20190122, 20190123, 20190124, 20190125, 20190128, 20190129, 20190130, 20190131, 20190201, 20190204, 20190205, 20190206, 20190207, 20190208, 20190211, 20190212, 20190213, 20190214, 20190215, 20190219, 20190220, 20190221, 20190222, 20190225, 20190226, 20190227, 20190228, 20190301, 20190304, 20190305, 20190306, 20190307, 20190308, 20190311, 20190312, 20190313, 20190314, 20190315, 20190318, 20190319, 20190320, 20190321, 20190322, 20190325, 20190326, 20190327, 20190328, 20190329, 20190401, 20190402, 20190403, 20190404, 20190405, 20190408, 20190409, 20190410, 20190411, 20190412, 20190415, 20190416, 20190417, 20190418, 20190422, 20190423, 20190424, 20190425, 20190426, 20190429, 20190430, 20190501, 20190502, 20190503, 20190506, 20190507, 20190508, 20190509, 20190510, 20190513, 20190514, 20190515, 20190516, 20190517, 20190520, 20190521, 20190522, 20190523, 20190524, 20190528, 20190529, 20190530, 20190531, 20190603, 20190604, 20190605, 20190606, 20190607, 20190610, 20190611, 20190612, 20190613, 20190614, 20190617, 20190618, 20190619, 20190620, 20190621, 20190624, 20190625, 20190626, 20190627, 20190628, 20190701, 20190702, 20190703, 20190705, 20190708, 20190709, 20190710, 20190711, 20190712, 20190715, 20190716, 20190717, 20190718, 20190719, 20190722, 20190723, 20190724, 20190725, 20190726, 20190729, 20190730, 20190731, 20190801, 20190802, 20190805, 20190806, 20190807, 20190808, 20190809, 20190812, 20190813, 20190814, 20190815, 20190816, 20190819, 20190820, 20190821, 20190822, 20190823, 20190826, 20190827, 20190828, 20190829, 20190830, 20190903, 20190904, 20190905, 20190906, 20190909, 20190910, 20190911, 20190912, 20190913, 20190916, 20190917, 20190918, 20190919, 20190920, 20190923, 20190924, 20190925, 20190926, 20190927, 20190930, 20191001, 20191002, 20191003, 20191004, 20191007, 20191008, 20191009, 20191010, 20191011, 20191014, 20191015, 20191016, 20191017, 20191018, 20191021, 20191022, 20191023, 20191024, 20191025, 20191028, 20191029, 20191030, 20191031, 20191101, 20191104, 20191105, 20191106, 20191107, 20191108, 20191111, 20191112, 20191113, 20191114, 20191115, 20191118, 20191119, 20191120], \"y\": [53.3204, 53.16759999999999, 53.0248, 52.84919999999999, 52.443999999999996, 52.1524, 51.898, 51.635999999999996, 51.4352, 51.264799999999994, 51.166, 51.16079999999999, 51.1204, 51.061600000000006, 50.982000000000006, 50.902648, 50.84864400000001, 50.746244, 50.63464399999999, 50.515443999999995, 50.415444, 50.343444, 50.320644, 50.31024399999999, 50.282244, 50.263044, 50.288644, 50.299844, 50.31824399999999, 50.526644000000005, 50.69464400000001, 50.812244, 50.94064399999999, 51.015043999999996, 51.115044, 51.163444, 51.21224399999999, 51.235043999999995, 51.266244, 51.33624399999999, 51.44639600000001, 51.5428, 51.6304, 51.7204, 51.8024, 51.90399999999999, 51.972, 52.1092, 52.21360000000001, 52.397200000000005, 52.5884, 52.754000000000005, 52.89800000000001, 53.074000000000005, 53.1668, 53.226800000000004, 53.1792, 53.1576, 53.1484, 53.0536, 52.9796, 52.8608, 52.7684, 52.674400000000006, 52.554, 52.41879999999999, 52.294799999999995, 52.20440000000001, 52.174400000000006, 52.165200000000006, 52.1792, 52.167199999999994, 52.0964, 52.0488, 51.974399999999996, 51.8944, 51.857200000000006, 51.80480000000001, 51.736000000000004, 51.7184, 51.6824, 51.75679999999999, 51.803599999999996, 51.876000000000005, 51.964000000000006, 52.098, 52.1876, 52.3, 52.372, 52.4664, 52.59400000000001, 52.696000000000005, 52.778, 52.756, 52.782, 52.783199999999994, 52.8112, 52.757200000000005, 52.824000000000005, 52.77319999999999, 52.6808, 52.3636, 52.1888, 51.9332, 51.75959999999999, 51.627599999999994, 51.56360000000001, 51.4884, 51.398, 51.239200000000004, 51.062, 50.92800000000001, 50.842, 50.800399999999996, 50.6984, 50.63960000000001, 50.618399999999994, 50.635600000000004, 50.7396, 50.8046, 50.8274, 50.8518, 50.97180000000001, 50.961400000000005, 51.0158, 51.10819999999999, 51.369400000000006, 51.5662, 51.8658, 52.0882, 52.2882, 52.4366, 52.559, 52.619, 52.775, 52.9646, 53.193400000000004, 53.3222, 53.4058, 53.5366, 53.576600000000006, 53.6474, 53.7222, 53.7926, 53.8204, 53.93280000000001, 54.0216, 54.0992, 54.16879999999999, 54.3072, 54.41799999999999, 54.5292, 54.562400000000004, 54.57, 54.6272, 54.709599999999995, 54.7412, 54.802, 54.9804, 55.141200000000005, 55.202799999999996, 55.295199999999994, 55.4036, 55.557199999999995, 55.724000000000004, 55.944, 56.11679999999999, 56.202799999999996, 56.244400000000006, 56.37440000000001, 56.4324, 56.556400000000004, 56.625600000000006, 56.6976, 56.73519999999999, 56.7704, 56.78320000000001, 56.8016, 56.834219999999995, 56.84182, 56.861819999999994, 56.983019999999996, 57.102219999999996, 57.19262, 57.30662, 57.47222000000001, 57.56742, 57.61382, 57.66022, 57.668620000000004, 57.66022000000001, 57.646220000000014, 57.71022, 57.80941999999999, 57.771820000000005, 57.86861999999999, 57.90862000000001, 58.022220000000004, 58.16182, 58.32022, 58.48542, 58.66902, 58.85462, 58.972, 59.0976, 59.2012, 59.25199999999999, 59.324000000000005, 59.41400000000001, 59.537600000000005, 59.666399999999996, 59.787200000000006, 59.946799999999996, 60.1004, 60.26240000000001, 60.39679999999999, 60.55039999999999, 60.6976, 60.85280000000001, 61.050399999999996, 61.1796, 61.318000000000005, 61.4572, 61.574, 61.67400000000001, 61.805195999999995, 61.90679600000001, 62.039596, 62.24079600000001, 62.405596, 62.524395999999996, 62.673995999999995, 62.833596, 62.923595999999996, 62.935596000000004, 62.937996000000005, 62.96439600000001, 63.021596, 63.08079600000001, 63.157996000000004, 63.222395999999996, 63.240396000000004, 63.24519599999999, 63.247595999999994, 63.264396, 63.259196, 63.232396, 63.180796, 63.144796, 63.103595999999996, 63.0492, 62.998400000000004, 62.982, 62.955200000000005, 62.937999999999995, 62.9592, 62.91480000000001, 62.8376, 62.77439999999999, 62.742, 62.732400000000005, 62.696799999999996, 62.656400000000005, 62.595600000000005, 62.5508, 62.586800000000004, 62.62559999999999, 62.6284, 62.64320000000001, 62.6308, 62.5784, 62.615199999999994, 62.6796, 62.708, 62.7692, 62.8636, 62.95360000000001, 62.9868, 63.02079999999999, 63.012, 62.9996, 63.07320000000001, 63.16959999999999, 63.294399999999996, 63.4488, 63.57640000000001, 63.730199999999996, 63.9226, 64.0842, 64.253, 64.4026, 64.5534, 64.749, 64.917, 65.1138, 65.36099999999999, 65.5094, 65.549, 65.5994, 65.6698, 65.69859999999998, 65.743184, 65.825184, 65.895384, 66.023384, 66.090184, 66.095584, 66.026784, 65.94558400000001, 65.885984, 65.899584, 65.87618400000001, 65.846184, 65.81498400000001, 65.782984, 65.72218400000001, 65.730584, 65.737384, 65.778584, 65.85058400000001, 65.878984, 65.891784, 66.014984, 66.132984, 66.25578399999999, 66.438184, 66.6168, 66.7788, 67.0082, 67.2102, 67.4514, 67.6884, 68.0356, 68.3084, 68.5526, 68.7642, 69.03379999999999, 69.2734, 69.5686, 69.71660000000001, 69.88900000000001, 70.03659999999999, 70.1234, 70.1438, 70.157, 70.259, 70.4666, 70.63619999999999, 70.8342, 70.9546, 71.0562, 71.1914, 71.295, 71.2558, 71.2014, 71.1626, 71.15100000000001, 71.1266, 71.18579999999999, 71.226, 71.2244, 71.1892, 71.14920000000001, 71.036, 71.0256, 70.9734, 70.9146, 70.8982, 70.9122, 70.929, 70.8924, 70.8148, 70.7616, 70.7064, 70.70840000000001, 70.6984, 70.6324, 70.602, 70.65920000000001, 70.76920000000001, 70.8132, 70.8404, 70.83640000000001, 70.80760000000001, 70.79360000000001, 70.7692, 70.7564, 70.708004, 70.730404, 70.776804, 70.863404, 70.928604, 71.015404, 71.14100400000001, 71.228604, 71.337804, 71.423004, 71.45740400000001, 71.52140399999999, 71.523404, 71.533404, 71.691004, 71.811804, 71.925804, 72.035804, 72.13580400000001, 72.229404, 72.322204, 72.435804, 72.535004, 72.653004, 72.828204, 73.0082, 73.157, 73.2746, 73.3694, 73.53659999999999, 73.6498, 73.73780000000001, 73.8494, 73.906, 73.9432, 73.9708, 74.0244, 74.10600000000001, 74.16879999999999, 74.1196, 74.0892, 74.0716, 74.0188, 74.0328, 74.08760000000001, 74.1468, 74.2028, 74.26280000000001, 74.312, 74.3, 74.3344, 74.3596, 74.4092, 74.5068, 74.54, 74.6276, 74.71, 74.7932, 74.869, 75.0198, 75.1974, 75.3222, 75.4262, 75.5278, 75.59899999999999, 75.7294, 75.9158, 76.0326, 76.1458, 76.2282, 76.32260000000001, 76.435, 76.5882, 76.7554, 76.90429999999999, 77.0543, 77.1983, 77.3443, 77.4703, 77.5967, 77.6887, 77.79790000000001, 77.92150000000001, 78.0313, 78.0873, 78.09410000000001, 78.0601, 78.0737, 78.0805, 78.11330000000001, 78.0605, 77.9345, 77.9313, 77.9049, 77.8497, 77.7189, 77.5505, 77.4105, 77.24329999999999, 77.0544, 76.83800000000001, 76.6708, 76.4632, 76.318, 76.24520000000001, 76.19040000000001, 76.1304, 76.0892, 76.08, 76.0352, 76.0612, 76.1008, 76.1108, 76.0932, 76.0684, 76.0864, 76.0748, 75.9948, 75.9208, 75.9112, 76.0028, 76.1324, 76.1768, 76.22800000000001, 76.344, 76.4688, 76.5976, 76.7992, 76.8908, 76.93679999999999, 76.9452, 76.9792, 76.99560000000001, 77.044, 77.1564, 77.2796, 77.3712, 77.5188, 77.6692, 77.82759999999999, 78.01400000000001, 78.23240000000001, 78.5064, 78.7656, 79.106, 79.3724, 79.6248, 79.9316, 80.25319999999999, 80.5316, 80.79759999999999, 81.0116, 81.204, 81.46960000000001, 81.6688, 81.8772, 82.0364, 82.2148, 82.39280000000001, 82.5104, 82.6608, 82.84120000000001, 83.00399999999999, 83.0992, 83.162, 83.18879999999999, 83.208, 83.11040000000001, 83.02120000000001, 82.7808, 82.5584, 82.3016, 81.9776, 81.7596, 81.55720000000001, 81.2728, 80.9804, 80.6232, 80.29799999999999, 80.0496, 79.8064, 79.57839999999999, 79.206, 78.6512, 78.20120000000001, 77.66720000000001, 77.2256, 76.7936, 76.4768, 76.138, 75.75, 75.37559999999999, 75.0924, 74.8076, 74.57560000000001, 74.3944, 74.36, 74.4156, 74.3896, 74.2848, 74.21560000000001, 74.13959999999999, 74.1336, 74.07839999999999, 73.958, 73.87, 73.7372, 73.6528, 73.73759999999999, 73.7552, 73.8176, 73.7336, 73.6292, 73.48, 73.3916, 73.3204, 73.19760000000001, 73.06160000000001, 73.00200000000001, 72.9076, 72.7636, 72.5224, 72.2128, 71.84240000000001, 71.53, 71.3172, 71.12280000000001, 70.9004, 70.6968, 70.5212, 70.334, 70.1404, 70.0348, 69.91080000000001, 69.7812, 69.6456, 69.5632, 69.59079999999999, 69.74080000000001, 69.886, 70.07719999999999, 70.2796, 70.4692, 70.6336, 70.80351999999999, 71.06312, 71.26632, 71.44312000000001, 71.61872, 71.79871999999999, 71.97431999999999, 72.19792, 72.42352, 72.58832000000001, 72.75352000000001, 72.90912000000002, 73.08232000000001, 73.20232000000001, 73.34512, 73.45312, 73.54952, 73.74352, 73.84352, 73.85792, 73.86392, 73.94832000000001, 74.15552, 74.36712, 74.57592000000001, 74.824, 74.95420000000001, 75.1778, 75.4542, 75.749, 76.0466, 76.32260000000001, 76.51180000000001, 76.7062, 76.8994, 77.0806, 77.3038, 77.51859999999999, 77.7218, 77.8982, 78.14020000000001, 78.3994, 78.6202, 78.8134, 78.9718, 79.1058, 79.145, 79.0758, 79.00619999999999, 78.90620000000001, 78.82220000000001, 78.79639999999999, 78.6912, 78.5868, 78.48200000000001, 78.3668, 78.24759999999999, 78.196, 78.176, 78.184, 78.1512, 78.0376, 77.95599999999999, 77.8892, 77.84679999999999, 77.85640000000001, 77.85439999999998, 77.8092, 77.7748, 77.8104, 77.8408, 77.8692, 77.9576, 78.0584, 78.156, 78.2216, 78.294396, 78.434396, 78.583596, 78.759596, 78.96919600000001, 79.14439599999999, 79.327996, 79.502396, 79.665196, 79.879196, 80.156796, 80.424796, 80.71239599999998, 80.84159600000001, 80.88399599999998, 80.982396, 81.04559599999999, 81.155996, 81.211196, 81.210796, 81.223996, 81.192396, 81.191196, 81.228796, 81.302396, 81.37, 81.3908, 81.4112, 81.45360000000001, 81.4888, 81.5552, 81.63, 81.66199999999999, 81.65079999999999, 81.53120000000001, 81.4192, 81.3396, 81.268, 81.3412, 81.4324, 81.442, 81.5368, 81.5408, 81.5536, 81.6688, 81.7356, 81.7904, 81.816, 81.87, 81.8808, 81.83279999999999, 81.8108, 81.756, 81.6648, 81.5928, 81.4752, 81.3212, 81.2432, 81.1988, 81.2564, 81.2728, 81.264, 81.2348, 81.20400000000001, 81.2028, 81.2452, 81.23, 81.2016, 81.2068, 81.2056, 81.26800000000001, 81.35920000000002, 81.46520000000001, 81.5532, 81.64320000000001, 81.75880000000001, 81.832, 81.9272, 82.0116, 82.0828, 82.3, 82.554, 82.80560000000001, 83.0192, 83.29039999999999, 83.5708, 83.8664, 84.1672, 84.4812, 84.772, 85.04440000000001, 85.3368, 85.62800000000001, 85.89640000000001, 86.16400000000002, 86.40679999999999, 86.63680000000001, 86.8748, 87.08479999999997, 87.2984, 87.542, 87.75479999999999, 87.96480000000001, 88.2052, 88.45379999999999, 88.62099999999998, 88.72740000000002, 88.76499999999999, 88.8274, 88.8218, 88.8294, 88.8002, 88.79820000000001, 88.77980000000001, 88.77860000000001, 88.79620000000001, 88.805, 88.86139999999999, 88.9118, 88.92220000000002, 88.9454, 89.0078, 89.03579999999998, 89.0578, 89.0834, 89.12539999999998, 89.20379999999999, 89.29060000000001, 89.38695200000001, 89.463952, 89.525952, 89.604352, 89.714752, 89.79715199999998, 89.966752, 90.103552, 90.301152, 90.49075199999999, 90.663552, 90.80235199999998, 90.933952, 91.06555199999998, 91.34675200000001, 91.551952, 91.781952, 92.04275200000001, 92.23355199999997, 92.47155199999999, 92.77195199999998, 93.09575199999999, 93.38815199999999, 93.64295199999998, 93.89495199999999, 94.14059999999999, 94.40339999999998, 94.71539999999999, 95.0122, 95.27420000000001, 95.5978, 95.8246, 96.1074, 96.3382, 96.5702, 96.82619999999999, 97.08140000000002, 97.3366, 97.5666, 97.6594, 97.8678, 98.0662, 98.18579999999999, 98.29579999999999, 98.38260000000002, 98.3334, 98.21600000000001, 98.11399999999999, 98.0692, 98.01679999999999, 97.92079999999999, 97.76440000000001, 97.5308, 97.3368, 97.1164, 96.89719999999997, 96.71400000000001, 96.5024, 96.2956, 96.08480000000002, 95.8884, 95.7, 95.48119999999999, 95.368, 95.21600000000002, 95.02, 94.7688, 94.56119999999999, 94.3396, 94.12400400000003, 94.01000400000001, 93.96640399999998, 93.970404, 93.94520399999999, 93.94360399999998, 93.984804, 94.06040399999998, 94.158404, 94.19360399999998, 94.42560399999998, 94.58870399999999, 94.719504, 94.84910400000001, 95.017104, 95.25910400000001, 95.482304, 95.700304, 95.94710399999998, 96.124304, 96.33070400000001, 96.52430400000001, 96.785504, 97.07830399999999, 97.433504, 97.76350000000001, 98.12109999999998, 98.4571, 98.8023, 99.19149999999998, 99.5591, 99.8507, 100.0807, 100.29349999999998, 100.55429999999998, 100.6183, 100.74840000000002, 100.9068, 101.04, 101.13160000000002, 101.09360000000001, 101.0132, 100.87240000000001, 100.77879999999999, 100.734, 100.71440000000001, 100.75920000000002, 100.8016, 100.8048, 100.79719999999999, 100.7948, 100.7248, 100.58040000000001, 100.3984, 100.14120000000001, 99.8576, 99.6768, 99.59400000000001, 99.50320000000002, 99.39920000000001, 99.2932, 99.15280000000001, 98.9736, 98.86120000000001, 98.7492, 98.6744, 98.72879999999999, 98.8804, 99.06479999999999, 99.13680000000001, 99.11399999999999, 99.0248, 98.8776, 98.76119999999999, 98.69840000000002, 98.58760000000001, 98.5572, 98.4312, 98.3132, 98.2368, 98.15719999999999, 98.0368, 97.8976, 97.7696, 97.65839999999999, 97.55320000000002, 97.5232, 97.56400000000002, 97.45199999999998, 97.4808, 97.5516, 97.56559999999999, 97.55720000000001, 97.418, 97.34440000000001, 97.3576, 97.28399999999999, 97.2692, 97.13840000000002, 96.96119999999999, 96.8976, 96.68319999999999, 96.6384, 96.53479999999999, 96.49960000000002, 96.446, 96.51599999999999, 96.56559999999999, 96.60400000000001, 96.5748, 96.5728, 96.55320000000002, 96.50640000000001, 96.49119999999999, 96.4368, 96.36800000000001, 96.3232, 96.22200000000001, 96.15360000000001, 96.02400000000002, 95.85799999999999, 95.78559999999999, 95.7016, 95.7252, 95.81, 95.80040000000001, 95.86399999999999, 95.8808, 95.9532, 95.97679999999998, 96.00599999999999, 95.90679999999999, 95.8316, 95.68440000000001, 95.64599599999998, 95.57519600000002, 95.51919600000001, 95.39319599999999, 95.33999599999999, 95.225196, 95.15159599999998, 94.97479599999998, 94.84239600000001, 94.69719599999999, 94.67639600000001, 94.69119599999998, 94.706396, 94.744396, 94.71079600000002, 94.595596, 94.52839600000001, 94.479596, 94.48639599999998, 94.432796, 94.302796, 94.25759599999999, 94.214796, 94.09279599999999, 94.14079600000001, 94.0608, 94.0376, 93.93519999999998, 93.9232, 93.94599999999998, 93.92840000000002, 93.8216, 93.8088, 93.83200000000001, 93.96199999999999, 94.00919999999998, 94.02359999999999, 94.0672, 94.054, 94.0712, 94.0852, 94.02999999999999, 93.9776, 93.93039999999999, 93.94320000000002, 94.02959999999999, 94.0872, 94.1024, 94.16879999999999, 94.19760000000001, 94.324, 94.4296, 94.5848, 94.7896, 94.92880000000001, 95.08320000000002, 95.2696, 95.3996, 95.55439999999999, 95.5792, 95.60040000000001, 95.6352, 95.6384, 95.63159999999999, 95.6472, 95.692, 95.81880000000001, 95.95439999999999, 96.1296, 96.31960000000001, 96.53960000000001, 96.7544, 96.97240000000002, 97.20759999999999, 97.43440000000001, 97.6652, 97.86480000000002, 98.04319999999998, 98.08480000000002, 98.07440000000001, 97.87000000000002, 97.57440000000001, 97.3732, 97.20720000000001, 97.07240000000002, 96.9008, 96.6352, 96.4276, 96.26960000000001, 96.0252, 95.81160000000001, 95.5152, 95.18039999999999, 94.81480000000002, 94.38240000000002, 93.9452, 93.52879999999999, 93.1908, 92.79960000000001, 92.37280000000001, 91.876, 91.40559999999999, 90.9092, 90.5076, 90.1292, 89.9324, 89.8688, 89.7428, 89.57759999999999, 89.5784, 89.6248, 89.7924, 89.9692, 90.1276, 90.3576, 90.5056, 90.6792, 90.9332, 91.2092, 91.52640000000001, 91.8216, 92.03960000000001, 92.35679999999998, 92.70519999999999, 93.06679999999999, 93.47679999999998, 93.96240000000002, 94.5208, 95.05759999999998, 95.73119999999999, 96.3824, 96.95559999999998, 97.45559999999998, 97.78759999999998, 97.9348, 98.0864, 98.22200000000001, 98.294, 98.3516, 98.48240000000001, 98.6684, 98.93360000000001, 99.13199999999999, 99.3048, 99.4312, 99.52480000000001, 99.64560000000002, 99.65440000000001, 99.664, 99.7316, 99.7388, 99.6188, 99.44040000000001, 99.2592, 98.97200000000001, 98.662, 98.3732, 98.0848, 97.9272, 97.866, 97.8792, 97.8188, 97.7084, 97.67559999999999, 97.60960000000001, 97.6156, 97.55359999999999, 97.50439999999999, 97.4612, 97.38719999999999, 97.29440000000001, 97.20199999999998, 97.1596, 97.11319999999999, 96.89839999999998, 96.68720000000002, 96.552692, 96.43269199999999, 96.210292, 96.02189200000001, 95.73509200000001, 95.56749199999999, 95.38109200000001, 95.21869199999999, 95.12429199999998, 94.87029199999999, 94.722292, 94.63749199999998, 94.51269199999999, 94.43149199999999, 94.21509199999998, 93.94349199999999, 93.62789199999997, 93.29669199999998, 92.92549200000002, 92.463892, 92.01748800000001, 91.506288, 90.92068799999998, 90.515488, 90.20428799999999, 89.871996, 89.591596, 89.39679600000001, 89.27979599999999, 89.26499600000001, 89.18979599999999, 89.20779599999997, 89.206996, 89.164596, 89.30579599999999, 89.39379599999998, 89.506196, 89.644596, 89.693396, 89.75339600000001, 89.88139600000001, 90.049396, 90.34259600000001, 90.71259599999999, 91.23339600000001, 91.80220000000001, 92.43780000000001, 93.1134, 93.69980000000001, 94.2414, 94.77739999999999, 95.29179999999998, 95.81139999999999, 96.3196, 96.89200000000001, 97.4284, 98.03040000000001, 98.61959999999999, 99.09479999999999, 99.536, 99.9516, 100.37, 100.7732, 101.19599999999998, 101.66120000000001, 102.0864, 102.5292, 102.8116, 103.10879999999999, 103.36480000000002, 103.4744, 103.5732, 103.7192, 103.8968, 104.07879999999999, 104.258796, 104.44039599999998, 104.650396, 104.739196, 104.86679600000001, 105.021996, 105.10479600000001, 105.18199599999998, 105.306396, 105.30759600000002, 105.37159600000001, 105.37039600000001, 105.389196, 105.357596, 105.290396, 105.23359599999999, 105.162796, 105.110396, 105.102796, 105.156396, 105.26599600000002, 105.340396, 105.40519600000002, 105.43359599999998, 105.515196, 105.6308, 105.7228, 105.81720000000001, 105.9532, 106.09679999999999, 106.19720000000001, 106.2928, 106.3316, 106.336, 106.414, 106.4296, 106.4984, 106.61759999999998, 106.778, 107.054, 107.23440000000001, 107.42999999999998, 107.6936, 108.0016, 108.29940000000002, 108.6302, 108.9402, 109.2042, 109.473, 109.7574, 110.06379999999999, 110.4206, 110.74539999999999, 111.12180000000001, 111.4302, 111.7298, 112.0658, 112.47019999999999, 112.87979999999999, 113.28540000000001, 113.6814, 114.06299999999999, 114.38740000000001, 114.6782, 114.85060000000001, 115.20139999999999, 115.5166, 115.71419999999998, 115.85219599999999, 115.970796, 116.05719599999999, 116.141996, 116.29519599999999, 116.44399600000001, 116.55839600000002, 116.60519599999999, 116.52559600000001, 116.467196, 116.36199599999998, 116.233996, 116.16719600000002, 116.08079599999999, 115.953596, 115.823596, 115.641596, 115.520396, 115.369596, 115.20239600000001, 115.08919600000002, 115.06279599999999, 115.065596, 115.06879599999998, 115.08919599999999, 114.8404, 114.62800000000001, 114.32319999999999, 114.01880000000001, 113.7348, 113.42040000000001, 113.1296, 112.84199999999998, 112.68079999999998, 112.58999999999997, 112.53559999999997, 112.50039999999998, 112.41879999999999, 112.31620000000001, 112.2738, 112.2362, 112.195, 112.07019999999999, 111.84020000000001, 111.64540000000001, 111.43299999999999, 111.20620000000001, 110.8794, 110.66660000000002, 110.525, 110.65019999999998, 110.73259999999999, 110.97399999999999, 111.25359999999999, 111.4816, 111.788, 112.0872, 112.41239999999999, 112.6372, 112.7634, 112.88939999999998, 113.0694, 113.17420000000001, 113.18520000000001, 113.1764, 113.16279999999999, 113.28, 113.4876, 113.55680000000001, 113.44519999999999, 113.33359999999999, 113.0592, 112.8424, 112.5984, 112.3124, 111.9824, 111.6856, 111.44059999999999, 111.0994, 110.79899999999999, 110.4162, 109.99099999999999, 109.412572, 108.822572, 108.309172, 107.74477200000001, 107.089372, 106.570372, 106.14897200000001, 105.734972, 105.36897200000001, 105.00657200000002, 104.629372, 104.504572, 104.50737199999999, 104.557372, 104.734172, 104.857372, 104.904572, 104.924172, 104.97137199999999, 104.96857200000001, 104.903372, 104.859772, 104.81377200000001, 104.821772, 104.816972, 104.93660000000001, 105.09900000000002, 105.1358, 105.20819999999998, 105.34119999999999, 105.391, 105.50119999999998, 105.61040000000001, 105.6348, 105.55680000000001, 105.488, 105.428, 105.3524, 105.24599999999998, 105.1192, 104.9752, 104.85079999999999, 104.74080000000002, 104.65439999999998, 104.6304, 104.51960000000001, 104.4308, 104.35640000000001, 104.33359999999999, 104.3152, 104.40639999999999, 104.47640000000001, 104.6208, 104.77560000000001, 104.93279999999999, 105.2404, 105.4752, 105.73640000000002, 106.0704, 106.43520000000001, 106.8346, 107.2058, 107.64140000000002, 108.09460000000001, 108.5318, 108.9998, 109.3898, 109.7774, 110.15739999999998, 110.5186, 110.93459999999999, 111.40700000000001, 111.839, 112.2454, 112.6642, 113.031, 113.4922, 113.965, 114.431, 114.905, 115.27500000000002, 115.67739999999999, 116.02619999999999, 116.3758, 116.69220000000001, 116.93160000000002, 117.20280000000001, 117.4896, 117.74879999999999, 118.05760000000001, 118.3796, 118.7424, 119.12800000000001, 119.55320000000002, 119.9768, 120.40280000000001, 120.764, 121.1372, 121.4532, 121.77080000000002, 122.01360000000001, 122.3172, 122.59839999999998, 122.928, 123.24600000000001, 123.50120000000001, 123.7544, 124.0636, 124.35560000000001, 124.63040000000001, 124.884, 125.1092, 125.2732, 125.5052, 125.74119999999999, 126.0624, 126.2888, 126.5168, 126.6956, 126.85119999999999, 127.03560000000002, 127.302, 127.56880000000001, 127.9004, 128.1916, 128.5516, 128.8124, 129.038, 129.1992, 129.35199999999998, 129.518, 129.6308, 129.6524, 129.7052, 129.80200000000002, 129.9536, 130.1816, 130.43280000000001, 130.5688, 130.66000000000003, 130.66400000000002, 130.8184, 130.9836, 131.2664, 131.498, 131.6408, 131.69400000000002, 131.70680000000002, 131.71200000000002, 131.7288, 131.7812, 131.7336, 131.7112, 131.8036, 131.9004, 132.10479999999998, 132.3124, 132.5936, 132.80399999999997, 133.03439999999998, 133.2752, 133.484, 133.65480000000002, 133.9136, 134.084, 134.3852, 134.56959999999998, 134.71399999999997, 134.8048, 134.9092, 135.0976, 135.3256, 135.5348, 135.7592, 135.96839999999997, 136.1112, 136.4584, 136.802, 137.08800000000002, 137.312, 137.47039999999998, 137.7028, 137.93800000000002, 138.2348, 138.53879999999998, 138.8328, 139.178, 139.5492, 139.9376, 140.4636, 140.75639999999999, 141.1764, 141.57999999999998, 141.93720000000002, 142.25119999999998, 142.59439999999998, 142.8688, 143.1492, 143.34959999999998, 143.67600000000002, 144.0, 144.04399999999998, 144.0736, 144.00719999999998, 143.92399999999998, 143.82039999999998, 143.62959999999998, 143.3356, 143.0644, 142.7764, 142.43959999999998, 142.07479999999998, 141.79399999999998, 141.468, 141.1076, 140.7948, 140.40040000000002, 140.072, 139.79319999999998, 139.5936, 139.3308, 139.0992, 138.8528, 138.67600000000002, 138.43720000000002, 138.1968, 138.0784, 137.95600000000002, 137.9168, 137.97279999999998, 138.1908, 138.48, 138.7216, 138.9764, 139.1952, 139.7684, 140.3252, 140.8804, 141.37840000000003, 141.8388, 142.3244, 142.8204, 143.2612, 143.5964, 143.832, 144.09879999999998, 144.4012, 144.7144, 145.0956, 145.4636, 145.858, 146.18800000000002, 146.4532, 146.6584, 146.65480000000002, 146.57080000000002, 146.4968, 146.63, 146.63279999999997, 146.70960000000002, 146.4432, 146.1684, 145.80440000000002, 145.44240000000002, 145.13400000000001, 144.806, 144.43439999999998, 144.05960000000002, 143.75, 143.5492, 143.3616, 143.2128, 143.1076, 142.8326, 142.56580000000002, 142.2058, 141.97379999999998, 141.89700000000002, 141.793, 141.7374, 141.505, 141.1526, 140.813, 140.5778, 140.22299999999998, 139.987, 139.8058, 139.7066, 139.66660000000002, 139.6722, 139.803, 139.9682, 140.1914, 140.47379999999998, 140.72060000000002, 140.8658, 140.8306, 140.731, 140.6864, 140.3664, 140.22320000000002, 140.0524, 139.82479999999998, 139.60119999999998, 139.54840000000002, 139.7516, 140.00039999999998, 140.0744, 140.17, 140.438, 140.64399999999998, 140.6996, 140.626, 140.36720000000003, 140.0068, 139.6728, 139.4548, 139.24960000000002, 139.05, 138.91039999999998, 138.90680000000003, 139.07600000000002, 139.32920000000001, 139.59519999999998, 140.11, 140.4736, 140.8776, 141.1148, 141.4204, 141.6588, 141.8668, 142.0776, 142.35080000000002, 142.602, 142.67680000000001, 142.62640000000002, 142.7216, 142.912, 143.186, 143.5564, 143.5796, 143.6212, 143.5552, 143.4016, 143.20159999999998, 143.03879999999998, 142.78160000000003, 142.41559999999998, 142.016, 141.686, 141.358, 140.93560000000002, 140.71880000000002, 140.40679999999998, 140.1744, 139.77120000000002, 139.4496, 139.18200000000002, 138.92520000000002, 138.70080000000002, 138.52120000000002, 138.3192, 138.1512, 138.0184, 137.82039999999998, 137.78799999999998, 137.6492, 137.5888, 137.5148, 137.4672, 137.3116, 137.1916, 137.1164, 137.1036, 137.0156, 137.05200000000002, 137.1508, 137.216, 137.40879999999999, 137.5084, 137.6636, 137.7308, 137.7688, 137.8, 137.9184, 137.89600000000002, 137.9128, 137.8604, 137.83720000000002, 137.8684, 138.0296, 138.2384, 138.48239999999998, 138.6856, 138.9172, 139.1676, 139.46400000000003, 139.81119999999999, 140.1448, 140.4264, 140.7392, 141.0416, 141.19, 141.32920000000001, 141.5092, 141.6984, 141.9012, 142.0288, 142.12879999999998, 142.1428, 142.35199999999998, 142.43359999999998, 142.52360000000002, 142.65679999999998, 142.8108, 142.9684, 143.0764, 143.0664, 143.3104, 143.59159999999997, 143.786, 144.0572, 144.326, 144.54, 144.79319999999998, 144.9888, 145.1244, 145.3968, 145.62, 145.866, 146.1048, 146.4232, 146.78279999999998, 147.0656, 147.3328, 147.56159999999997, 147.8692, 148.17079999999999, 148.512, 148.81279999999998, 148.9908, 149.18, 149.3968, 149.372, 149.28799999999998, 149.21959999999999, 149.0728, 148.918, 148.8096, 148.7824, 148.6472, 148.5896, 148.528, 148.4668, 148.43439999999998, 148.38479999999998, 148.1932, 147.99159999999998, 147.78119999999998, 147.59199999999998, 147.5084, 147.346, 147.1748, 146.926, 146.68560000000002, 146.5196, 146.3876, 146.3672, 146.2848, 145.98919999999998, 145.8044, 145.62040000000002, 145.5676, 145.54399999999998, 145.47, 145.5884, 145.6492, 145.784, 145.9588, 146.1496, 146.19400000000002, 146.4552, 147.0644, 147.538, 147.9216, 148.1924, 148.65800000000002, 149.1396, 149.6632, 150.2604, 150.8696, 151.4792, 152.0264, 152.7412, 153.59959999999998, 154.4824, 155.4216, 156.0188, 156.5648, 157.1072, 157.5652, 158.0784, 158.5264, 158.9596, 159.3964, 159.9372, 160.36159999999998, 160.62920000000003, 161.0532, 161.5804, 162.16279999999998, 162.52439999999999, 162.95159999999998, 163.3372, 163.402, 163.4924, 163.55519999999999, 163.562, 163.3488, 163.00560000000002, 162.74560000000002, 162.4412, 162.36800000000002, 162.2824, 162.136, 161.97639999999998, 161.8232, 161.71280000000002, 161.644, 161.4528, 161.4004, 161.3152, 161.0648, 160.93280000000001, 160.8352, 160.7452, 160.7416, 160.6572, 160.624, 160.81119999999999, 161.0156, 161.2332, 161.5516, 162.0488, 162.83, 163.34799999999998, 163.8396, 164.38279999999997, 164.86559999999997, 165.4772, 166.0704, 166.65960000000004, 167.19240000000002, 167.6972, 168.34240000000003, 168.984, 169.6332, 170.24640000000002, 170.78041199999998, 171.296812, 171.691212, 172.065612, 172.405612, 172.818412, 173.311212, 173.80161199999998, 174.323612, 174.813212, 175.263212, 175.60321199999998, 176.195212, 176.792012, 177.386812, 178.09401199999996, 178.772012, 179.43041200000005, 180.107212, 180.86521199999999, 181.70641200000003, 182.422012, 183.135612, 183.832012, 184.529212, 185.2672, 186.08319999999995, 186.81400000000002, 187.66, 188.50360000000003, 189.1816, 189.8936, 190.556, 191.18199999999996, 191.7796, 192.3748, 193.03079999999997, 193.4644, 193.7188, 193.87359999999998, 194.07199999999997, 194.206, 194.3908, 194.52359999999996, 194.55440000000002, 194.49439999999998, 194.4452, 194.42679999999996, 194.33759999999998, 194.2848, 194.2052, 194.0116, 193.90560000000002, 193.7692, 193.6984, 193.80399999999997, 193.84759999999997, 193.9216, 194.07160000000005, 194.28760000000003, 194.5172, 194.52599999999998, 194.6096, 194.94959999999995, 195.4216, 195.82559999999998, 196.2328, 196.53640000000004, 196.9968, 197.65560000000002, 198.29519999999997, 198.9168, 199.616, 200.4136, 201.19799999999998, 201.8748, 202.5448, 203.43040000000002, 204.12239999999997, 204.87080000000003, 205.60719999999998, 206.3084, 207.03279999999998, 207.7188, 208.24800000000002, 208.73559999999998, 209.2388, 209.5448, 209.75120000000004, 209.80720000000002, 209.77759999999998, 209.9044, 210.26, 210.43239999999997, 210.44080000000002, 210.52399999999997, 210.6656, 210.52679999999998, 210.3764, 210.308, 210.2868, 210.27879999999996, 210.146, 210.0112, 209.82920000000001, 209.5364, 209.1548, 208.726, 208.20559999999998, 207.8184, 207.5812, 207.61199999999997, 207.838, 208.22280000000003, 208.608, 209.1932, 209.6668, 210.11, 210.6384, 211.17079999999999, 211.56320000000002, 211.8576, 212.45559999999998, 213.018, 213.59599999999998, 214.25119999999998, 214.9968, 215.7904, 216.6336, 217.572, 218.4928, 219.50599999999997, 220.5036, 221.52200000000002, 222.81119999999999, 224.10519999999997, 225.07359999999997, 225.916, 226.26399999999998, 226.35559999999998, 226.18640000000002, 226.24039999999997, 226.01160000000004, 225.792, 225.7516, 225.76800000000003, 226.09599999999998, 226.1512, 226.2468, 226.24960000000002, 226.1052, 225.942, 225.562, 225.33520000000001, 224.9648, 224.5728, 224.3244, 224.14399999999998, 223.9508, 223.4812, 222.9052, 222.52640000000002, 222.20200000000006, 222.2616, 222.5756, 222.94480000000004, 223.22760000000002, 223.77720000000005, 224.2784, 224.4964, 224.6764, 224.5296, 224.1714, 223.5842, 223.21900000000002, 223.03140000000002, 222.82260000000002, 222.67260000000002, 222.27460000000002, 221.6798, 221.1994, 220.46779999999998, 219.66660000000005, 218.9506, 218.18220000000002, 217.5314, 216.954, 216.514, 216.06199999999998, 215.71920000000003, 215.55759999999998, 215.05880000000002, 214.296, 213.51319999999998, 213.0336, 212.58839999999998, 212.2132, 211.9318, 211.9494, 211.8402, 211.69060000000002, 211.4554, 211.3486]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('3adcbd0f-31fc-4544-9297-5417c01c1f84');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"e843f956-7cfc-459d-bf78-d96029a42cf5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"e843f956-7cfc-459d-bf78-d96029a42cf5\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'e843f956-7cfc-459d-bf78-d96029a42cf5',\n",
              "                        [{\"type\": \"scatter\", \"x\": [20110225, 20110228, 20110301, 20110302, 20110303, 20110304, 20110307, 20110308, 20110309, 20110310, 20110311, 20110314, 20110315, 20110316, 20110317, 20110318, 20110321, 20110322, 20110323, 20110324, 20110325, 20110328, 20110329, 20110330, 20110331, 20110401, 20110404, 20110405, 20110406, 20110407, 20110408, 20110411, 20110412, 20110413, 20110414, 20110415, 20110418, 20110419, 20110420, 20110421, 20110425, 20110426, 20110427, 20110428, 20110429, 20110502, 20110503, 20110504, 20110505, 20110506, 20110509, 20110510, 20110511, 20110512, 20110513, 20110516, 20110517, 20110518, 20110519, 20110520, 20110523, 20110524, 20110525, 20110526, 20110527, 20110531, 20110601, 20110602, 20110603, 20110606, 20110607, 20110608, 20110609, 20110610, 20110613, 20110614, 20110615, 20110616, 20110617, 20110620, 20110621, 20110622, 20110623, 20110624, 20110627, 20110628, 20110629, 20110630, 20110701, 20110705, 20110706, 20110707, 20110708, 20110711, 20110712, 20110713, 20110714, 20110715, 20110718, 20110719, 20110720, 20110721, 20110722, 20110725, 20110726, 20110727, 20110728, 20110729, 20110801, 20110802, 20110803, 20110804, 20110805, 20110808, 20110809, 20110810, 20110811, 20110812, 20110815, 20110816, 20110817, 20110818, 20110819, 20110822, 20110823, 20110824, 20110825, 20110826, 20110829, 20110830, 20110831, 20110901, 20110902, 20110906, 20110907, 20110908, 20110909, 20110912, 20110913, 20110914, 20110915, 20110916, 20110919, 20110920, 20110921, 20110922, 20110923, 20110926, 20110927, 20110928, 20110929, 20110930, 20111003, 20111004, 20111005, 20111006, 20111007, 20111010, 20111011, 20111012, 20111013, 20111014, 20111017, 20111018, 20111019, 20111020, 20111021, 20111024, 20111025, 20111026, 20111027, 20111028, 20111031, 20111101, 20111102, 20111103, 20111104, 20111107, 20111108, 20111109, 20111110, 20111111, 20111114, 20111115, 20111116, 20111117, 20111118, 20111121, 20111122, 20111123, 20111125, 20111128, 20111129, 20111130, 20111201, 20111202, 20111205, 20111206, 20111207, 20111208, 20111209, 20111212, 20111213, 20111214, 20111215, 20111216, 20111219, 20111220, 20111221, 20111222, 20111223, 20111227, 20111228, 20111229, 20111230, 20120103, 20120104, 20120105, 20120106, 20120109, 20120110, 20120111, 20120112, 20120113, 20120117, 20120118, 20120119, 20120120, 20120123, 20120124, 20120125, 20120126, 20120127, 20120130, 20120131, 20120201, 20120202, 20120203, 20120206, 20120207, 20120208, 20120209, 20120210, 20120213, 20120214, 20120215, 20120216, 20120217, 20120221, 20120222, 20120223, 20120224, 20120227, 20120228, 20120229, 20120301, 20120302, 20120305, 20120306, 20120307, 20120308, 20120309, 20120312, 20120313, 20120314, 20120315, 20120316, 20120319, 20120320, 20120321, 20120322, 20120323, 20120326, 20120327, 20120328, 20120329, 20120330, 20120402, 20120403, 20120404, 20120405, 20120409, 20120410, 20120411, 20120412, 20120413, 20120416, 20120417, 20120418, 20120419, 20120420, 20120423, 20120424, 20120425, 20120426, 20120427, 20120430, 20120501, 20120502, 20120503, 20120504, 20120507, 20120508, 20120509, 20120510, 20120511, 20120514, 20120515, 20120516, 20120517, 20120518, 20120521, 20120522, 20120523, 20120524, 20120525, 20120529, 20120530, 20120531, 20120601, 20120604, 20120605, 20120606, 20120607, 20120608, 20120611, 20120612, 20120613, 20120614, 20120615, 20120618, 20120619, 20120620, 20120621, 20120622, 20120625, 20120626, 20120627, 20120628, 20120629, 20120702, 20120703, 20120705, 20120706, 20120709, 20120710, 20120711, 20120712, 20120713, 20120716, 20120717, 20120718, 20120719, 20120720, 20120723, 20120724, 20120725, 20120726, 20120727, 20120730, 20120731, 20120801, 20120802, 20120803, 20120806, 20120807, 20120808, 20120809, 20120810, 20120813, 20120814, 20120815, 20120816, 20120817, 20120820, 20120821, 20120822, 20120823, 20120824, 20120827, 20120828, 20120829, 20120830, 20120831, 20120904, 20120905, 20120906, 20120907, 20120910, 20120911, 20120912, 20120913, 20120914, 20120917, 20120918, 20120919, 20120920, 20120921, 20120924, 20120925, 20120926, 20120927, 20120928, 20121001, 20121002, 20121003, 20121004, 20121005, 20121008, 20121009, 20121010, 20121011, 20121012, 20121015, 20121016, 20121017, 20121018, 20121019, 20121022, 20121023, 20121024, 20121025, 20121026, 20121031, 20121101, 20121102, 20121105, 20121106, 20121107, 20121108, 20121109, 20121112, 20121113, 20121114, 20121115, 20121116, 20121119, 20121120, 20121121, 20121123, 20121126, 20121127, 20121128, 20121129, 20121130, 20121203, 20121204, 20121205, 20121206, 20121207, 20121210, 20121211, 20121212, 20121213, 20121214, 20121217, 20121218, 20121219, 20121220, 20121221, 20121224, 20121226, 20121227, 20121228, 20121231, 20130102, 20130103, 20130104, 20130107, 20130108, 20130109, 20130110, 20130111, 20130114, 20130115, 20130116, 20130117, 20130118, 20130122, 20130123, 20130124, 20130125, 20130128, 20130129, 20130130, 20130131, 20130201, 20130204, 20130205, 20130206, 20130207, 20130208, 20130211, 20130212, 20130213, 20130214, 20130215, 20130219, 20130220, 20130221, 20130222, 20130225, 20130226, 20130227, 20130228, 20130301, 20130304, 20130305, 20130306, 20130307, 20130308, 20130311, 20130312, 20130313, 20130314, 20130315, 20130318, 20130319, 20130320, 20130321, 20130322, 20130325, 20130326, 20130327, 20130328, 20130401, 20130402, 20130403, 20130404, 20130405, 20130408, 20130409, 20130410, 20130411, 20130412, 20130415, 20130416, 20130417, 20130418, 20130419, 20130422, 20130423, 20130424, 20130425, 20130426, 20130429, 20130430, 20130501, 20130502, 20130503, 20130506, 20130507, 20130508, 20130509, 20130510, 20130513, 20130514, 20130515, 20130516, 20130517, 20130520, 20130521, 20130522, 20130523, 20130524, 20130528, 20130529, 20130530, 20130531, 20130603, 20130604, 20130605, 20130606, 20130607, 20130610, 20130611, 20130612, 20130613, 20130614, 20130617, 20130618, 20130619, 20130620, 20130621, 20130624, 20130625, 20130626, 20130627, 20130628, 20130701, 20130702, 20130703, 20130705, 20130708, 20130709, 20130710, 20130711, 20130712, 20130715, 20130716, 20130717, 20130718, 20130719, 20130722, 20130723, 20130724, 20130725, 20130726, 20130729, 20130730, 20130731, 20130801, 20130802, 20130805, 20130806, 20130807, 20130808, 20130809, 20130812, 20130813, 20130814, 20130815, 20130816, 20130819, 20130820, 20130821, 20130822, 20130823, 20130826, 20130827, 20130828, 20130829, 20130830, 20130903, 20130904, 20130905, 20130906, 20130909, 20130910, 20130911, 20130912, 20130913, 20130916, 20130917, 20130918, 20130919, 20130920, 20130923, 20130924, 20130925, 20130926, 20130927, 20130930, 20131001, 20131002, 20131003, 20131004, 20131007, 20131008, 20131009, 20131010, 20131011, 20131014, 20131015, 20131016, 20131017, 20131018, 20131021, 20131022, 20131023, 20131024, 20131025, 20131028, 20131029, 20131030, 20131031, 20131101, 20131104, 20131105, 20131106, 20131107, 20131108, 20131111, 20131112, 20131113, 20131114, 20131115, 20131118, 20131119, 20131120, 20131121, 20131122, 20131125, 20131126, 20131127, 20131129, 20131202, 20131203, 20131204, 20131205, 20131206, 20131209, 20131210, 20131211, 20131212, 20131213, 20131216, 20131217, 20131218, 20131219, 20131220, 20131223, 20131224, 20131226, 20131227, 20131230, 20131231, 20140102, 20140103, 20140106, 20140107, 20140108, 20140109, 20140110, 20140113, 20140114, 20140115, 20140116, 20140117, 20140121, 20140122, 20140123, 20140124, 20140127, 20140128, 20140129, 20140130, 20140131, 20140203, 20140204, 20140205, 20140206, 20140207, 20140210, 20140211, 20140212, 20140213, 20140214, 20140218, 20140219, 20140220, 20140221, 20140224, 20140225, 20140226, 20140227, 20140228, 20140303, 20140304, 20140305, 20140306, 20140307, 20140310, 20140311, 20140312, 20140313, 20140314, 20140317, 20140318, 20140319, 20140320, 20140321, 20140324, 20140325, 20140326, 20140327, 20140328, 20140331, 20140401, 20140402, 20140403, 20140404, 20140407, 20140408, 20140409, 20140410, 20140411, 20140414, 20140415, 20140416, 20140417, 20140421, 20140422, 20140423, 20140424, 20140425, 20140428, 20140429, 20140430, 20140501, 20140502, 20140505, 20140506, 20140507, 20140508, 20140509, 20140512, 20140513, 20140514, 20140515, 20140516, 20140519, 20140520, 20140521, 20140522, 20140523, 20140527, 20140528, 20140529, 20140530, 20140602, 20140603, 20140604, 20140605, 20140606, 20140609, 20140610, 20140611, 20140612, 20140613, 20140616, 20140617, 20140618, 20140619, 20140620, 20140623, 20140624, 20140625, 20140626, 20140627, 20140630, 20140701, 20140702, 20140703, 20140707, 20140708, 20140709, 20140710, 20140711, 20140714, 20140715, 20140716, 20140717, 20140718, 20140721, 20140722, 20140723, 20140724, 20140725, 20140728, 20140729, 20140730, 20140731, 20140801, 20140804, 20140805, 20140806, 20140807, 20140808, 20140811, 20140812, 20140813, 20140814, 20140815, 20140818, 20140819, 20140820, 20140821, 20140822, 20140825, 20140826, 20140827, 20140828, 20140829, 20140902, 20140903, 20140904, 20140905, 20140908, 20140909, 20140910, 20140911, 20140912, 20140915, 20140916, 20140917, 20140918, 20140919, 20140922, 20140923, 20140924, 20140925, 20140926, 20140929, 20140930, 20141001, 20141002, 20141003, 20141006, 20141007, 20141008, 20141009, 20141010, 20141013, 20141014, 20141015, 20141016, 20141017, 20141020, 20141021, 20141022, 20141023, 20141024, 20141027, 20141028, 20141029, 20141030, 20141031, 20141103, 20141104, 20141105, 20141106, 20141107, 20141110, 20141111, 20141112, 20141113, 20141114, 20141117, 20141118, 20141119, 20141120, 20141121, 20141124, 20141125, 20141126, 20141128, 20141201, 20141202, 20141203, 20141204, 20141205, 20141208, 20141209, 20141210, 20141211, 20141212, 20141215, 20141216, 20141217, 20141218, 20141219, 20141222, 20141223, 20141224, 20141226, 20141229, 20141230, 20141231, 20150102, 20150105, 20150106, 20150107, 20150108, 20150109, 20150112, 20150113, 20150114, 20150115, 20150116, 20150120, 20150121, 20150122, 20150123, 20150126, 20150127, 20150128, 20150129, 20150130, 20150202, 20150203, 20150204, 20150205, 20150206, 20150209, 20150210, 20150211, 20150212, 20150213, 20150217, 20150218, 20150219, 20150220, 20150223, 20150224, 20150225, 20150226, 20150227, 20150302, 20150303, 20150304, 20150305, 20150306, 20150309, 20150310, 20150311, 20150312, 20150313, 20150316, 20150317, 20150318, 20150319, 20150320, 20150323, 20150324, 20150325, 20150326, 20150327, 20150330, 20150331, 20150401, 20150402, 20150406, 20150407, 20150408, 20150409, 20150410, 20150413, 20150414, 20150415, 20150416, 20150417, 20150420, 20150421, 20150422, 20150423, 20150424, 20150427, 20150428, 20150429, 20150430, 20150501, 20150504, 20150505, 20150506, 20150507, 20150508, 20150511, 20150512, 20150513, 20150514, 20150515, 20150518, 20150519, 20150520, 20150521, 20150522, 20150526, 20150527, 20150528, 20150529, 20150601, 20150602, 20150603, 20150604, 20150605, 20150608, 20150609, 20150610, 20150611, 20150612, 20150615, 20150616, 20150617, 20150618, 20150619, 20150622, 20150623, 20150624, 20150625, 20150626, 20150629, 20150630, 20150701, 20150702, 20150706, 20150707, 20150708, 20150709, 20150710, 20150713, 20150714, 20150715, 20150716, 20150717, 20150720, 20150721, 20150722, 20150723, 20150724, 20150727, 20150728, 20150729, 20150730, 20150731, 20150803, 20150804, 20150805, 20150806, 20150807, 20150810, 20150811, 20150812, 20150813, 20150814, 20150817, 20150818, 20150819, 20150820, 20150821, 20150824, 20150825, 20150826, 20150827, 20150828, 20150831, 20150901, 20150902, 20150903, 20150904, 20150908, 20150909, 20150910, 20150911, 20150914, 20150915, 20150916, 20150917, 20150918, 20150921, 20150922, 20150923, 20150924, 20150925, 20150928, 20150929, 20150930, 20151001, 20151002, 20151005, 20151006, 20151007, 20151008, 20151009, 20151012, 20151013, 20151014, 20151015, 20151016, 20151019, 20151020, 20151021, 20151022, 20151023, 20151026, 20151027, 20151028, 20151029, 20151030, 20151102, 20151103, 20151104, 20151105, 20151106, 20151109, 20151110, 20151111, 20151112, 20151113, 20151116, 20151117, 20151118, 20151119, 20151120, 20151123, 20151124, 20151125, 20151127, 20151130, 20151201, 20151202, 20151203, 20151204, 20151207, 20151208, 20151209, 20151210, 20151211, 20151214, 20151215, 20151216, 20151217, 20151218, 20151221, 20151222, 20151223, 20151224, 20151228, 20151229, 20151230, 20151231, 20160104, 20160105, 20160106, 20160107, 20160108, 20160111, 20160112, 20160113, 20160114, 20160115, 20160119, 20160120, 20160121, 20160122, 20160125, 20160126, 20160127, 20160128, 20160129, 20160201, 20160202, 20160203, 20160204, 20160205, 20160208, 20160209, 20160210, 20160211, 20160212, 20160216, 20160217, 20160218, 20160219, 20160222, 20160223, 20160224, 20160225, 20160226, 20160229, 20160301, 20160302, 20160303, 20160304, 20160307, 20160308, 20160309, 20160310, 20160311, 20160314, 20160315, 20160316, 20160317, 20160318, 20160321, 20160322, 20160323, 20160324, 20160328, 20160329, 20160330, 20160331, 20160401, 20160404, 20160405, 20160406, 20160407, 20160408, 20160411, 20160412, 20160413, 20160414, 20160415, 20160418, 20160419, 20160420, 20160421, 20160422, 20160425, 20160426, 20160427, 20160428, 20160429, 20160502, 20160503, 20160504, 20160505, 20160506, 20160509, 20160510, 20160511, 20160512, 20160513, 20160516, 20160517, 20160518, 20160519, 20160520, 20160523, 20160524, 20160525, 20160526, 20160527, 20160531, 20160601, 20160602, 20160603, 20160606, 20160607, 20160608, 20160609, 20160610, 20160613, 20160614, 20160615, 20160616, 20160617, 20160620, 20160621, 20160622, 20160623, 20160624, 20160627, 20160628, 20160629, 20160630, 20160701, 20160705, 20160706, 20160707, 20160708, 20160711, 20160712, 20160713, 20160714, 20160715, 20160718, 20160719, 20160720, 20160721, 20160722, 20160725, 20160726, 20160727, 20160728, 20160729, 20160801, 20160802, 20160803, 20160804, 20160805, 20160808, 20160809, 20160810, 20160811, 20160812, 20160815, 20160816, 20160817, 20160818, 20160819, 20160822, 20160823, 20160824, 20160825, 20160826, 20160829, 20160830, 20160831, 20160901, 20160902, 20160906, 20160907, 20160908, 20160909, 20160912, 20160913, 20160914, 20160915, 20160916, 20160919, 20160920, 20160921, 20160922, 20160923, 20160926, 20160927, 20160928, 20160929, 20160930, 20161003, 20161004, 20161005, 20161006, 20161007, 20161010, 20161011, 20161012, 20161013, 20161014, 20161017, 20161018, 20161019, 20161020, 20161021, 20161024, 20161025, 20161026, 20161027, 20161028, 20161031, 20161101, 20161102, 20161103, 20161104, 20161107, 20161108, 20161109, 20161110, 20161111, 20161114, 20161115, 20161116, 20161117, 20161118, 20161121, 20161122, 20161123, 20161125, 20161128, 20161129, 20161130, 20161201, 20161202, 20161205, 20161206, 20161207, 20161208, 20161209, 20161212, 20161213, 20161214, 20161215, 20161216, 20161219, 20161220, 20161221, 20161222, 20161223, 20161227, 20161228, 20161229, 20161230, 20170103, 20170104, 20170105, 20170106, 20170109, 20170110, 20170111, 20170112, 20170113, 20170117, 20170118, 20170119, 20170120, 20170123, 20170124, 20170125, 20170126, 20170127, 20170130, 20170131, 20170201, 20170202, 20170203, 20170206, 20170207, 20170208, 20170209, 20170210, 20170213, 20170214, 20170215, 20170216, 20170217, 20170221, 20170222, 20170223, 20170224, 20170227, 20170228, 20170301, 20170302, 20170303, 20170306, 20170307, 20170308, 20170309, 20170310, 20170313, 20170314, 20170315, 20170316, 20170317, 20170320, 20170321, 20170322, 20170323, 20170324, 20170327, 20170328, 20170329, 20170330, 20170331, 20170403, 20170404, 20170405, 20170406, 20170407, 20170410, 20170411, 20170412, 20170413, 20170417, 20170418, 20170419, 20170420, 20170421, 20170424, 20170425, 20170426, 20170427, 20170428, 20170501, 20170502, 20170503, 20170504, 20170505, 20170508, 20170509, 20170510, 20170511, 20170512, 20170515, 20170516, 20170517, 20170518, 20170519, 20170522, 20170523, 20170524, 20170525, 20170526, 20170530, 20170531, 20170601, 20170602, 20170605, 20170606, 20170607, 20170608, 20170609, 20170612, 20170613, 20170614, 20170615, 20170616, 20170619, 20170620, 20170621, 20170622, 20170623, 20170626, 20170627, 20170628, 20170629, 20170630, 20170703, 20170705, 20170706, 20170707, 20170710, 20170711, 20170712, 20170713, 20170714, 20170717, 20170718, 20170719, 20170720, 20170721, 20170724, 20170725, 20170726, 20170727, 20170728, 20170731, 20170801, 20170802, 20170803, 20170804, 20170807, 20170808, 20170809, 20170810, 20170811, 20170814, 20170815, 20170816, 20170817, 20170818, 20170821, 20170822, 20170823, 20170824, 20170825, 20170828, 20170829, 20170830, 20170831, 20170901, 20170905, 20170906, 20170907, 20170908, 20170911, 20170912, 20170913, 20170914, 20170915, 20170918, 20170919, 20170920, 20170921, 20170922, 20170925, 20170926, 20170927, 20170928, 20170929, 20171002, 20171003, 20171004, 20171005, 20171006, 20171009, 20171010, 20171011, 20171012, 20171013, 20171016, 20171017, 20171018, 20171019, 20171020, 20171023, 20171024, 20171025, 20171026, 20171027, 20171030, 20171031, 20171101, 20171102, 20171103, 20171106, 20171107, 20171108, 20171109, 20171110, 20171113, 20171114, 20171115, 20171116, 20171117, 20171120, 20171121, 20171122, 20171124, 20171127, 20171128, 20171129, 20171130, 20171201, 20171204, 20171205, 20171206, 20171207, 20171208, 20171211, 20171212, 20171213, 20171214, 20171215, 20171218, 20171219, 20171220, 20171221, 20171222, 20171226, 20171227, 20171228, 20171229, 20180102, 20180103, 20180104, 20180105, 20180108, 20180109, 20180110, 20180111, 20180112, 20180116, 20180117, 20180118, 20180119, 20180122, 20180123, 20180124, 20180125, 20180126, 20180129, 20180130, 20180131, 20180201, 20180202, 20180205, 20180206, 20180207, 20180208, 20180209, 20180212, 20180213, 20180214, 20180215, 20180216, 20180220, 20180221, 20180222, 20180223, 20180226, 20180227, 20180228, 20180301, 20180302, 20180305, 20180306, 20180307, 20180308, 20180309, 20180312, 20180313, 20180314, 20180315, 20180316, 20180319, 20180320, 20180321, 20180322, 20180323, 20180326, 20180327, 20180328, 20180329, 20180402, 20180403, 20180404, 20180405, 20180406, 20180409, 20180410, 20180411, 20180412, 20180413, 20180416, 20180417, 20180418, 20180419, 20180420, 20180423, 20180424, 20180425, 20180426, 20180427, 20180430, 20180501, 20180502, 20180503, 20180504, 20180507, 20180508, 20180509, 20180510, 20180511, 20180514, 20180515, 20180516, 20180517, 20180518, 20180521, 20180522, 20180523, 20180524, 20180525, 20180529, 20180530, 20180531, 20180601, 20180604, 20180605, 20180606, 20180607, 20180608, 20180611, 20180612, 20180613, 20180614, 20180615, 20180618, 20180619, 20180620, 20180621, 20180622, 20180625, 20180626, 20180627, 20180628, 20180629, 20180702, 20180703, 20180705, 20180706, 20180709, 20180710, 20180711, 20180712, 20180713, 20180716, 20180717, 20180718, 20180719, 20180720, 20180723, 20180724, 20180725, 20180726, 20180727, 20180730, 20180731, 20180801, 20180802, 20180803, 20180806, 20180807, 20180808, 20180809, 20180810, 20180813, 20180814, 20180815, 20180816, 20180817, 20180820, 20180821, 20180822, 20180823, 20180824, 20180827, 20180828, 20180829, 20180830, 20180831, 20180904, 20180905, 20180906, 20180907, 20180910, 20180911, 20180912, 20180913, 20180914, 20180917, 20180918, 20180919, 20180920, 20180921, 20180924, 20180925, 20180926, 20180927, 20180928, 20181001, 20181002, 20181003, 20181004, 20181005, 20181008, 20181009, 20181010, 20181011, 20181012, 20181015, 20181016, 20181017, 20181018, 20181019, 20181022, 20181023, 20181024, 20181025, 20181026, 20181029, 20181030, 20181031, 20181101, 20181102, 20181105, 20181106, 20181107, 20181108, 20181109, 20181112, 20181113, 20181114, 20181115, 20181116, 20181119, 20181120, 20181121, 20181123, 20181126, 20181127, 20181128, 20181129, 20181130, 20181203, 20181204, 20181206, 20181207, 20181210, 20181211, 20181212, 20181213, 20181214, 20181217, 20181218, 20181219, 20181220, 20181221, 20181224, 20181226, 20181227, 20181228, 20181231, 20190102, 20190103, 20190104, 20190107, 20190108, 20190109, 20190110, 20190111, 20190114, 20190115, 20190116, 20190117, 20190118, 20190122, 20190123, 20190124, 20190125, 20190128, 20190129, 20190130, 20190131, 20190201, 20190204, 20190205, 20190206, 20190207, 20190208, 20190211, 20190212, 20190213, 20190214, 20190215, 20190219, 20190220, 20190221, 20190222, 20190225, 20190226, 20190227, 20190228, 20190301, 20190304, 20190305, 20190306, 20190307, 20190308, 20190311, 20190312, 20190313, 20190314, 20190315, 20190318, 20190319, 20190320, 20190321, 20190322, 20190325, 20190326, 20190327, 20190328, 20190329, 20190401, 20190402, 20190403, 20190404, 20190405, 20190408, 20190409, 20190410, 20190411, 20190412, 20190415, 20190416, 20190417, 20190418, 20190422, 20190423, 20190424, 20190425, 20190426, 20190429, 20190430, 20190501, 20190502, 20190503, 20190506, 20190507, 20190508, 20190509, 20190510, 20190513, 20190514, 20190515, 20190516, 20190517, 20190520, 20190521, 20190522, 20190523, 20190524, 20190528, 20190529, 20190530, 20190531, 20190603, 20190604, 20190605, 20190606, 20190607, 20190610, 20190611, 20190612, 20190613, 20190614, 20190617, 20190618, 20190619, 20190620, 20190621, 20190624, 20190625, 20190626, 20190627, 20190628, 20190701, 20190702, 20190703, 20190705, 20190708, 20190709, 20190710, 20190711, 20190712, 20190715, 20190716, 20190717, 20190718, 20190719, 20190722, 20190723, 20190724, 20190725, 20190726, 20190729, 20190730, 20190731, 20190801, 20190802, 20190805, 20190806, 20190807, 20190808, 20190809, 20190812, 20190813, 20190814, 20190815, 20190816, 20190819, 20190820, 20190821, 20190822, 20190823, 20190826, 20190827, 20190828, 20190829, 20190830, 20190903, 20190904, 20190905, 20190906, 20190909, 20190910, 20190911, 20190912, 20190913, 20190916, 20190917, 20190918, 20190919, 20190920, 20190923, 20190924, 20190925, 20190926, 20190927, 20190930, 20191001, 20191002, 20191003, 20191004, 20191007, 20191008, 20191009, 20191010, 20191011, 20191014, 20191015, 20191016, 20191017, 20191018, 20191021, 20191022, 20191023, 20191024, 20191025, 20191028, 20191029, 20191030, 20191031, 20191101, 20191104, 20191105, 20191106, 20191107, 20191108, 20191111, 20191112, 20191113, 20191114, 20191115, 20191118, 20191119, 20191120], \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e843f956-7cfc-459d-bf78-d96029a42cf5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyScHKrhFn7z"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anqa-UqNFn7z"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.05]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        " \n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"AMT\", step_sizes=4, th= th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "fc-wOxfRFn7z",
        "outputId": "b5128fcc-036a-4f46-ba9d-8d7e4839cf80"
      },
      "source": [
        "Result_cross.to_csv('AMT_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.881633</td>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.064516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.393258</td>\n",
              "      <td>0.834694</td>\n",
              "      <td>0.463576</td>\n",
              "      <td>0.564516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.581395</td>\n",
              "      <td>0.887755</td>\n",
              "      <td>0.476190</td>\n",
              "      <td>0.403226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.535714</td>\n",
              "      <td>0.881633</td>\n",
              "      <td>0.508475</td>\n",
              "      <td>0.483871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.864333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.561644</td>\n",
              "      <td>0.884026</td>\n",
              "      <td>0.607407</td>\n",
              "      <td>0.661290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.778993</td>\n",
              "      <td>0.435754</td>\n",
              "      <td>0.629032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.406593</td>\n",
              "      <td>0.827133</td>\n",
              "      <td>0.483660</td>\n",
              "      <td>0.596774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.271845</td>\n",
              "      <td>0.761488</td>\n",
              "      <td>0.339394</td>\n",
              "      <td>0.451613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.883673</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>0.096774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.864333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.849015</td>\n",
              "      <td>0.028169</td>\n",
              "      <td>0.016129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.870897</td>\n",
              "      <td>0.119403</td>\n",
              "      <td>0.064516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.857768</td>\n",
              "      <td>0.029851</td>\n",
              "      <td>0.016129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.829322</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.016129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.881633</td>\n",
              "      <td>0.147059</td>\n",
              "      <td>0.080645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.889796</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.387097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>0.889796</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.209677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.889796</td>\n",
              "      <td>0.289474</td>\n",
              "      <td>0.177419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.851204</td>\n",
              "      <td>0.260870</td>\n",
              "      <td>0.193548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.431579</td>\n",
              "      <td>0.835886</td>\n",
              "      <td>0.522293</td>\n",
              "      <td>0.661290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.349057</td>\n",
              "      <td>0.794311</td>\n",
              "      <td>0.440476</td>\n",
              "      <td>0.596774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.492308</td>\n",
              "      <td>0.862144</td>\n",
              "      <td>0.503937</td>\n",
              "      <td>0.516129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.317647</td>\n",
              "      <td>0.796499</td>\n",
              "      <td>0.367347</td>\n",
              "      <td>0.435484</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "1            GRU 0.1  AMT  1.000000  0.881633  0.121212  0.064516\n",
              "2        XGBoost 0.1  AMT  0.393258  0.834694  0.463576  0.564516\n",
              "3         Logreg 0.1  AMT  0.581395  0.887755  0.476190  0.403226\n",
              "4            SVM 0.1  AMT  0.535714  0.881633  0.508475  0.483871\n",
              "5      LSTM beta 0.1  AMT  0.000000  0.864333  0.000000  0.000000\n",
              "6       GRU beta 0.1  AMT  0.561644  0.884026  0.607407  0.661290\n",
              "7   XGBoost beta 0.1  AMT  0.333333  0.778993  0.435754  0.629032\n",
              "8    logreg beta 0.1  AMT  0.406593  0.827133  0.483660  0.596774\n",
              "9       svm beta 0.1  AMT  0.271845  0.761488  0.339394  0.451613\n",
              "0           LSTM 0.2  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "1            GRU 0.2  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "2        XGBoost 0.2  AMT  0.857143  0.883673  0.173913  0.096774\n",
              "3         Logreg 0.2  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "4            SVM 0.2  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "5      LSTM beta 0.2  AMT  0.000000  0.864333  0.000000  0.000000\n",
              "6       GRU beta 0.2  AMT  0.111111  0.849015  0.028169  0.016129\n",
              "7   XGBoost beta 0.2  AMT  0.800000  0.870897  0.119403  0.064516\n",
              "8    logreg beta 0.2  AMT  0.200000  0.857768  0.029851  0.016129\n",
              "9       svm beta 0.2  AMT  0.055556  0.829322  0.025000  0.016129\n",
              "0          LSTM 0.15  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "1           GRU 0.15  AMT  0.833333  0.881633  0.147059  0.080645\n",
              "2       XGBoost 0.15  AMT  0.600000  0.889796  0.470588  0.387097\n",
              "3        Logreg 0.15  AMT  0.722222  0.889796  0.325000  0.209677\n",
              "4           SVM 0.15  AMT  0.785714  0.889796  0.289474  0.177419\n",
              "5     LSTM beta 0.15  AMT  0.400000  0.851204  0.260870  0.193548\n",
              "6      GRU beta 0.15  AMT  0.431579  0.835886  0.522293  0.661290\n",
              "7  XGBoost beta 0.15  AMT  0.349057  0.794311  0.440476  0.596774\n",
              "8   logreg beta 0.15  AMT  0.492308  0.862144  0.503937  0.516129\n",
              "9      svm beta 0.15  AMT  0.317647  0.796499  0.367347  0.435484"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0FFk1-MFn70"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x53P8CtyFn70"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez3sbdK-Fn70",
        "outputId": "0c4bc83f-09ce-4f60-aed7-8eff063ac711"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"AMT\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6624 - accuracy: 0.6409 - val_loss: 0.5089 - val_accuracy: 0.8735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6549 - accuracy: 0.6430 - val_loss: 0.5227 - val_accuracy: 0.8735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6500 - accuracy: 0.6423 - val_loss: 0.5266 - val_accuracy: 0.8735\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6328 - accuracy: 0.6644 - val_loss: 0.4690 - val_accuracy: 0.8735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6200 - accuracy: 0.6846 - val_loss: 0.4383 - val_accuracy: 0.8796\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 17ms/step - loss: 0.6606 - accuracy: 0.6430 - val_loss: 0.5285 - val_accuracy: 0.8735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6510 - accuracy: 0.6450 - val_loss: 0.5425 - val_accuracy: 0.8735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6348 - accuracy: 0.6544 - val_loss: 0.4651 - val_accuracy: 0.8918\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.6079 - accuracy: 0.6765 - val_loss: 0.4496 - val_accuracy: 0.8898\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5924 - accuracy: 0.7094 - val_loss: 0.4511 - val_accuracy: 0.8796\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.699634\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.71143\n",
            "[2]\tvalidation_0-auc:0.717949\n",
            "[3]\tvalidation_0-auc:0.721755\n",
            "[4]\tvalidation_0-auc:0.733325\n",
            "[5]\tvalidation_0-auc:0.755276\n",
            "[6]\tvalidation_0-auc:0.761513\n",
            "[7]\tvalidation_0-auc:0.765413\n",
            "[8]\tvalidation_0-auc:0.786969\n",
            "[9]\tvalidation_0-auc:0.779771\n",
            "[10]\tvalidation_0-auc:0.781693\n",
            "[11]\tvalidation_0-auc:0.787477\n",
            "[12]\tvalidation_0-auc:0.793714\n",
            "[13]\tvalidation_0-auc:0.794035\n",
            "[14]\tvalidation_0-auc:0.791755\n",
            "[15]\tvalidation_0-auc:0.792\n",
            "[16]\tvalidation_0-auc:0.796729\n",
            "[17]\tvalidation_0-auc:0.800667\n",
            "[18]\tvalidation_0-auc:0.796823\n",
            "[19]\tvalidation_0-auc:0.800215\n",
            "[20]\tvalidation_0-auc:0.797219\n",
            "[21]\tvalidation_0-auc:0.798199\n",
            "[22]\tvalidation_0-auc:0.797709\n",
            "[23]\tvalidation_0-auc:0.794393\n",
            "[24]\tvalidation_0-auc:0.797257\n",
            "[25]\tvalidation_0-auc:0.803795\n",
            "[26]\tvalidation_0-auc:0.801609\n",
            "[27]\tvalidation_0-auc:0.801854\n",
            "[28]\tvalidation_0-auc:0.804247\n",
            "[29]\tvalidation_0-auc:0.805717\n",
            "[30]\tvalidation_0-auc:0.805415\n",
            "[31]\tvalidation_0-auc:0.806357\n",
            "[32]\tvalidation_0-auc:0.807526\n",
            "[33]\tvalidation_0-auc:0.809146\n",
            "[34]\tvalidation_0-auc:0.809937\n",
            "[35]\tvalidation_0-auc:0.807394\n",
            "[36]\tvalidation_0-auc:0.806263\n",
            "[37]\tvalidation_0-auc:0.807733\n",
            "[38]\tvalidation_0-auc:0.807413\n",
            "[39]\tvalidation_0-auc:0.806621\n",
            "[40]\tvalidation_0-auc:0.808618\n",
            "[41]\tvalidation_0-auc:0.811068\n",
            "[42]\tvalidation_0-auc:0.811671\n",
            "[43]\tvalidation_0-auc:0.811709\n",
            "[44]\tvalidation_0-auc:0.810993\n",
            "[45]\tvalidation_0-auc:0.809749\n",
            "[46]\tvalidation_0-auc:0.809259\n",
            "[47]\tvalidation_0-auc:0.809448\n",
            "[48]\tvalidation_0-auc:0.810804\n",
            "[49]\tvalidation_0-auc:0.811369\n",
            "[50]\tvalidation_0-auc:0.809975\n",
            "[51]\tvalidation_0-auc:0.812349\n",
            "[52]\tvalidation_0-auc:0.813348\n",
            "[53]\tvalidation_0-auc:0.814629\n",
            "[54]\tvalidation_0-auc:0.814893\n",
            "[55]\tvalidation_0-auc:0.813159\n",
            "[56]\tvalidation_0-auc:0.812896\n",
            "[57]\tvalidation_0-auc:0.814064\n",
            "[58]\tvalidation_0-auc:0.811916\n",
            "[59]\tvalidation_0-auc:0.813084\n",
            "[60]\tvalidation_0-auc:0.813009\n",
            "[61]\tvalidation_0-auc:0.814403\n",
            "[62]\tvalidation_0-auc:0.814177\n",
            "[63]\tvalidation_0-auc:0.814252\n",
            "[64]\tvalidation_0-auc:0.812783\n",
            "[65]\tvalidation_0-auc:0.81429\n",
            "[66]\tvalidation_0-auc:0.814516\n",
            "[67]\tvalidation_0-auc:0.814139\n",
            "[68]\tvalidation_0-auc:0.813009\n",
            "[69]\tvalidation_0-auc:0.810898\n",
            "[70]\tvalidation_0-auc:0.810974\n",
            "[71]\tvalidation_0-auc:0.811577\n",
            "[72]\tvalidation_0-auc:0.811614\n",
            "[73]\tvalidation_0-auc:0.811011\n",
            "[74]\tvalidation_0-auc:0.810823\n",
            "[75]\tvalidation_0-auc:0.811011\n",
            "[76]\tvalidation_0-auc:0.810069\n",
            "[77]\tvalidation_0-auc:0.811577\n",
            "[78]\tvalidation_0-auc:0.810408\n",
            "[79]\tvalidation_0-auc:0.810069\n",
            "[80]\tvalidation_0-auc:0.81022\n",
            "[81]\tvalidation_0-auc:0.810258\n",
            "[82]\tvalidation_0-auc:0.810446\n",
            "[83]\tvalidation_0-auc:0.812707\n",
            "[84]\tvalidation_0-auc:0.813649\n",
            "[85]\tvalidation_0-auc:0.812104\n",
            "[86]\tvalidation_0-auc:0.811841\n",
            "[87]\tvalidation_0-auc:0.811351\n",
            "[88]\tvalidation_0-auc:0.811539\n",
            "[89]\tvalidation_0-auc:0.813612\n",
            "[90]\tvalidation_0-auc:0.813951\n",
            "[91]\tvalidation_0-auc:0.810898\n",
            "[92]\tvalidation_0-auc:0.81169\n",
            "[93]\tvalidation_0-auc:0.81267\n",
            "[94]\tvalidation_0-auc:0.812933\n",
            "[95]\tvalidation_0-auc:0.814478\n",
            "[96]\tvalidation_0-auc:0.814931\n",
            "[97]\tvalidation_0-auc:0.816174\n",
            "[98]\tvalidation_0-auc:0.814177\n",
            "[99]\tvalidation_0-auc:0.813725\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6633 - accuracy: 0.6280 - val_loss: 0.5723 - val_accuracy: 0.8643\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6607 - accuracy: 0.6342 - val_loss: 0.5521 - val_accuracy: 0.8643\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6432 - accuracy: 0.6342 - val_loss: 0.4725 - val_accuracy: 0.8381\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6169 - accuracy: 0.6699 - val_loss: 0.4996 - val_accuracy: 0.8621\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5992 - accuracy: 0.6754 - val_loss: 0.4668 - val_accuracy: 0.8556\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6515 - accuracy: 0.6314 - val_loss: 0.4655 - val_accuracy: 0.8490\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5990 - accuracy: 0.6863 - val_loss: 0.5268 - val_accuracy: 0.8140\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5745 - accuracy: 0.7042 - val_loss: 0.4313 - val_accuracy: 0.8578\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5623 - accuracy: 0.7138 - val_loss: 0.5451 - val_accuracy: 0.7856\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5499 - accuracy: 0.7207 - val_loss: 0.4289 - val_accuracy: 0.8425\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.756472\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.754247\n",
            "[2]\tvalidation_0-auc:0.755267\n",
            "[3]\tvalidation_0-auc:0.756513\n",
            "[4]\tvalidation_0-auc:0.761066\n",
            "[5]\tvalidation_0-auc:0.763883\n",
            "[6]\tvalidation_0-auc:0.762536\n",
            "[7]\tvalidation_0-auc:0.763352\n",
            "[8]\tvalidation_0-auc:0.763454\n",
            "[9]\tvalidation_0-auc:0.764108\n",
            "[10]\tvalidation_0-auc:0.768844\n",
            "[11]\tvalidation_0-auc:0.768987\n",
            "[12]\tvalidation_0-auc:0.769641\n",
            "[13]\tvalidation_0-auc:0.769498\n",
            "[14]\tvalidation_0-auc:0.772764\n",
            "[15]\tvalidation_0-auc:0.77207\n",
            "[16]\tvalidation_0-auc:0.772132\n",
            "[17]\tvalidation_0-auc:0.766354\n",
            "[18]\tvalidation_0-auc:0.76223\n",
            "[19]\tvalidation_0-auc:0.762454\n",
            "[20]\tvalidation_0-auc:0.764434\n",
            "[21]\tvalidation_0-auc:0.770825\n",
            "[22]\tvalidation_0-auc:0.773356\n",
            "[23]\tvalidation_0-auc:0.772948\n",
            "[24]\tvalidation_0-auc:0.776399\n",
            "[25]\tvalidation_0-auc:0.777889\n",
            "[26]\tvalidation_0-auc:0.778011\n",
            "[27]\tvalidation_0-auc:0.777276\n",
            "[28]\tvalidation_0-auc:0.777807\n",
            "[29]\tvalidation_0-auc:0.777195\n",
            "[30]\tvalidation_0-auc:0.777848\n",
            "[31]\tvalidation_0-auc:0.778195\n",
            "[32]\tvalidation_0-auc:0.777276\n",
            "[33]\tvalidation_0-auc:0.777521\n",
            "[34]\tvalidation_0-auc:0.777971\n",
            "[35]\tvalidation_0-auc:0.776133\n",
            "[36]\tvalidation_0-auc:0.780053\n",
            "[37]\tvalidation_0-auc:0.778052\n",
            "[38]\tvalidation_0-auc:0.775766\n",
            "[39]\tvalidation_0-auc:0.776051\n",
            "[40]\tvalidation_0-auc:0.778256\n",
            "[41]\tvalidation_0-auc:0.777726\n",
            "[42]\tvalidation_0-auc:0.778787\n",
            "[43]\tvalidation_0-auc:0.776439\n",
            "[44]\tvalidation_0-auc:0.775664\n",
            "[45]\tvalidation_0-auc:0.776684\n",
            "[46]\tvalidation_0-auc:0.776766\n",
            "[47]\tvalidation_0-auc:0.777501\n",
            "[48]\tvalidation_0-auc:0.776562\n",
            "[49]\tvalidation_0-auc:0.776684\n",
            "[50]\tvalidation_0-auc:0.776521\n",
            "[51]\tvalidation_0-auc:0.775255\n",
            "[52]\tvalidation_0-auc:0.774153\n",
            "[53]\tvalidation_0-auc:0.773826\n",
            "[54]\tvalidation_0-auc:0.774398\n",
            "[55]\tvalidation_0-auc:0.773132\n",
            "[56]\tvalidation_0-auc:0.771866\n",
            "[57]\tvalidation_0-auc:0.771131\n",
            "[58]\tvalidation_0-auc:0.769784\n",
            "[59]\tvalidation_0-auc:0.768354\n",
            "[60]\tvalidation_0-auc:0.76815\n",
            "[61]\tvalidation_0-auc:0.767007\n",
            "[62]\tvalidation_0-auc:0.767497\n",
            "[63]\tvalidation_0-auc:0.767456\n",
            "[64]\tvalidation_0-auc:0.766517\n",
            "[65]\tvalidation_0-auc:0.765374\n",
            "[66]\tvalidation_0-auc:0.76472\n",
            "[67]\tvalidation_0-auc:0.764843\n",
            "[68]\tvalidation_0-auc:0.763944\n",
            "[69]\tvalidation_0-auc:0.76423\n",
            "[70]\tvalidation_0-auc:0.764108\n",
            "[71]\tvalidation_0-auc:0.762025\n",
            "[72]\tvalidation_0-auc:0.763822\n",
            "[73]\tvalidation_0-auc:0.763904\n",
            "[74]\tvalidation_0-auc:0.762924\n",
            "[75]\tvalidation_0-auc:0.760923\n",
            "[76]\tvalidation_0-auc:0.758963\n",
            "[77]\tvalidation_0-auc:0.759045\n",
            "[78]\tvalidation_0-auc:0.758024\n",
            "[79]\tvalidation_0-auc:0.756227\n",
            "[80]\tvalidation_0-auc:0.757166\n",
            "[81]\tvalidation_0-auc:0.757574\n",
            "[82]\tvalidation_0-auc:0.756513\n",
            "[83]\tvalidation_0-auc:0.754124\n",
            "[84]\tvalidation_0-auc:0.754614\n",
            "[85]\tvalidation_0-auc:0.75392\n",
            "[86]\tvalidation_0-auc:0.753797\n",
            "Stopping. Best iteration:\n",
            "[36]\tvalidation_0-auc:0.780053\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.8795918367346939 |         1.0         | 0.04838709677419355 | 0.09230769230769231 |\n",
            "|     GRU 0.1      | 0.8795918367346939 |  0.5272727272727272 | 0.46774193548387094 |  0.4957264957264957 |\n",
            "|   XGBoost 0.1    | 0.8346938775510204 | 0.39325842696629215 |  0.5645161290322581 | 0.46357615894039744 |\n",
            "|    Logreg 0.1    | 0.8877551020408163 |  0.5813953488372093 |  0.4032258064516129 |  0.4761904761904762 |\n",
            "|     SVM 0.1      | 0.8816326530612245 |  0.5357142857142857 |  0.4838709677419355 |  0.5084745762711865 |\n",
            "|  LSTM beta 0.1   | 0.8555798687089715 |  0.4787234042553192 |  0.7258064516129032 |  0.576923076923077  |\n",
            "|   GRU beta 0.1   | 0.8424507658643327 |  0.4431818181818182 |  0.6290322580645161 |  0.5199999999999999 |\n",
            "| XGBoost beta 0.1 | 0.7789934354485777 |  0.3333333333333333 |  0.6290322580645161 |  0.435754189944134  |\n",
            "| logreg beta 0.1  | 0.8271334792122538 |  0.4065934065934066 |  0.5967741935483871 | 0.48366013071895425 |\n",
            "|   svm beta 0.1   | 0.7614879649890591 | 0.27184466019417475 | 0.45161290322580644 |  0.3393939393939394 |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.5472 - accuracy: 0.7799 - val_loss: 0.4126 - val_accuracy: 0.8735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5332 - accuracy: 0.7805 - val_loss: 0.3988 - val_accuracy: 0.8735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5402 - accuracy: 0.7805 - val_loss: 0.4227 - val_accuracy: 0.8735\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5339 - accuracy: 0.7805 - val_loss: 0.4059 - val_accuracy: 0.8735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5342 - accuracy: 0.7805 - val_loss: 0.4156 - val_accuracy: 0.8735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 2s 13ms/step - loss: 0.5514 - accuracy: 0.7765 - val_loss: 0.4112 - val_accuracy: 0.8735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5332 - accuracy: 0.7805 - val_loss: 0.4025 - val_accuracy: 0.8735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5306 - accuracy: 0.7805 - val_loss: 0.3941 - val_accuracy: 0.8735\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5232 - accuracy: 0.7819 - val_loss: 0.3749 - val_accuracy: 0.8735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5182 - accuracy: 0.7805 - val_loss: 0.3700 - val_accuracy: 0.8735\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.76383\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.790436\n",
            "[2]\tvalidation_0-auc:0.78955\n",
            "[3]\tvalidation_0-auc:0.806508\n",
            "[4]\tvalidation_0-auc:0.802438\n",
            "[5]\tvalidation_0-auc:0.802476\n",
            "[6]\tvalidation_0-auc:0.821789\n",
            "[7]\tvalidation_0-auc:0.828667\n",
            "[8]\tvalidation_0-auc:0.838691\n",
            "[9]\tvalidation_0-auc:0.83756\n",
            "[10]\tvalidation_0-auc:0.831568\n",
            "[11]\tvalidation_0-auc:0.835676\n",
            "[12]\tvalidation_0-auc:0.834809\n",
            "[13]\tvalidation_0-auc:0.837994\n",
            "[14]\tvalidation_0-auc:0.839539\n",
            "[15]\tvalidation_0-auc:0.838823\n",
            "[16]\tvalidation_0-auc:0.839237\n",
            "[17]\tvalidation_0-auc:0.840142\n",
            "[18]\tvalidation_0-auc:0.839689\n",
            "[19]\tvalidation_0-auc:0.839162\n",
            "[20]\tvalidation_0-auc:0.839124\n",
            "[21]\tvalidation_0-auc:0.835921\n",
            "[22]\tvalidation_0-auc:0.836882\n",
            "[23]\tvalidation_0-auc:0.834696\n",
            "[24]\tvalidation_0-auc:0.837183\n",
            "[25]\tvalidation_0-auc:0.836166\n",
            "[26]\tvalidation_0-auc:0.838239\n",
            "[27]\tvalidation_0-auc:0.837673\n",
            "[28]\tvalidation_0-auc:0.836844\n",
            "[29]\tvalidation_0-auc:0.836392\n",
            "[30]\tvalidation_0-auc:0.836185\n",
            "[31]\tvalidation_0-auc:0.835017\n",
            "[32]\tvalidation_0-auc:0.83415\n",
            "[33]\tvalidation_0-auc:0.832134\n",
            "[34]\tvalidation_0-auc:0.829533\n",
            "[35]\tvalidation_0-auc:0.829157\n",
            "[36]\tvalidation_0-auc:0.832228\n",
            "[37]\tvalidation_0-auc:0.833472\n",
            "[38]\tvalidation_0-auc:0.83366\n",
            "[39]\tvalidation_0-auc:0.833208\n",
            "[40]\tvalidation_0-auc:0.831889\n",
            "[41]\tvalidation_0-auc:0.834583\n",
            "[42]\tvalidation_0-auc:0.83594\n",
            "[43]\tvalidation_0-auc:0.835412\n",
            "[44]\tvalidation_0-auc:0.837296\n",
            "[45]\tvalidation_0-auc:0.837786\n",
            "[46]\tvalidation_0-auc:0.841084\n",
            "[47]\tvalidation_0-auc:0.839878\n",
            "[48]\tvalidation_0-auc:0.838031\n",
            "[49]\tvalidation_0-auc:0.836712\n",
            "[50]\tvalidation_0-auc:0.83773\n",
            "[51]\tvalidation_0-auc:0.839916\n",
            "[52]\tvalidation_0-auc:0.841235\n",
            "[53]\tvalidation_0-auc:0.840632\n",
            "[54]\tvalidation_0-auc:0.840933\n",
            "[55]\tvalidation_0-auc:0.841008\n",
            "[56]\tvalidation_0-auc:0.84033\n",
            "[57]\tvalidation_0-auc:0.840594\n",
            "[58]\tvalidation_0-auc:0.84082\n",
            "[59]\tvalidation_0-auc:0.840971\n",
            "[60]\tvalidation_0-auc:0.840858\n",
            "[61]\tvalidation_0-auc:0.842082\n",
            "[62]\tvalidation_0-auc:0.841253\n",
            "[63]\tvalidation_0-auc:0.841668\n",
            "[64]\tvalidation_0-auc:0.840688\n",
            "[65]\tvalidation_0-auc:0.839463\n",
            "[66]\tvalidation_0-auc:0.839652\n",
            "[67]\tvalidation_0-auc:0.839426\n",
            "[68]\tvalidation_0-auc:0.839614\n",
            "[69]\tvalidation_0-auc:0.841272\n",
            "[70]\tvalidation_0-auc:0.841121\n",
            "[71]\tvalidation_0-auc:0.841611\n",
            "[72]\tvalidation_0-auc:0.842365\n",
            "[73]\tvalidation_0-auc:0.841951\n",
            "[74]\tvalidation_0-auc:0.840556\n",
            "[75]\tvalidation_0-auc:0.837956\n",
            "[76]\tvalidation_0-auc:0.837202\n",
            "[77]\tvalidation_0-auc:0.836939\n",
            "[78]\tvalidation_0-auc:0.838446\n",
            "[79]\tvalidation_0-auc:0.839087\n",
            "[80]\tvalidation_0-auc:0.837504\n",
            "[81]\tvalidation_0-auc:0.835657\n",
            "[82]\tvalidation_0-auc:0.835318\n",
            "[83]\tvalidation_0-auc:0.838031\n",
            "[84]\tvalidation_0-auc:0.836825\n",
            "[85]\tvalidation_0-auc:0.836675\n",
            "[86]\tvalidation_0-auc:0.835996\n",
            "[87]\tvalidation_0-auc:0.835205\n",
            "[88]\tvalidation_0-auc:0.833509\n",
            "[89]\tvalidation_0-auc:0.834753\n",
            "[90]\tvalidation_0-auc:0.83479\n",
            "[91]\tvalidation_0-auc:0.834715\n",
            "[92]\tvalidation_0-auc:0.833924\n",
            "[93]\tvalidation_0-auc:0.833698\n",
            "[94]\tvalidation_0-auc:0.834715\n",
            "[95]\tvalidation_0-auc:0.832605\n",
            "[96]\tvalidation_0-auc:0.832982\n",
            "[97]\tvalidation_0-auc:0.832793\n",
            "[98]\tvalidation_0-auc:0.831813\n",
            "[99]\tvalidation_0-auc:0.830607\n",
            "end training. \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5474 - accuracy: 0.7749 - val_loss: 0.4342 - val_accuracy: 0.8643\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5453 - accuracy: 0.7756 - val_loss: 0.4287 - val_accuracy: 0.8643\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5402 - accuracy: 0.7756 - val_loss: 0.4154 - val_accuracy: 0.8643\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5397 - accuracy: 0.7756 - val_loss: 0.4224 - val_accuracy: 0.8643\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5371 - accuracy: 0.7756 - val_loss: 0.4116 - val_accuracy: 0.8643\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.5533 - accuracy: 0.7742 - val_loss: 0.4162 - val_accuracy: 0.8643\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5392 - accuracy: 0.7756 - val_loss: 0.3879 - val_accuracy: 0.8643\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5179 - accuracy: 0.7769 - val_loss: 0.3747 - val_accuracy: 0.8643\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5104 - accuracy: 0.7763 - val_loss: 0.3533 - val_accuracy: 0.8600\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5020 - accuracy: 0.7742 - val_loss: 0.3914 - val_accuracy: 0.8556\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.711515\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.722458\n",
            "[2]\tvalidation_0-auc:0.738505\n",
            "[3]\tvalidation_0-auc:0.752185\n",
            "[4]\tvalidation_0-auc:0.750715\n",
            "[5]\tvalidation_0-auc:0.752144\n",
            "[6]\tvalidation_0-auc:0.74116\n",
            "[7]\tvalidation_0-auc:0.744753\n",
            "[8]\tvalidation_0-auc:0.745427\n",
            "[9]\tvalidation_0-auc:0.751266\n",
            "[10]\tvalidation_0-auc:0.746448\n",
            "[11]\tvalidation_0-auc:0.739179\n",
            "[12]\tvalidation_0-auc:0.730523\n",
            "[13]\tvalidation_0-auc:0.732973\n",
            "[14]\tvalidation_0-auc:0.74312\n",
            "[15]\tvalidation_0-auc:0.745202\n",
            "[16]\tvalidation_0-auc:0.733687\n",
            "[17]\tvalidation_0-auc:0.732238\n",
            "[18]\tvalidation_0-auc:0.73236\n",
            "[19]\tvalidation_0-auc:0.724622\n",
            "[20]\tvalidation_0-auc:0.718028\n",
            "[21]\tvalidation_0-auc:0.711944\n",
            "[22]\tvalidation_0-auc:0.711678\n",
            "[23]\tvalidation_0-auc:0.709759\n",
            "[24]\tvalidation_0-auc:0.709575\n",
            "[25]\tvalidation_0-auc:0.706921\n",
            "[26]\tvalidation_0-auc:0.700551\n",
            "[27]\tvalidation_0-auc:0.697285\n",
            "[28]\tvalidation_0-auc:0.693242\n",
            "[29]\tvalidation_0-auc:0.693324\n",
            "[30]\tvalidation_0-auc:0.686505\n",
            "[31]\tvalidation_0-auc:0.682728\n",
            "[32]\tvalidation_0-auc:0.680278\n",
            "[33]\tvalidation_0-auc:0.673622\n",
            "[34]\tvalidation_0-auc:0.672826\n",
            "[35]\tvalidation_0-auc:0.670621\n",
            "[36]\tvalidation_0-auc:0.665067\n",
            "[37]\tvalidation_0-auc:0.660535\n",
            "[38]\tvalidation_0-auc:0.658534\n",
            "[39]\tvalidation_0-auc:0.652817\n",
            "[40]\tvalidation_0-auc:0.654512\n",
            "[41]\tvalidation_0-auc:0.653083\n",
            "[42]\tvalidation_0-auc:0.649857\n",
            "[43]\tvalidation_0-auc:0.64463\n",
            "[44]\tvalidation_0-auc:0.6402\n",
            "[45]\tvalidation_0-auc:0.63873\n",
            "[46]\tvalidation_0-auc:0.638975\n",
            "[47]\tvalidation_0-auc:0.638444\n",
            "[48]\tvalidation_0-auc:0.638567\n",
            "[49]\tvalidation_0-auc:0.635096\n",
            "[50]\tvalidation_0-auc:0.633136\n",
            "[51]\tvalidation_0-auc:0.631829\n",
            "[52]\tvalidation_0-auc:0.62844\n",
            "[53]\tvalidation_0-auc:0.62305\n",
            "Stopping. Best iteration:\n",
            "[3]\tvalidation_0-auc:0.752185\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+----------------------+----------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall        |       F1 score       |\n",
            "+------------------+--------------------+---------------------+----------------------+----------------------+\n",
            "|     LSTM 0.2     | 0.8734693877551021 |         0.0         |         0.0          |         0.0          |\n",
            "|     GRU 0.2      | 0.8734693877551021 |         0.0         |         0.0          |         0.0          |\n",
            "|   XGBoost 0.2    | 0.8836734693877552 |  0.8571428571428571 |  0.0967741935483871  | 0.17391304347826084  |\n",
            "|    Logreg 0.2    | 0.8734693877551021 |         0.0         |         0.0          |         0.0          |\n",
            "|     SVM 0.2      | 0.8734693877551021 |         0.0         |         0.0          |         0.0          |\n",
            "|  LSTM beta 0.2   | 0.8643326039387309 |         0.0         |         0.0          |         0.0          |\n",
            "|   GRU beta 0.2   | 0.8555798687089715 |         0.25        | 0.03225806451612903  | 0.05714285714285715  |\n",
            "| XGBoost beta 0.2 | 0.8708971553610503 |         0.8         | 0.06451612903225806  | 0.11940298507462686  |\n",
            "| logreg beta 0.2  | 0.8577680525164114 |         0.2         | 0.016129032258064516 | 0.029850746268656716 |\n",
            "|   svm beta 0.2   | 0.8293216630196937 | 0.05555555555555555 | 0.016129032258064516 | 0.024999999999999998 |\n",
            "+------------------+--------------------+---------------------+----------------------+----------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6254 - accuracy: 0.7013 - val_loss: 0.4487 - val_accuracy: 0.8735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6144 - accuracy: 0.7020 - val_loss: 0.4385 - val_accuracy: 0.8735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6119 - accuracy: 0.7020 - val_loss: 0.4578 - val_accuracy: 0.8735\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6116 - accuracy: 0.7020 - val_loss: 0.4463 - val_accuracy: 0.8735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6155 - accuracy: 0.7020 - val_loss: 0.4611 - val_accuracy: 0.8735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6176 - accuracy: 0.6933 - val_loss: 0.5102 - val_accuracy: 0.8735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6046 - accuracy: 0.7054 - val_loss: 0.3833 - val_accuracy: 0.8735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5913 - accuracy: 0.7174 - val_loss: 0.4165 - val_accuracy: 0.8816\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5762 - accuracy: 0.7248 - val_loss: 0.4081 - val_accuracy: 0.8857\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5754 - accuracy: 0.7255 - val_loss: 0.4071 - val_accuracy: 0.8878\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.689478\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.721228\n",
            "[2]\tvalidation_0-auc:0.722773\n",
            "[3]\tvalidation_0-auc:0.755653\n",
            "[4]\tvalidation_0-auc:0.765658\n",
            "[5]\tvalidation_0-auc:0.765338\n",
            "[6]\tvalidation_0-auc:0.765262\n",
            "[7]\tvalidation_0-auc:0.764094\n",
            "[8]\tvalidation_0-auc:0.773854\n",
            "[9]\tvalidation_0-auc:0.771932\n",
            "[10]\tvalidation_0-auc:0.780732\n",
            "[11]\tvalidation_0-auc:0.781372\n",
            "[12]\tvalidation_0-auc:0.783502\n",
            "[13]\tvalidation_0-auc:0.783577\n",
            "[14]\tvalidation_0-auc:0.79671\n",
            "[15]\tvalidation_0-auc:0.797049\n",
            "[16]\tvalidation_0-auc:0.798029\n",
            "[17]\tvalidation_0-auc:0.796465\n",
            "[18]\tvalidation_0-auc:0.798915\n",
            "[19]\tvalidation_0-auc:0.800874\n",
            "[20]\tvalidation_0-auc:0.800516\n",
            "[21]\tvalidation_0-auc:0.795467\n",
            "[22]\tvalidation_0-auc:0.796861\n",
            "[23]\tvalidation_0-auc:0.795014\n",
            "[24]\tvalidation_0-auc:0.792207\n",
            "[25]\tvalidation_0-auc:0.797275\n",
            "[26]\tvalidation_0-auc:0.797615\n",
            "[27]\tvalidation_0-auc:0.796729\n",
            "[28]\tvalidation_0-auc:0.794336\n",
            "[29]\tvalidation_0-auc:0.793733\n",
            "[30]\tvalidation_0-auc:0.794524\n",
            "[31]\tvalidation_0-auc:0.792565\n",
            "[32]\tvalidation_0-auc:0.796578\n",
            "[33]\tvalidation_0-auc:0.79541\n",
            "[34]\tvalidation_0-auc:0.797671\n",
            "[35]\tvalidation_0-auc:0.7985\n",
            "[36]\tvalidation_0-auc:0.798576\n",
            "[37]\tvalidation_0-auc:0.797633\n",
            "[38]\tvalidation_0-auc:0.796578\n",
            "[39]\tvalidation_0-auc:0.79541\n",
            "[40]\tvalidation_0-auc:0.796842\n",
            "[41]\tvalidation_0-auc:0.800309\n",
            "[42]\tvalidation_0-auc:0.799857\n",
            "[43]\tvalidation_0-auc:0.800196\n",
            "[44]\tvalidation_0-auc:0.798689\n",
            "[45]\tvalidation_0-auc:0.797407\n",
            "[46]\tvalidation_0-auc:0.799329\n",
            "[47]\tvalidation_0-auc:0.800158\n",
            "[48]\tvalidation_0-auc:0.800554\n",
            "[49]\tvalidation_0-auc:0.804134\n",
            "[50]\tvalidation_0-auc:0.804549\n",
            "[51]\tvalidation_0-auc:0.804285\n",
            "[52]\tvalidation_0-auc:0.803757\n",
            "[53]\tvalidation_0-auc:0.804624\n",
            "[54]\tvalidation_0-auc:0.80387\n",
            "[55]\tvalidation_0-auc:0.80323\n",
            "[56]\tvalidation_0-auc:0.798293\n",
            "[57]\tvalidation_0-auc:0.799423\n",
            "[58]\tvalidation_0-auc:0.799876\n",
            "[59]\tvalidation_0-auc:0.799235\n",
            "[60]\tvalidation_0-auc:0.804247\n",
            "[61]\tvalidation_0-auc:0.802514\n",
            "[62]\tvalidation_0-auc:0.803682\n",
            "[63]\tvalidation_0-auc:0.804511\n",
            "[64]\tvalidation_0-auc:0.805076\n",
            "[65]\tvalidation_0-auc:0.805038\n",
            "[66]\tvalidation_0-auc:0.805604\n",
            "[67]\tvalidation_0-auc:0.807469\n",
            "[68]\tvalidation_0-auc:0.810635\n",
            "[69]\tvalidation_0-auc:0.810974\n",
            "[70]\tvalidation_0-auc:0.810182\n",
            "[71]\tvalidation_0-auc:0.81022\n",
            "[72]\tvalidation_0-auc:0.810635\n",
            "[73]\tvalidation_0-auc:0.810823\n",
            "[74]\tvalidation_0-auc:0.808826\n",
            "[75]\tvalidation_0-auc:0.808788\n",
            "[76]\tvalidation_0-auc:0.807884\n",
            "[77]\tvalidation_0-auc:0.806602\n",
            "[78]\tvalidation_0-auc:0.807092\n",
            "[79]\tvalidation_0-auc:0.806414\n",
            "[80]\tvalidation_0-auc:0.806188\n",
            "[81]\tvalidation_0-auc:0.806527\n",
            "[82]\tvalidation_0-auc:0.806414\n",
            "[83]\tvalidation_0-auc:0.807582\n",
            "[84]\tvalidation_0-auc:0.8086\n",
            "[85]\tvalidation_0-auc:0.809617\n",
            "[86]\tvalidation_0-auc:0.808411\n",
            "[87]\tvalidation_0-auc:0.808336\n",
            "[88]\tvalidation_0-auc:0.809391\n",
            "[89]\tvalidation_0-auc:0.809391\n",
            "[90]\tvalidation_0-auc:0.808675\n",
            "[91]\tvalidation_0-auc:0.808298\n",
            "[92]\tvalidation_0-auc:0.806376\n",
            "[93]\tvalidation_0-auc:0.805999\n",
            "[94]\tvalidation_0-auc:0.80517\n",
            "[95]\tvalidation_0-auc:0.805396\n",
            "[96]\tvalidation_0-auc:0.805811\n",
            "[97]\tvalidation_0-auc:0.805849\n",
            "[98]\tvalidation_0-auc:0.80615\n",
            "[99]\tvalidation_0-auc:0.80468\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6244 - accuracy: 0.6939 - val_loss: 0.4480 - val_accuracy: 0.8643\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6094 - accuracy: 0.6953 - val_loss: 0.4273 - val_accuracy: 0.8381\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5755 - accuracy: 0.7014 - val_loss: 0.4069 - val_accuracy: 0.8578\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5684 - accuracy: 0.7062 - val_loss: 0.4307 - val_accuracy: 0.8249\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5529 - accuracy: 0.7028 - val_loss: 0.4247 - val_accuracy: 0.8468\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6216 - accuracy: 0.6911 - val_loss: 0.5054 - val_accuracy: 0.8643\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5773 - accuracy: 0.7117 - val_loss: 0.5078 - val_accuracy: 0.8709\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5356 - accuracy: 0.7220 - val_loss: 0.4708 - val_accuracy: 0.8359\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5262 - accuracy: 0.7344 - val_loss: 0.3958 - val_accuracy: 0.8775\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5281 - accuracy: 0.7399 - val_loss: 0.3990 - val_accuracy: 0.8709\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.760229\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.761086\n",
            "[2]\tvalidation_0-auc:0.761045\n",
            "[3]\tvalidation_0-auc:0.760514\n",
            "[4]\tvalidation_0-auc:0.759453\n",
            "[5]\tvalidation_0-auc:0.759596\n",
            "[6]\tvalidation_0-auc:0.760249\n",
            "[7]\tvalidation_0-auc:0.758657\n",
            "[8]\tvalidation_0-auc:0.754961\n",
            "[9]\tvalidation_0-auc:0.755329\n",
            "[10]\tvalidation_0-auc:0.754982\n",
            "[11]\tvalidation_0-auc:0.761576\n",
            "[12]\tvalidation_0-auc:0.762413\n",
            "[13]\tvalidation_0-auc:0.760596\n",
            "[14]\tvalidation_0-auc:0.762883\n",
            "[15]\tvalidation_0-auc:0.771478\n",
            "[16]\tvalidation_0-auc:0.770784\n",
            "[17]\tvalidation_0-auc:0.768824\n",
            "[18]\tvalidation_0-auc:0.773193\n",
            "[19]\tvalidation_0-auc:0.773479\n",
            "[20]\tvalidation_0-auc:0.773765\n",
            "[21]\tvalidation_0-auc:0.775112\n",
            "[22]\tvalidation_0-auc:0.771274\n",
            "[23]\tvalidation_0-auc:0.771151\n",
            "[24]\tvalidation_0-auc:0.77205\n",
            "[25]\tvalidation_0-auc:0.77158\n",
            "[26]\tvalidation_0-auc:0.771213\n",
            "[27]\tvalidation_0-auc:0.769171\n",
            "[28]\tvalidation_0-auc:0.769294\n",
            "[29]\tvalidation_0-auc:0.768599\n",
            "[30]\tvalidation_0-auc:0.76717\n",
            "[31]\tvalidation_0-auc:0.767497\n",
            "[32]\tvalidation_0-auc:0.764598\n",
            "[33]\tvalidation_0-auc:0.764884\n",
            "[34]\tvalidation_0-auc:0.765047\n",
            "[35]\tvalidation_0-auc:0.763883\n",
            "[36]\tvalidation_0-auc:0.76372\n",
            "[37]\tvalidation_0-auc:0.763965\n",
            "[38]\tvalidation_0-auc:0.761392\n",
            "[39]\tvalidation_0-auc:0.759147\n",
            "[40]\tvalidation_0-auc:0.758493\n",
            "[41]\tvalidation_0-auc:0.757309\n",
            "[42]\tvalidation_0-auc:0.754737\n",
            "[43]\tvalidation_0-auc:0.753389\n",
            "[44]\tvalidation_0-auc:0.753307\n",
            "[45]\tvalidation_0-auc:0.751266\n",
            "[46]\tvalidation_0-auc:0.751062\n",
            "[47]\tvalidation_0-auc:0.750041\n",
            "[48]\tvalidation_0-auc:0.749837\n",
            "[49]\tvalidation_0-auc:0.750429\n",
            "[50]\tvalidation_0-auc:0.74953\n",
            "[51]\tvalidation_0-auc:0.748673\n",
            "[52]\tvalidation_0-auc:0.74806\n",
            "[53]\tvalidation_0-auc:0.749204\n",
            "[54]\tvalidation_0-auc:0.748244\n",
            "[55]\tvalidation_0-auc:0.746284\n",
            "[56]\tvalidation_0-auc:0.745018\n",
            "[57]\tvalidation_0-auc:0.746529\n",
            "[58]\tvalidation_0-auc:0.746938\n",
            "[59]\tvalidation_0-auc:0.745345\n",
            "[60]\tvalidation_0-auc:0.7451\n",
            "[61]\tvalidation_0-auc:0.745304\n",
            "[62]\tvalidation_0-auc:0.744283\n",
            "[63]\tvalidation_0-auc:0.744202\n",
            "[64]\tvalidation_0-auc:0.744896\n",
            "[65]\tvalidation_0-auc:0.743957\n",
            "[66]\tvalidation_0-auc:0.743344\n",
            "[67]\tvalidation_0-auc:0.742323\n",
            "[68]\tvalidation_0-auc:0.741262\n",
            "[69]\tvalidation_0-auc:0.741017\n",
            "[70]\tvalidation_0-auc:0.740813\n",
            "[71]\tvalidation_0-auc:0.739588\n",
            "Stopping. Best iteration:\n",
            "[21]\tvalidation_0-auc:0.775112\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.8734693877551021 |         0.0         |         0.0         |         0.0         |\n",
            "|      GRU 0.15     | 0.8877551020408163 |  0.8181818181818182 | 0.14516129032258066 | 0.24657534246575347 |\n",
            "|    XGBoost 0.15   | 0.889795918367347  |         0.6         |  0.3870967741935484 | 0.47058823529411764 |\n",
            "|    Logreg 0.15    | 0.889795918367347  |  0.7222222222222222 | 0.20967741935483872 |        0.325        |\n",
            "|      SVM 0.15     | 0.889795918367347  |  0.7857142857142857 |  0.1774193548387097 |  0.2894736842105263 |\n",
            "|   LSTM beta 0.15  | 0.8468271334792122 | 0.45555555555555555 |  0.6612903225806451 |  0.5394736842105263 |\n",
            "|   GRU beta 0.15   | 0.8708971553610503 |  0.5238095238095238 |  0.532258064516129  |  0.5280000000000001 |\n",
            "| XGBoost beta 0.15 | 0.7943107221006565 |  0.3490566037735849 |  0.5967741935483871 |  0.4404761904761905 |\n",
            "|  logreg beta 0.15 | 0.862144420131291  | 0.49230769230769234 |  0.5161290322580645 |  0.5039370078740157 |\n",
            "|   svm beta 0.15   | 0.7964989059080962 |  0.3176470588235294 | 0.43548387096774194 |  0.3673469387755102 |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "fju2_PabFn71",
        "outputId": "e3ec2d66-758d-4ebc-f023-40b2f11c7695"
      },
      "source": [
        "Result_purging.to_csv('AMT_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.879592</td>\n",
              "      <td>0.092308</td>\n",
              "      <td>0.048387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.527273</td>\n",
              "      <td>0.879592</td>\n",
              "      <td>0.495726</td>\n",
              "      <td>0.467742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.393258</td>\n",
              "      <td>0.834694</td>\n",
              "      <td>0.463576</td>\n",
              "      <td>0.564516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.581395</td>\n",
              "      <td>0.887755</td>\n",
              "      <td>0.476190</td>\n",
              "      <td>0.403226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.535714</td>\n",
              "      <td>0.881633</td>\n",
              "      <td>0.508475</td>\n",
              "      <td>0.483871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.478723</td>\n",
              "      <td>0.855580</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.725806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.443182</td>\n",
              "      <td>0.842451</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.629032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.778993</td>\n",
              "      <td>0.435754</td>\n",
              "      <td>0.629032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.406593</td>\n",
              "      <td>0.827133</td>\n",
              "      <td>0.483660</td>\n",
              "      <td>0.596774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.271845</td>\n",
              "      <td>0.761488</td>\n",
              "      <td>0.339394</td>\n",
              "      <td>0.451613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.883673</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>0.096774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.864333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.855580</td>\n",
              "      <td>0.057143</td>\n",
              "      <td>0.032258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.870897</td>\n",
              "      <td>0.119403</td>\n",
              "      <td>0.064516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.857768</td>\n",
              "      <td>0.029851</td>\n",
              "      <td>0.016129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.829322</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.016129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.887755</td>\n",
              "      <td>0.246575</td>\n",
              "      <td>0.145161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.889796</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.387097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>0.889796</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.209677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.889796</td>\n",
              "      <td>0.289474</td>\n",
              "      <td>0.177419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.455556</td>\n",
              "      <td>0.846827</td>\n",
              "      <td>0.539474</td>\n",
              "      <td>0.661290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.523810</td>\n",
              "      <td>0.870897</td>\n",
              "      <td>0.528000</td>\n",
              "      <td>0.532258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.349057</td>\n",
              "      <td>0.794311</td>\n",
              "      <td>0.440476</td>\n",
              "      <td>0.596774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.492308</td>\n",
              "      <td>0.862144</td>\n",
              "      <td>0.503937</td>\n",
              "      <td>0.516129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.317647</td>\n",
              "      <td>0.796499</td>\n",
              "      <td>0.367347</td>\n",
              "      <td>0.435484</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  AMT  1.000000  0.879592  0.092308  0.048387\n",
              "1            GRU 0.1  AMT  0.527273  0.879592  0.495726  0.467742\n",
              "2        XGBoost 0.1  AMT  0.393258  0.834694  0.463576  0.564516\n",
              "3         Logreg 0.1  AMT  0.581395  0.887755  0.476190  0.403226\n",
              "4            SVM 0.1  AMT  0.535714  0.881633  0.508475  0.483871\n",
              "5      LSTM beta 0.1  AMT  0.478723  0.855580  0.576923  0.725806\n",
              "6       GRU beta 0.1  AMT  0.443182  0.842451  0.520000  0.629032\n",
              "7   XGBoost beta 0.1  AMT  0.333333  0.778993  0.435754  0.629032\n",
              "8    logreg beta 0.1  AMT  0.406593  0.827133  0.483660  0.596774\n",
              "9       svm beta 0.1  AMT  0.271845  0.761488  0.339394  0.451613\n",
              "0           LSTM 0.2  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "1            GRU 0.2  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "2        XGBoost 0.2  AMT  0.857143  0.883673  0.173913  0.096774\n",
              "3         Logreg 0.2  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "4            SVM 0.2  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "5      LSTM beta 0.2  AMT  0.000000  0.864333  0.000000  0.000000\n",
              "6       GRU beta 0.2  AMT  0.250000  0.855580  0.057143  0.032258\n",
              "7   XGBoost beta 0.2  AMT  0.800000  0.870897  0.119403  0.064516\n",
              "8    logreg beta 0.2  AMT  0.200000  0.857768  0.029851  0.016129\n",
              "9       svm beta 0.2  AMT  0.055556  0.829322  0.025000  0.016129\n",
              "0          LSTM 0.15  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "1           GRU 0.15  AMT  0.818182  0.887755  0.246575  0.145161\n",
              "2       XGBoost 0.15  AMT  0.600000  0.889796  0.470588  0.387097\n",
              "3        Logreg 0.15  AMT  0.722222  0.889796  0.325000  0.209677\n",
              "4           SVM 0.15  AMT  0.785714  0.889796  0.289474  0.177419\n",
              "5     LSTM beta 0.15  AMT  0.455556  0.846827  0.539474  0.661290\n",
              "6      GRU beta 0.15  AMT  0.523810  0.870897  0.528000  0.532258\n",
              "7  XGBoost beta 0.15  AMT  0.349057  0.794311  0.440476  0.596774\n",
              "8   logreg beta 0.15  AMT  0.492308  0.862144  0.503937  0.516129\n",
              "9      svm beta 0.15  AMT  0.317647  0.796499  0.367347  0.435484"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdAbX2K3Fn72"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_pcL-boGa9k"
      },
      "source": [
        "## CL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "zKh5rMjiGa-A",
        "outputId": "3c72cd9d-d6d2-4b16-856f-c050d66e5107"
      },
      "source": [
        "dfs = pd.read_csv(\"CL.csv\")\n",
        "# dfs = dfs[::-1].reset_index()\n",
        "# Denoise prices\n",
        "dfs = denoise_data(dfs, '<CLOSE>')\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>US1.CL</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>76.46</td>\n",
              "      <td>77.24</td>\n",
              "      <td>76.25</td>\n",
              "      <td>77.916262</td>\n",
              "      <td>2872127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>US1.CL</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>75.90</td>\n",
              "      <td>76.10</td>\n",
              "      <td>74.40</td>\n",
              "      <td>77.916262</td>\n",
              "      <td>7170535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>US1.CL</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>74.73</td>\n",
              "      <td>75.04</td>\n",
              "      <td>73.75</td>\n",
              "      <td>77.916262</td>\n",
              "      <td>4674296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>US1.CL</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>74.63</td>\n",
              "      <td>74.63</td>\n",
              "      <td>73.67</td>\n",
              "      <td>77.916262</td>\n",
              "      <td>3563991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US1.CL</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>73.77</td>\n",
              "      <td>75.04</td>\n",
              "      <td>73.62</td>\n",
              "      <td>77.916262</td>\n",
              "      <td>3042250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>US1.CL</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>76.58</td>\n",
              "      <td>76.65</td>\n",
              "      <td>75.92</td>\n",
              "      <td>80.566900</td>\n",
              "      <td>144858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>US1.CL</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>75.79</td>\n",
              "      <td>76.15</td>\n",
              "      <td>75.41</td>\n",
              "      <td>80.566900</td>\n",
              "      <td>227270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>US1.CL</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>75.89</td>\n",
              "      <td>77.35</td>\n",
              "      <td>75.72</td>\n",
              "      <td>80.566900</td>\n",
              "      <td>244930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>US1.CL</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>77.24</td>\n",
              "      <td>77.24</td>\n",
              "      <td>75.59</td>\n",
              "      <td>80.566900</td>\n",
              "      <td>208601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>US1.CL</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>75.76</td>\n",
              "      <td>75.98</td>\n",
              "      <td>75.07</td>\n",
              "      <td>80.566900</td>\n",
              "      <td>154087</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2769 rows Ã— 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     <TICKER> <PER>    <DATE>  <TIME>  ...  <HIGH>  <LOW>    <CLOSE>    <VOL>\n",
              "0      US1.CL     D  20101004       0  ...   77.24  76.25  77.916262  2872127\n",
              "1      US1.CL     D  20101005       0  ...   76.10  74.40  77.916262  7170535\n",
              "2      US1.CL     D  20101006       0  ...   75.04  73.75  77.916262  4674296\n",
              "3      US1.CL     D  20101007       0  ...   74.63  73.67  77.916262  3563991\n",
              "4      US1.CL     D  20101008       0  ...   75.04  73.62  77.916262  3042250\n",
              "...       ...   ...       ...     ...  ...     ...    ...        ...      ...\n",
              "2764   US1.CL     D  20210927       0  ...   76.65  75.92  80.566900   144858\n",
              "2765   US1.CL     D  20210928       0  ...   76.15  75.41  80.566900   227270\n",
              "2766   US1.CL     D  20210929       0  ...   77.35  75.72  80.566900   244930\n",
              "2767   US1.CL     D  20210930       0  ...   77.24  75.59  80.566900   208601\n",
              "2768   US1.CL     D  20211001       0  ...   75.98  75.07  80.566900   154087\n",
              "\n",
              "[2769 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G6biKoROGa-B",
        "outputId": "1d29d547-c175-4b18-aa00-b7c315236563"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"41633d24-b761-47f5-8c35-87aeb17fbfb6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"41633d24-b761-47f5-8c35-87aeb17fbfb6\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '41633d24-b761-47f5-8c35-87aeb17fbfb6',\n",
              "                        [{\"type\": \"scatter\", \"x\": [20120926, 20120927, 20120928, 20121001, 20121002, 20121003, 20121004, 20121005, 20121008, 20121009, 20121010, 20121011, 20121012, 20121015, 20121016, 20121017, 20121018, 20121019, 20121022, 20121023, 20121024, 20121025, 20121026, 20121031, 20121101, 20121102, 20121105, 20121106, 20121107, 20121108, 20121109, 20121112, 20121113, 20121114, 20121115, 20121116, 20121119, 20121120, 20121121, 20121123, 20121126, 20121127, 20121128, 20121129, 20121130, 20121203, 20121204, 20121205, 20121206, 20121207, 20121210, 20121211, 20121212, 20121213, 20121214, 20121217, 20121218, 20121219, 20121220, 20121221, 20121224, 20121226, 20121227, 20121228, 20121231, 20130102, 20130103, 20130104, 20130107, 20130108, 20130109, 20130110, 20130111, 20130114, 20130115, 20130116, 20130117, 20130118, 20130122, 20130123, 20130124, 20130125, 20130128, 20130129, 20130130, 20130131, 20130201, 20130204, 20130205, 20130206, 20130207, 20130208, 20130211, 20130212, 20130213, 20130214, 20130215, 20130219, 20130220, 20130221, 20130222, 20130225, 20130226, 20130227, 20130228, 20130301, 20130304, 20130305, 20130306, 20130307, 20130308, 20130311, 20130312, 20130313, 20130314, 20130315, 20130318, 20130319, 20130320, 20130321, 20130322, 20130325, 20130326, 20130327, 20130328, 20130401, 20130402, 20130403, 20130404, 20130405, 20130408, 20130409, 20130410, 20130411, 20130412, 20130415, 20130416, 20130417, 20130418, 20130419, 20130422, 20130423, 20130424, 20130425, 20130426, 20130429, 20130430, 20130501, 20130502, 20130503, 20130506, 20130507, 20130508, 20130509, 20130510, 20130513, 20130514, 20130515, 20130516, 20130517, 20130520, 20130521, 20130522, 20130523, 20130524, 20130528, 20130529, 20130530, 20130531, 20130603, 20130604, 20130605, 20130606, 20130607, 20130610, 20130611, 20130612, 20130613, 20130614, 20130617, 20130618, 20130619, 20130620, 20130621, 20130624, 20130625, 20130626, 20130627, 20130628, 20130701, 20130702, 20130703, 20130705, 20130708, 20130709, 20130710, 20130711, 20130712, 20130715, 20130716, 20130717, 20130718, 20130719, 20130722, 20130723, 20130724, 20130725, 20130726, 20130729, 20130730, 20130731, 20130801, 20130802, 20130805, 20130806, 20130807, 20130808, 20130809, 20130812, 20130813, 20130814, 20130815, 20130816, 20130819, 20130820, 20130821, 20130822, 20130823, 20130826, 20130827, 20130828, 20130829, 20130830, 20130903, 20130904, 20130905, 20130906, 20130909, 20130910, 20130911, 20130912, 20130913, 20130916, 20130917, 20130918, 20130919, 20130920, 20130923, 20130924, 20130925, 20130926, 20130927, 20130930, 20131001, 20131002, 20131003, 20131004, 20131007, 20131008, 20131009, 20131010, 20131011, 20131014, 20131015, 20131016, 20131017, 20131018, 20131021, 20131022, 20131023, 20131024, 20131025, 20131028, 20131029, 20131030, 20131031, 20131101, 20131104, 20131105, 20131106, 20131107, 20131108, 20131111, 20131112, 20131113, 20131114, 20131115, 20131118, 20131119, 20131120, 20131121, 20131122, 20131125, 20131126, 20131127, 20131129, 20131202, 20131203, 20131204, 20131205, 20131206, 20131209, 20131210, 20131211, 20131212, 20131213, 20131216, 20131217, 20131218, 20131219, 20131220, 20131223, 20131224, 20131226, 20131227, 20131230, 20131231, 20140102, 20140103, 20140106, 20140107, 20140108, 20140109, 20140110, 20140113, 20140114, 20140115, 20140116, 20140117, 20140121, 20140122, 20140123, 20140124, 20140127, 20140128, 20140129, 20140130, 20140131, 20140203, 20140204, 20140205, 20140206, 20140207, 20140210, 20140211, 20140212, 20140213, 20140214, 20140218, 20140219, 20140220, 20140221, 20140224, 20140225, 20140226, 20140227, 20140228, 20140303, 20140304, 20140305, 20140306, 20140307, 20140310, 20140311, 20140312, 20140313, 20140314, 20140317, 20140318, 20140319, 20140320, 20140321, 20140324, 20140325, 20140326, 20140327, 20140328, 20140331, 20140401, 20140402, 20140403, 20140404, 20140407, 20140408, 20140409, 20140410, 20140411, 20140414, 20140415, 20140416, 20140417, 20140421, 20140422, 20140423, 20140424, 20140425, 20140428, 20140429, 20140430, 20140501, 20140502, 20140505, 20140506, 20140507, 20140508, 20140509, 20140512, 20140513, 20140514, 20140515, 20140516, 20140519, 20140520, 20140521, 20140522, 20140523, 20140527, 20140528, 20140529, 20140530, 20140602, 20140603, 20140604, 20140605, 20140606, 20140609, 20140610, 20140611, 20140612, 20140613, 20140616, 20140617, 20140618, 20140619, 20140620, 20140623, 20140624, 20140625, 20140626, 20140627, 20140630, 20140701, 20140702, 20140703, 20140707, 20140708, 20140709, 20140710, 20140711, 20140714, 20140715, 20140716, 20140717, 20140718, 20140721, 20140722, 20140723, 20140724, 20140725, 20140728, 20140729, 20140730, 20140731, 20140801, 20140804, 20140805, 20140806, 20140807, 20140808, 20140811, 20140812, 20140813, 20140814, 20140815, 20140818, 20140819, 20140820, 20140821, 20140822, 20140825, 20140826, 20140827, 20140828, 20140829, 20140902, 20140903, 20140904, 20140905, 20140908, 20140909, 20140910, 20140911, 20140912, 20140915, 20140916, 20140917, 20140918, 20140919, 20140922, 20140923, 20140924, 20140925, 20140926, 20140929, 20140930, 20141001, 20141002, 20141003, 20141006, 20141007, 20141008, 20141009, 20141010, 20141013, 20141014, 20141015, 20141016, 20141017, 20141020, 20141021, 20141022, 20141023, 20141024, 20141027, 20141028, 20141029, 20141030, 20141031, 20141103, 20141104, 20141105, 20141106, 20141107, 20141110, 20141111, 20141112, 20141113, 20141114, 20141117, 20141118, 20141119, 20141120, 20141121, 20141124, 20141125, 20141126, 20141128, 20141201, 20141202, 20141203, 20141204, 20141205, 20141208, 20141209, 20141210, 20141211, 20141212, 20141215, 20141216, 20141217, 20141218, 20141219, 20141222, 20141223, 20141224, 20141226, 20141229, 20141230, 20141231, 20150102, 20150105, 20150106, 20150107, 20150108, 20150109, 20150112, 20150113, 20150114, 20150115, 20150116, 20150120, 20150121, 20150122, 20150123, 20150126, 20150127, 20150128, 20150129, 20150130, 20150202, 20150203, 20150204, 20150205, 20150206, 20150209, 20150210, 20150211, 20150212, 20150213, 20150217, 20150218, 20150219, 20150220, 20150223, 20150224, 20150225, 20150226, 20150227, 20150302, 20150303, 20150304, 20150305, 20150306, 20150309, 20150310, 20150311, 20150312, 20150313, 20150316, 20150317, 20150318, 20150319, 20150320, 20150323, 20150324, 20150325, 20150326, 20150327, 20150330, 20150331, 20150401, 20150402, 20150406, 20150407, 20150408, 20150409, 20150410, 20150413, 20150414, 20150415, 20150416, 20150417, 20150420, 20150421, 20150422, 20150423, 20150424, 20150427, 20150428, 20150429, 20150430, 20150501, 20150504, 20150505, 20150506, 20150507, 20150508, 20150511, 20150512, 20150513, 20150514, 20150515, 20150518, 20150519, 20150520, 20150521, 20150522, 20150526, 20150527, 20150528, 20150529, 20150601, 20150602, 20150603, 20150604, 20150605, 20150608, 20150609, 20150610, 20150611, 20150612, 20150615, 20150616, 20150617, 20150618, 20150619, 20150622, 20150623, 20150624, 20150625, 20150626, 20150629, 20150630, 20150701, 20150702, 20150706, 20150707, 20150708, 20150709, 20150710, 20150713, 20150714, 20150715, 20150716, 20150717, 20150720, 20150721, 20150722, 20150723, 20150724, 20150727, 20150728, 20150729, 20150730, 20150731, 20150803, 20150804, 20150805, 20150806, 20150807, 20150810, 20150811, 20150812, 20150813, 20150814, 20150817, 20150818, 20150819, 20150820, 20150821, 20150824, 20150825, 20150826, 20150827, 20150828, 20150831, 20150901, 20150902, 20150903, 20150904, 20150908, 20150909, 20150910, 20150911, 20150914, 20150915, 20150916, 20150917, 20150918, 20150921, 20150922, 20150923, 20150924, 20150925, 20150928, 20150929, 20150930, 20151001, 20151002, 20151005, 20151006, 20151007, 20151008, 20151009, 20151012, 20151013, 20151014, 20151015, 20151016, 20151019, 20151020, 20151021, 20151022, 20151023, 20151026, 20151027, 20151028, 20151029, 20151030, 20151102, 20151103, 20151104, 20151105, 20151106, 20151109, 20151110, 20151111, 20151112, 20151113, 20151116, 20151117, 20151118, 20151119, 20151120, 20151123, 20151124, 20151125, 20151127, 20151130, 20151201, 20151202, 20151203, 20151204, 20151207, 20151208, 20151209, 20151210, 20151211, 20151214, 20151215, 20151216, 20151217, 20151218, 20151221, 20151222, 20151223, 20151224, 20151228, 20151229, 20151230, 20151231, 20160104, 20160105, 20160106, 20160107, 20160108, 20160111, 20160112, 20160113, 20160114, 20160115, 20160119, 20160120, 20160121, 20160122, 20160125, 20160126, 20160127, 20160128, 20160129, 20160201, 20160202, 20160203, 20160204, 20160205, 20160208, 20160209, 20160210, 20160211, 20160212, 20160216, 20160217, 20160218, 20160219, 20160222, 20160223, 20160224, 20160225, 20160226, 20160229, 20160301, 20160302, 20160303, 20160304, 20160307, 20160308, 20160309, 20160310, 20160311, 20160314, 20160315, 20160316, 20160317, 20160318, 20160321, 20160322, 20160323, 20160324, 20160328, 20160329, 20160330, 20160331, 20160401, 20160404, 20160405, 20160406, 20160407, 20160408, 20160411, 20160412, 20160413, 20160414, 20160415, 20160418, 20160419, 20160420, 20160421, 20160422, 20160425, 20160426, 20160427, 20160428, 20160429, 20160502, 20160503, 20160504, 20160505, 20160506, 20160509, 20160510, 20160511, 20160512, 20160513, 20160516, 20160517, 20160518, 20160519, 20160520, 20160523, 20160524, 20160525, 20160526, 20160527, 20160531, 20160601, 20160602, 20160603, 20160606, 20160607, 20160608, 20160609, 20160610, 20160613, 20160614, 20160615, 20160616, 20160617, 20160620, 20160621, 20160622, 20160623, 20160624, 20160627, 20160628, 20160629, 20160630, 20160701, 20160705, 20160706, 20160707, 20160708, 20160711, 20160712, 20160713, 20160714, 20160715, 20160718, 20160719, 20160720, 20160721, 20160722, 20160725, 20160726, 20160727, 20160728, 20160729, 20160801, 20160802, 20160803, 20160804, 20160805, 20160808, 20160809, 20160810, 20160811, 20160812, 20160815, 20160816, 20160817, 20160818, 20160819, 20160822, 20160823, 20160824, 20160825, 20160826, 20160829, 20160830, 20160831, 20160901, 20160902, 20160906, 20160907, 20160908, 20160909, 20160912, 20160913, 20160914, 20160915, 20160916, 20160919, 20160920, 20160921, 20160922, 20160923, 20160926, 20160927, 20160928, 20160929, 20160930, 20161003, 20161004, 20161005, 20161006, 20161007, 20161010, 20161011, 20161012, 20161013, 20161014, 20161017, 20161018, 20161019, 20161020, 20161021, 20161024, 20161025, 20161026, 20161027, 20161028, 20161031, 20161101, 20161102, 20161103, 20161104, 20161107, 20161108, 20161109, 20161110, 20161111, 20161114, 20161115, 20161116, 20161117, 20161118, 20161121, 20161122, 20161123, 20161125, 20161128, 20161129, 20161130, 20161201, 20161202, 20161205, 20161206, 20161207, 20161208, 20161209, 20161212, 20161213, 20161214, 20161215, 20161216, 20161219, 20161220, 20161221, 20161222, 20161223, 20161227, 20161228, 20161229, 20161230, 20170103, 20170104, 20170105, 20170106, 20170109, 20170110, 20170111, 20170112, 20170113, 20170117, 20170118, 20170119, 20170120, 20170123, 20170124, 20170125, 20170126, 20170127, 20170130, 20170131, 20170201, 20170202, 20170203, 20170206, 20170207, 20170208, 20170209, 20170210, 20170213, 20170214, 20170215, 20170216, 20170217, 20170221, 20170222, 20170223, 20170224, 20170227, 20170228, 20170301, 20170302, 20170303, 20170306, 20170307, 20170308, 20170309, 20170310, 20170313, 20170314, 20170315, 20170316, 20170317, 20170320, 20170321, 20170322, 20170323, 20170324, 20170327, 20170328, 20170329, 20170330, 20170331, 20170403, 20170404, 20170405, 20170406, 20170407, 20170410, 20170411, 20170412, 20170413, 20170417, 20170418, 20170419, 20170420, 20170421, 20170424, 20170425, 20170426, 20170427, 20170428, 20170501, 20170502, 20170503, 20170504, 20170505, 20170508, 20170509, 20170510, 20170511, 20170512, 20170515, 20170516, 20170517, 20170518, 20170519, 20170522, 20170523, 20170524, 20170525, 20170526, 20170530, 20170531, 20170601, 20170602, 20170605, 20170606, 20170607, 20170608, 20170609, 20170612, 20170613, 20170614, 20170615, 20170616, 20170619, 20170620, 20170621, 20170622, 20170623, 20170626, 20170627, 20170628, 20170629, 20170630, 20170703, 20170705, 20170706, 20170707, 20170710, 20170711, 20170712, 20170713, 20170714, 20170717, 20170718, 20170719, 20170720, 20170721, 20170724, 20170725, 20170726, 20170727, 20170728, 20170731, 20170801, 20170802, 20170803, 20170804, 20170807, 20170808, 20170809, 20170810, 20170811, 20170814, 20170815, 20170816, 20170817, 20170818, 20170821, 20170822, 20170823, 20170824, 20170825, 20170828, 20170829, 20170830, 20170831, 20170901, 20170905, 20170906, 20170907, 20170908, 20170911, 20170912, 20170913, 20170914, 20170915, 20170918, 20170919, 20170920, 20170921, 20170922, 20170925, 20170926, 20170927, 20170928, 20170929, 20171002, 20171003, 20171004, 20171005, 20171006, 20171009, 20171010, 20171011, 20171012, 20171013, 20171016, 20171017, 20171018, 20171019, 20171020, 20171023, 20171024, 20171025, 20171026, 20171027, 20171030, 20171031, 20171101, 20171102, 20171103, 20171106, 20171107, 20171108, 20171109, 20171110, 20171113, 20171114, 20171115, 20171116, 20171117, 20171120, 20171121, 20171122, 20171124, 20171127, 20171128, 20171129, 20171130, 20171201, 20171204, 20171205, 20171206, 20171207, 20171208, 20171211, 20171212, 20171213, 20171214, 20171215, 20171218, 20171219, 20171220, 20171221, 20171222, 20171226, 20171227, 20171228, 20171229, 20180102, 20180103, 20180104, 20180105, 20180108, 20180109, 20180110, 20180111, 20180112, 20180116, 20180117, 20180118, 20180119, 20180122, 20180123, 20180124, 20180125, 20180126, 20180129, 20180130, 20180131, 20180201, 20180202, 20180205, 20180206, 20180207, 20180208, 20180209, 20180212, 20180213, 20180214, 20180215, 20180216, 20180220, 20180221, 20180222, 20180223, 20180226, 20180227, 20180228, 20180301, 20180302, 20180305, 20180306, 20180307, 20180308, 20180309, 20180312, 20180313, 20180314, 20180315, 20180316, 20180319, 20180320, 20180321, 20180322, 20180323, 20180326, 20180327, 20180328, 20180329, 20180402, 20180403, 20180404, 20180405, 20180406, 20180409, 20180410, 20180411, 20180412, 20180413, 20180416, 20180417, 20180418, 20180419, 20180420, 20180423, 20180424, 20180425, 20180426, 20180427, 20180430, 20180501, 20180502, 20180503, 20180504, 20180507, 20180508, 20180509, 20180510, 20180511, 20180514, 20180515, 20180516, 20180517, 20180518, 20180521, 20180522, 20180523, 20180524, 20180525, 20180529, 20180530, 20180531, 20180601, 20180604, 20180605, 20180606, 20180607, 20180608, 20180611, 20180612, 20180613, 20180614, 20180615, 20180618, 20180619, 20180620, 20180621, 20180622, 20180625, 20180626, 20180627, 20180628, 20180629, 20180702, 20180703, 20180705, 20180706, 20180709, 20180710, 20180711, 20180712, 20180713, 20180716, 20180717, 20180718, 20180719, 20180720, 20180723, 20180724, 20180725, 20180726, 20180727, 20180730, 20180731, 20180801, 20180802, 20180803, 20180806, 20180807, 20180808, 20180809, 20180810, 20180813, 20180814, 20180815, 20180816, 20180817, 20180820, 20180821, 20180822, 20180823, 20180824, 20180827, 20180828, 20180829, 20180830, 20180831, 20180904, 20180905, 20180906, 20180907, 20180910, 20180911, 20180912], \"y\": [106.202301, 106.218701, 106.244801, 106.25870099999999, 106.28080100000001, 106.31110100000002, 106.35210099999999, 106.40260099999998, 106.39920099999999, 106.385401, 106.36190099999997, 106.33890099999999, 106.32040099999996, 106.31930099999998, 106.29490100000001, 106.27660100000001, 106.29070099999998, 106.30470100000001, 106.33410099999999, 106.34900099999999, 106.35580100000001, 106.374901, 106.387801, 106.405301, 106.42350100000002, 106.42350100000002, 106.444201, 106.480201, 106.52250099999998, 106.549201, 106.579101, 106.613501, 106.65070099999998, 106.69410099999999, 106.74010100000001, 106.776101, 106.78550100000001, 106.808801, 106.83300099999998, 106.89060099999998, 106.96040099999999, 107.03570099999999, 107.07549999999999, 107.12099999999998, 107.14999999999998, 107.16829999999999, 107.1926, 107.22460000000001, 107.27990000000004, 107.33150000000002, 107.3951, 107.46719999999999, 107.53379999999999, 107.60080000000002, 107.6661, 107.73140000000001, 107.79289999999999, 107.8567, 107.9293, 107.998, 108.07730000000001, 108.15749999999998, 108.23329999999999, 108.36940000000003, 108.42120000000001, 108.43680000000002, 108.45320000000001, 108.50119999999998, 108.5707, 108.5707, 108.64229999999999, 108.72110000000002, 108.819, 108.95389999999999, 109.07159999999998, 109.1931, 109.32050000000001, 109.45710000000001, 109.57350000000001, 109.6946, 109.82080000000002, 109.95920000000002, 110.08820000000001, 110.22789999999999, 110.381, 110.5272, 110.64479999999998, 110.76928999999998, 110.87859, 110.99638999999998, 111.11408999999998, 111.23929, 111.37709, 111.47819000000001, 111.59269000000003, 111.69729000000001, 111.81029000000001, 111.92799, 112.04728999999998, 112.18788999999997, 112.31598999999999, 112.45018999999998, 112.57458999999997, 112.70949, 112.84969, 113.14488999999999, 113.30589, 113.49089000000002, 113.05658999999997, 112.62552999999998, 112.19042999999999, 111.75923000000003, 111.75923000000003, 111.32053, 110.88972999999999, 110.45582999999999, 110.00903, 109.53829799999998, 109.055798, 108.573398, 108.096198, 107.623698, 107.12884800000002, 106.63794800000001, 106.149648, 105.66014799999998, 105.16834799999998, 104.660248, 104.15524799999997, 103.65914799999999, 103.16754800000001, 102.66674799999998, 102.14534800000001, 101.602748, 101.06184800000001, 100.51474800000001, 100.002648, 99.474548, 98.961348, 98.43794800000002, 97.91934800000001, 97.39484799999998, 96.89034799999997, 96.38594799999998, 95.89124799999999, 95.402148, 94.905048, 94.394348, 93.866848, 93.334048, 92.78824800000001, 92.23184800000001, 91.679548, 91.12914799999999, 90.57044799999998, 90.01054800000001, 89.45344800000001, 88.90104800000003, 88.34654800000004, 87.793848, 87.242348, 86.684048, 86.133448, 85.588648, 85.049248, 84.521148, 84.008948, 83.495748, 82.97674799999999, 82.44009800000002, 81.913798, 81.376298, 80.830198, 80.251448, 79.675648, 79.086148, 78.49424800000001, 77.89014800000001, 77.30704800000001, 76.718148, 76.127748, 75.523748, 74.92374800000002, 74.318348, 73.704748, 73.099048, 72.50564800000001, 71.89235800000002, 71.299558, 70.69965800000001, 70.095058, 69.483758, 68.285158, 67.688758, 67.11055800000001, 66.52665800000001, 65.937558, 65.348058, 65.348058, 64.739558, 64.12155800000001, 63.52335799999999, 62.919557999999995, 62.307857999999996, 61.706858000000004, 61.09445800000001, 60.477658, 59.837358, 59.183858, 59.149958000000005, 59.128218000000004, 59.12061799999999, 59.11931799999999, 59.12051800000001, 59.123318, 59.134218000000004, 59.15221800000001, 59.184350000000016, 59.210350000000005, 59.26655000000001, 59.31555, 59.35754999999999, 59.4147, 59.48190000000002, 59.550500000000014, 59.6148, 59.67609999999999, 59.74849999999999, 59.82119999999999, 59.88659999999999, 59.9502, 60.0013, 60.06970000000001, 60.15689999999999, 60.23579999999999, 60.32509999999999, 60.5065, 60.58420000000001, 60.666000000000004, 60.73920000000001, 60.82200000000001, 60.90410000000001, 60.9041, 60.985200000000006, 61.05950000000001, 61.12799999999999, 61.199600000000004, 61.26299999999999, 61.32529999999999, 61.387299999999996, 61.4478, 61.5211, 61.59, 61.64809999999999, 61.71119999999999, 61.766699999999986, 61.8216, 61.8657, 61.8989, 61.944399999999995, 61.984700000000004, 62.033600000000014, 62.0743, 62.1158, 62.154500000000006, 62.19850000000001, 62.24340000000001, 62.2861, 62.3222, 62.35445, 62.38515000000001, 62.42045, 62.45775, 62.513099999999994, 62.5725, 62.62830000000001, 62.691500000000005, 62.7546, 62.8795, 62.95039999999999, 63.017799999999994, 63.079600000000006, 63.1263, 63.17300000000001, 63.22370000000001, 63.22370000000001, 63.26410000000001, 63.2989, 63.32540000000001, 63.34139999999999, 63.352500000000006, 63.36380000000001, 63.37570000000001, 63.38400000000001, 63.4011, 63.4132, 63.42229999999999, 63.4409, 63.46589999999999, 63.49119999999999, 63.51309999999999, 63.52759999999999, 63.54380000000001, 63.57019999999999, 63.5901, 63.61529999999999, 63.64419999999999, 63.679300000000005, 63.709500000000006, 63.74969999999999, 63.7775, 63.79990000000001, 63.816500000000005, 63.831900000000005, 63.85240000000001, 63.8619, 63.8601, 63.8704, 63.891999999999996, 63.900600000000004, 63.8996, 63.9049, 63.9027, 63.8883, 63.86649999999999, 63.856799999999986, 63.85, 63.83709999999999, 63.82849999999999, 63.817, 63.7999, 63.794700000000006, 63.78859999999999, 63.7915, 63.800599999999996, 63.8071, 63.81049999999999, 63.80570000000001, 63.8125, 63.823299999999996, 63.842600000000004, 63.851900000000015, 63.85780000000002, 63.86180000000001, 63.8604, 63.866099999999996, 63.870599999999996, 63.89629999999999, 63.91299999999999, 63.93589999999998, 63.96069999999999, 63.97519999999999, 63.987249999999996, 64.00755, 64.03045, 64.06175, 64.09864999999999, 64.13024999999999, 64.167, 64.1902, 64.2119, 64.2335, 64.25450000000001, 64.2731, 64.2927, 64.318, 64.333, 64.3513, 64.37859999999999, 64.4214, 64.4615, 64.4615, 64.49870000000001, 64.5258, 64.5543, 64.5878, 64.6238, 64.663, 64.69914999999999, 64.73045, 64.76155, 64.80215, 64.84665000000001, 64.91005000000001, 64.98325000000001, 65.03835000000001, 65.10055, 65.16534999999999, 65.23284999999998, 65.31004999999999, 65.39025000000001, 65.46765, 65.54625000000001, 65.62245000000001, 65.69545, 65.76015, 65.83625000000002, 65.90960000000001, 65.97715000000001, 66.04865000000001, 66.13125000000001, 66.20955, 66.28845, 66.35504999999999, 66.42344999999999, 66.49164999999999, 66.55645000000001, 66.60985000000001, 66.66865000000001, 66.76150000000001, 66.79509999999999, 66.825, 66.82849999999999, 66.8352, 66.8408, 66.8408, 66.84609999999999, 66.85570000000001, 66.8562, 66.8647, 66.8813, 66.8895, 66.8992, 66.9082, 66.9118, 66.9154, 66.91749999999999, 66.92299999999999, 66.92249999999999, 66.9262, 66.932, 66.93709999999999, 66.943, 66.939168, 66.93016800000001, 66.917058, 66.90195800000001, 66.89195799999999, 66.88185800000001, 66.85445800000001, 66.816658, 66.787358, 66.762858, 66.743908, 66.73020799999999, 66.71760799999998, 66.707658, 66.68205800000001, 66.664558, 66.648258, 66.63225800000001, 66.60940799999999, 66.59910799999997, 66.58300799999998, 66.564208, 66.536508, 66.512408, 66.49435799999999, 66.49435799999998, 66.477458, 66.462158, 66.45105799999999, 66.436058, 66.423158, 66.397008, 66.375008, 66.343308, 66.307358, 66.283358, 66.259358, 66.229358, 66.200558, 66.17855800000001, 66.154858, 66.13645800000002, 66.114158, 66.088258, 66.065358, 66.048108, 66.043808, 66.04570799999999, 66.04630800000001, 66.048608, 66.041708, 66.03290799999999, 66.033808, 66.04270799999999, 66.04610799999999, 66.04220799999999, 66.042408, 66.038408, 66.039608, 66.035308, 66.034108, 66.025508, 66.02020800000001, 66.011908, 66.01135800000002, 66.011107, 66.014207, 66.00680700000001, 66.00440699999999, 66.00350699999998, 66.011807, 66.015807, 66.016907, 66.02360700000001, 66.027707, 66.02600699999999, 66.02520700000001, 66.038657, 66.06965699999999, 66.108657, 66.17715700000001, 66.247657, 66.31455700000001, 66.385557, 66.442557, 66.498357, 66.53825699999999, 66.578257, 66.619757, 66.653757, 66.694857, 66.746057, 66.78575699999999, 66.825857, 66.86675699999999, 66.903157, 66.94065699999999, 66.983457, 67.024857, 67.060357, 67.108789, 67.13308899999998, 67.151699, 67.16409900000001, 67.172399, 67.25329899999998, 67.30919899999999, 67.360999, 67.408899, 67.462049, 67.50994899999999, 67.509949, 67.54894900000001, 67.590899, 67.632499, 67.677799, 67.72184899999999, 67.769549, 67.812749, 67.860149, 67.905749, 67.956549, 68.014549, 68.07564899999998, 68.139049, 68.19279900000001, 68.250299, 68.30829899999999, 68.356999, 68.419499, 68.45924900000001, 68.513299, 68.553399, 68.59689900000001, 68.65104900000001, 68.69844900000001, 68.743349, 68.77334900000001, 68.80334900000001, 68.83594900000001, 68.878049, 68.918849, 68.95424899999999, 68.984849, 69.00824899999999, 69.026749, 69.045649, 69.058549, 69.06814899999999, 69.099449, 69.11104900000001, 69.12514900000001, 69.137449, 69.150549, 69.168049, 69.168049, 69.18504899999999, 69.204049, 69.222149, 69.234649, 69.23564900000001, 69.24624899999999, 69.255649, 69.261049, 69.256849, 69.25245, 69.24164999999999, 69.23604999999999, 69.21424999999999, 69.19725, 69.18144999999998, 69.16195, 69.15115, 69.13725000000001, 69.13335000000001, 69.13234999999999, 69.13615, 69.12565, 69.11705, 69.10315, 69.08675, 69.06805, 69.05234999999999, 69.03184999999999, 69.01195, 68.98315, 68.96695000000001, 68.94975000000001, 68.93244999999999, 68.92394999999999, 68.90785000000001, 68.85425000000001, 68.82634999999999, 68.79244999999997, 68.76794999999998, 68.75524999999999, 68.72915, 68.70264999999999, 68.70264999999999, 68.67239999999998, 68.6385, 68.63109999999999, 68.6398, 68.652, 68.67299999999999, 68.6528, 68.64240000000001, 68.6172, 68.5905, 68.5526, 68.5086, 68.4775, 68.44849999999998, 68.41359999999999, 68.39229999999999, 68.35870000000001, 68.31955000000002, 68.28405000000001, 68.25295000000001, 68.2234, 68.19019999999999, 68.15929999999999, 68.124, 68.0921, 68.0502, 68.01195, 67.97255, 67.93575000000001, 67.90175, 67.87095000000001, 67.86760000000001, 67.8519, 67.8478, 67.8533, 67.8526, 67.8591, 67.85549999999999, 67.8563, 67.86050000000002, 67.8602, 67.8449, 67.81959999999998, 67.8044, 67.7922, 67.78179999999999, 67.76189999999998, 67.729, 67.6855, 67.61399999999999, 67.5245, 67.4471, 67.381, 67.3154, 67.24419999999999, 67.1609, 67.0897, 67.0227, 66.9385, 66.872, 66.8031, 66.73790000000002, 66.66700000000002, 66.59530000000001, 66.5274, 66.4663, 66.40960000000001, 66.34909999999999, 66.2954, 66.2443, 66.18849999999999, 66.13245, 66.09034999999999, 66.03405, 65.98594999999999, 65.93884999999999, 65.89755, 65.86115, 65.83835, 65.76724999999999, 65.73985, 65.71185, 65.68525, 65.65675, 65.62894999999999, 65.62895, 65.62320000000001, 65.6188, 65.6181, 65.6207, 65.6198, 65.63909899999999, 65.66299899999999, 65.690699, 65.725299, 65.759899, 65.79209900000001, 65.784999, 65.79423200000001, 65.811532, 65.83298200000002, 65.849582, 65.84398200000001, 65.830082, 65.825182, 65.817282, 65.799582, 65.78023199999998, 65.77133199999999, 65.75963200000001, 65.766232, 65.776032, 65.775132, 65.782582, 65.791382, 65.78248200000002, 65.795882, 65.790882, 65.78288200000001, 65.76528200000001, 65.748632, 65.742632, 65.73003200000001, 65.699532, 65.691232, 65.673232, 65.661032, 65.658432, 65.66863200000002, 65.668632, 65.658032, 65.62473200000001, 65.604432, 65.59043199999999, 65.577532, 65.56293199999999, 65.540832, 65.530732, 65.52083199999998, 65.499432, 65.463732, 65.433432, 65.398532, 65.35053199999999, 65.29913199999999, 65.250232, 65.21673200000001, 65.18573199999999, 65.171032, 65.185032, 65.22433199999999, 65.22653199999999, 65.23223199999998, 65.24503199999998, 65.25623199999998, 65.28673199999999, 65.307732, 65.328632, 65.38913199999999, 65.416232, 65.445832, 65.486832, 65.527332, 65.568232, 65.60473200000001, 65.658332, 65.675432, 65.701232, 65.73783200000001, 65.78753200000001, 65.837082, 65.879982, 65.879982, 65.934282, 65.97528199999999, 66.003782, 66.04108200000002, 66.061382, 66.06438200000001, 66.089482, 66.106882, 66.12398200000001, 66.14208200000002, 66.15518200000001, 66.170782, 66.19358199999999, 66.21023199999999, 66.222832, 66.232532, 66.247582, 66.26628199999999, 66.27868299999999, 66.29378299999999, 66.310983, 66.317183, 66.326883, 66.333983, 66.369483, 66.39995, 66.42685, 66.45855000000002, 66.49430000000001, 66.54750000000001, 66.6013, 66.65140000000001, 66.69319999999999, 66.75139999999999, 66.81505, 66.8716, 66.92650000000002, 66.97380000000001, 67.02280000000002, 67.07499999999999, 67.12254999999999, 67.15744999999998, 67.18285, 67.19535, 67.23275, 67.26395000000001, 67.29995, 67.35555, 67.40165, 67.46175, 67.51815, 67.5764, 67.63024999999999, 67.69645, 67.75545, 67.80995, 67.84684999999999, 67.89995, 67.96064999999999, 68.02125, 68.06354999999999, 68.09325, 68.12765000000002, 68.15895, 68.18355, 68.21709999999999, 68.2574, 68.31535000000001, 68.37115, 68.43365, 68.51545000000002, 68.60405, 68.69195, 68.77635000000001, 68.86355, 68.94855000000001, 69.03855, 69.11654999999999, 69.20885, 69.36435, 69.44485, 69.51565000000001, 69.58895000000001, 69.65675, 69.70395, 69.70394999999999, 69.76894999999999, 69.82095, 69.86059999999999, 69.9122, 69.96875000000001, 70.03985, 70.10985000000001, 70.18445000000001, 70.27285, 70.349749, 70.433949, 70.50824899999999, 70.575549, 70.64574900000001, 70.716549, 70.79204900000002, 70.873749, 70.94504900000001, 71.02324899999999, 71.105749, 71.178749, 71.250449, 71.31434899999999, 71.36584899999998, 71.42264899999999, 71.491599, 71.562499, 71.62889899999999, 71.685699, 71.751199, 71.811049, 71.86814899999999, 71.917199, 71.96119900000001, 72.00584900000001, 72.06004899999999, 72.10904899999997, 72.199549, 72.240149, 72.27784899999999, 72.31624899999998, 72.34964899999999, 72.381649, 72.381649, 72.41674899999998, 72.44744899999999, 72.48914900000001, 72.523549, 72.557649, 72.587849, 72.628549, 72.673449, 72.698349, 72.71509899999998, 72.70749899999998, 72.73189900000001, 72.76059900000001, 72.788399, 72.815099, 72.844999, 72.874049, 72.887349, 72.907749, 72.919349, 72.933549, 72.94609899999999, 72.965649, 72.98344900000001, 72.99884899999999, 73.018049, 73.034249, 73.035849, 73.04894900000001, 73.061149, 73.08324900000001, 73.10804900000001, 73.125749, 73.14984899999999, 73.170449, 73.199499, 73.213749, 73.22504900000001, 73.22999899999999, 73.227499, 73.223499, 73.220699, 73.220699, 73.21499899999999, 73.204499, 73.19619900000002, 73.186299, 73.17512400000001, 73.170124, 73.154624, 73.133124, 73.11862400000001, 73.120424, 73.107524, 73.061524, 73.013624, 72.953024, 72.913424, 72.88077400000002, 72.82912400000001, 72.76927400000001, 72.699482, 72.633682, 72.56058200000001, 72.49318199999999, 72.423583, 72.346683, 72.254483, 72.16588300000001, 72.06838300000001, 71.96788300000001, 71.87098300000001, 71.785183, 71.696583, 71.618483, 71.547283, 71.470083, 71.386883, 71.309383, 71.238683, 71.166983, 71.086633, 70.996333, 70.911333, 70.833533, 70.745833, 70.65623299999999, 70.571033, 70.47768300000001, 70.384583, 70.298233, 70.217033, 70.14353299999999, 70.064533, 69.972933, 69.880333, 69.793933, 69.70768299999999, 69.63318099999998, 69.56798099999999, 69.49208100000001, 69.426781, 69.35443099999999, 69.291931, 69.232831, 69.172581, 69.070381, 68.962881, 68.87108099999999, 68.78833100000001, 68.73623100000002, 68.66793100000001, 68.60443099999999, 68.55168099999999, 68.49658099999999, 68.44308099999999, 68.398431, 68.350531, 68.298131, 68.24918100000001, 68.206481, 68.194731, 68.190931, 68.202631, 68.19483100000001, 68.18503100000001, 68.18683100000001, 68.191631, 68.191631, 68.196231, 68.201681, 68.213381, 68.22848099999999, 68.235681, 68.24788099999999, 68.26728100000001, 68.285381, 68.302481, 68.32468100000001, 68.35463100000001, 68.384631, 68.412521, 68.436421, 68.460671, 68.494071, 68.519171, 68.539971, 68.56784600000002, 68.59164600000001, 68.618146, 68.654346, 68.68284600000001, 68.703196, 68.734296, 68.79339599999999, 68.85364600000001, 68.92509600000001, 68.99614600000001, 69.068846, 69.14579599999999, 69.227896, 69.30948799999999, 69.380088, 69.45228800000001, 69.517388, 69.589588, 69.74858799999998, 69.830888, 69.903788, 69.975588, 70.032788, 70.084138, 70.084138, 70.146538, 70.201588, 70.251988, 70.298238, 70.35663799999999, 70.410838, 70.461038, 70.516788, 70.56868800000001, 70.664788, 70.740388, 70.82588799999999, 70.908388, 70.99078800000001, 71.077888, 71.176988, 71.27608799999999, 71.37428799999999, 71.46918800000002, 71.56258799999999, 71.66908799999999, 71.78738799999999, 71.904188, 72.017388, 72.116738, 72.20339000000001, 72.28029, 72.36668999999999, 72.44589, 72.53384, 72.61424, 72.69864, 72.77974, 72.89004, 73.00004000000001, 73.22504000000002, 73.32204000000002, 73.41304000000001, 73.49924, 73.57494, 73.64854, 73.72034, 73.72034, 73.77744, 73.83384, 73.88754, 73.92589, 73.95889, 73.96174, 73.95499, 73.94599, 73.92649, 73.91769, 73.91259, 73.91609, 73.90069, 73.88389000000001, 73.87814000000002, 73.87004, 73.85844000000002, 73.84894, 73.83214, 73.80574, 73.78294000000001, 73.76139, 73.73939, 73.71734000000001, 73.69364, 73.67125000000001, 73.64675, 73.6242, 73.603, 73.5854, 73.56420000000001, 73.5368, 73.5174, 73.5014, 73.4818, 73.45559999999999, 73.43124999999999, 73.40534999999998, 73.38715, 73.36179999999999, 73.3407, 73.31644999999999, 73.28755, 73.26555, 73.24045000000001, 73.21515000000001, 73.20155, 73.18945, 73.17554999999999, 73.16085000000001, 73.14145000000002, 73.13055000000001, 73.12555000000002, 73.13085000000001, 73.12655000000001, 73.13074999999999, 73.1501, 73.1647, 73.17755, 73.19095000000002, 73.20400000000001, 73.216, 73.2239, 73.24029999999999, 73.25585, 73.26704999999998, 73.23375, 73.22325000000002, 73.22215000000001, 73.23095, 73.25234999999999, 73.26386000000001, 73.25986, 73.249693, 73.220393, 73.180093, 73.13519299999999, 73.082593, 73.02446, 72.89916000000001, 72.83316, 72.78026, 72.73046000000001, 72.68286, 72.62565999999998, 72.62565999999998, 72.56466, 72.51056, 72.47456000000001, 72.44026, 72.41535999999999, 72.39706, 72.37235999999999, 72.33836000000001, 72.31085999999999, 72.27955999999999, 72.25906, 72.23980999999999, 72.21801, 72.19861, 72.18431000000001, 72.17871, 72.16771, 72.16761000000001, 72.17501, 72.19751000000001, 72.21836, 72.23506000000002, 72.24196, 72.25065, 72.26105000000001, 72.25255, 72.26715, 72.27895000000001, 72.28915, 72.29945000000001, 72.3224, 72.34554999999999, 72.37525, 72.41054999999999, 72.44914999999997, 72.48789999999998, 72.5219, 72.59615, 72.62905, 72.66976000000001, 72.70945999999999, 72.74655999999999, 72.78380999999999, 72.78380999999999, 72.81880999999998, 72.85401, 72.88391, 72.92211, 72.97670999999998, 73.03541, 73.09861, 73.16381, 73.22000999999999, 73.27881000000001, 73.33576, 73.35746, 73.38416000000001, 73.41636, 73.44326, 73.45815999999999, 73.46166, 73.44016, 73.42796, 73.40786, 73.37476000000001, 73.34826, 73.32206000000001, 73.29956, 73.28206, 73.28256, 73.26736, 73.23356000000001, 73.19426, 73.15675999999999, 73.13146, 73.11256, 73.08746000000001, 73.04826, 73.00476, 72.94766000000001, 72.91526, 72.86536000000001, 72.82406, 72.77656, 72.72985, 72.68884999999999, 72.68884999999999, 72.644617, 72.608617, 72.58241699999999, 72.561217, 72.539817, 72.51275000000001, 72.48855, 72.46815000000001, 72.45795, 72.44595, 72.44865, 72.45615, 72.45645000000002, 72.46345000000001, 72.47125000000001, 72.45665000000001, 72.44364999999999, 72.42595, 72.40615, 72.38535, 72.36965, 72.35914999999999, 72.3645, 72.3664, 72.3652, 72.34530000000001, 72.3023, 72.246, 72.1846, 72.1287, 72.0706, 72.0062, 71.91340000000001, 71.8161, 71.70499999999998, 71.606, 71.50991, 71.40641, 71.30441, 71.18950999999998, 71.08211, 70.96870999999999, 70.84900999999999, 70.71705999999999, 70.59270999999998, 70.46880999999999, 70.33940999999999, 70.20720999999999, 70.07751000000002, 69.95321000000001, 69.82300999999998, 69.70891, 69.59611, 69.48679999999999, 69.36349999999999, 69.2355, 69.10595, 68.98075, 68.86094999999999, 68.74245, 68.62175, 68.49225, 68.36305, 68.21845, 68.07775000000001, 67.95594999999999, 67.82095, 67.68754999999999, 67.59214999999999, 67.48924999999998, 67.38875, 67.29735, 67.21545, 67.13875, 67.08975000000001, 67.03365, 66.97765, 66.93485, 66.89685, 66.80005, 66.76095000000001, 66.70115000000001, 66.63805, 66.59535, 66.55995000000001, 66.55995, 66.52169999999998, 66.46685000000001, 66.41365, 66.37375, 66.34255000000002, 66.30705, 66.27385, 66.24275, 66.21305000000001, 66.18035, 66.14024999999998, 66.08864999999999, 66.05045, 66.01685000000002, 65.99045000000001, 65.96724999999998, 65.93634999999999, 65.89485, 65.85225, 65.82065, 65.78795, 65.76265000000001, 65.74805, 65.73034999999999, 65.69855, 65.65565, 65.62385, 65.58005, 65.53215, 65.48485, 65.43655, 65.39184999999999, 65.33704999999999, 65.28744999999999, 65.23065, 65.18085, 65.12650000000001, 65.0248, 64.999, 65.00359999999999, 65.0214, 65.0349, 65.0588, 65.0588, 65.0751, 65.0886, 65.12435, 65.16675000000001, 65.21465, 65.25304999999999, 65.28694999999999, 65.32604999999998, 65.37275, 65.42464999999999, 65.47364999999999, 65.50585, 65.53475, 65.56535, 65.59795, 65.62585000000001, 65.64245, 65.64325, 65.6448, 65.64330000000001, 65.64450000000001, 65.64240000000001, 65.6333, 65.6344, 65.63999999999999, 65.6471, 65.66669999999999, 65.68209999999999, 65.65190000000001, 65.6032, 65.5715, 65.52960000000002, 65.49300000000001, 65.4653, 65.43439999999998, 65.3723, 65.34149999999998, 65.34029999999998, 65.31909999999999, 65.2915, 65.2652, 65.23809999999999, 65.2381, 65.2234, 65.2019, 65.17689999999999]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('41633d24-b761-47f5-8c35-87aeb17fbfb6');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"58aa4b38-a7fd-494b-9bc6-905fe92c1fc8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"58aa4b38-a7fd-494b-9bc6-905fe92c1fc8\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '58aa4b38-a7fd-494b-9bc6-905fe92c1fc8',\n",
              "                        [{\"type\": \"scatter\", \"x\": [20120926, 20120927, 20120928, 20121001, 20121002, 20121003, 20121004, 20121005, 20121008, 20121009, 20121010, 20121011, 20121012, 20121015, 20121016, 20121017, 20121018, 20121019, 20121022, 20121023, 20121024, 20121025, 20121026, 20121031, 20121101, 20121102, 20121105, 20121106, 20121107, 20121108, 20121109, 20121112, 20121113, 20121114, 20121115, 20121116, 20121119, 20121120, 20121121, 20121123, 20121126, 20121127, 20121128, 20121129, 20121130, 20121203, 20121204, 20121205, 20121206, 20121207, 20121210, 20121211, 20121212, 20121213, 20121214, 20121217, 20121218, 20121219, 20121220, 20121221, 20121224, 20121226, 20121227, 20121228, 20121231, 20130102, 20130103, 20130104, 20130107, 20130108, 20130109, 20130110, 20130111, 20130114, 20130115, 20130116, 20130117, 20130118, 20130122, 20130123, 20130124, 20130125, 20130128, 20130129, 20130130, 20130131, 20130201, 20130204, 20130205, 20130206, 20130207, 20130208, 20130211, 20130212, 20130213, 20130214, 20130215, 20130219, 20130220, 20130221, 20130222, 20130225, 20130226, 20130227, 20130228, 20130301, 20130304, 20130305, 20130306, 20130307, 20130308, 20130311, 20130312, 20130313, 20130314, 20130315, 20130318, 20130319, 20130320, 20130321, 20130322, 20130325, 20130326, 20130327, 20130328, 20130401, 20130402, 20130403, 20130404, 20130405, 20130408, 20130409, 20130410, 20130411, 20130412, 20130415, 20130416, 20130417, 20130418, 20130419, 20130422, 20130423, 20130424, 20130425, 20130426, 20130429, 20130430, 20130501, 20130502, 20130503, 20130506, 20130507, 20130508, 20130509, 20130510, 20130513, 20130514, 20130515, 20130516, 20130517, 20130520, 20130521, 20130522, 20130523, 20130524, 20130528, 20130529, 20130530, 20130531, 20130603, 20130604, 20130605, 20130606, 20130607, 20130610, 20130611, 20130612, 20130613, 20130614, 20130617, 20130618, 20130619, 20130620, 20130621, 20130624, 20130625, 20130626, 20130627, 20130628, 20130701, 20130702, 20130703, 20130705, 20130708, 20130709, 20130710, 20130711, 20130712, 20130715, 20130716, 20130717, 20130718, 20130719, 20130722, 20130723, 20130724, 20130725, 20130726, 20130729, 20130730, 20130731, 20130801, 20130802, 20130805, 20130806, 20130807, 20130808, 20130809, 20130812, 20130813, 20130814, 20130815, 20130816, 20130819, 20130820, 20130821, 20130822, 20130823, 20130826, 20130827, 20130828, 20130829, 20130830, 20130903, 20130904, 20130905, 20130906, 20130909, 20130910, 20130911, 20130912, 20130913, 20130916, 20130917, 20130918, 20130919, 20130920, 20130923, 20130924, 20130925, 20130926, 20130927, 20130930, 20131001, 20131002, 20131003, 20131004, 20131007, 20131008, 20131009, 20131010, 20131011, 20131014, 20131015, 20131016, 20131017, 20131018, 20131021, 20131022, 20131023, 20131024, 20131025, 20131028, 20131029, 20131030, 20131031, 20131101, 20131104, 20131105, 20131106, 20131107, 20131108, 20131111, 20131112, 20131113, 20131114, 20131115, 20131118, 20131119, 20131120, 20131121, 20131122, 20131125, 20131126, 20131127, 20131129, 20131202, 20131203, 20131204, 20131205, 20131206, 20131209, 20131210, 20131211, 20131212, 20131213, 20131216, 20131217, 20131218, 20131219, 20131220, 20131223, 20131224, 20131226, 20131227, 20131230, 20131231, 20140102, 20140103, 20140106, 20140107, 20140108, 20140109, 20140110, 20140113, 20140114, 20140115, 20140116, 20140117, 20140121, 20140122, 20140123, 20140124, 20140127, 20140128, 20140129, 20140130, 20140131, 20140203, 20140204, 20140205, 20140206, 20140207, 20140210, 20140211, 20140212, 20140213, 20140214, 20140218, 20140219, 20140220, 20140221, 20140224, 20140225, 20140226, 20140227, 20140228, 20140303, 20140304, 20140305, 20140306, 20140307, 20140310, 20140311, 20140312, 20140313, 20140314, 20140317, 20140318, 20140319, 20140320, 20140321, 20140324, 20140325, 20140326, 20140327, 20140328, 20140331, 20140401, 20140402, 20140403, 20140404, 20140407, 20140408, 20140409, 20140410, 20140411, 20140414, 20140415, 20140416, 20140417, 20140421, 20140422, 20140423, 20140424, 20140425, 20140428, 20140429, 20140430, 20140501, 20140502, 20140505, 20140506, 20140507, 20140508, 20140509, 20140512, 20140513, 20140514, 20140515, 20140516, 20140519, 20140520, 20140521, 20140522, 20140523, 20140527, 20140528, 20140529, 20140530, 20140602, 20140603, 20140604, 20140605, 20140606, 20140609, 20140610, 20140611, 20140612, 20140613, 20140616, 20140617, 20140618, 20140619, 20140620, 20140623, 20140624, 20140625, 20140626, 20140627, 20140630, 20140701, 20140702, 20140703, 20140707, 20140708, 20140709, 20140710, 20140711, 20140714, 20140715, 20140716, 20140717, 20140718, 20140721, 20140722, 20140723, 20140724, 20140725, 20140728, 20140729, 20140730, 20140731, 20140801, 20140804, 20140805, 20140806, 20140807, 20140808, 20140811, 20140812, 20140813, 20140814, 20140815, 20140818, 20140819, 20140820, 20140821, 20140822, 20140825, 20140826, 20140827, 20140828, 20140829, 20140902, 20140903, 20140904, 20140905, 20140908, 20140909, 20140910, 20140911, 20140912, 20140915, 20140916, 20140917, 20140918, 20140919, 20140922, 20140923, 20140924, 20140925, 20140926, 20140929, 20140930, 20141001, 20141002, 20141003, 20141006, 20141007, 20141008, 20141009, 20141010, 20141013, 20141014, 20141015, 20141016, 20141017, 20141020, 20141021, 20141022, 20141023, 20141024, 20141027, 20141028, 20141029, 20141030, 20141031, 20141103, 20141104, 20141105, 20141106, 20141107, 20141110, 20141111, 20141112, 20141113, 20141114, 20141117, 20141118, 20141119, 20141120, 20141121, 20141124, 20141125, 20141126, 20141128, 20141201, 20141202, 20141203, 20141204, 20141205, 20141208, 20141209, 20141210, 20141211, 20141212, 20141215, 20141216, 20141217, 20141218, 20141219, 20141222, 20141223, 20141224, 20141226, 20141229, 20141230, 20141231, 20150102, 20150105, 20150106, 20150107, 20150108, 20150109, 20150112, 20150113, 20150114, 20150115, 20150116, 20150120, 20150121, 20150122, 20150123, 20150126, 20150127, 20150128, 20150129, 20150130, 20150202, 20150203, 20150204, 20150205, 20150206, 20150209, 20150210, 20150211, 20150212, 20150213, 20150217, 20150218, 20150219, 20150220, 20150223, 20150224, 20150225, 20150226, 20150227, 20150302, 20150303, 20150304, 20150305, 20150306, 20150309, 20150310, 20150311, 20150312, 20150313, 20150316, 20150317, 20150318, 20150319, 20150320, 20150323, 20150324, 20150325, 20150326, 20150327, 20150330, 20150331, 20150401, 20150402, 20150406, 20150407, 20150408, 20150409, 20150410, 20150413, 20150414, 20150415, 20150416, 20150417, 20150420, 20150421, 20150422, 20150423, 20150424, 20150427, 20150428, 20150429, 20150430, 20150501, 20150504, 20150505, 20150506, 20150507, 20150508, 20150511, 20150512, 20150513, 20150514, 20150515, 20150518, 20150519, 20150520, 20150521, 20150522, 20150526, 20150527, 20150528, 20150529, 20150601, 20150602, 20150603, 20150604, 20150605, 20150608, 20150609, 20150610, 20150611, 20150612, 20150615, 20150616, 20150617, 20150618, 20150619, 20150622, 20150623, 20150624, 20150625, 20150626, 20150629, 20150630, 20150701, 20150702, 20150706, 20150707, 20150708, 20150709, 20150710, 20150713, 20150714, 20150715, 20150716, 20150717, 20150720, 20150721, 20150722, 20150723, 20150724, 20150727, 20150728, 20150729, 20150730, 20150731, 20150803, 20150804, 20150805, 20150806, 20150807, 20150810, 20150811, 20150812, 20150813, 20150814, 20150817, 20150818, 20150819, 20150820, 20150821, 20150824, 20150825, 20150826, 20150827, 20150828, 20150831, 20150901, 20150902, 20150903, 20150904, 20150908, 20150909, 20150910, 20150911, 20150914, 20150915, 20150916, 20150917, 20150918, 20150921, 20150922, 20150923, 20150924, 20150925, 20150928, 20150929, 20150930, 20151001, 20151002, 20151005, 20151006, 20151007, 20151008, 20151009, 20151012, 20151013, 20151014, 20151015, 20151016, 20151019, 20151020, 20151021, 20151022, 20151023, 20151026, 20151027, 20151028, 20151029, 20151030, 20151102, 20151103, 20151104, 20151105, 20151106, 20151109, 20151110, 20151111, 20151112, 20151113, 20151116, 20151117, 20151118, 20151119, 20151120, 20151123, 20151124, 20151125, 20151127, 20151130, 20151201, 20151202, 20151203, 20151204, 20151207, 20151208, 20151209, 20151210, 20151211, 20151214, 20151215, 20151216, 20151217, 20151218, 20151221, 20151222, 20151223, 20151224, 20151228, 20151229, 20151230, 20151231, 20160104, 20160105, 20160106, 20160107, 20160108, 20160111, 20160112, 20160113, 20160114, 20160115, 20160119, 20160120, 20160121, 20160122, 20160125, 20160126, 20160127, 20160128, 20160129, 20160201, 20160202, 20160203, 20160204, 20160205, 20160208, 20160209, 20160210, 20160211, 20160212, 20160216, 20160217, 20160218, 20160219, 20160222, 20160223, 20160224, 20160225, 20160226, 20160229, 20160301, 20160302, 20160303, 20160304, 20160307, 20160308, 20160309, 20160310, 20160311, 20160314, 20160315, 20160316, 20160317, 20160318, 20160321, 20160322, 20160323, 20160324, 20160328, 20160329, 20160330, 20160331, 20160401, 20160404, 20160405, 20160406, 20160407, 20160408, 20160411, 20160412, 20160413, 20160414, 20160415, 20160418, 20160419, 20160420, 20160421, 20160422, 20160425, 20160426, 20160427, 20160428, 20160429, 20160502, 20160503, 20160504, 20160505, 20160506, 20160509, 20160510, 20160511, 20160512, 20160513, 20160516, 20160517, 20160518, 20160519, 20160520, 20160523, 20160524, 20160525, 20160526, 20160527, 20160531, 20160601, 20160602, 20160603, 20160606, 20160607, 20160608, 20160609, 20160610, 20160613, 20160614, 20160615, 20160616, 20160617, 20160620, 20160621, 20160622, 20160623, 20160624, 20160627, 20160628, 20160629, 20160630, 20160701, 20160705, 20160706, 20160707, 20160708, 20160711, 20160712, 20160713, 20160714, 20160715, 20160718, 20160719, 20160720, 20160721, 20160722, 20160725, 20160726, 20160727, 20160728, 20160729, 20160801, 20160802, 20160803, 20160804, 20160805, 20160808, 20160809, 20160810, 20160811, 20160812, 20160815, 20160816, 20160817, 20160818, 20160819, 20160822, 20160823, 20160824, 20160825, 20160826, 20160829, 20160830, 20160831, 20160901, 20160902, 20160906, 20160907, 20160908, 20160909, 20160912, 20160913, 20160914, 20160915, 20160916, 20160919, 20160920, 20160921, 20160922, 20160923, 20160926, 20160927, 20160928, 20160929, 20160930, 20161003, 20161004, 20161005, 20161006, 20161007, 20161010, 20161011, 20161012, 20161013, 20161014, 20161017, 20161018, 20161019, 20161020, 20161021, 20161024, 20161025, 20161026, 20161027, 20161028, 20161031, 20161101, 20161102, 20161103, 20161104, 20161107, 20161108, 20161109, 20161110, 20161111, 20161114, 20161115, 20161116, 20161117, 20161118, 20161121, 20161122, 20161123, 20161125, 20161128, 20161129, 20161130, 20161201, 20161202, 20161205, 20161206, 20161207, 20161208, 20161209, 20161212, 20161213, 20161214, 20161215, 20161216, 20161219, 20161220, 20161221, 20161222, 20161223, 20161227, 20161228, 20161229, 20161230, 20170103, 20170104, 20170105, 20170106, 20170109, 20170110, 20170111, 20170112, 20170113, 20170117, 20170118, 20170119, 20170120, 20170123, 20170124, 20170125, 20170126, 20170127, 20170130, 20170131, 20170201, 20170202, 20170203, 20170206, 20170207, 20170208, 20170209, 20170210, 20170213, 20170214, 20170215, 20170216, 20170217, 20170221, 20170222, 20170223, 20170224, 20170227, 20170228, 20170301, 20170302, 20170303, 20170306, 20170307, 20170308, 20170309, 20170310, 20170313, 20170314, 20170315, 20170316, 20170317, 20170320, 20170321, 20170322, 20170323, 20170324, 20170327, 20170328, 20170329, 20170330, 20170331, 20170403, 20170404, 20170405, 20170406, 20170407, 20170410, 20170411, 20170412, 20170413, 20170417, 20170418, 20170419, 20170420, 20170421, 20170424, 20170425, 20170426, 20170427, 20170428, 20170501, 20170502, 20170503, 20170504, 20170505, 20170508, 20170509, 20170510, 20170511, 20170512, 20170515, 20170516, 20170517, 20170518, 20170519, 20170522, 20170523, 20170524, 20170525, 20170526, 20170530, 20170531, 20170601, 20170602, 20170605, 20170606, 20170607, 20170608, 20170609, 20170612, 20170613, 20170614, 20170615, 20170616, 20170619, 20170620, 20170621, 20170622, 20170623, 20170626, 20170627, 20170628, 20170629, 20170630, 20170703, 20170705, 20170706, 20170707, 20170710, 20170711, 20170712, 20170713, 20170714, 20170717, 20170718, 20170719, 20170720, 20170721, 20170724, 20170725, 20170726, 20170727, 20170728, 20170731, 20170801, 20170802, 20170803, 20170804, 20170807, 20170808, 20170809, 20170810, 20170811, 20170814, 20170815, 20170816, 20170817, 20170818, 20170821, 20170822, 20170823, 20170824, 20170825, 20170828, 20170829, 20170830, 20170831, 20170901, 20170905, 20170906, 20170907, 20170908, 20170911, 20170912, 20170913, 20170914, 20170915, 20170918, 20170919, 20170920, 20170921, 20170922, 20170925, 20170926, 20170927, 20170928, 20170929, 20171002, 20171003, 20171004, 20171005, 20171006, 20171009, 20171010, 20171011, 20171012, 20171013, 20171016, 20171017, 20171018, 20171019, 20171020, 20171023, 20171024, 20171025, 20171026, 20171027, 20171030, 20171031, 20171101, 20171102, 20171103, 20171106, 20171107, 20171108, 20171109, 20171110, 20171113, 20171114, 20171115, 20171116, 20171117, 20171120, 20171121, 20171122, 20171124, 20171127, 20171128, 20171129, 20171130, 20171201, 20171204, 20171205, 20171206, 20171207, 20171208, 20171211, 20171212, 20171213, 20171214, 20171215, 20171218, 20171219, 20171220, 20171221, 20171222, 20171226, 20171227, 20171228, 20171229, 20180102, 20180103, 20180104, 20180105, 20180108, 20180109, 20180110, 20180111, 20180112, 20180116, 20180117, 20180118, 20180119, 20180122, 20180123, 20180124, 20180125, 20180126, 20180129, 20180130, 20180131, 20180201, 20180202, 20180205, 20180206, 20180207, 20180208, 20180209, 20180212, 20180213, 20180214, 20180215, 20180216, 20180220, 20180221, 20180222, 20180223, 20180226, 20180227, 20180228, 20180301, 20180302, 20180305, 20180306, 20180307, 20180308, 20180309, 20180312, 20180313, 20180314, 20180315, 20180316, 20180319, 20180320, 20180321, 20180322, 20180323, 20180326, 20180327, 20180328, 20180329, 20180402, 20180403, 20180404, 20180405, 20180406, 20180409, 20180410, 20180411, 20180412, 20180413, 20180416, 20180417, 20180418, 20180419, 20180420, 20180423, 20180424, 20180425, 20180426, 20180427, 20180430, 20180501, 20180502, 20180503, 20180504, 20180507, 20180508, 20180509, 20180510, 20180511, 20180514, 20180515, 20180516, 20180517, 20180518, 20180521, 20180522, 20180523, 20180524, 20180525, 20180529, 20180530, 20180531, 20180601, 20180604, 20180605, 20180606, 20180607, 20180608, 20180611, 20180612, 20180613, 20180614, 20180615, 20180618, 20180619, 20180620, 20180621, 20180622, 20180625, 20180626, 20180627, 20180628, 20180629, 20180702, 20180703, 20180705, 20180706, 20180709, 20180710, 20180711, 20180712, 20180713, 20180716, 20180717, 20180718, 20180719, 20180720, 20180723, 20180724, 20180725, 20180726, 20180727, 20180730, 20180731, 20180801, 20180802, 20180803, 20180806, 20180807, 20180808, 20180809, 20180810, 20180813, 20180814, 20180815, 20180816, 20180817, 20180820, 20180821, 20180822, 20180823, 20180824, 20180827, 20180828, 20180829, 20180830, 20180831, 20180904, 20180905, 20180906, 20180907, 20180910, 20180911, 20180912], \"y\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('58aa4b38-a7fd-494b-9bc6-905fe92c1fc8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp4WLtN5Ga-B"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO6ZOjONGa-B",
        "outputId": "1cf553af-94b7-4aa2-9e90-eaaaf8f90e7d"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.05, .1, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"CL\", step_sizes=4, th= th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6860 - accuracy: 0.5738 - val_loss: 0.6342 - val_accuracy: 0.8327\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6841 - accuracy: 0.5745 - val_loss: 0.6326 - val_accuracy: 0.8327\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6845 - accuracy: 0.5725 - val_loss: 0.6040 - val_accuracy: 0.8327\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6775 - accuracy: 0.5765 - val_loss: 0.5790 - val_accuracy: 0.8224\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6670 - accuracy: 0.5987 - val_loss: 0.6031 - val_accuracy: 0.8367\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6894 - accuracy: 0.5570 - val_loss: 0.5776 - val_accuracy: 0.8265\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.6771 - accuracy: 0.5886 - val_loss: 0.5975 - val_accuracy: 0.8429\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6450 - accuracy: 0.6242 - val_loss: 0.6406 - val_accuracy: 0.7449\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.6231 - accuracy: 0.6490 - val_loss: 0.5581 - val_accuracy: 0.8224\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6392 - accuracy: 0.6477 - val_loss: 0.5150 - val_accuracy: 0.8469\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.791577\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.80437\n",
            "[2]\tvalidation_0-auc:0.801814\n",
            "[3]\tvalidation_0-auc:0.810273\n",
            "[4]\tvalidation_0-auc:0.807493\n",
            "[5]\tvalidation_0-auc:0.810766\n",
            "[6]\tvalidation_0-auc:0.813247\n",
            "[7]\tvalidation_0-auc:0.81782\n",
            "[8]\tvalidation_0-auc:0.817372\n",
            "[9]\tvalidation_0-auc:0.816221\n",
            "[10]\tvalidation_0-auc:0.815399\n",
            "[11]\tvalidation_0-auc:0.816057\n",
            "[12]\tvalidation_0-auc:0.816162\n",
            "[13]\tvalidation_0-auc:0.818194\n",
            "[14]\tvalidation_0-auc:0.819076\n",
            "[15]\tvalidation_0-auc:0.820511\n",
            "[16]\tvalidation_0-auc:0.822065\n",
            "[17]\tvalidation_0-auc:0.822125\n",
            "[18]\tvalidation_0-auc:0.822065\n",
            "[19]\tvalidation_0-auc:0.82063\n",
            "[20]\tvalidation_0-auc:0.821034\n",
            "[21]\tvalidation_0-auc:0.821721\n",
            "[22]\tvalidation_0-auc:0.821063\n",
            "[23]\tvalidation_0-auc:0.821811\n",
            "[24]\tvalidation_0-auc:0.821228\n",
            "[25]\tvalidation_0-auc:0.820511\n",
            "[26]\tvalidation_0-auc:0.820989\n",
            "[27]\tvalidation_0-auc:0.820002\n",
            "[28]\tvalidation_0-auc:0.820271\n",
            "[29]\tvalidation_0-auc:0.821347\n",
            "[30]\tvalidation_0-auc:0.821063\n",
            "[31]\tvalidation_0-auc:0.820017\n",
            "[32]\tvalidation_0-auc:0.819539\n",
            "[33]\tvalidation_0-auc:0.820182\n",
            "[34]\tvalidation_0-auc:0.821721\n",
            "[35]\tvalidation_0-auc:0.820256\n",
            "[36]\tvalidation_0-auc:0.820451\n",
            "[37]\tvalidation_0-auc:0.822095\n",
            "[38]\tvalidation_0-auc:0.821258\n",
            "[39]\tvalidation_0-auc:0.818583\n",
            "[40]\tvalidation_0-auc:0.817686\n",
            "[41]\tvalidation_0-auc:0.818747\n",
            "[42]\tvalidation_0-auc:0.818956\n",
            "[43]\tvalidation_0-auc:0.818478\n",
            "[44]\tvalidation_0-auc:0.818523\n",
            "[45]\tvalidation_0-auc:0.818074\n",
            "[46]\tvalidation_0-auc:0.816984\n",
            "[47]\tvalidation_0-auc:0.816774\n",
            "[48]\tvalidation_0-auc:0.818119\n",
            "[49]\tvalidation_0-auc:0.818777\n",
            "[50]\tvalidation_0-auc:0.81788\n",
            "[51]\tvalidation_0-auc:0.817432\n",
            "[52]\tvalidation_0-auc:0.816984\n",
            "[53]\tvalidation_0-auc:0.817163\n",
            "[54]\tvalidation_0-auc:0.81794\n",
            "[55]\tvalidation_0-auc:0.817462\n",
            "[56]\tvalidation_0-auc:0.81785\n",
            "[57]\tvalidation_0-auc:0.818926\n",
            "[58]\tvalidation_0-auc:0.81782\n",
            "[59]\tvalidation_0-auc:0.81803\n",
            "[60]\tvalidation_0-auc:0.817133\n",
            "[61]\tvalidation_0-auc:0.816057\n",
            "[62]\tvalidation_0-auc:0.816057\n",
            "[63]\tvalidation_0-auc:0.814562\n",
            "[64]\tvalidation_0-auc:0.814084\n",
            "[65]\tvalidation_0-auc:0.813965\n",
            "[66]\tvalidation_0-auc:0.812874\n",
            "[67]\tvalidation_0-auc:0.811917\n",
            "Stopping. Best iteration:\n",
            "[17]\tvalidation_0-auc:0.822125\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6859 - accuracy: 0.5566 - val_loss: 0.6339 - val_accuracy: 0.8425\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6848 - accuracy: 0.5546 - val_loss: 0.6403 - val_accuracy: 0.7987\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6748 - accuracy: 0.5745 - val_loss: 0.5496 - val_accuracy: 0.7637\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6547 - accuracy: 0.6170 - val_loss: 0.5580 - val_accuracy: 0.7681\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6552 - accuracy: 0.6156 - val_loss: 0.5866 - val_accuracy: 0.7637\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6827 - accuracy: 0.5539 - val_loss: 0.6873 - val_accuracy: 0.8162\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6311 - accuracy: 0.6493 - val_loss: 0.7249 - val_accuracy: 0.8096\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6145 - accuracy: 0.6692 - val_loss: 0.7961 - val_accuracy: 0.7724\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6070 - accuracy: 0.6712 - val_loss: 0.7886 - val_accuracy: 0.7899\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6038 - accuracy: 0.6685 - val_loss: 0.7805 - val_accuracy: 0.7812\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.780519\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.809019\n",
            "[2]\tvalidation_0-auc:0.817785\n",
            "[3]\tvalidation_0-auc:0.83768\n",
            "[4]\tvalidation_0-auc:0.844895\n",
            "[5]\tvalidation_0-auc:0.850216\n",
            "[6]\tvalidation_0-auc:0.853662\n",
            "[7]\tvalidation_0-auc:0.852327\n",
            "[8]\tvalidation_0-auc:0.870112\n",
            "[9]\tvalidation_0-auc:0.864015\n",
            "[10]\tvalidation_0-auc:0.863167\n",
            "[11]\tvalidation_0-auc:0.856926\n",
            "[12]\tvalidation_0-auc:0.857973\n",
            "[13]\tvalidation_0-auc:0.85561\n",
            "[14]\tvalidation_0-auc:0.858947\n",
            "[15]\tvalidation_0-auc:0.853734\n",
            "[16]\tvalidation_0-auc:0.848034\n",
            "[17]\tvalidation_0-auc:0.844318\n",
            "[18]\tvalidation_0-auc:0.846609\n",
            "[19]\tvalidation_0-auc:0.842731\n",
            "[20]\tvalidation_0-auc:0.843074\n",
            "[21]\tvalidation_0-auc:0.840566\n",
            "[22]\tvalidation_0-auc:0.836851\n",
            "[23]\tvalidation_0-auc:0.834343\n",
            "[24]\tvalidation_0-auc:0.834235\n",
            "[25]\tvalidation_0-auc:0.835823\n",
            "[26]\tvalidation_0-auc:0.83483\n",
            "[27]\tvalidation_0-auc:0.831097\n",
            "[28]\tvalidation_0-auc:0.82987\n",
            "[29]\tvalidation_0-auc:0.829473\n",
            "[30]\tvalidation_0-auc:0.827417\n",
            "[31]\tvalidation_0-auc:0.828716\n",
            "[32]\tvalidation_0-auc:0.831782\n",
            "[33]\tvalidation_0-auc:0.833622\n",
            "[34]\tvalidation_0-auc:0.831782\n",
            "[35]\tvalidation_0-auc:0.831205\n",
            "[36]\tvalidation_0-auc:0.830556\n",
            "[37]\tvalidation_0-auc:0.826046\n",
            "[38]\tvalidation_0-auc:0.824459\n",
            "[39]\tvalidation_0-auc:0.823088\n",
            "[40]\tvalidation_0-auc:0.822727\n",
            "[41]\tvalidation_0-auc:0.819156\n",
            "[42]\tvalidation_0-auc:0.818831\n",
            "[43]\tvalidation_0-auc:0.816234\n",
            "[44]\tvalidation_0-auc:0.819841\n",
            "[45]\tvalidation_0-auc:0.817605\n",
            "[46]\tvalidation_0-auc:0.814646\n",
            "[47]\tvalidation_0-auc:0.813907\n",
            "[48]\tvalidation_0-auc:0.810949\n",
            "[49]\tvalidation_0-auc:0.813889\n",
            "[50]\tvalidation_0-auc:0.816378\n",
            "[51]\tvalidation_0-auc:0.815729\n",
            "[52]\tvalidation_0-auc:0.815584\n",
            "[53]\tvalidation_0-auc:0.816414\n",
            "[54]\tvalidation_0-auc:0.821032\n",
            "[55]\tvalidation_0-auc:0.822565\n",
            "[56]\tvalidation_0-auc:0.821573\n",
            "[57]\tvalidation_0-auc:0.822511\n",
            "[58]\tvalidation_0-auc:0.817893\n",
            "Stopping. Best iteration:\n",
            "[8]\tvalidation_0-auc:0.870112\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.8367346938775511 |  0.5121951219512195 | 0.5121951219512195 | 0.5121951219512195 |\n",
            "|     GRU 0.1      | 0.8469387755102041 |  0.5555555555555556 | 0.4268292682926829 | 0.4827586206896552 |\n",
            "|   XGBoost 0.1    |        0.8         | 0.42857142857142855 | 0.5853658536585366 | 0.4948453608247423 |\n",
            "|    Logreg 0.1    | 0.810204081632653  |  0.4485981308411215 | 0.5853658536585366 | 0.507936507936508  |\n",
            "|     SVM 0.1      | 0.8244897959183674 | 0.47619047619047616 | 0.4878048780487805 | 0.4819277108433735 |\n",
            "|  LSTM beta 0.1   | 0.7636761487964989 |  0.3448275862068966 | 0.5555555555555556 | 0.4255319148936171 |\n",
            "|   GRU beta 0.1   | 0.7811816192560175 |  0.4014084507042254 | 0.7916666666666666 | 0.5327102803738317 |\n",
            "| XGBoost beta 0.1 | 0.8533916849015317 |  0.5257731958762887 | 0.7083333333333334 | 0.6035502958579881 |\n",
            "| logreg beta 0.1  | 0.7964989059080962 | 0.41025641025641024 | 0.6666666666666666 | 0.5079365079365079 |\n",
            "|   svm beta 0.1   | 0.8096280087527352 | 0.43119266055045874 | 0.6527777777777778 | 0.5193370165745858 |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.4536 - accuracy: 0.8470 - val_loss: 0.2351 - val_accuracy: 0.9490\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4351 - accuracy: 0.8490 - val_loss: 0.2498 - val_accuracy: 0.9490\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4373 - accuracy: 0.8490 - val_loss: 0.2401 - val_accuracy: 0.9490\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4328 - accuracy: 0.8490 - val_loss: 0.2695 - val_accuracy: 0.9490\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4303 - accuracy: 0.8490 - val_loss: 0.2484 - val_accuracy: 0.9490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 2s 13ms/step - loss: 0.4538 - accuracy: 0.8463 - val_loss: 0.2953 - val_accuracy: 0.9490\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.4366 - accuracy: 0.8490 - val_loss: 0.2641 - val_accuracy: 0.9490\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.4269 - accuracy: 0.8490 - val_loss: 0.2257 - val_accuracy: 0.9429\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4301 - accuracy: 0.8490 - val_loss: 0.3445 - val_accuracy: 0.9367\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4214 - accuracy: 0.8490 - val_loss: 0.2469 - val_accuracy: 0.9388\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.631355\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.631398\n",
            "[2]\tvalidation_0-auc:0.629763\n",
            "[3]\tvalidation_0-auc:0.667613\n",
            "[4]\tvalidation_0-auc:0.668645\n",
            "[5]\tvalidation_0-auc:0.718495\n",
            "[6]\tvalidation_0-auc:0.719011\n",
            "[7]\tvalidation_0-auc:0.718925\n",
            "[8]\tvalidation_0-auc:0.718538\n",
            "[9]\tvalidation_0-auc:0.749806\n",
            "[10]\tvalidation_0-auc:0.770796\n",
            "[11]\tvalidation_0-auc:0.776043\n",
            "[12]\tvalidation_0-auc:0.784473\n",
            "[13]\tvalidation_0-auc:0.78757\n",
            "[14]\tvalidation_0-auc:0.777936\n",
            "[15]\tvalidation_0-auc:0.782925\n",
            "[16]\tvalidation_0-auc:0.790151\n",
            "[17]\tvalidation_0-auc:0.80228\n",
            "[18]\tvalidation_0-auc:0.799742\n",
            "[19]\tvalidation_0-auc:0.814624\n",
            "[20]\tvalidation_0-auc:0.810409\n",
            "[21]\tvalidation_0-auc:0.805462\n",
            "[22]\tvalidation_0-auc:0.802624\n",
            "[23]\tvalidation_0-auc:0.813806\n",
            "[24]\tvalidation_0-auc:0.811312\n",
            "[25]\tvalidation_0-auc:0.815398\n",
            "[26]\tvalidation_0-auc:0.811871\n",
            "[27]\tvalidation_0-auc:0.832473\n",
            "[28]\tvalidation_0-auc:0.830237\n",
            "[29]\tvalidation_0-auc:0.839656\n",
            "[30]\tvalidation_0-auc:0.831656\n",
            "[31]\tvalidation_0-auc:0.843914\n",
            "[32]\tvalidation_0-auc:0.846022\n",
            "[33]\tvalidation_0-auc:0.847054\n",
            "[34]\tvalidation_0-auc:0.84757\n",
            "[35]\tvalidation_0-auc:0.847312\n",
            "[36]\tvalidation_0-auc:0.856344\n",
            "[37]\tvalidation_0-auc:0.851699\n",
            "[38]\tvalidation_0-auc:0.851699\n",
            "[39]\tvalidation_0-auc:0.851183\n",
            "[40]\tvalidation_0-auc:0.852645\n",
            "[41]\tvalidation_0-auc:0.845591\n",
            "[42]\tvalidation_0-auc:0.844989\n",
            "[43]\tvalidation_0-auc:0.846968\n",
            "[44]\tvalidation_0-auc:0.843355\n",
            "[45]\tvalidation_0-auc:0.843613\n",
            "[46]\tvalidation_0-auc:0.842839\n",
            "[47]\tvalidation_0-auc:0.842323\n",
            "[48]\tvalidation_0-auc:0.844645\n",
            "[49]\tvalidation_0-auc:0.837763\n",
            "[50]\tvalidation_0-auc:0.836301\n",
            "[51]\tvalidation_0-auc:0.837247\n",
            "[52]\tvalidation_0-auc:0.832774\n",
            "[53]\tvalidation_0-auc:0.832946\n",
            "[54]\tvalidation_0-auc:0.833376\n",
            "[55]\tvalidation_0-auc:0.833548\n",
            "[56]\tvalidation_0-auc:0.827699\n",
            "[57]\tvalidation_0-auc:0.821075\n",
            "[58]\tvalidation_0-auc:0.821677\n",
            "[59]\tvalidation_0-auc:0.821763\n",
            "[60]\tvalidation_0-auc:0.82228\n",
            "[61]\tvalidation_0-auc:0.820731\n",
            "[62]\tvalidation_0-auc:0.823398\n",
            "[63]\tvalidation_0-auc:0.822882\n",
            "[64]\tvalidation_0-auc:0.818065\n",
            "[65]\tvalidation_0-auc:0.81428\n",
            "[66]\tvalidation_0-auc:0.813763\n",
            "[67]\tvalidation_0-auc:0.818495\n",
            "[68]\tvalidation_0-auc:0.817634\n",
            "[69]\tvalidation_0-auc:0.813763\n",
            "[70]\tvalidation_0-auc:0.815914\n",
            "[71]\tvalidation_0-auc:0.816688\n",
            "[72]\tvalidation_0-auc:0.820387\n",
            "[73]\tvalidation_0-auc:0.821505\n",
            "[74]\tvalidation_0-auc:0.822452\n",
            "[75]\tvalidation_0-auc:0.824516\n",
            "[76]\tvalidation_0-auc:0.819613\n",
            "[77]\tvalidation_0-auc:0.821505\n",
            "[78]\tvalidation_0-auc:0.819699\n",
            "[79]\tvalidation_0-auc:0.819785\n",
            "[80]\tvalidation_0-auc:0.817548\n",
            "[81]\tvalidation_0-auc:0.817634\n",
            "[82]\tvalidation_0-auc:0.814882\n",
            "[83]\tvalidation_0-auc:0.818409\n",
            "[84]\tvalidation_0-auc:0.817548\n",
            "[85]\tvalidation_0-auc:0.818495\n",
            "[86]\tvalidation_0-auc:0.816344\n",
            "Stopping. Best iteration:\n",
            "[36]\tvalidation_0-auc:0.856344\n",
            "\n",
            "end training. \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.4568 - accuracy: 0.8428 - val_loss: 0.2789 - val_accuracy: 0.9453\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4439 - accuracy: 0.8456 - val_loss: 0.2507 - val_accuracy: 0.9453\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4375 - accuracy: 0.8456 - val_loss: 0.2647 - val_accuracy: 0.9453\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4398 - accuracy: 0.8456 - val_loss: 0.2634 - val_accuracy: 0.9453\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4381 - accuracy: 0.8456 - val_loss: 0.2546 - val_accuracy: 0.9453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 2s 13ms/step - loss: 0.4662 - accuracy: 0.8442 - val_loss: 0.2623 - val_accuracy: 0.9453\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4405 - accuracy: 0.8456 - val_loss: 0.2478 - val_accuracy: 0.9453\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4309 - accuracy: 0.8456 - val_loss: 0.2983 - val_accuracy: 0.8972\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4163 - accuracy: 0.8463 - val_loss: 0.3448 - val_accuracy: 0.8796\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4131 - accuracy: 0.8497 - val_loss: 0.3515 - val_accuracy: 0.8796\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.474259\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.474259\n",
            "[2]\tvalidation_0-auc:0.474074\n",
            "[3]\tvalidation_0-auc:0.474074\n",
            "[4]\tvalidation_0-auc:0.473704\n",
            "[5]\tvalidation_0-auc:0.545463\n",
            "[6]\tvalidation_0-auc:0.545556\n",
            "[7]\tvalidation_0-auc:0.545556\n",
            "[8]\tvalidation_0-auc:0.618704\n",
            "[9]\tvalidation_0-auc:0.618796\n",
            "[10]\tvalidation_0-auc:0.667917\n",
            "[11]\tvalidation_0-auc:0.724583\n",
            "[12]\tvalidation_0-auc:0.71662\n",
            "[13]\tvalidation_0-auc:0.711991\n",
            "[14]\tvalidation_0-auc:0.711991\n",
            "[15]\tvalidation_0-auc:0.713009\n",
            "[16]\tvalidation_0-auc:0.711157\n",
            "[17]\tvalidation_0-auc:0.686713\n",
            "[18]\tvalidation_0-auc:0.712639\n",
            "[19]\tvalidation_0-auc:0.709028\n",
            "[20]\tvalidation_0-auc:0.701065\n",
            "[21]\tvalidation_0-auc:0.716389\n",
            "[22]\tvalidation_0-auc:0.701759\n",
            "[23]\tvalidation_0-auc:0.701574\n",
            "[24]\tvalidation_0-auc:0.718148\n",
            "[25]\tvalidation_0-auc:0.712593\n",
            "[26]\tvalidation_0-auc:0.706667\n",
            "[27]\tvalidation_0-auc:0.699537\n",
            "[28]\tvalidation_0-auc:0.70713\n",
            "[29]\tvalidation_0-auc:0.702454\n",
            "[30]\tvalidation_0-auc:0.695324\n",
            "[31]\tvalidation_0-auc:0.691898\n",
            "[32]\tvalidation_0-auc:0.70162\n",
            "[33]\tvalidation_0-auc:0.693519\n",
            "[34]\tvalidation_0-auc:0.696667\n",
            "[35]\tvalidation_0-auc:0.698333\n",
            "[36]\tvalidation_0-auc:0.694444\n",
            "[37]\tvalidation_0-auc:0.694537\n",
            "[38]\tvalidation_0-auc:0.697454\n",
            "[39]\tvalidation_0-auc:0.695185\n",
            "[40]\tvalidation_0-auc:0.685972\n",
            "[41]\tvalidation_0-auc:0.678102\n",
            "[42]\tvalidation_0-auc:0.683981\n",
            "[43]\tvalidation_0-auc:0.67875\n",
            "[44]\tvalidation_0-auc:0.672731\n",
            "[45]\tvalidation_0-auc:0.671528\n",
            "[46]\tvalidation_0-auc:0.67625\n",
            "[47]\tvalidation_0-auc:0.676991\n",
            "[48]\tvalidation_0-auc:0.682176\n",
            "[49]\tvalidation_0-auc:0.680417\n",
            "[50]\tvalidation_0-auc:0.682917\n",
            "[51]\tvalidation_0-auc:0.690787\n",
            "[52]\tvalidation_0-auc:0.692731\n",
            "[53]\tvalidation_0-auc:0.690833\n",
            "[54]\tvalidation_0-auc:0.690463\n",
            "[55]\tvalidation_0-auc:0.692315\n",
            "[56]\tvalidation_0-auc:0.69213\n",
            "[57]\tvalidation_0-auc:0.690926\n",
            "[58]\tvalidation_0-auc:0.691944\n",
            "[59]\tvalidation_0-auc:0.696111\n",
            "[60]\tvalidation_0-auc:0.693889\n",
            "[61]\tvalidation_0-auc:0.696204\n",
            "Stopping. Best iteration:\n",
            "[11]\tvalidation_0-auc:0.724583\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------+----------------------+\n",
            "|      Model       |      Accuracy      |      Precision      | Recall |       F1 score       |\n",
            "+------------------+--------------------+---------------------+--------+----------------------+\n",
            "|     LSTM 0.2     | 0.9489795918367347 |         0.0         |  0.0   |         0.0          |\n",
            "|     GRU 0.2      | 0.9387755102040817 | 0.14285714285714285 |  0.04  |        0.0625        |\n",
            "|   XGBoost 0.2    | 0.926530612244898  | 0.13333333333333333 |  0.08  |         0.1          |\n",
            "|    Logreg 0.2    | 0.9326530612244898 |         0.1         |  0.04  | 0.05714285714285714  |\n",
            "|     SVM 0.2      | 0.9489795918367347 |         0.0         |  0.0   |         0.0          |\n",
            "|  LSTM beta 0.2   | 0.9452954048140044 |         0.0         |  0.0   |         0.0          |\n",
            "|   GRU beta 0.2   | 0.8796498905908097 |       0.03125       |  0.04  | 0.03508771929824561  |\n",
            "| XGBoost beta 0.2 | 0.8687089715536105 | 0.02702702702702703 |  0.04  | 0.03225806451612903  |\n",
            "| logreg beta 0.2  | 0.862144420131291  |        0.025        |  0.04  | 0.030769230769230767 |\n",
            "|   svm beta 0.2   | 0.9452954048140044 |         0.0         |  0.0   |         0.0          |\n",
            "+------------------+--------------------+---------------------+--------+----------------------+\n",
            "Threshhold =  0.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6506 - accuracy: 0.6490 - val_loss: 0.4890 - val_accuracy: 0.9490\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6505 - accuracy: 0.6537 - val_loss: 0.4716 - val_accuracy: 0.9490\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6484 - accuracy: 0.6537 - val_loss: 0.4499 - val_accuracy: 0.9490\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6463 - accuracy: 0.6537 - val_loss: 0.4410 - val_accuracy: 0.9490\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6468 - accuracy: 0.6537 - val_loss: 0.4417 - val_accuracy: 0.9490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 2s 13ms/step - loss: 0.6550 - accuracy: 0.6503 - val_loss: 0.4339 - val_accuracy: 0.9490\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6462 - accuracy: 0.6537 - val_loss: 0.4675 - val_accuracy: 0.9388\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6401 - accuracy: 0.6537 - val_loss: 0.4431 - val_accuracy: 0.9347\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6204 - accuracy: 0.6698 - val_loss: 0.3973 - val_accuracy: 0.9265\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6124 - accuracy: 0.6846 - val_loss: 0.4005 - val_accuracy: 0.9143\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.791785\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.812774\n",
            "[2]\tvalidation_0-auc:0.837505\n",
            "[3]\tvalidation_0-auc:0.840903\n",
            "[4]\tvalidation_0-auc:0.843097\n",
            "[5]\tvalidation_0-auc:0.847398\n",
            "[6]\tvalidation_0-auc:0.851312\n",
            "[7]\tvalidation_0-auc:0.852774\n",
            "[8]\tvalidation_0-auc:0.854538\n",
            "[9]\tvalidation_0-auc:0.854237\n",
            "[10]\tvalidation_0-auc:0.854968\n",
            "[11]\tvalidation_0-auc:0.850237\n",
            "[12]\tvalidation_0-auc:0.851527\n",
            "[13]\tvalidation_0-auc:0.854065\n",
            "[14]\tvalidation_0-auc:0.85372\n",
            "[15]\tvalidation_0-auc:0.854065\n",
            "[16]\tvalidation_0-auc:0.851484\n",
            "[17]\tvalidation_0-auc:0.851054\n",
            "[18]\tvalidation_0-auc:0.860774\n",
            "[19]\tvalidation_0-auc:0.860387\n",
            "[20]\tvalidation_0-auc:0.860731\n",
            "[21]\tvalidation_0-auc:0.860473\n",
            "[22]\tvalidation_0-auc:0.862366\n",
            "[23]\tvalidation_0-auc:0.860559\n",
            "[24]\tvalidation_0-auc:0.857118\n",
            "[25]\tvalidation_0-auc:0.855914\n",
            "[26]\tvalidation_0-auc:0.857118\n",
            "[27]\tvalidation_0-auc:0.856516\n",
            "[28]\tvalidation_0-auc:0.858323\n",
            "[29]\tvalidation_0-auc:0.863828\n",
            "[30]\tvalidation_0-auc:0.863656\n",
            "[31]\tvalidation_0-auc:0.865548\n",
            "[32]\tvalidation_0-auc:0.867011\n",
            "[33]\tvalidation_0-auc:0.868129\n",
            "[34]\tvalidation_0-auc:0.873806\n",
            "[35]\tvalidation_0-auc:0.873634\n",
            "[36]\tvalidation_0-auc:0.874323\n",
            "[37]\tvalidation_0-auc:0.869936\n",
            "[38]\tvalidation_0-auc:0.862108\n",
            "[39]\tvalidation_0-auc:0.861247\n",
            "[40]\tvalidation_0-auc:0.865118\n",
            "[41]\tvalidation_0-auc:0.863312\n",
            "[42]\tvalidation_0-auc:0.860129\n",
            "[43]\tvalidation_0-auc:0.861935\n",
            "[44]\tvalidation_0-auc:0.861935\n",
            "[45]\tvalidation_0-auc:0.857548\n",
            "[46]\tvalidation_0-auc:0.856172\n",
            "[47]\tvalidation_0-auc:0.857118\n",
            "[48]\tvalidation_0-auc:0.859269\n",
            "[49]\tvalidation_0-auc:0.857634\n",
            "[50]\tvalidation_0-auc:0.856602\n",
            "[51]\tvalidation_0-auc:0.852559\n",
            "[52]\tvalidation_0-auc:0.85471\n",
            "[53]\tvalidation_0-auc:0.850581\n",
            "[54]\tvalidation_0-auc:0.850237\n",
            "[55]\tvalidation_0-auc:0.850495\n",
            "[56]\tvalidation_0-auc:0.851011\n",
            "[57]\tvalidation_0-auc:0.850753\n",
            "[58]\tvalidation_0-auc:0.856258\n",
            "[59]\tvalidation_0-auc:0.854538\n",
            "[60]\tvalidation_0-auc:0.853419\n",
            "[61]\tvalidation_0-auc:0.850839\n",
            "[62]\tvalidation_0-auc:0.850667\n",
            "[63]\tvalidation_0-auc:0.849806\n",
            "[64]\tvalidation_0-auc:0.847398\n",
            "[65]\tvalidation_0-auc:0.84671\n",
            "[66]\tvalidation_0-auc:0.847484\n",
            "[67]\tvalidation_0-auc:0.848344\n",
            "[68]\tvalidation_0-auc:0.845935\n",
            "[69]\tvalidation_0-auc:0.843441\n",
            "[70]\tvalidation_0-auc:0.842409\n",
            "[71]\tvalidation_0-auc:0.843613\n",
            "[72]\tvalidation_0-auc:0.848172\n",
            "[73]\tvalidation_0-auc:0.847914\n",
            "[74]\tvalidation_0-auc:0.84972\n",
            "[75]\tvalidation_0-auc:0.84929\n",
            "[76]\tvalidation_0-auc:0.846882\n",
            "[77]\tvalidation_0-auc:0.848172\n",
            "[78]\tvalidation_0-auc:0.846366\n",
            "[79]\tvalidation_0-auc:0.848344\n",
            "[80]\tvalidation_0-auc:0.848172\n",
            "[81]\tvalidation_0-auc:0.847054\n",
            "[82]\tvalidation_0-auc:0.844731\n",
            "[83]\tvalidation_0-auc:0.842409\n",
            "[84]\tvalidation_0-auc:0.849032\n",
            "[85]\tvalidation_0-auc:0.846366\n",
            "[86]\tvalidation_0-auc:0.84757\n",
            "Stopping. Best iteration:\n",
            "[36]\tvalidation_0-auc:0.874323\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6605 - accuracy: 0.6438 - val_loss: 0.5073 - val_accuracy: 0.9453\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6533 - accuracy: 0.6458 - val_loss: 0.5334 - val_accuracy: 0.9453\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6442 - accuracy: 0.6465 - val_loss: 0.4779 - val_accuracy: 0.8709\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6403 - accuracy: 0.6472 - val_loss: 0.4813 - val_accuracy: 0.8753\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6189 - accuracy: 0.6802 - val_loss: 0.6087 - val_accuracy: 0.8053\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6570 - accuracy: 0.6431 - val_loss: 0.5092 - val_accuracy: 0.8972\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6266 - accuracy: 0.6603 - val_loss: 0.5861 - val_accuracy: 0.8621\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5989 - accuracy: 0.7062 - val_loss: 0.5549 - val_accuracy: 0.8796\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5872 - accuracy: 0.7131 - val_loss: 0.6105 - val_accuracy: 0.8228\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5844 - accuracy: 0.7021 - val_loss: 0.5786 - val_accuracy: 0.8600\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.753611\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.704352\n",
            "[2]\tvalidation_0-auc:0.704537\n",
            "[3]\tvalidation_0-auc:0.7\n",
            "[4]\tvalidation_0-auc:0.703519\n",
            "[5]\tvalidation_0-auc:0.723565\n",
            "[6]\tvalidation_0-auc:0.708102\n",
            "[7]\tvalidation_0-auc:0.707083\n",
            "[8]\tvalidation_0-auc:0.705278\n",
            "[9]\tvalidation_0-auc:0.705463\n",
            "[10]\tvalidation_0-auc:0.693889\n",
            "[11]\tvalidation_0-auc:0.695278\n",
            "[12]\tvalidation_0-auc:0.695648\n",
            "[13]\tvalidation_0-auc:0.695926\n",
            "[14]\tvalidation_0-auc:0.693843\n",
            "[15]\tvalidation_0-auc:0.695046\n",
            "[16]\tvalidation_0-auc:0.694861\n",
            "[17]\tvalidation_0-auc:0.661343\n",
            "[18]\tvalidation_0-auc:0.624861\n",
            "[19]\tvalidation_0-auc:0.636806\n",
            "[20]\tvalidation_0-auc:0.624259\n",
            "[21]\tvalidation_0-auc:0.624537\n",
            "[22]\tvalidation_0-auc:0.607963\n",
            "[23]\tvalidation_0-auc:0.608241\n",
            "[24]\tvalidation_0-auc:0.593796\n",
            "[25]\tvalidation_0-auc:0.592454\n",
            "[26]\tvalidation_0-auc:0.577037\n",
            "[27]\tvalidation_0-auc:0.589444\n",
            "[28]\tvalidation_0-auc:0.588056\n",
            "[29]\tvalidation_0-auc:0.579444\n",
            "[30]\tvalidation_0-auc:0.578519\n",
            "[31]\tvalidation_0-auc:0.580463\n",
            "[32]\tvalidation_0-auc:0.58037\n",
            "[33]\tvalidation_0-auc:0.578056\n",
            "[34]\tvalidation_0-auc:0.576574\n",
            "[35]\tvalidation_0-auc:0.578611\n",
            "[36]\tvalidation_0-auc:0.568981\n",
            "[37]\tvalidation_0-auc:0.568426\n",
            "[38]\tvalidation_0-auc:0.572407\n",
            "[39]\tvalidation_0-auc:0.571389\n",
            "[40]\tvalidation_0-auc:0.568056\n",
            "[41]\tvalidation_0-auc:0.564722\n",
            "[42]\tvalidation_0-auc:0.565648\n",
            "[43]\tvalidation_0-auc:0.566389\n",
            "[44]\tvalidation_0-auc:0.563241\n",
            "[45]\tvalidation_0-auc:0.563981\n",
            "[46]\tvalidation_0-auc:0.563194\n",
            "[47]\tvalidation_0-auc:0.561435\n",
            "[48]\tvalidation_0-auc:0.565139\n",
            "[49]\tvalidation_0-auc:0.564213\n",
            "[50]\tvalidation_0-auc:0.565833\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.753611\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+--------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      | Recall |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+--------+---------------------+\n",
            "|     LSTM 0.15     | 0.9489795918367347 |         0.0         |  0.0   |         0.0         |\n",
            "|      GRU 0.15     | 0.9142857142857143 |  0.2571428571428571 |  0.36  |         0.3         |\n",
            "|    XGBoost 0.15   | 0.9040816326530612 |         0.25        |  0.44  | 0.31884057971014496 |\n",
            "|    Logreg 0.15    | 0.9102040816326531 |  0.2564102564102564 |  0.4   |        0.3125       |\n",
            "|      SVM 0.15     | 0.9224489795918367 |  0.3333333333333333 |  0.52  | 0.40625000000000006 |\n",
            "|   LSTM beta 0.15  | 0.8052516411378556 | 0.07894736842105263 |  0.24  | 0.11881188118811879 |\n",
            "|   GRU beta 0.15   | 0.8599562363238512 | 0.15789473684210525 |  0.36  | 0.21951219512195122 |\n",
            "| XGBoost beta 0.15 | 0.9168490153172867 | 0.25925925925925924 |  0.28  |  0.2692307692307692 |\n",
            "|  logreg beta 0.15 | 0.8358862144420132 |  0.1323529411764706 |  0.36  | 0.19354838709677422 |\n",
            "|   svm beta 0.15   | 0.862144420131291  | 0.16071428571428573 |  0.36  | 0.22222222222222224 |\n",
            "+-------------------+--------------------+---------------------+--------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "81QNjEISGa-C",
        "outputId": "8e7720ff-32ce-48ad-b12f-4954ab9d70cb"
      },
      "source": [
        "Result_cross.to_csv('CL_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.512195</td>\n",
              "      <td>0.836735</td>\n",
              "      <td>0.512195</td>\n",
              "      <td>0.512195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.846939</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.426829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.494845</td>\n",
              "      <td>0.585366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.448598</td>\n",
              "      <td>0.810204</td>\n",
              "      <td>0.507937</td>\n",
              "      <td>0.585366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.476190</td>\n",
              "      <td>0.824490</td>\n",
              "      <td>0.481928</td>\n",
              "      <td>0.487805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.344828</td>\n",
              "      <td>0.763676</td>\n",
              "      <td>0.425532</td>\n",
              "      <td>0.555556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.401408</td>\n",
              "      <td>0.781182</td>\n",
              "      <td>0.532710</td>\n",
              "      <td>0.791667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.525773</td>\n",
              "      <td>0.853392</td>\n",
              "      <td>0.603550</td>\n",
              "      <td>0.708333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.410256</td>\n",
              "      <td>0.796499</td>\n",
              "      <td>0.507937</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.431193</td>\n",
              "      <td>0.809628</td>\n",
              "      <td>0.519337</td>\n",
              "      <td>0.652778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.948980</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.938776</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.926531</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.080000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.932653</td>\n",
              "      <td>0.057143</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.948980</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.945295</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.879650</td>\n",
              "      <td>0.035088</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.868709</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.862144</td>\n",
              "      <td>0.030769</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.945295</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.948980</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>0.914286</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.360000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.904082</td>\n",
              "      <td>0.318841</td>\n",
              "      <td>0.440000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.256410</td>\n",
              "      <td>0.910204</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.922449</td>\n",
              "      <td>0.406250</td>\n",
              "      <td>0.520000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.078947</td>\n",
              "      <td>0.805252</td>\n",
              "      <td>0.118812</td>\n",
              "      <td>0.240000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.157895</td>\n",
              "      <td>0.859956</td>\n",
              "      <td>0.219512</td>\n",
              "      <td>0.360000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.259259</td>\n",
              "      <td>0.916849</td>\n",
              "      <td>0.269231</td>\n",
              "      <td>0.280000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.132353</td>\n",
              "      <td>0.835886</td>\n",
              "      <td>0.193548</td>\n",
              "      <td>0.360000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.160714</td>\n",
              "      <td>0.862144</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.360000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1   CL  0.512195  0.836735  0.512195  0.512195\n",
              "1            GRU 0.1   CL  0.555556  0.846939  0.482759  0.426829\n",
              "2        XGBoost 0.1   CL  0.428571  0.800000  0.494845  0.585366\n",
              "3         Logreg 0.1   CL  0.448598  0.810204  0.507937  0.585366\n",
              "4            SVM 0.1   CL  0.476190  0.824490  0.481928  0.487805\n",
              "5      LSTM beta 0.1   CL  0.344828  0.763676  0.425532  0.555556\n",
              "6       GRU beta 0.1   CL  0.401408  0.781182  0.532710  0.791667\n",
              "7   XGBoost beta 0.1   CL  0.525773  0.853392  0.603550  0.708333\n",
              "8    logreg beta 0.1   CL  0.410256  0.796499  0.507937  0.666667\n",
              "9       svm beta 0.1   CL  0.431193  0.809628  0.519337  0.652778\n",
              "0           LSTM 0.2   CL  0.000000  0.948980  0.000000  0.000000\n",
              "1            GRU 0.2   CL  0.142857  0.938776  0.062500  0.040000\n",
              "2        XGBoost 0.2   CL  0.133333  0.926531  0.100000  0.080000\n",
              "3         Logreg 0.2   CL  0.100000  0.932653  0.057143  0.040000\n",
              "4            SVM 0.2   CL  0.000000  0.948980  0.000000  0.000000\n",
              "5      LSTM beta 0.2   CL  0.000000  0.945295  0.000000  0.000000\n",
              "6       GRU beta 0.2   CL  0.031250  0.879650  0.035088  0.040000\n",
              "7   XGBoost beta 0.2   CL  0.027027  0.868709  0.032258  0.040000\n",
              "8    logreg beta 0.2   CL  0.025000  0.862144  0.030769  0.040000\n",
              "9       svm beta 0.2   CL  0.000000  0.945295  0.000000  0.000000\n",
              "0          LSTM 0.15   CL  0.000000  0.948980  0.000000  0.000000\n",
              "1           GRU 0.15   CL  0.257143  0.914286  0.300000  0.360000\n",
              "2       XGBoost 0.15   CL  0.250000  0.904082  0.318841  0.440000\n",
              "3        Logreg 0.15   CL  0.256410  0.910204  0.312500  0.400000\n",
              "4           SVM 0.15   CL  0.333333  0.922449  0.406250  0.520000\n",
              "5     LSTM beta 0.15   CL  0.078947  0.805252  0.118812  0.240000\n",
              "6      GRU beta 0.15   CL  0.157895  0.859956  0.219512  0.360000\n",
              "7  XGBoost beta 0.15   CL  0.259259  0.916849  0.269231  0.280000\n",
              "8   logreg beta 0.15   CL  0.132353  0.835886  0.193548  0.360000\n",
              "9      svm beta 0.15   CL  0.160714  0.862144  0.222222  0.360000"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz6pbErBGa-D"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV6gJ7eKGa-D"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3j3CQsqGa-D",
        "outputId": "5d2a1bba-7ce3-4e47-d1cd-173ebdf63892"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"CL\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6883 - accuracy: 0.5544 - val_loss: 0.6046 - val_accuracy: 0.8327\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6823 - accuracy: 0.5745 - val_loss: 0.6078 - val_accuracy: 0.8265\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6754 - accuracy: 0.5906 - val_loss: 0.5504 - val_accuracy: 0.8184\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6761 - accuracy: 0.5752 - val_loss: 0.5590 - val_accuracy: 0.8449\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6552 - accuracy: 0.6060 - val_loss: 0.6935 - val_accuracy: 0.5102\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6864 - accuracy: 0.5631 - val_loss: 0.6243 - val_accuracy: 0.8204\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6735 - accuracy: 0.5940 - val_loss: 0.6361 - val_accuracy: 0.8265\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6505 - accuracy: 0.6342 - val_loss: 0.5175 - val_accuracy: 0.8469\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6349 - accuracy: 0.6403 - val_loss: 0.5314 - val_accuracy: 0.8388\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6344 - accuracy: 0.6443 - val_loss: 0.6193 - val_accuracy: 0.7408\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.791577\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.80437\n",
            "[2]\tvalidation_0-auc:0.801814\n",
            "[3]\tvalidation_0-auc:0.810273\n",
            "[4]\tvalidation_0-auc:0.807493\n",
            "[5]\tvalidation_0-auc:0.810766\n",
            "[6]\tvalidation_0-auc:0.813247\n",
            "[7]\tvalidation_0-auc:0.81782\n",
            "[8]\tvalidation_0-auc:0.817372\n",
            "[9]\tvalidation_0-auc:0.816221\n",
            "[10]\tvalidation_0-auc:0.815399\n",
            "[11]\tvalidation_0-auc:0.816057\n",
            "[12]\tvalidation_0-auc:0.816162\n",
            "[13]\tvalidation_0-auc:0.818194\n",
            "[14]\tvalidation_0-auc:0.819076\n",
            "[15]\tvalidation_0-auc:0.820511\n",
            "[16]\tvalidation_0-auc:0.822065\n",
            "[17]\tvalidation_0-auc:0.822125\n",
            "[18]\tvalidation_0-auc:0.822065\n",
            "[19]\tvalidation_0-auc:0.82063\n",
            "[20]\tvalidation_0-auc:0.821034\n",
            "[21]\tvalidation_0-auc:0.821721\n",
            "[22]\tvalidation_0-auc:0.821063\n",
            "[23]\tvalidation_0-auc:0.821811\n",
            "[24]\tvalidation_0-auc:0.821228\n",
            "[25]\tvalidation_0-auc:0.820511\n",
            "[26]\tvalidation_0-auc:0.820989\n",
            "[27]\tvalidation_0-auc:0.820002\n",
            "[28]\tvalidation_0-auc:0.820271\n",
            "[29]\tvalidation_0-auc:0.821347\n",
            "[30]\tvalidation_0-auc:0.821063\n",
            "[31]\tvalidation_0-auc:0.820017\n",
            "[32]\tvalidation_0-auc:0.819539\n",
            "[33]\tvalidation_0-auc:0.820182\n",
            "[34]\tvalidation_0-auc:0.821721\n",
            "[35]\tvalidation_0-auc:0.820256\n",
            "[36]\tvalidation_0-auc:0.820451\n",
            "[37]\tvalidation_0-auc:0.822095\n",
            "[38]\tvalidation_0-auc:0.821258\n",
            "[39]\tvalidation_0-auc:0.818583\n",
            "[40]\tvalidation_0-auc:0.817686\n",
            "[41]\tvalidation_0-auc:0.818747\n",
            "[42]\tvalidation_0-auc:0.818956\n",
            "[43]\tvalidation_0-auc:0.818478\n",
            "[44]\tvalidation_0-auc:0.818523\n",
            "[45]\tvalidation_0-auc:0.818074\n",
            "[46]\tvalidation_0-auc:0.816984\n",
            "[47]\tvalidation_0-auc:0.816774\n",
            "[48]\tvalidation_0-auc:0.818119\n",
            "[49]\tvalidation_0-auc:0.818777\n",
            "[50]\tvalidation_0-auc:0.81788\n",
            "[51]\tvalidation_0-auc:0.817432\n",
            "[52]\tvalidation_0-auc:0.816984\n",
            "[53]\tvalidation_0-auc:0.817163\n",
            "[54]\tvalidation_0-auc:0.81794\n",
            "[55]\tvalidation_0-auc:0.817462\n",
            "[56]\tvalidation_0-auc:0.81785\n",
            "[57]\tvalidation_0-auc:0.818926\n",
            "[58]\tvalidation_0-auc:0.81782\n",
            "[59]\tvalidation_0-auc:0.81803\n",
            "[60]\tvalidation_0-auc:0.817133\n",
            "[61]\tvalidation_0-auc:0.816057\n",
            "[62]\tvalidation_0-auc:0.816057\n",
            "[63]\tvalidation_0-auc:0.814562\n",
            "[64]\tvalidation_0-auc:0.814084\n",
            "[65]\tvalidation_0-auc:0.813965\n",
            "[66]\tvalidation_0-auc:0.812874\n",
            "[67]\tvalidation_0-auc:0.811917\n",
            "Stopping. Best iteration:\n",
            "[17]\tvalidation_0-auc:0.822125\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 20ms/step - loss: 0.6886 - accuracy: 0.5601 - val_loss: 0.6505 - val_accuracy: 0.8425\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6887 - accuracy: 0.5621 - val_loss: 0.6278 - val_accuracy: 0.8425\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6819 - accuracy: 0.5655 - val_loss: 0.6181 - val_accuracy: 0.7768\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6696 - accuracy: 0.5923 - val_loss: 0.6169 - val_accuracy: 0.7505\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6476 - accuracy: 0.6273 - val_loss: 0.5740 - val_accuracy: 0.7943\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6851 - accuracy: 0.5635 - val_loss: 0.6208 - val_accuracy: 0.7768\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6475 - accuracy: 0.6294 - val_loss: 0.6991 - val_accuracy: 0.8096\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6145 - accuracy: 0.6767 - val_loss: 0.7123 - val_accuracy: 0.8009\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6175 - accuracy: 0.6637 - val_loss: 0.6824 - val_accuracy: 0.8206\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6038 - accuracy: 0.6898 - val_loss: 0.6688 - val_accuracy: 0.8184\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.780519\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.809019\n",
            "[2]\tvalidation_0-auc:0.817785\n",
            "[3]\tvalidation_0-auc:0.83768\n",
            "[4]\tvalidation_0-auc:0.844895\n",
            "[5]\tvalidation_0-auc:0.850216\n",
            "[6]\tvalidation_0-auc:0.853662\n",
            "[7]\tvalidation_0-auc:0.852327\n",
            "[8]\tvalidation_0-auc:0.870112\n",
            "[9]\tvalidation_0-auc:0.864015\n",
            "[10]\tvalidation_0-auc:0.863167\n",
            "[11]\tvalidation_0-auc:0.856926\n",
            "[12]\tvalidation_0-auc:0.857973\n",
            "[13]\tvalidation_0-auc:0.85561\n",
            "[14]\tvalidation_0-auc:0.858947\n",
            "[15]\tvalidation_0-auc:0.853734\n",
            "[16]\tvalidation_0-auc:0.848034\n",
            "[17]\tvalidation_0-auc:0.844318\n",
            "[18]\tvalidation_0-auc:0.846609\n",
            "[19]\tvalidation_0-auc:0.842731\n",
            "[20]\tvalidation_0-auc:0.843074\n",
            "[21]\tvalidation_0-auc:0.840566\n",
            "[22]\tvalidation_0-auc:0.836851\n",
            "[23]\tvalidation_0-auc:0.834343\n",
            "[24]\tvalidation_0-auc:0.834235\n",
            "[25]\tvalidation_0-auc:0.835823\n",
            "[26]\tvalidation_0-auc:0.83483\n",
            "[27]\tvalidation_0-auc:0.831097\n",
            "[28]\tvalidation_0-auc:0.82987\n",
            "[29]\tvalidation_0-auc:0.829473\n",
            "[30]\tvalidation_0-auc:0.827417\n",
            "[31]\tvalidation_0-auc:0.828716\n",
            "[32]\tvalidation_0-auc:0.831782\n",
            "[33]\tvalidation_0-auc:0.833622\n",
            "[34]\tvalidation_0-auc:0.831782\n",
            "[35]\tvalidation_0-auc:0.831205\n",
            "[36]\tvalidation_0-auc:0.830556\n",
            "[37]\tvalidation_0-auc:0.826046\n",
            "[38]\tvalidation_0-auc:0.824459\n",
            "[39]\tvalidation_0-auc:0.823088\n",
            "[40]\tvalidation_0-auc:0.822727\n",
            "[41]\tvalidation_0-auc:0.819156\n",
            "[42]\tvalidation_0-auc:0.818831\n",
            "[43]\tvalidation_0-auc:0.816234\n",
            "[44]\tvalidation_0-auc:0.819841\n",
            "[45]\tvalidation_0-auc:0.817605\n",
            "[46]\tvalidation_0-auc:0.814646\n",
            "[47]\tvalidation_0-auc:0.813907\n",
            "[48]\tvalidation_0-auc:0.810949\n",
            "[49]\tvalidation_0-auc:0.813889\n",
            "[50]\tvalidation_0-auc:0.816378\n",
            "[51]\tvalidation_0-auc:0.815729\n",
            "[52]\tvalidation_0-auc:0.815584\n",
            "[53]\tvalidation_0-auc:0.816414\n",
            "[54]\tvalidation_0-auc:0.821032\n",
            "[55]\tvalidation_0-auc:0.822565\n",
            "[56]\tvalidation_0-auc:0.821573\n",
            "[57]\tvalidation_0-auc:0.822511\n",
            "[58]\tvalidation_0-auc:0.817893\n",
            "Stopping. Best iteration:\n",
            "[8]\tvalidation_0-auc:0.870112\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.5102040816326531 | 0.23841059602649006 |  0.8780487804878049 |        0.375        |\n",
            "|     GRU 0.1      | 0.7408163265306122 |  0.3668639053254438 |  0.7560975609756098 |  0.4940239043824701 |\n",
            "|   XGBoost 0.1    |        0.8         | 0.42857142857142855 |  0.5853658536585366 |  0.4948453608247423 |\n",
            "|    Logreg 0.1    | 0.810204081632653  |  0.4485981308411215 |  0.5853658536585366 |  0.507936507936508  |\n",
            "|     SVM 0.1      | 0.8244897959183674 | 0.47619047619047616 |  0.4878048780487805 |  0.4819277108433735 |\n",
            "|  LSTM beta 0.1   | 0.7943107221006565 |  0.2708333333333333 | 0.18055555555555555 | 0.21666666666666667 |\n",
            "|   GRU beta 0.1   | 0.8183807439824945 | 0.44660194174757284 |  0.6388888888888888 |  0.5257142857142858 |\n",
            "| XGBoost beta 0.1 | 0.8533916849015317 |  0.5257731958762887 |  0.7083333333333334 |  0.6035502958579881 |\n",
            "| logreg beta 0.1  | 0.7964989059080962 | 0.41025641025641024 |  0.6666666666666666 |  0.5079365079365079 |\n",
            "|   svm beta 0.1   | 0.8096280087527352 | 0.43119266055045874 |  0.6527777777777778 |  0.5193370165745858 |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.4564 - accuracy: 0.8456 - val_loss: 0.2541 - val_accuracy: 0.9490\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4322 - accuracy: 0.8490 - val_loss: 0.2854 - val_accuracy: 0.9490\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4383 - accuracy: 0.8490 - val_loss: 0.2735 - val_accuracy: 0.9490\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4388 - accuracy: 0.8490 - val_loss: 0.2482 - val_accuracy: 0.9490\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4328 - accuracy: 0.8490 - val_loss: 0.2787 - val_accuracy: 0.9490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.4610 - accuracy: 0.8409 - val_loss: 0.2818 - val_accuracy: 0.9490\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4419 - accuracy: 0.8490 - val_loss: 0.3098 - val_accuracy: 0.9490\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4254 - accuracy: 0.8490 - val_loss: 0.2663 - val_accuracy: 0.9408\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4221 - accuracy: 0.8490 - val_loss: 0.2528 - val_accuracy: 0.9367\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4152 - accuracy: 0.8503 - val_loss: 0.3438 - val_accuracy: 0.9347\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.631355\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.631398\n",
            "[2]\tvalidation_0-auc:0.629763\n",
            "[3]\tvalidation_0-auc:0.667613\n",
            "[4]\tvalidation_0-auc:0.668645\n",
            "[5]\tvalidation_0-auc:0.718495\n",
            "[6]\tvalidation_0-auc:0.719011\n",
            "[7]\tvalidation_0-auc:0.718925\n",
            "[8]\tvalidation_0-auc:0.718538\n",
            "[9]\tvalidation_0-auc:0.749806\n",
            "[10]\tvalidation_0-auc:0.770796\n",
            "[11]\tvalidation_0-auc:0.776043\n",
            "[12]\tvalidation_0-auc:0.784473\n",
            "[13]\tvalidation_0-auc:0.78757\n",
            "[14]\tvalidation_0-auc:0.777936\n",
            "[15]\tvalidation_0-auc:0.782925\n",
            "[16]\tvalidation_0-auc:0.790151\n",
            "[17]\tvalidation_0-auc:0.80228\n",
            "[18]\tvalidation_0-auc:0.799742\n",
            "[19]\tvalidation_0-auc:0.814624\n",
            "[20]\tvalidation_0-auc:0.810409\n",
            "[21]\tvalidation_0-auc:0.805462\n",
            "[22]\tvalidation_0-auc:0.802624\n",
            "[23]\tvalidation_0-auc:0.813806\n",
            "[24]\tvalidation_0-auc:0.811312\n",
            "[25]\tvalidation_0-auc:0.815398\n",
            "[26]\tvalidation_0-auc:0.811871\n",
            "[27]\tvalidation_0-auc:0.832473\n",
            "[28]\tvalidation_0-auc:0.830237\n",
            "[29]\tvalidation_0-auc:0.839656\n",
            "[30]\tvalidation_0-auc:0.831656\n",
            "[31]\tvalidation_0-auc:0.843914\n",
            "[32]\tvalidation_0-auc:0.846022\n",
            "[33]\tvalidation_0-auc:0.847054\n",
            "[34]\tvalidation_0-auc:0.84757\n",
            "[35]\tvalidation_0-auc:0.847312\n",
            "[36]\tvalidation_0-auc:0.856344\n",
            "[37]\tvalidation_0-auc:0.851699\n",
            "[38]\tvalidation_0-auc:0.851699\n",
            "[39]\tvalidation_0-auc:0.851183\n",
            "[40]\tvalidation_0-auc:0.852645\n",
            "[41]\tvalidation_0-auc:0.845591\n",
            "[42]\tvalidation_0-auc:0.844989\n",
            "[43]\tvalidation_0-auc:0.846968\n",
            "[44]\tvalidation_0-auc:0.843355\n",
            "[45]\tvalidation_0-auc:0.843613\n",
            "[46]\tvalidation_0-auc:0.842839\n",
            "[47]\tvalidation_0-auc:0.842323\n",
            "[48]\tvalidation_0-auc:0.844645\n",
            "[49]\tvalidation_0-auc:0.837763\n",
            "[50]\tvalidation_0-auc:0.836301\n",
            "[51]\tvalidation_0-auc:0.837247\n",
            "[52]\tvalidation_0-auc:0.832774\n",
            "[53]\tvalidation_0-auc:0.832946\n",
            "[54]\tvalidation_0-auc:0.833376\n",
            "[55]\tvalidation_0-auc:0.833548\n",
            "[56]\tvalidation_0-auc:0.827699\n",
            "[57]\tvalidation_0-auc:0.821075\n",
            "[58]\tvalidation_0-auc:0.821677\n",
            "[59]\tvalidation_0-auc:0.821763\n",
            "[60]\tvalidation_0-auc:0.82228\n",
            "[61]\tvalidation_0-auc:0.820731\n",
            "[62]\tvalidation_0-auc:0.823398\n",
            "[63]\tvalidation_0-auc:0.822882\n",
            "[64]\tvalidation_0-auc:0.818065\n",
            "[65]\tvalidation_0-auc:0.81428\n",
            "[66]\tvalidation_0-auc:0.813763\n",
            "[67]\tvalidation_0-auc:0.818495\n",
            "[68]\tvalidation_0-auc:0.817634\n",
            "[69]\tvalidation_0-auc:0.813763\n",
            "[70]\tvalidation_0-auc:0.815914\n",
            "[71]\tvalidation_0-auc:0.816688\n",
            "[72]\tvalidation_0-auc:0.820387\n",
            "[73]\tvalidation_0-auc:0.821505\n",
            "[74]\tvalidation_0-auc:0.822452\n",
            "[75]\tvalidation_0-auc:0.824516\n",
            "[76]\tvalidation_0-auc:0.819613\n",
            "[77]\tvalidation_0-auc:0.821505\n",
            "[78]\tvalidation_0-auc:0.819699\n",
            "[79]\tvalidation_0-auc:0.819785\n",
            "[80]\tvalidation_0-auc:0.817548\n",
            "[81]\tvalidation_0-auc:0.817634\n",
            "[82]\tvalidation_0-auc:0.814882\n",
            "[83]\tvalidation_0-auc:0.818409\n",
            "[84]\tvalidation_0-auc:0.817548\n",
            "[85]\tvalidation_0-auc:0.818495\n",
            "[86]\tvalidation_0-auc:0.816344\n",
            "Stopping. Best iteration:\n",
            "[36]\tvalidation_0-auc:0.856344\n",
            "\n",
            "end training. \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.4676 - accuracy: 0.8428 - val_loss: 0.2832 - val_accuracy: 0.9453\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4419 - accuracy: 0.8456 - val_loss: 0.2936 - val_accuracy: 0.9453\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4514 - accuracy: 0.8456 - val_loss: 0.2437 - val_accuracy: 0.9453\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4473 - accuracy: 0.8456 - val_loss: 0.2702 - val_accuracy: 0.9453\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4284 - accuracy: 0.8456 - val_loss: 0.2894 - val_accuracy: 0.9453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.4662 - accuracy: 0.8428 - val_loss: 0.2343 - val_accuracy: 0.9453\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4384 - accuracy: 0.8456 - val_loss: 0.2766 - val_accuracy: 0.9059\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4264 - accuracy: 0.8456 - val_loss: 0.3505 - val_accuracy: 0.8862\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4145 - accuracy: 0.8524 - val_loss: 0.3957 - val_accuracy: 0.8796\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4125 - accuracy: 0.8476 - val_loss: 0.4261 - val_accuracy: 0.8796\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.474259\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.474259\n",
            "[2]\tvalidation_0-auc:0.474074\n",
            "[3]\tvalidation_0-auc:0.474074\n",
            "[4]\tvalidation_0-auc:0.473704\n",
            "[5]\tvalidation_0-auc:0.545463\n",
            "[6]\tvalidation_0-auc:0.545556\n",
            "[7]\tvalidation_0-auc:0.545556\n",
            "[8]\tvalidation_0-auc:0.618704\n",
            "[9]\tvalidation_0-auc:0.618796\n",
            "[10]\tvalidation_0-auc:0.667917\n",
            "[11]\tvalidation_0-auc:0.724583\n",
            "[12]\tvalidation_0-auc:0.71662\n",
            "[13]\tvalidation_0-auc:0.711991\n",
            "[14]\tvalidation_0-auc:0.711991\n",
            "[15]\tvalidation_0-auc:0.713009\n",
            "[16]\tvalidation_0-auc:0.711157\n",
            "[17]\tvalidation_0-auc:0.686713\n",
            "[18]\tvalidation_0-auc:0.712639\n",
            "[19]\tvalidation_0-auc:0.709028\n",
            "[20]\tvalidation_0-auc:0.701065\n",
            "[21]\tvalidation_0-auc:0.716389\n",
            "[22]\tvalidation_0-auc:0.701759\n",
            "[23]\tvalidation_0-auc:0.701574\n",
            "[24]\tvalidation_0-auc:0.718148\n",
            "[25]\tvalidation_0-auc:0.712593\n",
            "[26]\tvalidation_0-auc:0.706667\n",
            "[27]\tvalidation_0-auc:0.699537\n",
            "[28]\tvalidation_0-auc:0.70713\n",
            "[29]\tvalidation_0-auc:0.702454\n",
            "[30]\tvalidation_0-auc:0.695324\n",
            "[31]\tvalidation_0-auc:0.691898\n",
            "[32]\tvalidation_0-auc:0.70162\n",
            "[33]\tvalidation_0-auc:0.693519\n",
            "[34]\tvalidation_0-auc:0.696667\n",
            "[35]\tvalidation_0-auc:0.698333\n",
            "[36]\tvalidation_0-auc:0.694444\n",
            "[37]\tvalidation_0-auc:0.694537\n",
            "[38]\tvalidation_0-auc:0.697454\n",
            "[39]\tvalidation_0-auc:0.695185\n",
            "[40]\tvalidation_0-auc:0.685972\n",
            "[41]\tvalidation_0-auc:0.678102\n",
            "[42]\tvalidation_0-auc:0.683981\n",
            "[43]\tvalidation_0-auc:0.67875\n",
            "[44]\tvalidation_0-auc:0.672731\n",
            "[45]\tvalidation_0-auc:0.671528\n",
            "[46]\tvalidation_0-auc:0.67625\n",
            "[47]\tvalidation_0-auc:0.676991\n",
            "[48]\tvalidation_0-auc:0.682176\n",
            "[49]\tvalidation_0-auc:0.680417\n",
            "[50]\tvalidation_0-auc:0.682917\n",
            "[51]\tvalidation_0-auc:0.690787\n",
            "[52]\tvalidation_0-auc:0.692731\n",
            "[53]\tvalidation_0-auc:0.690833\n",
            "[54]\tvalidation_0-auc:0.690463\n",
            "[55]\tvalidation_0-auc:0.692315\n",
            "[56]\tvalidation_0-auc:0.69213\n",
            "[57]\tvalidation_0-auc:0.690926\n",
            "[58]\tvalidation_0-auc:0.691944\n",
            "[59]\tvalidation_0-auc:0.696111\n",
            "[60]\tvalidation_0-auc:0.693889\n",
            "[61]\tvalidation_0-auc:0.696204\n",
            "Stopping. Best iteration:\n",
            "[11]\tvalidation_0-auc:0.724583\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------+----------------------+\n",
            "|      Model       |      Accuracy      |      Precision      | Recall |       F1 score       |\n",
            "+------------------+--------------------+---------------------+--------+----------------------+\n",
            "|     LSTM 0.2     | 0.9489795918367347 |         0.0         |  0.0   |         0.0          |\n",
            "|     GRU 0.2      | 0.9346938775510204 |  0.1111111111111111 |  0.04  | 0.058823529411764705 |\n",
            "|   XGBoost 0.2    | 0.926530612244898  | 0.13333333333333333 |  0.08  |         0.1          |\n",
            "|    Logreg 0.2    | 0.9326530612244898 |         0.1         |  0.04  | 0.05714285714285714  |\n",
            "|     SVM 0.2      | 0.9489795918367347 |         0.0         |  0.0   |         0.0          |\n",
            "|  LSTM beta 0.2   | 0.9452954048140044 |         0.0         |  0.0   |         0.0          |\n",
            "|   GRU beta 0.2   | 0.8796498905908097 |       0.03125       |  0.04  | 0.03508771929824561  |\n",
            "| XGBoost beta 0.2 | 0.8687089715536105 | 0.02702702702702703 |  0.04  | 0.03225806451612903  |\n",
            "| logreg beta 0.2  | 0.862144420131291  |        0.025        |  0.04  | 0.030769230769230767 |\n",
            "|   svm beta 0.2   | 0.9452954048140044 |         0.0         |  0.0   |         0.0          |\n",
            "+------------------+--------------------+---------------------+--------+----------------------+\n",
            "Threshhold =  0.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6513 - accuracy: 0.6477 - val_loss: 0.5052 - val_accuracy: 0.9490\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6505 - accuracy: 0.6537 - val_loss: 0.4315 - val_accuracy: 0.9490\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6466 - accuracy: 0.6537 - val_loss: 0.5670 - val_accuracy: 0.9490\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6485 - accuracy: 0.6517 - val_loss: 0.4457 - val_accuracy: 0.9490\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6415 - accuracy: 0.6550 - val_loss: 0.4451 - val_accuracy: 0.9306\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6501 - accuracy: 0.6537 - val_loss: 0.5251 - val_accuracy: 0.9388\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6463 - accuracy: 0.6564 - val_loss: 0.5168 - val_accuracy: 0.9367\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6285 - accuracy: 0.6638 - val_loss: 0.4406 - val_accuracy: 0.9327\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6172 - accuracy: 0.6852 - val_loss: 0.5238 - val_accuracy: 0.8939\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6137 - accuracy: 0.6933 - val_loss: 0.4512 - val_accuracy: 0.9143\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.791785\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.812774\n",
            "[2]\tvalidation_0-auc:0.837505\n",
            "[3]\tvalidation_0-auc:0.840903\n",
            "[4]\tvalidation_0-auc:0.843097\n",
            "[5]\tvalidation_0-auc:0.847398\n",
            "[6]\tvalidation_0-auc:0.851312\n",
            "[7]\tvalidation_0-auc:0.852774\n",
            "[8]\tvalidation_0-auc:0.854538\n",
            "[9]\tvalidation_0-auc:0.854237\n",
            "[10]\tvalidation_0-auc:0.854968\n",
            "[11]\tvalidation_0-auc:0.850237\n",
            "[12]\tvalidation_0-auc:0.851527\n",
            "[13]\tvalidation_0-auc:0.854065\n",
            "[14]\tvalidation_0-auc:0.85372\n",
            "[15]\tvalidation_0-auc:0.854065\n",
            "[16]\tvalidation_0-auc:0.851484\n",
            "[17]\tvalidation_0-auc:0.851054\n",
            "[18]\tvalidation_0-auc:0.860774\n",
            "[19]\tvalidation_0-auc:0.860387\n",
            "[20]\tvalidation_0-auc:0.860731\n",
            "[21]\tvalidation_0-auc:0.860473\n",
            "[22]\tvalidation_0-auc:0.862366\n",
            "[23]\tvalidation_0-auc:0.860559\n",
            "[24]\tvalidation_0-auc:0.857118\n",
            "[25]\tvalidation_0-auc:0.855914\n",
            "[26]\tvalidation_0-auc:0.857118\n",
            "[27]\tvalidation_0-auc:0.856516\n",
            "[28]\tvalidation_0-auc:0.858323\n",
            "[29]\tvalidation_0-auc:0.863828\n",
            "[30]\tvalidation_0-auc:0.863656\n",
            "[31]\tvalidation_0-auc:0.865548\n",
            "[32]\tvalidation_0-auc:0.867011\n",
            "[33]\tvalidation_0-auc:0.868129\n",
            "[34]\tvalidation_0-auc:0.873806\n",
            "[35]\tvalidation_0-auc:0.873634\n",
            "[36]\tvalidation_0-auc:0.874323\n",
            "[37]\tvalidation_0-auc:0.869936\n",
            "[38]\tvalidation_0-auc:0.862108\n",
            "[39]\tvalidation_0-auc:0.861247\n",
            "[40]\tvalidation_0-auc:0.865118\n",
            "[41]\tvalidation_0-auc:0.863312\n",
            "[42]\tvalidation_0-auc:0.860129\n",
            "[43]\tvalidation_0-auc:0.861935\n",
            "[44]\tvalidation_0-auc:0.861935\n",
            "[45]\tvalidation_0-auc:0.857548\n",
            "[46]\tvalidation_0-auc:0.856172\n",
            "[47]\tvalidation_0-auc:0.857118\n",
            "[48]\tvalidation_0-auc:0.859269\n",
            "[49]\tvalidation_0-auc:0.857634\n",
            "[50]\tvalidation_0-auc:0.856602\n",
            "[51]\tvalidation_0-auc:0.852559\n",
            "[52]\tvalidation_0-auc:0.85471\n",
            "[53]\tvalidation_0-auc:0.850581\n",
            "[54]\tvalidation_0-auc:0.850237\n",
            "[55]\tvalidation_0-auc:0.850495\n",
            "[56]\tvalidation_0-auc:0.851011\n",
            "[57]\tvalidation_0-auc:0.850753\n",
            "[58]\tvalidation_0-auc:0.856258\n",
            "[59]\tvalidation_0-auc:0.854538\n",
            "[60]\tvalidation_0-auc:0.853419\n",
            "[61]\tvalidation_0-auc:0.850839\n",
            "[62]\tvalidation_0-auc:0.850667\n",
            "[63]\tvalidation_0-auc:0.849806\n",
            "[64]\tvalidation_0-auc:0.847398\n",
            "[65]\tvalidation_0-auc:0.84671\n",
            "[66]\tvalidation_0-auc:0.847484\n",
            "[67]\tvalidation_0-auc:0.848344\n",
            "[68]\tvalidation_0-auc:0.845935\n",
            "[69]\tvalidation_0-auc:0.843441\n",
            "[70]\tvalidation_0-auc:0.842409\n",
            "[71]\tvalidation_0-auc:0.843613\n",
            "[72]\tvalidation_0-auc:0.848172\n",
            "[73]\tvalidation_0-auc:0.847914\n",
            "[74]\tvalidation_0-auc:0.84972\n",
            "[75]\tvalidation_0-auc:0.84929\n",
            "[76]\tvalidation_0-auc:0.846882\n",
            "[77]\tvalidation_0-auc:0.848172\n",
            "[78]\tvalidation_0-auc:0.846366\n",
            "[79]\tvalidation_0-auc:0.848344\n",
            "[80]\tvalidation_0-auc:0.848172\n",
            "[81]\tvalidation_0-auc:0.847054\n",
            "[82]\tvalidation_0-auc:0.844731\n",
            "[83]\tvalidation_0-auc:0.842409\n",
            "[84]\tvalidation_0-auc:0.849032\n",
            "[85]\tvalidation_0-auc:0.846366\n",
            "[86]\tvalidation_0-auc:0.84757\n",
            "Stopping. Best iteration:\n",
            "[36]\tvalidation_0-auc:0.874323\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6619 - accuracy: 0.6431 - val_loss: 0.4564 - val_accuracy: 0.9453\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6535 - accuracy: 0.6458 - val_loss: 0.4751 - val_accuracy: 0.9453\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6473 - accuracy: 0.6410 - val_loss: 0.4112 - val_accuracy: 0.9453\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6391 - accuracy: 0.6507 - val_loss: 0.4466 - val_accuracy: 0.8753\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6173 - accuracy: 0.6733 - val_loss: 0.4732 - val_accuracy: 0.8709\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6490 - accuracy: 0.6376 - val_loss: 0.4651 - val_accuracy: 0.8796\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6119 - accuracy: 0.6870 - val_loss: 0.6200 - val_accuracy: 0.8753\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5858 - accuracy: 0.7159 - val_loss: 0.6990 - val_accuracy: 0.8118\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5861 - accuracy: 0.7076 - val_loss: 0.6514 - val_accuracy: 0.8490\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5795 - accuracy: 0.7261 - val_loss: 0.7624 - val_accuracy: 0.7462\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.753611\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.704352\n",
            "[2]\tvalidation_0-auc:0.704537\n",
            "[3]\tvalidation_0-auc:0.7\n",
            "[4]\tvalidation_0-auc:0.703519\n",
            "[5]\tvalidation_0-auc:0.723565\n",
            "[6]\tvalidation_0-auc:0.708102\n",
            "[7]\tvalidation_0-auc:0.707083\n",
            "[8]\tvalidation_0-auc:0.705278\n",
            "[9]\tvalidation_0-auc:0.705463\n",
            "[10]\tvalidation_0-auc:0.693889\n",
            "[11]\tvalidation_0-auc:0.695278\n",
            "[12]\tvalidation_0-auc:0.695648\n",
            "[13]\tvalidation_0-auc:0.695926\n",
            "[14]\tvalidation_0-auc:0.693843\n",
            "[15]\tvalidation_0-auc:0.695046\n",
            "[16]\tvalidation_0-auc:0.694861\n",
            "[17]\tvalidation_0-auc:0.661343\n",
            "[18]\tvalidation_0-auc:0.624861\n",
            "[19]\tvalidation_0-auc:0.636806\n",
            "[20]\tvalidation_0-auc:0.624259\n",
            "[21]\tvalidation_0-auc:0.624537\n",
            "[22]\tvalidation_0-auc:0.607963\n",
            "[23]\tvalidation_0-auc:0.608241\n",
            "[24]\tvalidation_0-auc:0.593796\n",
            "[25]\tvalidation_0-auc:0.592454\n",
            "[26]\tvalidation_0-auc:0.577037\n",
            "[27]\tvalidation_0-auc:0.589444\n",
            "[28]\tvalidation_0-auc:0.588056\n",
            "[29]\tvalidation_0-auc:0.579444\n",
            "[30]\tvalidation_0-auc:0.578519\n",
            "[31]\tvalidation_0-auc:0.580463\n",
            "[32]\tvalidation_0-auc:0.58037\n",
            "[33]\tvalidation_0-auc:0.578056\n",
            "[34]\tvalidation_0-auc:0.576574\n",
            "[35]\tvalidation_0-auc:0.578611\n",
            "[36]\tvalidation_0-auc:0.568981\n",
            "[37]\tvalidation_0-auc:0.568426\n",
            "[38]\tvalidation_0-auc:0.572407\n",
            "[39]\tvalidation_0-auc:0.571389\n",
            "[40]\tvalidation_0-auc:0.568056\n",
            "[41]\tvalidation_0-auc:0.564722\n",
            "[42]\tvalidation_0-auc:0.565648\n",
            "[43]\tvalidation_0-auc:0.566389\n",
            "[44]\tvalidation_0-auc:0.563241\n",
            "[45]\tvalidation_0-auc:0.563981\n",
            "[46]\tvalidation_0-auc:0.563194\n",
            "[47]\tvalidation_0-auc:0.561435\n",
            "[48]\tvalidation_0-auc:0.565139\n",
            "[49]\tvalidation_0-auc:0.564213\n",
            "[50]\tvalidation_0-auc:0.565833\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.753611\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+----------------------+--------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision       | Recall |       F1 score      |\n",
            "+-------------------+--------------------+----------------------+--------+---------------------+\n",
            "|     LSTM 0.15     | 0.9306122448979591 |         0.0          |  0.0   |         0.0         |\n",
            "|      GRU 0.15     | 0.9142857142857143 | 0.32653061224489793  |  0.64  | 0.43243243243243235 |\n",
            "|    XGBoost 0.15   | 0.9040816326530612 |         0.25         |  0.44  | 0.31884057971014496 |\n",
            "|    Logreg 0.15    | 0.9102040816326531 |  0.2564102564102564  |  0.4   |        0.3125       |\n",
            "|      SVM 0.15     | 0.9224489795918367 |  0.3333333333333333  |  0.52  | 0.40625000000000006 |\n",
            "|   LSTM beta 0.15  | 0.8708971553610503 | 0.027777777777777776 |  0.04  | 0.03278688524590164 |\n",
            "|   GRU beta 0.15   | 0.7461706783369803 | 0.10434782608695652  |  0.48  | 0.17142857142857143 |\n",
            "| XGBoost beta 0.15 | 0.9168490153172867 | 0.25925925925925924  |  0.28  |  0.2692307692307692 |\n",
            "|  logreg beta 0.15 | 0.8358862144420132 |  0.1323529411764706  |  0.36  | 0.19354838709677422 |\n",
            "|   svm beta 0.15   | 0.862144420131291  | 0.16071428571428573  |  0.36  | 0.22222222222222224 |\n",
            "+-------------------+--------------------+----------------------+--------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "wiYyeVi7Ga-E",
        "outputId": "228a1b35-cb0d-409b-aa45-515fd45c0b04"
      },
      "source": [
        "Result_purging.to_csv('CL_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.238411</td>\n",
              "      <td>0.510204</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.878049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.366864</td>\n",
              "      <td>0.740816</td>\n",
              "      <td>0.494024</td>\n",
              "      <td>0.756098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.494845</td>\n",
              "      <td>0.585366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.448598</td>\n",
              "      <td>0.810204</td>\n",
              "      <td>0.507937</td>\n",
              "      <td>0.585366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.476190</td>\n",
              "      <td>0.824490</td>\n",
              "      <td>0.481928</td>\n",
              "      <td>0.487805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.270833</td>\n",
              "      <td>0.794311</td>\n",
              "      <td>0.216667</td>\n",
              "      <td>0.180556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.446602</td>\n",
              "      <td>0.818381</td>\n",
              "      <td>0.525714</td>\n",
              "      <td>0.638889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.525773</td>\n",
              "      <td>0.853392</td>\n",
              "      <td>0.603550</td>\n",
              "      <td>0.708333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.410256</td>\n",
              "      <td>0.796499</td>\n",
              "      <td>0.507937</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.431193</td>\n",
              "      <td>0.809628</td>\n",
              "      <td>0.519337</td>\n",
              "      <td>0.652778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.948980</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.934694</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.926531</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.080000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.932653</td>\n",
              "      <td>0.057143</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.948980</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.945295</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.879650</td>\n",
              "      <td>0.035088</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.868709</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.862144</td>\n",
              "      <td>0.030769</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.945295</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.930612</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.326531</td>\n",
              "      <td>0.914286</td>\n",
              "      <td>0.432432</td>\n",
              "      <td>0.640000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.904082</td>\n",
              "      <td>0.318841</td>\n",
              "      <td>0.440000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.256410</td>\n",
              "      <td>0.910204</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.922449</td>\n",
              "      <td>0.406250</td>\n",
              "      <td>0.520000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.027778</td>\n",
              "      <td>0.870897</td>\n",
              "      <td>0.032787</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.104348</td>\n",
              "      <td>0.746171</td>\n",
              "      <td>0.171429</td>\n",
              "      <td>0.480000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.259259</td>\n",
              "      <td>0.916849</td>\n",
              "      <td>0.269231</td>\n",
              "      <td>0.280000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.132353</td>\n",
              "      <td>0.835886</td>\n",
              "      <td>0.193548</td>\n",
              "      <td>0.360000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.160714</td>\n",
              "      <td>0.862144</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.360000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1   CL  0.238411  0.510204  0.375000  0.878049\n",
              "1            GRU 0.1   CL  0.366864  0.740816  0.494024  0.756098\n",
              "2        XGBoost 0.1   CL  0.428571  0.800000  0.494845  0.585366\n",
              "3         Logreg 0.1   CL  0.448598  0.810204  0.507937  0.585366\n",
              "4            SVM 0.1   CL  0.476190  0.824490  0.481928  0.487805\n",
              "5      LSTM beta 0.1   CL  0.270833  0.794311  0.216667  0.180556\n",
              "6       GRU beta 0.1   CL  0.446602  0.818381  0.525714  0.638889\n",
              "7   XGBoost beta 0.1   CL  0.525773  0.853392  0.603550  0.708333\n",
              "8    logreg beta 0.1   CL  0.410256  0.796499  0.507937  0.666667\n",
              "9       svm beta 0.1   CL  0.431193  0.809628  0.519337  0.652778\n",
              "0           LSTM 0.2   CL  0.000000  0.948980  0.000000  0.000000\n",
              "1            GRU 0.2   CL  0.111111  0.934694  0.058824  0.040000\n",
              "2        XGBoost 0.2   CL  0.133333  0.926531  0.100000  0.080000\n",
              "3         Logreg 0.2   CL  0.100000  0.932653  0.057143  0.040000\n",
              "4            SVM 0.2   CL  0.000000  0.948980  0.000000  0.000000\n",
              "5      LSTM beta 0.2   CL  0.000000  0.945295  0.000000  0.000000\n",
              "6       GRU beta 0.2   CL  0.031250  0.879650  0.035088  0.040000\n",
              "7   XGBoost beta 0.2   CL  0.027027  0.868709  0.032258  0.040000\n",
              "8    logreg beta 0.2   CL  0.025000  0.862144  0.030769  0.040000\n",
              "9       svm beta 0.2   CL  0.000000  0.945295  0.000000  0.000000\n",
              "0          LSTM 0.15   CL  0.000000  0.930612  0.000000  0.000000\n",
              "1           GRU 0.15   CL  0.326531  0.914286  0.432432  0.640000\n",
              "2       XGBoost 0.15   CL  0.250000  0.904082  0.318841  0.440000\n",
              "3        Logreg 0.15   CL  0.256410  0.910204  0.312500  0.400000\n",
              "4           SVM 0.15   CL  0.333333  0.922449  0.406250  0.520000\n",
              "5     LSTM beta 0.15   CL  0.027778  0.870897  0.032787  0.040000\n",
              "6      GRU beta 0.15   CL  0.104348  0.746171  0.171429  0.480000\n",
              "7  XGBoost beta 0.15   CL  0.259259  0.916849  0.269231  0.280000\n",
              "8   logreg beta 0.15   CL  0.132353  0.835886  0.193548  0.360000\n",
              "9      svm beta 0.15   CL  0.160714  0.862144  0.222222  0.360000"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKg3AipZGa-E"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEjp-1zWHUKQ"
      },
      "source": [
        "## CPRI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "aEooKxjUHUKp",
        "outputId": "de675405-f0b5-4169-c1d5-ebe280dd67c6"
      },
      "source": [
        "dfs = pd.read_csv(\"CPRI.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "# Denoise prices\n",
        "dfs = denoise_data(dfs, '<CLOSE>')\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2464</td>\n",
              "      <td>US1.CPRI</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>48.61</td>\n",
              "      <td>50.18</td>\n",
              "      <td>47.93</td>\n",
              "      <td>49.78</td>\n",
              "      <td>37955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2463</td>\n",
              "      <td>US1.CPRI</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>50.23</td>\n",
              "      <td>50.73</td>\n",
              "      <td>48.33</td>\n",
              "      <td>48.40</td>\n",
              "      <td>51156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2462</td>\n",
              "      <td>US1.CPRI</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>52.41</td>\n",
              "      <td>52.96</td>\n",
              "      <td>51.17</td>\n",
              "      <td>51.19</td>\n",
              "      <td>16510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2461</td>\n",
              "      <td>US1.CPRI</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>53.13</td>\n",
              "      <td>53.13</td>\n",
              "      <td>52.07</td>\n",
              "      <td>52.50</td>\n",
              "      <td>15965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2460</td>\n",
              "      <td>US1.CPRI</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>52.69</td>\n",
              "      <td>53.40</td>\n",
              "      <td>52.69</td>\n",
              "      <td>52.99</td>\n",
              "      <td>43844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2460</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.CPRI</td>\n",
              "      <td>D</td>\n",
              "      <td>20111221</td>\n",
              "      <td>0</td>\n",
              "      <td>25.64</td>\n",
              "      <td>26.78</td>\n",
              "      <td>25.46</td>\n",
              "      <td>26.50</td>\n",
              "      <td>2485638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2461</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.CPRI</td>\n",
              "      <td>D</td>\n",
              "      <td>20111220</td>\n",
              "      <td>0</td>\n",
              "      <td>25.02</td>\n",
              "      <td>25.76</td>\n",
              "      <td>25.02</td>\n",
              "      <td>25.40</td>\n",
              "      <td>1697064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2462</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.CPRI</td>\n",
              "      <td>D</td>\n",
              "      <td>20111219</td>\n",
              "      <td>0</td>\n",
              "      <td>24.50</td>\n",
              "      <td>25.09</td>\n",
              "      <td>24.31</td>\n",
              "      <td>24.83</td>\n",
              "      <td>3133441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2463</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.CPRI</td>\n",
              "      <td>D</td>\n",
              "      <td>20111216</td>\n",
              "      <td>0</td>\n",
              "      <td>24.45</td>\n",
              "      <td>24.80</td>\n",
              "      <td>23.51</td>\n",
              "      <td>24.21</td>\n",
              "      <td>3921157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2464</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.CPRI</td>\n",
              "      <td>D</td>\n",
              "      <td>20111215</td>\n",
              "      <td>0</td>\n",
              "      <td>24.82</td>\n",
              "      <td>25.19</td>\n",
              "      <td>23.51</td>\n",
              "      <td>24.39</td>\n",
              "      <td>31936188</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2465 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  <TICKER> <PER>    <DATE>  ...  <HIGH>  <LOW>  <CLOSE>     <VOL>\n",
              "0      2464  US1.CPRI     D  20211001  ...   50.18  47.93    49.78     37955\n",
              "1      2463  US1.CPRI     D  20210930  ...   50.73  48.33    48.40     51156\n",
              "2      2462  US1.CPRI     D  20210929  ...   52.96  51.17    51.19     16510\n",
              "3      2461  US1.CPRI     D  20210928  ...   53.13  52.07    52.50     15965\n",
              "4      2460  US1.CPRI     D  20210927  ...   53.40  52.69    52.99     43844\n",
              "...     ...       ...   ...       ...  ...     ...    ...      ...       ...\n",
              "2460      4  US1.CPRI     D  20111221  ...   26.78  25.46    26.50   2485638\n",
              "2461      3  US1.CPRI     D  20111220  ...   25.76  25.02    25.40   1697064\n",
              "2462      2  US1.CPRI     D  20111219  ...   25.09  24.31    24.83   3133441\n",
              "2463      1  US1.CPRI     D  20111216  ...   24.80  23.51    24.21   3921157\n",
              "2464      0  US1.CPRI     D  20111215  ...   25.19  23.51    24.39  31936188\n",
              "\n",
              "[2465 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9sbOQsCGHUKr",
        "outputId": "61e8aa55-263a-45d3-d88d-c74ec4a18deb"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"d32df22c-eec2-4cf7-980a-8245a7878a89\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"d32df22c-eec2-4cf7-980a-8245a7878a89\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'd32df22c-eec2-4cf7-980a-8245a7878a89',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [27.92, 28.83, 29.33, 29.48, 29.61, 31.45, 33.16, 32.62, 32.74, 32.86, 32.72, 32.8, 32.19, 32.56, 32.31, 31.66, 32.26, 32.01, 32.21, 32.04, 31.41, 29.76, 28.07, 28.3, 26.49, 25.56, 26.37, 26.54, 26.21, 25.875, 26.75, 26.76, 28.01, 27.91, 27.68, 28.58, 28.25, 26.98, 28.68, 30.35, 29.86, 30.98, 32.69, 32.58, 31.6, 31.71, 33.19, 33.96, 35.59, 35.83, 36.315, 36.07, 36.53, 36.72, 36.34, 35.65, 36.33, 35.95, 36.61, 37.05, 36.95, 36.47, 35.11, 34.63, 34.3, 34.5, 34.75, 34.76, 34.25, 34.85, 34.66, 33.99, 34.25, 33.66, 34.14, 34.71, 34.94, 34.25, 34.68, 33.89, 33.5, 33.68, 33.12, 33.93, 34.5, 34.21, 33.95, 35.05, 35.386, 34.02, 32.47, 33.78, 35.05, 38.88, 39.01, 39.5, 39.74, 40.62, 39.95, 39.99, 40.28, 40.19, 40.74, 40.48, 42.8, 43.09, 42.29, 41.97, 43.73, 44.08, 43.68, 43.19, 44.08, 44.87, 44.67, 44.52, 46.12, 46.09, 46.64, 49.21, 48.66, 47.845, 48.09, 48.45, 48.04, 49.83, 48.57, 48.85, 48.1, 49.31, 46.94, 46.62, 47.15, 45.75, 46.65, 45.38, 44.37, 43.86, 43.69, 45.36, 45.4, 46.57, 47.09, 45.92, 46.23, 46.41, 45.55, 44.69, 43.55, 44.69, 46.01, 45.21, 45.09, 46.81, 45.63, 45.44, 45.6, 45.17, 43.83, 43.6, 43.7, 44.04, 44.5, 44.89, 45.12, 45.48, 45.51, 46.04, 47.25, 48.53, 43.54, 42.33, 42.43, 42.48, 42.9, 42.84, 42.56, 42.405, 41.05, 40.82, 41.26, 42.42, 41.1, 40.4, 40.33, 40.23, 40.21, 40.29, 41.86, 40.9, 40.0, 38.98, 37.11, 39.44, 37.93, 37.18, 37.88, 37.76, 35.54, 36.41, 37.07, 37.92, 38.0, 38.11, 37.94, 37.48, 38.82, 38.45, 39.65, 40.5, 42.92, 42.43, 45.05, 43.76, 43.79, 44.96, 44.21, 45.34, 44.88, 45.3, 44.23, 44.6, 45.88, 47.24, 48.05, 48.22, 47.75, 47.78, 50.09, 49.0501, 57.51, 58.31, 58.29, 57.37, 55.41, 54.65, 55.04, 55.66, 56.92, 56.3, 57.88, 58.24, 58.05, 60.24, 62.37, 64.06, 62.46, 63.59, 61.98, 62.28, 67.06, 68.02, 66.88, 66.59, 68.09, 66.1, 68.37, 68.56, 69.45, 68.02, 68.01, 66.7, 72.67, 72.71, 73.05, 73.15, 73.36, 73.05, 72.61, 73.05, 73.94, 73.37, 72.36, 72.81, 73.25, 72.08, 72.63, 72.12, 73.73, 73.84, 74.42, 74.82, 74.25, 74.21, 75.41, 74.96, 72.97, 71.94, 71.32, 73.31, 71.41, 72.67, 72.34, 70.01, 65.59, 64.63, 64.26, 64.66, 65.77, 66.72, 66.17, 66.72, 68.34, 68.32, 67.27, 67.63, 67.61, 67.96, 67.08, 67.67, 67.17, 66.55, 66.43, 66.49, 67.3, 67.4, 66.98, 65.97, 65.88, 66.47, 66.6, 66.97, 66.8, 67.99, 66.7, 66.64, 67.26, 68.0, 67.01, 68.62, 67.52, 65.87, 66.84, 68.175, 66.35, 64.65, 63.42, 63.46, 63.52, 60.55, 59.84, 57.39, 60.4, 68.21, 68.62, 68.3635, 67.31, 66.16, 67.09, 66.17, 66.71, 65.8, 64.45, 63.2, 63.09, 62.92, 62.39, 61.39, 61.55, 61.54, 61.42, 65.44, 66.38, 68.44, 69.06, 68.49, 66.91, 67.7, 67.86, 65.02, 65.72, 65.53, 66.83, 65.51, 65.38, 66.08, 65.05, 65.22, 64.27, 64.85, 66.03, 64.31, 62.78, 61.94, 62.08, 61.21, 61.51, 62.73, 61.16, 62.11, 63.24, 62.65, 61.75, 61.49, 60.85, 61.11, 62.27, 61.41, 61.39, 60.71, 61.66, 62.08, 61.93, 61.84, 63.23, 62.92, 63.6, 64.91, 64.76, 63.79, 63.62, 63.98, 63.9, 64.73, 63.38, 62.38, 60.88, 59.79, 60.01, 66.09, 65.4, 63.94, 65.26, 65.93, 66.0, 66.92, 67.2, 68.14, 67.045, 67.27, 66.95, 65.69, 65.87, 65.02, 64.62, 63.83, 65.23, 64.44, 63.93, 63.83, 64.8, 63.91, 64.07, 63.79, 64.03, 62.96, 63.55, 63.585, 63.99, 63.41, 64.04, 63.71, 63.345, 63.19, 61.65, 61.43, 61.1, 61.11, 61.04, 61.81, 59.95, 59.85, 59.81, 58.78, 57.97, 58.45, 58.63, 57.87, 56.61, 56.61, 56.59, 56.31, 56.15, 56.28, 55.77, 55.17, 54.88, 54.1, 54.72, 54.54, 53.96, 54.6202, 54.6, 47.65, 47.93, 48.47, 48.81, 48.79, 49.01, 49.75, 49.73, 49.82, 49.45, 49.94, 48.86, 49.13, 48.99, 48.615, 47.75, 47.64, 47.01, 47.55, 47.29, 47.5, 47.61, 47.53, 47.6, 48.18, 47.83, 47.72, 48.55, 47.02, 45.88, 46.15, 45.93, 46.1203, 46.18, 44.67, 44.91, 44.42, 42.92, 42.59, 42.69, 42.17, 42.39, 42.63, 42.45, 42.8, 42.22, 42.07, 42.3, 42.45, 42.291, 41.79, 41.68, 41.79, 41.94, 41.96, 43.34, 44.02, 44.059, 45.02, 44.7099, 45.16, 45.77, 45.34, 37.24, 36.83, 36.469, 36.34, 36.81, 36.4408, 36.55, 36.23, 35.51, 34.93, 34.94, 35.26, 35.025, 34.72, 34.18, 34.52, 34.04, 33.45, 33.3, 33.26, 35.86, 36.04, 35.51, 36.41, 36.25, 36.25, 35.7, 35.97, 35.75, 35.67, 35.36, 34.87, 34.54, 34.56, 34.94, 34.6, 34.7, 34.55, 34.77, 35.41, 35.715, 35.091, 34.21, 34.63, 34.49, 33.05, 33.08, 33.18, 36.28, 36.75, 36.62, 36.74, 36.98, 36.89, 36.39, 35.45, 35.83, 35.93, 36.47, 36.75, 37.18, 38.23, 38.66, 38.2, 38.1, 37.5, 37.19, 37.9, 36.72, 37.32, 38.03, 38.04, 37.46, 36.64, 36.76, 37.59, 36.86, 36.91, 37.32, 37.57, 37.78, 37.77, 37.3, 37.555, 37.0801, 36.84, 36.81, 38.03, 38.12, 38.13, 38.295, 38.35, 37.8, 37.56, 37.22, 36.57, 36.74, 37.38, 37.82, 37.73, 37.61, 37.2, 36.92, 36.85, 36.46, 36.41, 36.03, 36.51, 36.41, 37.02, 36.89, 36.52, 37.59, 37.35, 37.3, 37.94, 38.3, 38.311, 38.21, 38.07, 38.09, 37.6, 38.39, 38.27, 37.865, 36.81, 41.28, 41.0299, 40.89, 41.34, 42.81, 41.79, 41.535, 42.06, 42.81, 43.4, 42.57, 42.45, 42.03, 43.21, 43.54, 42.39, 42.65, 42.87, 43.38, 42.05, 42.48, 42.11, 43.5, 43.06, 42.98, 42.85, 43.14, 42.92, 42.835, 42.74, 44.21, 44.27, 44.84, 45.21, 46.01, 47.29, 47.9, 47.88, 49.365, 50.61, 48.91, 48.37, 48.47, 46.21, 46.23, 46.49, 47.71, 48.03, 48.88, 48.595, 48.27, 48.14, 47.58, 48.57, 47.52, 46.9, 47.16, 49.67, 51.75, 50.15, 50.32, 50.3, 49.1, 48.24, 49.73, 50.37, 50.78, 50.13, 48.175, 48.57, 48.56, 48.84, 48.96, 49.6, 50.1, 47.52, 46.56, 46.67, 46.64, 46.78, 45.65, 46.41, 46.96, 47.52, 47.01, 47.36, 46.44, 46.8, 46.87, 48.25, 48.17, 47.99, 50.51, 50.11, 49.83, 49.11, 49.61, 47.85, 47.68, 47.49, 47.5, 48.32, 47.74, 49.16, 49.93, 48.75, 49.21, 49.62, 48.945, 49.26, 50.19, 49.59, 50.26, 50.34, 50.77, 50.12, 50.06, 49.31, 48.79, 49.91, 49.96, 48.785, 49.13, 48.71, 50.12, 50.27, 50.4, 49.67, 49.02, 50.32, 52.89, 51.71, 52.06, 52.35, 52.16, 51.69, 51.44, 51.34, 51.58, 51.21, 52.01, 51.64, 51.54, 51.71, 51.9, 51.1, 50.22, 49.06, 48.56, 49.16, 50.0805, 49.46, 48.41, 47.44, 46.18, 47.61, 50.79, 50.79, 50.66, 50.05, 49.96, 49.1, 49.88, 49.425, 49.67, 49.84, 50.73, 50.66, 50.845, 49.59, 48.45, 47.53, 45.52, 42.72, 41.79, 41.81, 41.8, 41.56, 41.52, 41.56, 40.7, 40.75, 41.63, 41.48, 41.915, 43.79, 44.005, 49.9, 50.68, 50.87, 50.18, 51.31, 52.43, 52.92, 51.65, 52.83, 55.12, 53.41, 51.84, 51.6, 51.65, 51.98, 51.98, 52.64, 52.28, 51.79, 53.14, 51.61, 51.38, 51.2, 53.74, 56.31, 55.9, 56.36, 56.98, 56.97, 56.88, 56.37, 56.27, 55.54, 55.93, 56.05, 56.91, 56.955, 56.79, 56.07, 57.29, 58.54, 58.49, 57.37, 58.12, 56.76, 57.4192, 57.13, 57.19, 57.84, 57.28, 56.64, 56.61, 56.46, 55.45, 54.78, 54.81, 52.73, 53.24, 52.4801, 50.745, 49.57, 49.13, 50.16, 50.25, 50.21, 51.85, 51.72, 51.73, 50.12, 40.36, 39.88, 38.72, 38.03, 37.79, 36.72, 38.4, 37.51, 36.5, 35.76, 35.57, 36.11, 36.15, 37.54, 37.115, 37.81, 38.97, 39.77, 40.71, 40.38, 40.06, 40.23, 41.03, 40.64, 40.86, 41.64, 40.83, 40.33, 39.72, 39.42, 40.94, 40.48, 40.16, 40.39, 42.22, 41.95, 41.99, 42.87, 42.34, 42.695, 43.03, 42.88, 43.0, 43.45, 42.89, 42.79, 42.23, 41.11, 40.88, 40.85, 39.31, 39.67, 38.99, 41.63, 41.3, 43.1, 42.83, 43.48, 43.48, 42.58, 39.32, 38.96, 38.65, 38.69, 38.54, 38.69, 38.74, 38.54, 39.46, 39.6, 39.91, 39.77, 40.27, 41.74, 41.74, 41.54, 41.9, 42.6, 43.49, 42.71, 43.37, 43.88, 43.515, 41.66, 42.24, 40.71, 40.565, 42.4, 42.72, 42.21, 43.05, 43.57, 43.18, 44.34, 45.1, 43.87, 43.16, 44.13, 43.68, 43.91, 44.5, 43.94, 44.91, 44.51, 42.8, 43.46, 43.1799, 43.29, 41.09, 39.39, 38.06, 40.08, 42.17, 43.18, 42.87, 43.525, 43.37, 43.62, 44.44, 45.38, 45.27, 43.85, 43.77, 39.47, 38.54, 38.71, 41.99, 42.11, 40.12, 39.42, 39.07, 39.33, 39.8, 40.12, 40.58, 39.91, 40.52, 41.25, 42.96, 43.475, 44.06, 42.48, 41.99, 41.69, 42.42, 41.83, 43.07, 43.12, 42.08, 43.36, 44.92, 45.36, 46.2, 46.8, 47.34, 46.61, 46.15, 46.41, 46.51, 47.08, 48.21, 47.84, 48.2, 48.58, 48.14, 48.89, 49.28, 48.573, 48.61, 47.75, 46.49, 45.72, 45.93, 60.62, 61.655, 61.34, 60.44, 60.71, 61.15, 61.02, 61.06, 61.1, 62.16, 62.83, 62.51, 62.43, 62.36, 62.57, 62.65, 62.55, 61.85, 62.5, 62.68, 63.015, 63.47, 63.32, 62.31, 62.28, 62.2, 63.26, 63.73, 63.7472, 62.94, 64.1448, 64.6, 65.25, 64.89, 64.38, 63.49, 63.38, 64.09, 65.75, 66.25, 66.97, 66.69, 68.02, 67.74, 67.9, 66.13, 66.81, 66.37, 64.81, 64.54, 64.71, 65.4225, 65.21, 64.7, 64.336, 65.96, 67.13, 67.14, 68.34, 67.95, 67.42, 67.88, 68.56, 68.35, 69.7, 68.98, 69.56, 69.78, 72.76, 73.27, 71.54, 71.02, 71.27, 71.39, 71.99, 69.78, 71.38, 72.22, 71.56, 70.8, 72.54, 70.02, 70.79, 69.8601, 69.36, 69.1, 68.51, 67.02, 67.02, 66.01, 66.49, 68.41, 69.36, 69.26, 70.0, 68.7, 66.86, 73.02, 74.79, 75.0999, 75.37, 74.95, 75.22, 75.3, 75.31, 74.36, 74.61, 74.3, 75.65, 75.88, 77.7, 77.02, 76.71, 75.58, 76.3, 76.16, 77.6636, 76.52, 75.73, 74.185, 73.3399, 76.71, 75.82, 76.8, 75.54, 73.97, 73.45, 72.45, 72.94, 72.32, 72.04, 70.75, 71.01, 68.89, 69.13, 69.942, 72.31, 72.39, 71.34, 77.9, 78.58, 77.91, 77.06, 78.16, 76.7, 75.72, 75.75, 75.07, 74.85, 73.14, 71.51, 72.06, 71.41, 72.15, 72.25, 73.67, 74.91, 77.08, 75.48, 75.67, 75.42, 74.08, 71.66, 71.4, 71.17, 71.49, 73.145, 74.6, 74.45, 75.09, 76.94, 77.22, 76.1, 76.38, 76.6, 76.81, 75.67, 75.63, 75.8, 76.57, 76.23, 79.97, 78.73, 80.56, 80.12, 81.0, 82.33, 82.94, 82.02, 82.62, 81.73, 82.85, 79.959, 79.34, 78.06, 78.36, 78.72, 77.43, 79.98, 79.26, 78.74, 79.7, 79.11, 76.99, 81.81, 81.47, 82.26, 83.66, 84.25, 81.12, 82.04, 81.71, 82.22, 82.22, 81.05, 81.87, 81.11, 79.4399, 85.72, 88.79, 89.18, 91.38, 89.91, 91.16, 90.27, 89.71, 89.92, 88.64, 90.0, 90.11, 90.13, 88.37, 88.52, 88.6, 89.33, 90.82, 91.06, 90.29, 93.89, 94.64, 95.37, 94.33, 93.89, 94.31, 93.33, 93.85, 94.05, 94.46, 94.38, 92.27, 97.01, 95.75, 96.39, 93.67, 93.3, 91.85, 93.03, 92.85, 91.26, 93.37, 93.71, 94.45, 91.82, 91.79, 91.37, 93.16, 94.73, 93.2, 92.4, 91.2, 90.3, 88.16, 89.92, 91.14, 91.43, 93.22, 90.67, 89.94, 90.12, 89.36, 87.12, 86.89, 89.65, 90.52, 89.0, 86.58, 89.54, 92.62, 95.03, 95.25, 93.27, 92.74, 91.55, 92.05, 93.45, 94.1, 97.47, 97.67, 98.7, 96.77, 98.53, 98.12, 97.02, 97.99, 98.07, 98.26, 98.275, 98.61, 98.99, 98.87, 97.96, 98.02, 99.57, 99.33, 99.84, 98.58, 97.57, 97.99, 97.79, 98.82, 98.44, 98.37, 96.0, 96.22, 95.56, 94.21, 91.5, 91.34, 89.92, 76.79, 79.94, 80.38, 78.73, 81.87, 81.08, 80.07, 80.29, 79.74, 77.18, 77.03, 75.69, 77.39, 77.04, 76.66, 79.81, 78.6, 77.91, 78.91, 82.05, 82.52, 82.48, 81.2, 80.62, 80.58, 81.03, 81.4, 80.8, 83.68, 83.92, 84.19, 82.92, 82.21, 82.27, 82.67, 80.59, 81.81, 80.61, 79.67, 80.77, 80.74, 80.4, 81.72, 81.35, 81.84, 80.6, 80.1, 80.63, 80.43, 79.08, 79.92, 80.95, 82.72, 81.86, 82.17, 82.0, 81.08, 79.4, 77.91, 79.47, 79.13, 74.78, 75.44, 76.93, 77.97, 77.41, 76.39, 76.21, 76.92, 77.59]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d32df22c-eec2-4cf7-980a-8245a7878a89');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"297c4966-74fb-45d8-9b82-f85b5705cea6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"297c4966-74fb-45d8-9b82-f85b5705cea6\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '297c4966-74fb-45d8-9b82-f85b5705cea6',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('297c4966-74fb-45d8-9b82-f85b5705cea6');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8bXzt1tHUKs"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaRzK0XhHUKs",
        "outputId": "624c6af6-e1f7-4a2e-f940-e7ed9a8616c7"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.05, .1, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2100)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"CPRI\", step_sizes=4, th= th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6860 - accuracy: 0.5685 - val_loss: 0.7166 - val_accuracy: 0.3621\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6553 - accuracy: 0.5953 - val_loss: 0.6748 - val_accuracy: 0.4724\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6304 - accuracy: 0.6564 - val_loss: 0.6339 - val_accuracy: 0.7207\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6199 - accuracy: 0.6651 - val_loss: 0.5717 - val_accuracy: 0.7552\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6036 - accuracy: 0.6738 - val_loss: 0.7500 - val_accuracy: 0.5138\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6854 - accuracy: 0.5671 - val_loss: 0.7362 - val_accuracy: 0.3621\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.6409 - accuracy: 0.6074 - val_loss: 0.6334 - val_accuracy: 0.6552\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5675 - accuracy: 0.7054 - val_loss: 0.5721 - val_accuracy: 0.7034\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5699 - accuracy: 0.7054 - val_loss: 0.5691 - val_accuracy: 0.7103\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5720 - accuracy: 0.6966 - val_loss: 0.5669 - val_accuracy: 0.7207\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.794131\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.809292\n",
            "[2]\tvalidation_0-auc:0.807207\n",
            "[3]\tvalidation_0-auc:0.813024\n",
            "[4]\tvalidation_0-auc:0.809575\n",
            "[5]\tvalidation_0-auc:0.809833\n",
            "[6]\tvalidation_0-auc:0.807362\n",
            "[7]\tvalidation_0-auc:0.80749\n",
            "[8]\tvalidation_0-auc:0.809447\n",
            "[9]\tvalidation_0-auc:0.810167\n",
            "[10]\tvalidation_0-auc:0.808391\n",
            "[11]\tvalidation_0-auc:0.80888\n",
            "[12]\tvalidation_0-auc:0.816628\n",
            "[13]\tvalidation_0-auc:0.818044\n",
            "[14]\tvalidation_0-auc:0.819485\n",
            "[15]\tvalidation_0-auc:0.818867\n",
            "[16]\tvalidation_0-auc:0.817864\n",
            "[17]\tvalidation_0-auc:0.817555\n",
            "[18]\tvalidation_0-auc:0.818867\n",
            "[19]\tvalidation_0-auc:0.820952\n",
            "[20]\tvalidation_0-auc:0.821184\n",
            "[21]\tvalidation_0-auc:0.822523\n",
            "[22]\tvalidation_0-auc:0.823089\n",
            "[23]\tvalidation_0-auc:0.825174\n",
            "[24]\tvalidation_0-auc:0.824865\n",
            "[25]\tvalidation_0-auc:0.824891\n",
            "[26]\tvalidation_0-auc:0.824221\n",
            "[27]\tvalidation_0-auc:0.824427\n",
            "[28]\tvalidation_0-auc:0.824041\n",
            "[29]\tvalidation_0-auc:0.824685\n",
            "[30]\tvalidation_0-auc:0.823758\n",
            "[31]\tvalidation_0-auc:0.823964\n",
            "[32]\tvalidation_0-auc:0.824067\n",
            "[33]\tvalidation_0-auc:0.822728\n",
            "[34]\tvalidation_0-auc:0.822728\n",
            "[35]\tvalidation_0-auc:0.823295\n",
            "[36]\tvalidation_0-auc:0.821828\n",
            "[37]\tvalidation_0-auc:0.820489\n",
            "[38]\tvalidation_0-auc:0.82018\n",
            "[39]\tvalidation_0-auc:0.818147\n",
            "[40]\tvalidation_0-auc:0.818095\n",
            "[41]\tvalidation_0-auc:0.81686\n",
            "[42]\tvalidation_0-auc:0.815624\n",
            "[43]\tvalidation_0-auc:0.813205\n",
            "[44]\tvalidation_0-auc:0.81305\n",
            "[45]\tvalidation_0-auc:0.813462\n",
            "[46]\tvalidation_0-auc:0.812741\n",
            "[47]\tvalidation_0-auc:0.811532\n",
            "[48]\tvalidation_0-auc:0.810862\n",
            "[49]\tvalidation_0-auc:0.810811\n",
            "[50]\tvalidation_0-auc:0.810347\n",
            "[51]\tvalidation_0-auc:0.810656\n",
            "[52]\tvalidation_0-auc:0.810553\n",
            "[53]\tvalidation_0-auc:0.809575\n",
            "[54]\tvalidation_0-auc:0.80834\n",
            "[55]\tvalidation_0-auc:0.807156\n",
            "[56]\tvalidation_0-auc:0.806795\n",
            "[57]\tvalidation_0-auc:0.805714\n",
            "[58]\tvalidation_0-auc:0.806075\n",
            "[59]\tvalidation_0-auc:0.806435\n",
            "[60]\tvalidation_0-auc:0.805457\n",
            "[61]\tvalidation_0-auc:0.80556\n",
            "[62]\tvalidation_0-auc:0.805869\n",
            "[63]\tvalidation_0-auc:0.805457\n",
            "[64]\tvalidation_0-auc:0.805714\n",
            "[65]\tvalidation_0-auc:0.80556\n",
            "[66]\tvalidation_0-auc:0.80556\n",
            "[67]\tvalidation_0-auc:0.804788\n",
            "[68]\tvalidation_0-auc:0.804427\n",
            "[69]\tvalidation_0-auc:0.804427\n",
            "[70]\tvalidation_0-auc:0.804118\n",
            "[71]\tvalidation_0-auc:0.804118\n",
            "[72]\tvalidation_0-auc:0.803398\n",
            "[73]\tvalidation_0-auc:0.803192\n",
            "Stopping. Best iteration:\n",
            "[23]\tvalidation_0-auc:0.825174\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6834 - accuracy: 0.5697 - val_loss: 0.7748 - val_accuracy: 0.2802\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6300 - accuracy: 0.6493 - val_loss: 0.7130 - val_accuracy: 0.4942\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6045 - accuracy: 0.6548 - val_loss: 0.6968 - val_accuracy: 0.4514\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5864 - accuracy: 0.6760 - val_loss: 0.5824 - val_accuracy: 0.7588\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5598 - accuracy: 0.7076 - val_loss: 0.5579 - val_accuracy: 0.7626\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6503 - accuracy: 0.6253 - val_loss: 0.5954 - val_accuracy: 0.7782\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5680 - accuracy: 0.6939 - val_loss: 0.5850 - val_accuracy: 0.6848\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5608 - accuracy: 0.7111 - val_loss: 0.5401 - val_accuracy: 0.7860\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5564 - accuracy: 0.7241 - val_loss: 0.6232 - val_accuracy: 0.6226\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5297 - accuracy: 0.7220 - val_loss: 0.5007 - val_accuracy: 0.7938\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.632658\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.720571\n",
            "[2]\tvalidation_0-auc:0.718018\n",
            "[3]\tvalidation_0-auc:0.702665\n",
            "[4]\tvalidation_0-auc:0.699812\n",
            "[5]\tvalidation_0-auc:0.697222\n",
            "[6]\tvalidation_0-auc:0.666141\n",
            "[7]\tvalidation_0-auc:0.639189\n",
            "[8]\tvalidation_0-auc:0.63964\n",
            "[9]\tvalidation_0-auc:0.658108\n",
            "[10]\tvalidation_0-auc:0.652628\n",
            "[11]\tvalidation_0-auc:0.655368\n",
            "[12]\tvalidation_0-auc:0.659647\n",
            "[13]\tvalidation_0-auc:0.653829\n",
            "[14]\tvalidation_0-auc:0.65488\n",
            "[15]\tvalidation_0-auc:0.643619\n",
            "[16]\tvalidation_0-auc:0.644707\n",
            "[17]\tvalidation_0-auc:0.636562\n",
            "[18]\tvalidation_0-auc:0.636974\n",
            "[19]\tvalidation_0-auc:0.638063\n",
            "[20]\tvalidation_0-auc:0.634272\n",
            "[21]\tvalidation_0-auc:0.633596\n",
            "[22]\tvalidation_0-auc:0.631532\n",
            "[23]\tvalidation_0-auc:0.631757\n",
            "[24]\tvalidation_0-auc:0.635135\n",
            "[25]\tvalidation_0-auc:0.635098\n",
            "[26]\tvalidation_0-auc:0.629317\n",
            "[27]\tvalidation_0-auc:0.63247\n",
            "[28]\tvalidation_0-auc:0.632132\n",
            "[29]\tvalidation_0-auc:0.636824\n",
            "[30]\tvalidation_0-auc:0.632695\n",
            "[31]\tvalidation_0-auc:0.635473\n",
            "[32]\tvalidation_0-auc:0.628116\n",
            "[33]\tvalidation_0-auc:0.63262\n",
            "[34]\tvalidation_0-auc:0.639152\n",
            "[35]\tvalidation_0-auc:0.637425\n",
            "[36]\tvalidation_0-auc:0.634722\n",
            "[37]\tvalidation_0-auc:0.636824\n",
            "[38]\tvalidation_0-auc:0.633408\n",
            "[39]\tvalidation_0-auc:0.638026\n",
            "[40]\tvalidation_0-auc:0.638476\n",
            "[41]\tvalidation_0-auc:0.633146\n",
            "[42]\tvalidation_0-auc:0.633596\n",
            "[43]\tvalidation_0-auc:0.636899\n",
            "[44]\tvalidation_0-auc:0.638551\n",
            "[45]\tvalidation_0-auc:0.638701\n",
            "[46]\tvalidation_0-auc:0.638701\n",
            "[47]\tvalidation_0-auc:0.639827\n",
            "[48]\tvalidation_0-auc:0.64039\n",
            "[49]\tvalidation_0-auc:0.641742\n",
            "[50]\tvalidation_0-auc:0.641216\n",
            "[51]\tvalidation_0-auc:0.641216\n",
            "Stopping. Best iteration:\n",
            "[1]\tvalidation_0-auc:0.720571\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.5137931034482759 |  0.4230769230769231 | 0.9428571428571428 |  0.5840707964601769 |\n",
            "|     GRU 0.1      | 0.7206896551724138 |  0.5769230769230769 | 0.8571428571428571 |  0.689655172413793  |\n",
            "|   XGBoost 0.1    | 0.7103448275862069 |  0.567741935483871  | 0.8380952380952381 |  0.676923076923077  |\n",
            "|    Logreg 0.1    | 0.6551724137931034 |  0.5135135135135135 | 0.9047619047619048 |  0.6551724137931034 |\n",
            "|     SVM 0.1      | 0.696551724137931  |  0.5502958579881657 | 0.8857142857142857 |  0.6788321167883212 |\n",
            "|  LSTM beta 0.1   | 0.7626459143968871 |  0.5901639344262295 |        0.5         |  0.5413533834586466 |\n",
            "|   GRU beta 0.1   | 0.7937743190661478 |  0.6610169491525424 | 0.5416666666666666 |  0.5954198473282444 |\n",
            "| XGBoost beta 0.1 | 0.603112840466926  | 0.36607142857142855 | 0.5694444444444444 | 0.44565217391304346 |\n",
            "| logreg beta 0.1  | 0.6731517509727627 | 0.45161290322580644 | 0.7777777777777778 |  0.5714285714285714 |\n",
            "|   svm beta 0.1   | 0.6809338521400778 | 0.43902439024390244 |        0.5         |  0.4675324675324676 |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6657 - accuracy: 0.6275 - val_loss: 0.8009 - val_accuracy: 0.3655\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6483 - accuracy: 0.6416 - val_loss: 0.9306 - val_accuracy: 0.3655\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6291 - accuracy: 0.6477 - val_loss: 0.7786 - val_accuracy: 0.4414\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6036 - accuracy: 0.6698 - val_loss: 0.6833 - val_accuracy: 0.6034\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6045 - accuracy: 0.6685 - val_loss: 0.7364 - val_accuracy: 0.5448\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6503 - accuracy: 0.6403 - val_loss: 0.8259 - val_accuracy: 0.3759\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.6087 - accuracy: 0.6691 - val_loss: 0.6781 - val_accuracy: 0.5690\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5907 - accuracy: 0.6779 - val_loss: 0.6796 - val_accuracy: 0.5897\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5948 - accuracy: 0.6886 - val_loss: 0.6942 - val_accuracy: 0.5655\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5887 - accuracy: 0.7020 - val_loss: 0.6734 - val_accuracy: 0.6000\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.716238\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.70888\n",
            "[2]\tvalidation_0-auc:0.693832\n",
            "[3]\tvalidation_0-auc:0.688577\n",
            "[4]\tvalidation_0-auc:0.697626\n",
            "[5]\tvalidation_0-auc:0.695703\n",
            "[6]\tvalidation_0-auc:0.693781\n",
            "[7]\tvalidation_0-auc:0.693576\n",
            "[8]\tvalidation_0-auc:0.696011\n",
            "[9]\tvalidation_0-auc:0.689525\n",
            "[10]\tvalidation_0-auc:0.68668\n",
            "[11]\tvalidation_0-auc:0.686782\n",
            "[12]\tvalidation_0-auc:0.685449\n",
            "[13]\tvalidation_0-auc:0.687987\n",
            "[14]\tvalidation_0-auc:0.687782\n",
            "[15]\tvalidation_0-auc:0.687961\n",
            "[16]\tvalidation_0-auc:0.686526\n",
            "[17]\tvalidation_0-auc:0.683962\n",
            "[18]\tvalidation_0-auc:0.681911\n",
            "[19]\tvalidation_0-auc:0.683296\n",
            "[20]\tvalidation_0-auc:0.682219\n",
            "[21]\tvalidation_0-auc:0.679732\n",
            "[22]\tvalidation_0-auc:0.68045\n",
            "[23]\tvalidation_0-auc:0.67963\n",
            "[24]\tvalidation_0-auc:0.679579\n",
            "[25]\tvalidation_0-auc:0.679373\n",
            "[26]\tvalidation_0-auc:0.678707\n",
            "[27]\tvalidation_0-auc:0.678245\n",
            "[28]\tvalidation_0-auc:0.674964\n",
            "[29]\tvalidation_0-auc:0.67481\n",
            "[30]\tvalidation_0-auc:0.672785\n",
            "[31]\tvalidation_0-auc:0.672067\n",
            "[32]\tvalidation_0-auc:0.672324\n",
            "[33]\tvalidation_0-auc:0.671401\n",
            "[34]\tvalidation_0-auc:0.670273\n",
            "[35]\tvalidation_0-auc:0.669401\n",
            "[36]\tvalidation_0-auc:0.669093\n",
            "[37]\tvalidation_0-auc:0.670145\n",
            "[38]\tvalidation_0-auc:0.667581\n",
            "[39]\tvalidation_0-auc:0.667325\n",
            "[40]\tvalidation_0-auc:0.665171\n",
            "[41]\tvalidation_0-auc:0.664761\n",
            "[42]\tvalidation_0-auc:0.660762\n",
            "[43]\tvalidation_0-auc:0.65907\n",
            "[44]\tvalidation_0-auc:0.659326\n",
            "[45]\tvalidation_0-auc:0.659736\n",
            "[46]\tvalidation_0-auc:0.659942\n",
            "[47]\tvalidation_0-auc:0.658045\n",
            "[48]\tvalidation_0-auc:0.657019\n",
            "[49]\tvalidation_0-auc:0.656045\n",
            "[50]\tvalidation_0-auc:0.655789\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.716238\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6402 - accuracy: 0.6424 - val_loss: 0.6713 - val_accuracy: 0.5837\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5624 - accuracy: 0.7172 - val_loss: 0.7053 - val_accuracy: 0.6109\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5628 - accuracy: 0.7083 - val_loss: 0.6195 - val_accuracy: 0.7082\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5431 - accuracy: 0.7090 - val_loss: 0.5703 - val_accuracy: 0.7471\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5146 - accuracy: 0.7474 - val_loss: 0.4958 - val_accuracy: 0.7938\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 2s 13ms/step - loss: 0.5943 - accuracy: 0.6644 - val_loss: 0.5291 - val_accuracy: 0.8366\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5220 - accuracy: 0.7268 - val_loss: 0.6562 - val_accuracy: 0.5603\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4908 - accuracy: 0.7584 - val_loss: 0.5112 - val_accuracy: 0.7510\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.4834 - accuracy: 0.7577 - val_loss: 0.5606 - val_accuracy: 0.6848\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4879 - accuracy: 0.7598 - val_loss: 0.4917 - val_accuracy: 0.7510\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.86588\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.896404\n",
            "[2]\tvalidation_0-auc:0.895399\n",
            "[3]\tvalidation_0-auc:0.902062\n",
            "[4]\tvalidation_0-auc:0.903551\n",
            "[5]\tvalidation_0-auc:0.902509\n",
            "[6]\tvalidation_0-auc:0.903923\n",
            "[7]\tvalidation_0-auc:0.904854\n",
            "[8]\tvalidation_0-auc:0.904891\n",
            "[9]\tvalidation_0-auc:0.906083\n",
            "[10]\tvalidation_0-auc:0.905896\n",
            "[11]\tvalidation_0-auc:0.906715\n",
            "[12]\tvalidation_0-auc:0.908502\n",
            "[13]\tvalidation_0-auc:0.909544\n",
            "[14]\tvalidation_0-auc:0.909917\n",
            "[15]\tvalidation_0-auc:0.910661\n",
            "[16]\tvalidation_0-auc:0.910661\n",
            "[17]\tvalidation_0-auc:0.910289\n",
            "[18]\tvalidation_0-auc:0.91014\n",
            "[19]\tvalidation_0-auc:0.912001\n",
            "[20]\tvalidation_0-auc:0.912746\n",
            "[21]\tvalidation_0-auc:0.912671\n",
            "[22]\tvalidation_0-auc:0.912299\n",
            "[23]\tvalidation_0-auc:0.912895\n",
            "[24]\tvalidation_0-auc:0.912597\n",
            "[25]\tvalidation_0-auc:0.912076\n",
            "[26]\tvalidation_0-auc:0.911741\n",
            "[27]\tvalidation_0-auc:0.913118\n",
            "[28]\tvalidation_0-auc:0.912895\n",
            "[29]\tvalidation_0-auc:0.91282\n",
            "[30]\tvalidation_0-auc:0.912522\n",
            "[31]\tvalidation_0-auc:0.913118\n",
            "[32]\tvalidation_0-auc:0.913714\n",
            "[33]\tvalidation_0-auc:0.913267\n",
            "[34]\tvalidation_0-auc:0.913416\n",
            "[35]\tvalidation_0-auc:0.912225\n",
            "[36]\tvalidation_0-auc:0.911406\n",
            "[37]\tvalidation_0-auc:0.910363\n",
            "[38]\tvalidation_0-auc:0.910512\n",
            "[39]\tvalidation_0-auc:0.909991\n",
            "[40]\tvalidation_0-auc:0.910587\n",
            "[41]\tvalidation_0-auc:0.910363\n",
            "[42]\tvalidation_0-auc:0.910363\n",
            "[43]\tvalidation_0-auc:0.909321\n",
            "[44]\tvalidation_0-auc:0.908465\n",
            "[45]\tvalidation_0-auc:0.908539\n",
            "[46]\tvalidation_0-auc:0.908986\n",
            "[47]\tvalidation_0-auc:0.908725\n",
            "[48]\tvalidation_0-auc:0.909917\n",
            "[49]\tvalidation_0-auc:0.910587\n",
            "[50]\tvalidation_0-auc:0.910587\n",
            "[51]\tvalidation_0-auc:0.910289\n",
            "[52]\tvalidation_0-auc:0.910363\n",
            "[53]\tvalidation_0-auc:0.910214\n",
            "[54]\tvalidation_0-auc:0.909842\n",
            "[55]\tvalidation_0-auc:0.909917\n",
            "[56]\tvalidation_0-auc:0.909768\n",
            "[57]\tvalidation_0-auc:0.911033\n",
            "[58]\tvalidation_0-auc:0.911927\n",
            "[59]\tvalidation_0-auc:0.912225\n",
            "[60]\tvalidation_0-auc:0.911406\n",
            "[61]\tvalidation_0-auc:0.912001\n",
            "[62]\tvalidation_0-auc:0.91215\n",
            "[63]\tvalidation_0-auc:0.912225\n",
            "[64]\tvalidation_0-auc:0.911555\n",
            "[65]\tvalidation_0-auc:0.911182\n",
            "[66]\tvalidation_0-auc:0.91215\n",
            "[67]\tvalidation_0-auc:0.912969\n",
            "[68]\tvalidation_0-auc:0.912597\n",
            "[69]\tvalidation_0-auc:0.912597\n",
            "[70]\tvalidation_0-auc:0.913043\n",
            "[71]\tvalidation_0-auc:0.912225\n",
            "[72]\tvalidation_0-auc:0.912373\n",
            "[73]\tvalidation_0-auc:0.911331\n",
            "[74]\tvalidation_0-auc:0.91148\n",
            "[75]\tvalidation_0-auc:0.909917\n",
            "[76]\tvalidation_0-auc:0.90947\n",
            "[77]\tvalidation_0-auc:0.909247\n",
            "[78]\tvalidation_0-auc:0.909247\n",
            "[79]\tvalidation_0-auc:0.909247\n",
            "[80]\tvalidation_0-auc:0.909917\n",
            "[81]\tvalidation_0-auc:0.909768\n",
            "[82]\tvalidation_0-auc:0.909619\n",
            "Stopping. Best iteration:\n",
            "[32]\tvalidation_0-auc:0.913714\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|     LSTM 0.2     | 0.5448275862068965 | 0.44537815126050423 |        1.0         | 0.616279069767442  |\n",
            "|     GRU 0.2      |        0.6         | 0.47474747474747475 | 0.8867924528301887 | 0.618421052631579  |\n",
            "|   XGBoost 0.2    | 0.5620689655172414 |  0.4461538461538462 | 0.8207547169811321 | 0.5780730897009967 |\n",
            "|    Logreg 0.2    |        0.5         | 0.41975308641975306 | 0.9622641509433962 | 0.5845272206303724 |\n",
            "|     SVM 0.2      | 0.5689655172413793 | 0.45701357466063347 | 0.9528301886792453 | 0.617737003058104  |\n",
            "|  LSTM beta 0.2   | 0.7937743190661478 |        0.625        | 0.684931506849315  |  0.65359477124183  |\n",
            "|   GRU beta 0.2   | 0.7509727626459144 |  0.5436893203883495 | 0.7671232876712328 | 0.6363636363636364 |\n",
            "| XGBoost beta 0.2 | 0.8093385214007782 |         0.62        | 0.8493150684931506 | 0.7167630057803469 |\n",
            "| logreg beta 0.2  | 0.6770428015564203 |  0.4652777777777778 | 0.9178082191780822 | 0.6175115207373272 |\n",
            "|   svm beta 0.2   | 0.7937743190661478 |  0.5961538461538461 | 0.8493150684931506 | 0.7005649717514123 |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6718 - accuracy: 0.6154 - val_loss: 0.7778 - val_accuracy: 0.3655\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6453 - accuracy: 0.6295 - val_loss: 0.8345 - val_accuracy: 0.4172\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6312 - accuracy: 0.6322 - val_loss: 0.6975 - val_accuracy: 0.5138\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6260 - accuracy: 0.6443 - val_loss: 0.7299 - val_accuracy: 0.5517\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6209 - accuracy: 0.6550 - val_loss: 0.7153 - val_accuracy: 0.5793\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6589 - accuracy: 0.6215 - val_loss: 0.7534 - val_accuracy: 0.3793\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.6173 - accuracy: 0.6463 - val_loss: 0.7204 - val_accuracy: 0.4759\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5935 - accuracy: 0.6772 - val_loss: 0.7112 - val_accuracy: 0.5172\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5904 - accuracy: 0.6846 - val_loss: 0.6913 - val_accuracy: 0.5483\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5870 - accuracy: 0.6819 - val_loss: 0.6755 - val_accuracy: 0.5897\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.665889\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.679502\n",
            "[2]\tvalidation_0-auc:0.685859\n",
            "[3]\tvalidation_0-auc:0.687885\n",
            "[4]\tvalidation_0-auc:0.686039\n",
            "[5]\tvalidation_0-auc:0.682091\n",
            "[6]\tvalidation_0-auc:0.681732\n",
            "[7]\tvalidation_0-auc:0.681732\n",
            "[8]\tvalidation_0-auc:0.682886\n",
            "[9]\tvalidation_0-auc:0.68186\n",
            "[10]\tvalidation_0-auc:0.671965\n",
            "[11]\tvalidation_0-auc:0.668478\n",
            "[12]\tvalidation_0-auc:0.671785\n",
            "[13]\tvalidation_0-auc:0.67199\n",
            "[14]\tvalidation_0-auc:0.66917\n",
            "[15]\tvalidation_0-auc:0.670914\n",
            "[16]\tvalidation_0-auc:0.666094\n",
            "[17]\tvalidation_0-auc:0.66653\n",
            "[18]\tvalidation_0-auc:0.668478\n",
            "[19]\tvalidation_0-auc:0.670016\n",
            "[20]\tvalidation_0-auc:0.674092\n",
            "[21]\tvalidation_0-auc:0.672375\n",
            "[22]\tvalidation_0-auc:0.672477\n",
            "[23]\tvalidation_0-auc:0.671067\n",
            "[24]\tvalidation_0-auc:0.669581\n",
            "[25]\tvalidation_0-auc:0.670555\n",
            "[26]\tvalidation_0-auc:0.666325\n",
            "[27]\tvalidation_0-auc:0.665094\n",
            "[28]\tvalidation_0-auc:0.667196\n",
            "[29]\tvalidation_0-auc:0.66571\n",
            "[30]\tvalidation_0-auc:0.667812\n",
            "[31]\tvalidation_0-auc:0.667453\n",
            "[32]\tvalidation_0-auc:0.668837\n",
            "[33]\tvalidation_0-auc:0.669145\n",
            "[34]\tvalidation_0-auc:0.669196\n",
            "[35]\tvalidation_0-auc:0.67058\n",
            "[36]\tvalidation_0-auc:0.668042\n",
            "[37]\tvalidation_0-auc:0.666299\n",
            "[38]\tvalidation_0-auc:0.665376\n",
            "[39]\tvalidation_0-auc:0.664351\n",
            "[40]\tvalidation_0-auc:0.663428\n",
            "[41]\tvalidation_0-auc:0.662146\n",
            "[42]\tvalidation_0-auc:0.662044\n",
            "[43]\tvalidation_0-auc:0.660326\n",
            "[44]\tvalidation_0-auc:0.660326\n",
            "[45]\tvalidation_0-auc:0.659762\n",
            "[46]\tvalidation_0-auc:0.659249\n",
            "[47]\tvalidation_0-auc:0.65848\n",
            "[48]\tvalidation_0-auc:0.658224\n",
            "[49]\tvalidation_0-auc:0.658224\n",
            "[50]\tvalidation_0-auc:0.658429\n",
            "[51]\tvalidation_0-auc:0.658121\n",
            "[52]\tvalidation_0-auc:0.65725\n",
            "[53]\tvalidation_0-auc:0.656686\n",
            "Stopping. Best iteration:\n",
            "[3]\tvalidation_0-auc:0.687885\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6659 - accuracy: 0.6115 - val_loss: 0.7240 - val_accuracy: 0.3268\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6033 - accuracy: 0.6740 - val_loss: 0.7639 - val_accuracy: 0.3113\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5732 - accuracy: 0.6932 - val_loss: 0.5717 - val_accuracy: 0.7588\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5618 - accuracy: 0.7028 - val_loss: 0.6250 - val_accuracy: 0.6809\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5338 - accuracy: 0.7330 - val_loss: 0.5306 - val_accuracy: 0.7393\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6132 - accuracy: 0.6582 - val_loss: 0.6571 - val_accuracy: 0.5486\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5426 - accuracy: 0.7227 - val_loss: 0.5649 - val_accuracy: 0.7237\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5096 - accuracy: 0.7358 - val_loss: 0.6291 - val_accuracy: 0.6109\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5083 - accuracy: 0.7426 - val_loss: 0.5328 - val_accuracy: 0.7549\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5005 - accuracy: 0.7591 - val_loss: 0.4720 - val_accuracy: 0.8210\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.860706\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.85985\n",
            "[2]\tvalidation_0-auc:0.886689\n",
            "[3]\tvalidation_0-auc:0.894468\n",
            "[4]\tvalidation_0-auc:0.896069\n",
            "[5]\tvalidation_0-auc:0.898786\n",
            "[6]\tvalidation_0-auc:0.900238\n",
            "[7]\tvalidation_0-auc:0.89994\n",
            "[8]\tvalidation_0-auc:0.900313\n",
            "[9]\tvalidation_0-auc:0.898563\n",
            "[10]\tvalidation_0-auc:0.902918\n",
            "[11]\tvalidation_0-auc:0.904147\n",
            "[12]\tvalidation_0-auc:0.905487\n",
            "[13]\tvalidation_0-auc:0.90679\n",
            "[14]\tvalidation_0-auc:0.908986\n",
            "[15]\tvalidation_0-auc:0.90973\n",
            "[16]\tvalidation_0-auc:0.910624\n",
            "[17]\tvalidation_0-auc:0.909507\n",
            "[18]\tvalidation_0-auc:0.909433\n",
            "[19]\tvalidation_0-auc:0.909656\n",
            "[20]\tvalidation_0-auc:0.910326\n",
            "[21]\tvalidation_0-auc:0.908614\n",
            "[22]\tvalidation_0-auc:0.909098\n",
            "[23]\tvalidation_0-auc:0.909023\n",
            "[24]\tvalidation_0-auc:0.908725\n",
            "[25]\tvalidation_0-auc:0.908018\n",
            "[26]\tvalidation_0-auc:0.908242\n",
            "[27]\tvalidation_0-auc:0.908018\n",
            "[28]\tvalidation_0-auc:0.907199\n",
            "[29]\tvalidation_0-auc:0.906231\n",
            "[30]\tvalidation_0-auc:0.905934\n",
            "[31]\tvalidation_0-auc:0.904817\n",
            "[32]\tvalidation_0-auc:0.90504\n",
            "[33]\tvalidation_0-auc:0.90504\n",
            "[34]\tvalidation_0-auc:0.904966\n",
            "[35]\tvalidation_0-auc:0.903477\n",
            "[36]\tvalidation_0-auc:0.901951\n",
            "[37]\tvalidation_0-auc:0.901951\n",
            "[38]\tvalidation_0-auc:0.901802\n",
            "[39]\tvalidation_0-auc:0.901951\n",
            "[40]\tvalidation_0-auc:0.902621\n",
            "[41]\tvalidation_0-auc:0.901206\n",
            "[42]\tvalidation_0-auc:0.900908\n",
            "[43]\tvalidation_0-auc:0.900089\n",
            "[44]\tvalidation_0-auc:0.89901\n",
            "[45]\tvalidation_0-auc:0.89901\n",
            "[46]\tvalidation_0-auc:0.898712\n",
            "[47]\tvalidation_0-auc:0.897521\n",
            "[48]\tvalidation_0-auc:0.897484\n",
            "[49]\tvalidation_0-auc:0.897856\n",
            "[50]\tvalidation_0-auc:0.898005\n",
            "[51]\tvalidation_0-auc:0.897298\n",
            "[52]\tvalidation_0-auc:0.897298\n",
            "[53]\tvalidation_0-auc:0.897595\n",
            "[54]\tvalidation_0-auc:0.897223\n",
            "[55]\tvalidation_0-auc:0.899196\n",
            "[56]\tvalidation_0-auc:0.897781\n",
            "[57]\tvalidation_0-auc:0.896888\n",
            "[58]\tvalidation_0-auc:0.896888\n",
            "[59]\tvalidation_0-auc:0.897484\n",
            "[60]\tvalidation_0-auc:0.898005\n",
            "[61]\tvalidation_0-auc:0.899457\n",
            "[62]\tvalidation_0-auc:0.899084\n",
            "[63]\tvalidation_0-auc:0.897074\n",
            "[64]\tvalidation_0-auc:0.896553\n",
            "[65]\tvalidation_0-auc:0.896181\n",
            "[66]\tvalidation_0-auc:0.89566\n",
            "Stopping. Best iteration:\n",
            "[16]\tvalidation_0-auc:0.910624\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.5793103448275863 |  0.4603960396039604 | 0.8773584905660378 | 0.6038961038961039 |\n",
            "|      GRU 0.15     | 0.5896551724137931 | 0.46766169154228854 | 0.8867924528301887 | 0.6123778501628665 |\n",
            "|    XGBoost 0.15   | 0.5413793103448276 | 0.42780748663101603 | 0.7547169811320755 | 0.5460750853242321 |\n",
            "|    Logreg 0.15    | 0.5241379310344828 | 0.43103448275862066 | 0.9433962264150944 | 0.5917159763313609 |\n",
            "|      SVM 0.15     | 0.5620689655172414 |  0.4507042253521127 | 0.9056603773584906 | 0.6018808777429466 |\n",
            "|   LSTM beta 0.15  | 0.7392996108949417 |  0.5272727272727272 | 0.7945205479452054 | 0.6338797814207651 |\n",
            "|   GRU beta 0.15   | 0.8210116731517509 |  0.7288135593220338 | 0.589041095890411  | 0.6515151515151515 |\n",
            "| XGBoost beta 0.15 | 0.7898832684824902 |  0.5887850467289719 | 0.863013698630137  |        0.7         |\n",
            "|  logreg beta 0.15 | 0.6731517509727627 | 0.46153846153846156 | 0.9041095890410958 | 0.6111111111111112 |\n",
            "|   svm beta 0.15   | 0.8132295719844358 |  0.6288659793814433 | 0.8356164383561644 | 0.7176470588235294 |\n",
            "+-------------------+--------------------+---------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "25tTnnbIHUKt",
        "outputId": "fea73f70-8c58-4191-d7b1-520ce56d9628"
      },
      "source": [
        "Result_cross.to_csv('CPRI_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.423077</td>\n",
              "      <td>0.513793</td>\n",
              "      <td>0.584071</td>\n",
              "      <td>0.942857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.720690</td>\n",
              "      <td>0.689655</td>\n",
              "      <td>0.857143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.567742</td>\n",
              "      <td>0.710345</td>\n",
              "      <td>0.676923</td>\n",
              "      <td>0.838095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.513514</td>\n",
              "      <td>0.655172</td>\n",
              "      <td>0.655172</td>\n",
              "      <td>0.904762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.550296</td>\n",
              "      <td>0.696552</td>\n",
              "      <td>0.678832</td>\n",
              "      <td>0.885714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.590164</td>\n",
              "      <td>0.762646</td>\n",
              "      <td>0.541353</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.661017</td>\n",
              "      <td>0.793774</td>\n",
              "      <td>0.595420</td>\n",
              "      <td>0.541667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.366071</td>\n",
              "      <td>0.603113</td>\n",
              "      <td>0.445652</td>\n",
              "      <td>0.569444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.451613</td>\n",
              "      <td>0.673152</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.777778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.439024</td>\n",
              "      <td>0.680934</td>\n",
              "      <td>0.467532</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.445378</td>\n",
              "      <td>0.544828</td>\n",
              "      <td>0.616279</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.474747</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.618421</td>\n",
              "      <td>0.886792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.446154</td>\n",
              "      <td>0.562069</td>\n",
              "      <td>0.578073</td>\n",
              "      <td>0.820755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.419753</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.584527</td>\n",
              "      <td>0.962264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.457014</td>\n",
              "      <td>0.568966</td>\n",
              "      <td>0.617737</td>\n",
              "      <td>0.952830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.793774</td>\n",
              "      <td>0.653595</td>\n",
              "      <td>0.684932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.543689</td>\n",
              "      <td>0.750973</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.767123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.620000</td>\n",
              "      <td>0.809339</td>\n",
              "      <td>0.716763</td>\n",
              "      <td>0.849315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.465278</td>\n",
              "      <td>0.677043</td>\n",
              "      <td>0.617512</td>\n",
              "      <td>0.917808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.596154</td>\n",
              "      <td>0.793774</td>\n",
              "      <td>0.700565</td>\n",
              "      <td>0.849315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.460396</td>\n",
              "      <td>0.579310</td>\n",
              "      <td>0.603896</td>\n",
              "      <td>0.877358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.467662</td>\n",
              "      <td>0.589655</td>\n",
              "      <td>0.612378</td>\n",
              "      <td>0.886792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.427807</td>\n",
              "      <td>0.541379</td>\n",
              "      <td>0.546075</td>\n",
              "      <td>0.754717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.431034</td>\n",
              "      <td>0.524138</td>\n",
              "      <td>0.591716</td>\n",
              "      <td>0.943396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.450704</td>\n",
              "      <td>0.562069</td>\n",
              "      <td>0.601881</td>\n",
              "      <td>0.905660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.527273</td>\n",
              "      <td>0.739300</td>\n",
              "      <td>0.633880</td>\n",
              "      <td>0.794521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.728814</td>\n",
              "      <td>0.821012</td>\n",
              "      <td>0.651515</td>\n",
              "      <td>0.589041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.588785</td>\n",
              "      <td>0.789883</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.863014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.673152</td>\n",
              "      <td>0.611111</td>\n",
              "      <td>0.904110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.628866</td>\n",
              "      <td>0.813230</td>\n",
              "      <td>0.717647</td>\n",
              "      <td>0.835616</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  CPRI  0.423077  0.513793  0.584071  0.942857\n",
              "1            GRU 0.1  CPRI  0.576923  0.720690  0.689655  0.857143\n",
              "2        XGBoost 0.1  CPRI  0.567742  0.710345  0.676923  0.838095\n",
              "3         Logreg 0.1  CPRI  0.513514  0.655172  0.655172  0.904762\n",
              "4            SVM 0.1  CPRI  0.550296  0.696552  0.678832  0.885714\n",
              "5      LSTM beta 0.1  CPRI  0.590164  0.762646  0.541353  0.500000\n",
              "6       GRU beta 0.1  CPRI  0.661017  0.793774  0.595420  0.541667\n",
              "7   XGBoost beta 0.1  CPRI  0.366071  0.603113  0.445652  0.569444\n",
              "8    logreg beta 0.1  CPRI  0.451613  0.673152  0.571429  0.777778\n",
              "9       svm beta 0.1  CPRI  0.439024  0.680934  0.467532  0.500000\n",
              "0           LSTM 0.2  CPRI  0.445378  0.544828  0.616279  1.000000\n",
              "1            GRU 0.2  CPRI  0.474747  0.600000  0.618421  0.886792\n",
              "2        XGBoost 0.2  CPRI  0.446154  0.562069  0.578073  0.820755\n",
              "3         Logreg 0.2  CPRI  0.419753  0.500000  0.584527  0.962264\n",
              "4            SVM 0.2  CPRI  0.457014  0.568966  0.617737  0.952830\n",
              "5      LSTM beta 0.2  CPRI  0.625000  0.793774  0.653595  0.684932\n",
              "6       GRU beta 0.2  CPRI  0.543689  0.750973  0.636364  0.767123\n",
              "7   XGBoost beta 0.2  CPRI  0.620000  0.809339  0.716763  0.849315\n",
              "8    logreg beta 0.2  CPRI  0.465278  0.677043  0.617512  0.917808\n",
              "9       svm beta 0.2  CPRI  0.596154  0.793774  0.700565  0.849315\n",
              "0          LSTM 0.15  CPRI  0.460396  0.579310  0.603896  0.877358\n",
              "1           GRU 0.15  CPRI  0.467662  0.589655  0.612378  0.886792\n",
              "2       XGBoost 0.15  CPRI  0.427807  0.541379  0.546075  0.754717\n",
              "3        Logreg 0.15  CPRI  0.431034  0.524138  0.591716  0.943396\n",
              "4           SVM 0.15  CPRI  0.450704  0.562069  0.601881  0.905660\n",
              "5     LSTM beta 0.15  CPRI  0.527273  0.739300  0.633880  0.794521\n",
              "6      GRU beta 0.15  CPRI  0.728814  0.821012  0.651515  0.589041\n",
              "7  XGBoost beta 0.15  CPRI  0.588785  0.789883  0.700000  0.863014\n",
              "8   logreg beta 0.15  CPRI  0.461538  0.673152  0.611111  0.904110\n",
              "9      svm beta 0.15  CPRI  0.628866  0.813230  0.717647  0.835616"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irmrld7pHUKu"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iehVFbMWHUKv"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sp9DHgTNHUKv",
        "outputId": "8b823134-2adf-4dc7-b3d8-87ca7bc9e6d2"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2100)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"CPRI\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6806 - accuracy: 0.5678 - val_loss: 0.6912 - val_accuracy: 0.4448\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6543 - accuracy: 0.6107 - val_loss: 0.6448 - val_accuracy: 0.6241\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6132 - accuracy: 0.6685 - val_loss: 0.6168 - val_accuracy: 0.6690\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5873 - accuracy: 0.6933 - val_loss: 0.6643 - val_accuracy: 0.5345\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5932 - accuracy: 0.6718 - val_loss: 0.5984 - val_accuracy: 0.6897\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6597 - accuracy: 0.6148 - val_loss: 0.6102 - val_accuracy: 0.7310\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5914 - accuracy: 0.7007 - val_loss: 0.5793 - val_accuracy: 0.7379\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5733 - accuracy: 0.7094 - val_loss: 0.6129 - val_accuracy: 0.6621\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5722 - accuracy: 0.7121 - val_loss: 0.5849 - val_accuracy: 0.6862\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5685 - accuracy: 0.7154 - val_loss: 0.5910 - val_accuracy: 0.6828\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.794131\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.809292\n",
            "[2]\tvalidation_0-auc:0.807207\n",
            "[3]\tvalidation_0-auc:0.813024\n",
            "[4]\tvalidation_0-auc:0.809575\n",
            "[5]\tvalidation_0-auc:0.809833\n",
            "[6]\tvalidation_0-auc:0.807362\n",
            "[7]\tvalidation_0-auc:0.80749\n",
            "[8]\tvalidation_0-auc:0.809447\n",
            "[9]\tvalidation_0-auc:0.810167\n",
            "[10]\tvalidation_0-auc:0.808391\n",
            "[11]\tvalidation_0-auc:0.80888\n",
            "[12]\tvalidation_0-auc:0.816628\n",
            "[13]\tvalidation_0-auc:0.818044\n",
            "[14]\tvalidation_0-auc:0.819485\n",
            "[15]\tvalidation_0-auc:0.818867\n",
            "[16]\tvalidation_0-auc:0.817864\n",
            "[17]\tvalidation_0-auc:0.817555\n",
            "[18]\tvalidation_0-auc:0.818867\n",
            "[19]\tvalidation_0-auc:0.820952\n",
            "[20]\tvalidation_0-auc:0.821184\n",
            "[21]\tvalidation_0-auc:0.822523\n",
            "[22]\tvalidation_0-auc:0.823089\n",
            "[23]\tvalidation_0-auc:0.825174\n",
            "[24]\tvalidation_0-auc:0.824865\n",
            "[25]\tvalidation_0-auc:0.824891\n",
            "[26]\tvalidation_0-auc:0.824221\n",
            "[27]\tvalidation_0-auc:0.824427\n",
            "[28]\tvalidation_0-auc:0.824041\n",
            "[29]\tvalidation_0-auc:0.824685\n",
            "[30]\tvalidation_0-auc:0.823758\n",
            "[31]\tvalidation_0-auc:0.823964\n",
            "[32]\tvalidation_0-auc:0.824067\n",
            "[33]\tvalidation_0-auc:0.822728\n",
            "[34]\tvalidation_0-auc:0.822728\n",
            "[35]\tvalidation_0-auc:0.823295\n",
            "[36]\tvalidation_0-auc:0.821828\n",
            "[37]\tvalidation_0-auc:0.820489\n",
            "[38]\tvalidation_0-auc:0.82018\n",
            "[39]\tvalidation_0-auc:0.818147\n",
            "[40]\tvalidation_0-auc:0.818095\n",
            "[41]\tvalidation_0-auc:0.81686\n",
            "[42]\tvalidation_0-auc:0.815624\n",
            "[43]\tvalidation_0-auc:0.813205\n",
            "[44]\tvalidation_0-auc:0.81305\n",
            "[45]\tvalidation_0-auc:0.813462\n",
            "[46]\tvalidation_0-auc:0.812741\n",
            "[47]\tvalidation_0-auc:0.811532\n",
            "[48]\tvalidation_0-auc:0.810862\n",
            "[49]\tvalidation_0-auc:0.810811\n",
            "[50]\tvalidation_0-auc:0.810347\n",
            "[51]\tvalidation_0-auc:0.810656\n",
            "[52]\tvalidation_0-auc:0.810553\n",
            "[53]\tvalidation_0-auc:0.809575\n",
            "[54]\tvalidation_0-auc:0.80834\n",
            "[55]\tvalidation_0-auc:0.807156\n",
            "[56]\tvalidation_0-auc:0.806795\n",
            "[57]\tvalidation_0-auc:0.805714\n",
            "[58]\tvalidation_0-auc:0.806075\n",
            "[59]\tvalidation_0-auc:0.806435\n",
            "[60]\tvalidation_0-auc:0.805457\n",
            "[61]\tvalidation_0-auc:0.80556\n",
            "[62]\tvalidation_0-auc:0.805869\n",
            "[63]\tvalidation_0-auc:0.805457\n",
            "[64]\tvalidation_0-auc:0.805714\n",
            "[65]\tvalidation_0-auc:0.80556\n",
            "[66]\tvalidation_0-auc:0.80556\n",
            "[67]\tvalidation_0-auc:0.804788\n",
            "[68]\tvalidation_0-auc:0.804427\n",
            "[69]\tvalidation_0-auc:0.804427\n",
            "[70]\tvalidation_0-auc:0.804118\n",
            "[71]\tvalidation_0-auc:0.804118\n",
            "[72]\tvalidation_0-auc:0.803398\n",
            "[73]\tvalidation_0-auc:0.803192\n",
            "Stopping. Best iteration:\n",
            "[23]\tvalidation_0-auc:0.825174\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6824 - accuracy: 0.5745 - val_loss: 0.7182 - val_accuracy: 0.3152\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6214 - accuracy: 0.6452 - val_loss: 0.6028 - val_accuracy: 0.7626\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6030 - accuracy: 0.6568 - val_loss: 0.6755 - val_accuracy: 0.5486\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5961 - accuracy: 0.6857 - val_loss: 0.6443 - val_accuracy: 0.6265\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5743 - accuracy: 0.6918 - val_loss: 0.7276 - val_accuracy: 0.4630\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 2s 13ms/step - loss: 0.6529 - accuracy: 0.6026 - val_loss: 0.7093 - val_accuracy: 0.4125\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5628 - accuracy: 0.7076 - val_loss: 0.6156 - val_accuracy: 0.6615\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5484 - accuracy: 0.7049 - val_loss: 0.5871 - val_accuracy: 0.6848\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5347 - accuracy: 0.7323 - val_loss: 0.5135 - val_accuracy: 0.8210\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5453 - accuracy: 0.7097 - val_loss: 0.5206 - val_accuracy: 0.8093\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.632658\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.720571\n",
            "[2]\tvalidation_0-auc:0.718018\n",
            "[3]\tvalidation_0-auc:0.702665\n",
            "[4]\tvalidation_0-auc:0.699812\n",
            "[5]\tvalidation_0-auc:0.697222\n",
            "[6]\tvalidation_0-auc:0.666141\n",
            "[7]\tvalidation_0-auc:0.639189\n",
            "[8]\tvalidation_0-auc:0.63964\n",
            "[9]\tvalidation_0-auc:0.658108\n",
            "[10]\tvalidation_0-auc:0.652628\n",
            "[11]\tvalidation_0-auc:0.655368\n",
            "[12]\tvalidation_0-auc:0.659647\n",
            "[13]\tvalidation_0-auc:0.653829\n",
            "[14]\tvalidation_0-auc:0.65488\n",
            "[15]\tvalidation_0-auc:0.643619\n",
            "[16]\tvalidation_0-auc:0.644707\n",
            "[17]\tvalidation_0-auc:0.636562\n",
            "[18]\tvalidation_0-auc:0.636974\n",
            "[19]\tvalidation_0-auc:0.638063\n",
            "[20]\tvalidation_0-auc:0.634272\n",
            "[21]\tvalidation_0-auc:0.633596\n",
            "[22]\tvalidation_0-auc:0.631532\n",
            "[23]\tvalidation_0-auc:0.631757\n",
            "[24]\tvalidation_0-auc:0.635135\n",
            "[25]\tvalidation_0-auc:0.635098\n",
            "[26]\tvalidation_0-auc:0.629317\n",
            "[27]\tvalidation_0-auc:0.63247\n",
            "[28]\tvalidation_0-auc:0.632132\n",
            "[29]\tvalidation_0-auc:0.636824\n",
            "[30]\tvalidation_0-auc:0.632695\n",
            "[31]\tvalidation_0-auc:0.635473\n",
            "[32]\tvalidation_0-auc:0.628116\n",
            "[33]\tvalidation_0-auc:0.63262\n",
            "[34]\tvalidation_0-auc:0.639152\n",
            "[35]\tvalidation_0-auc:0.637425\n",
            "[36]\tvalidation_0-auc:0.634722\n",
            "[37]\tvalidation_0-auc:0.636824\n",
            "[38]\tvalidation_0-auc:0.633408\n",
            "[39]\tvalidation_0-auc:0.638026\n",
            "[40]\tvalidation_0-auc:0.638476\n",
            "[41]\tvalidation_0-auc:0.633146\n",
            "[42]\tvalidation_0-auc:0.633596\n",
            "[43]\tvalidation_0-auc:0.636899\n",
            "[44]\tvalidation_0-auc:0.638551\n",
            "[45]\tvalidation_0-auc:0.638701\n",
            "[46]\tvalidation_0-auc:0.638701\n",
            "[47]\tvalidation_0-auc:0.639827\n",
            "[48]\tvalidation_0-auc:0.64039\n",
            "[49]\tvalidation_0-auc:0.641742\n",
            "[50]\tvalidation_0-auc:0.641216\n",
            "[51]\tvalidation_0-auc:0.641216\n",
            "Stopping. Best iteration:\n",
            "[1]\tvalidation_0-auc:0.720571\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+---------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |       Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+---------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.1     |  0.6896551724137931 |  0.543859649122807  | 0.8857142857142857 |  0.673913043478261  |\n",
            "|     GRU 0.1      |  0.6827586206896552 |  0.5371428571428571 | 0.8952380952380953 |  0.6714285714285714 |\n",
            "|   XGBoost 0.1    |  0.7103448275862069 |  0.567741935483871  | 0.8380952380952381 |  0.676923076923077  |\n",
            "|    Logreg 0.1    |  0.6551724137931034 |  0.5135135135135135 | 0.9047619047619048 |  0.6551724137931034 |\n",
            "|     SVM 0.1      |  0.696551724137931  |  0.5502958579881657 | 0.8857142857142857 |  0.6788321167883212 |\n",
            "|  LSTM beta 0.1   | 0.46303501945525294 |  0.3058823529411765 | 0.7222222222222222 | 0.42975206611570255 |\n",
            "|   GRU beta 0.1   |  0.8093385214007782 |  0.6825396825396826 | 0.5972222222222222 |  0.6370370370370372 |\n",
            "| XGBoost beta 0.1 |  0.603112840466926  | 0.36607142857142855 | 0.5694444444444444 | 0.44565217391304346 |\n",
            "| logreg beta 0.1  |  0.6731517509727627 | 0.45161290322580644 | 0.7777777777777778 |  0.5714285714285714 |\n",
            "|   svm beta 0.1   |  0.6809338521400778 | 0.43902439024390244 |        0.5         |  0.4675324675324676 |\n",
            "+------------------+---------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6666 - accuracy: 0.6295 - val_loss: 0.7489 - val_accuracy: 0.3655\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6616 - accuracy: 0.6309 - val_loss: 0.7737 - val_accuracy: 0.3655\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6450 - accuracy: 0.6309 - val_loss: 0.7459 - val_accuracy: 0.4345\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6094 - accuracy: 0.6698 - val_loss: 0.6920 - val_accuracy: 0.5448\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6018 - accuracy: 0.6658 - val_loss: 0.7212 - val_accuracy: 0.6000\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6516 - accuracy: 0.6289 - val_loss: 0.8412 - val_accuracy: 0.3793\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.6093 - accuracy: 0.6658 - val_loss: 0.6867 - val_accuracy: 0.5345\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5925 - accuracy: 0.6738 - val_loss: 0.6947 - val_accuracy: 0.5241\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5853 - accuracy: 0.6893 - val_loss: 0.7621 - val_accuracy: 0.5034\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5829 - accuracy: 0.6926 - val_loss: 0.6231 - val_accuracy: 0.6172\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.716238\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.70888\n",
            "[2]\tvalidation_0-auc:0.693832\n",
            "[3]\tvalidation_0-auc:0.688577\n",
            "[4]\tvalidation_0-auc:0.697626\n",
            "[5]\tvalidation_0-auc:0.695703\n",
            "[6]\tvalidation_0-auc:0.693781\n",
            "[7]\tvalidation_0-auc:0.693576\n",
            "[8]\tvalidation_0-auc:0.696011\n",
            "[9]\tvalidation_0-auc:0.689525\n",
            "[10]\tvalidation_0-auc:0.68668\n",
            "[11]\tvalidation_0-auc:0.686782\n",
            "[12]\tvalidation_0-auc:0.685449\n",
            "[13]\tvalidation_0-auc:0.687987\n",
            "[14]\tvalidation_0-auc:0.687782\n",
            "[15]\tvalidation_0-auc:0.687961\n",
            "[16]\tvalidation_0-auc:0.686526\n",
            "[17]\tvalidation_0-auc:0.683962\n",
            "[18]\tvalidation_0-auc:0.681911\n",
            "[19]\tvalidation_0-auc:0.683296\n",
            "[20]\tvalidation_0-auc:0.682219\n",
            "[21]\tvalidation_0-auc:0.679732\n",
            "[22]\tvalidation_0-auc:0.68045\n",
            "[23]\tvalidation_0-auc:0.67963\n",
            "[24]\tvalidation_0-auc:0.679579\n",
            "[25]\tvalidation_0-auc:0.679373\n",
            "[26]\tvalidation_0-auc:0.678707\n",
            "[27]\tvalidation_0-auc:0.678245\n",
            "[28]\tvalidation_0-auc:0.674964\n",
            "[29]\tvalidation_0-auc:0.67481\n",
            "[30]\tvalidation_0-auc:0.672785\n",
            "[31]\tvalidation_0-auc:0.672067\n",
            "[32]\tvalidation_0-auc:0.672324\n",
            "[33]\tvalidation_0-auc:0.671401\n",
            "[34]\tvalidation_0-auc:0.670273\n",
            "[35]\tvalidation_0-auc:0.669401\n",
            "[36]\tvalidation_0-auc:0.669093\n",
            "[37]\tvalidation_0-auc:0.670145\n",
            "[38]\tvalidation_0-auc:0.667581\n",
            "[39]\tvalidation_0-auc:0.667325\n",
            "[40]\tvalidation_0-auc:0.665171\n",
            "[41]\tvalidation_0-auc:0.664761\n",
            "[42]\tvalidation_0-auc:0.660762\n",
            "[43]\tvalidation_0-auc:0.65907\n",
            "[44]\tvalidation_0-auc:0.659326\n",
            "[45]\tvalidation_0-auc:0.659736\n",
            "[46]\tvalidation_0-auc:0.659942\n",
            "[47]\tvalidation_0-auc:0.658045\n",
            "[48]\tvalidation_0-auc:0.657019\n",
            "[49]\tvalidation_0-auc:0.656045\n",
            "[50]\tvalidation_0-auc:0.655789\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.716238\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6238 - accuracy: 0.6637 - val_loss: 0.8361 - val_accuracy: 0.3424\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5832 - accuracy: 0.6740 - val_loss: 0.5449 - val_accuracy: 0.7860\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5512 - accuracy: 0.7111 - val_loss: 0.7070 - val_accuracy: 0.4864\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5448 - accuracy: 0.7193 - val_loss: 0.6138 - val_accuracy: 0.6693\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5478 - accuracy: 0.7159 - val_loss: 0.5316 - val_accuracy: 0.7549\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 18ms/step - loss: 0.5932 - accuracy: 0.6754 - val_loss: 0.7000 - val_accuracy: 0.4591\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5193 - accuracy: 0.7310 - val_loss: 0.6229 - val_accuracy: 0.6226\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.4986 - accuracy: 0.7440 - val_loss: 0.5957 - val_accuracy: 0.6459\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5004 - accuracy: 0.7605 - val_loss: 0.5661 - val_accuracy: 0.6848\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4942 - accuracy: 0.7543 - val_loss: 0.5486 - val_accuracy: 0.7393\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.86588\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.896404\n",
            "[2]\tvalidation_0-auc:0.895399\n",
            "[3]\tvalidation_0-auc:0.902062\n",
            "[4]\tvalidation_0-auc:0.903551\n",
            "[5]\tvalidation_0-auc:0.902509\n",
            "[6]\tvalidation_0-auc:0.903923\n",
            "[7]\tvalidation_0-auc:0.904854\n",
            "[8]\tvalidation_0-auc:0.904891\n",
            "[9]\tvalidation_0-auc:0.906083\n",
            "[10]\tvalidation_0-auc:0.905896\n",
            "[11]\tvalidation_0-auc:0.906715\n",
            "[12]\tvalidation_0-auc:0.908502\n",
            "[13]\tvalidation_0-auc:0.909544\n",
            "[14]\tvalidation_0-auc:0.909917\n",
            "[15]\tvalidation_0-auc:0.910661\n",
            "[16]\tvalidation_0-auc:0.910661\n",
            "[17]\tvalidation_0-auc:0.910289\n",
            "[18]\tvalidation_0-auc:0.91014\n",
            "[19]\tvalidation_0-auc:0.912001\n",
            "[20]\tvalidation_0-auc:0.912746\n",
            "[21]\tvalidation_0-auc:0.912671\n",
            "[22]\tvalidation_0-auc:0.912299\n",
            "[23]\tvalidation_0-auc:0.912895\n",
            "[24]\tvalidation_0-auc:0.912597\n",
            "[25]\tvalidation_0-auc:0.912076\n",
            "[26]\tvalidation_0-auc:0.911741\n",
            "[27]\tvalidation_0-auc:0.913118\n",
            "[28]\tvalidation_0-auc:0.912895\n",
            "[29]\tvalidation_0-auc:0.91282\n",
            "[30]\tvalidation_0-auc:0.912522\n",
            "[31]\tvalidation_0-auc:0.913118\n",
            "[32]\tvalidation_0-auc:0.913714\n",
            "[33]\tvalidation_0-auc:0.913267\n",
            "[34]\tvalidation_0-auc:0.913416\n",
            "[35]\tvalidation_0-auc:0.912225\n",
            "[36]\tvalidation_0-auc:0.911406\n",
            "[37]\tvalidation_0-auc:0.910363\n",
            "[38]\tvalidation_0-auc:0.910512\n",
            "[39]\tvalidation_0-auc:0.909991\n",
            "[40]\tvalidation_0-auc:0.910587\n",
            "[41]\tvalidation_0-auc:0.910363\n",
            "[42]\tvalidation_0-auc:0.910363\n",
            "[43]\tvalidation_0-auc:0.909321\n",
            "[44]\tvalidation_0-auc:0.908465\n",
            "[45]\tvalidation_0-auc:0.908539\n",
            "[46]\tvalidation_0-auc:0.908986\n",
            "[47]\tvalidation_0-auc:0.908725\n",
            "[48]\tvalidation_0-auc:0.909917\n",
            "[49]\tvalidation_0-auc:0.910587\n",
            "[50]\tvalidation_0-auc:0.910587\n",
            "[51]\tvalidation_0-auc:0.910289\n",
            "[52]\tvalidation_0-auc:0.910363\n",
            "[53]\tvalidation_0-auc:0.910214\n",
            "[54]\tvalidation_0-auc:0.909842\n",
            "[55]\tvalidation_0-auc:0.909917\n",
            "[56]\tvalidation_0-auc:0.909768\n",
            "[57]\tvalidation_0-auc:0.911033\n",
            "[58]\tvalidation_0-auc:0.911927\n",
            "[59]\tvalidation_0-auc:0.912225\n",
            "[60]\tvalidation_0-auc:0.911406\n",
            "[61]\tvalidation_0-auc:0.912001\n",
            "[62]\tvalidation_0-auc:0.91215\n",
            "[63]\tvalidation_0-auc:0.912225\n",
            "[64]\tvalidation_0-auc:0.911555\n",
            "[65]\tvalidation_0-auc:0.911182\n",
            "[66]\tvalidation_0-auc:0.91215\n",
            "[67]\tvalidation_0-auc:0.912969\n",
            "[68]\tvalidation_0-auc:0.912597\n",
            "[69]\tvalidation_0-auc:0.912597\n",
            "[70]\tvalidation_0-auc:0.913043\n",
            "[71]\tvalidation_0-auc:0.912225\n",
            "[72]\tvalidation_0-auc:0.912373\n",
            "[73]\tvalidation_0-auc:0.911331\n",
            "[74]\tvalidation_0-auc:0.91148\n",
            "[75]\tvalidation_0-auc:0.909917\n",
            "[76]\tvalidation_0-auc:0.90947\n",
            "[77]\tvalidation_0-auc:0.909247\n",
            "[78]\tvalidation_0-auc:0.909247\n",
            "[79]\tvalidation_0-auc:0.909247\n",
            "[80]\tvalidation_0-auc:0.909917\n",
            "[81]\tvalidation_0-auc:0.909768\n",
            "[82]\tvalidation_0-auc:0.909619\n",
            "Stopping. Best iteration:\n",
            "[32]\tvalidation_0-auc:0.913714\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|     LSTM 0.2     |        0.6         |  0.4739583333333333 | 0.8584905660377359 | 0.6107382550335569 |\n",
            "|     GRU 0.2      | 0.6172413793103448 |  0.4825174825174825 | 0.6509433962264151 | 0.5542168674698795 |\n",
            "|   XGBoost 0.2    | 0.5620689655172414 |  0.4461538461538462 | 0.8207547169811321 | 0.5780730897009967 |\n",
            "|    Logreg 0.2    |        0.5         | 0.41975308641975306 | 0.9622641509433962 | 0.5845272206303724 |\n",
            "|     SVM 0.2      | 0.5689655172413793 | 0.45701357466063347 | 0.9528301886792453 | 0.617737003058104  |\n",
            "|  LSTM beta 0.2   | 0.754863813229572  |         0.55        | 0.7534246575342466 | 0.6358381502890174 |\n",
            "|   GRU beta 0.2   | 0.7392996108949417 |  0.5263157894736842 | 0.821917808219178  | 0.641711229946524  |\n",
            "| XGBoost beta 0.2 | 0.8093385214007782 |         0.62        | 0.8493150684931506 | 0.7167630057803469 |\n",
            "| logreg beta 0.2  | 0.6770428015564203 |  0.4652777777777778 | 0.9178082191780822 | 0.6175115207373272 |\n",
            "|   svm beta 0.2   | 0.7937743190661478 |  0.5961538461538461 | 0.8493150684931506 | 0.7005649717514123 |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6691 - accuracy: 0.6174 - val_loss: 0.7901 - val_accuracy: 0.3655\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6388 - accuracy: 0.6235 - val_loss: 0.7150 - val_accuracy: 0.5552\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6146 - accuracy: 0.6591 - val_loss: 0.6937 - val_accuracy: 0.6034\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6314 - accuracy: 0.6651 - val_loss: 0.8388 - val_accuracy: 0.3793\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6079 - accuracy: 0.6678 - val_loss: 0.8110 - val_accuracy: 0.5241\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6536 - accuracy: 0.6188 - val_loss: 0.7325 - val_accuracy: 0.4241\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6149 - accuracy: 0.6651 - val_loss: 0.7185 - val_accuracy: 0.5069\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5949 - accuracy: 0.6705 - val_loss: 0.7216 - val_accuracy: 0.4828\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5900 - accuracy: 0.6772 - val_loss: 0.7034 - val_accuracy: 0.5207\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5860 - accuracy: 0.6752 - val_loss: 0.6847 - val_accuracy: 0.5655\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.665889\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.679502\n",
            "[2]\tvalidation_0-auc:0.685859\n",
            "[3]\tvalidation_0-auc:0.687885\n",
            "[4]\tvalidation_0-auc:0.686039\n",
            "[5]\tvalidation_0-auc:0.682091\n",
            "[6]\tvalidation_0-auc:0.681732\n",
            "[7]\tvalidation_0-auc:0.681732\n",
            "[8]\tvalidation_0-auc:0.682886\n",
            "[9]\tvalidation_0-auc:0.68186\n",
            "[10]\tvalidation_0-auc:0.671965\n",
            "[11]\tvalidation_0-auc:0.668478\n",
            "[12]\tvalidation_0-auc:0.671785\n",
            "[13]\tvalidation_0-auc:0.67199\n",
            "[14]\tvalidation_0-auc:0.66917\n",
            "[15]\tvalidation_0-auc:0.670914\n",
            "[16]\tvalidation_0-auc:0.666094\n",
            "[17]\tvalidation_0-auc:0.66653\n",
            "[18]\tvalidation_0-auc:0.668478\n",
            "[19]\tvalidation_0-auc:0.670016\n",
            "[20]\tvalidation_0-auc:0.674092\n",
            "[21]\tvalidation_0-auc:0.672375\n",
            "[22]\tvalidation_0-auc:0.672477\n",
            "[23]\tvalidation_0-auc:0.671067\n",
            "[24]\tvalidation_0-auc:0.669581\n",
            "[25]\tvalidation_0-auc:0.670555\n",
            "[26]\tvalidation_0-auc:0.666325\n",
            "[27]\tvalidation_0-auc:0.665094\n",
            "[28]\tvalidation_0-auc:0.667196\n",
            "[29]\tvalidation_0-auc:0.66571\n",
            "[30]\tvalidation_0-auc:0.667812\n",
            "[31]\tvalidation_0-auc:0.667453\n",
            "[32]\tvalidation_0-auc:0.668837\n",
            "[33]\tvalidation_0-auc:0.669145\n",
            "[34]\tvalidation_0-auc:0.669196\n",
            "[35]\tvalidation_0-auc:0.67058\n",
            "[36]\tvalidation_0-auc:0.668042\n",
            "[37]\tvalidation_0-auc:0.666299\n",
            "[38]\tvalidation_0-auc:0.665376\n",
            "[39]\tvalidation_0-auc:0.664351\n",
            "[40]\tvalidation_0-auc:0.663428\n",
            "[41]\tvalidation_0-auc:0.662146\n",
            "[42]\tvalidation_0-auc:0.662044\n",
            "[43]\tvalidation_0-auc:0.660326\n",
            "[44]\tvalidation_0-auc:0.660326\n",
            "[45]\tvalidation_0-auc:0.659762\n",
            "[46]\tvalidation_0-auc:0.659249\n",
            "[47]\tvalidation_0-auc:0.65848\n",
            "[48]\tvalidation_0-auc:0.658224\n",
            "[49]\tvalidation_0-auc:0.658224\n",
            "[50]\tvalidation_0-auc:0.658429\n",
            "[51]\tvalidation_0-auc:0.658121\n",
            "[52]\tvalidation_0-auc:0.65725\n",
            "[53]\tvalidation_0-auc:0.656686\n",
            "Stopping. Best iteration:\n",
            "[3]\tvalidation_0-auc:0.687885\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6330 - accuracy: 0.6431 - val_loss: 0.6743 - val_accuracy: 0.5992\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5674 - accuracy: 0.6857 - val_loss: 0.6138 - val_accuracy: 0.7043\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5557 - accuracy: 0.6939 - val_loss: 0.5885 - val_accuracy: 0.7043\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5457 - accuracy: 0.7069 - val_loss: 0.5427 - val_accuracy: 0.7665\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5275 - accuracy: 0.7213 - val_loss: 0.5818 - val_accuracy: 0.7043\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 2s 13ms/step - loss: 0.6173 - accuracy: 0.6493 - val_loss: 0.5249 - val_accuracy: 0.7704\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5460 - accuracy: 0.7241 - val_loss: 0.5549 - val_accuracy: 0.7004\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5185 - accuracy: 0.7220 - val_loss: 0.5621 - val_accuracy: 0.6926\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5100 - accuracy: 0.7461 - val_loss: 0.5175 - val_accuracy: 0.7588\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4911 - accuracy: 0.7584 - val_loss: 0.7199 - val_accuracy: 0.5409\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.860706\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.85985\n",
            "[2]\tvalidation_0-auc:0.886689\n",
            "[3]\tvalidation_0-auc:0.894468\n",
            "[4]\tvalidation_0-auc:0.896069\n",
            "[5]\tvalidation_0-auc:0.898786\n",
            "[6]\tvalidation_0-auc:0.900238\n",
            "[7]\tvalidation_0-auc:0.89994\n",
            "[8]\tvalidation_0-auc:0.900313\n",
            "[9]\tvalidation_0-auc:0.898563\n",
            "[10]\tvalidation_0-auc:0.902918\n",
            "[11]\tvalidation_0-auc:0.904147\n",
            "[12]\tvalidation_0-auc:0.905487\n",
            "[13]\tvalidation_0-auc:0.90679\n",
            "[14]\tvalidation_0-auc:0.908986\n",
            "[15]\tvalidation_0-auc:0.90973\n",
            "[16]\tvalidation_0-auc:0.910624\n",
            "[17]\tvalidation_0-auc:0.909507\n",
            "[18]\tvalidation_0-auc:0.909433\n",
            "[19]\tvalidation_0-auc:0.909656\n",
            "[20]\tvalidation_0-auc:0.910326\n",
            "[21]\tvalidation_0-auc:0.908614\n",
            "[22]\tvalidation_0-auc:0.909098\n",
            "[23]\tvalidation_0-auc:0.909023\n",
            "[24]\tvalidation_0-auc:0.908725\n",
            "[25]\tvalidation_0-auc:0.908018\n",
            "[26]\tvalidation_0-auc:0.908242\n",
            "[27]\tvalidation_0-auc:0.908018\n",
            "[28]\tvalidation_0-auc:0.907199\n",
            "[29]\tvalidation_0-auc:0.906231\n",
            "[30]\tvalidation_0-auc:0.905934\n",
            "[31]\tvalidation_0-auc:0.904817\n",
            "[32]\tvalidation_0-auc:0.90504\n",
            "[33]\tvalidation_0-auc:0.90504\n",
            "[34]\tvalidation_0-auc:0.904966\n",
            "[35]\tvalidation_0-auc:0.903477\n",
            "[36]\tvalidation_0-auc:0.901951\n",
            "[37]\tvalidation_0-auc:0.901951\n",
            "[38]\tvalidation_0-auc:0.901802\n",
            "[39]\tvalidation_0-auc:0.901951\n",
            "[40]\tvalidation_0-auc:0.902621\n",
            "[41]\tvalidation_0-auc:0.901206\n",
            "[42]\tvalidation_0-auc:0.900908\n",
            "[43]\tvalidation_0-auc:0.900089\n",
            "[44]\tvalidation_0-auc:0.89901\n",
            "[45]\tvalidation_0-auc:0.89901\n",
            "[46]\tvalidation_0-auc:0.898712\n",
            "[47]\tvalidation_0-auc:0.897521\n",
            "[48]\tvalidation_0-auc:0.897484\n",
            "[49]\tvalidation_0-auc:0.897856\n",
            "[50]\tvalidation_0-auc:0.898005\n",
            "[51]\tvalidation_0-auc:0.897298\n",
            "[52]\tvalidation_0-auc:0.897298\n",
            "[53]\tvalidation_0-auc:0.897595\n",
            "[54]\tvalidation_0-auc:0.897223\n",
            "[55]\tvalidation_0-auc:0.899196\n",
            "[56]\tvalidation_0-auc:0.897781\n",
            "[57]\tvalidation_0-auc:0.896888\n",
            "[58]\tvalidation_0-auc:0.896888\n",
            "[59]\tvalidation_0-auc:0.897484\n",
            "[60]\tvalidation_0-auc:0.898005\n",
            "[61]\tvalidation_0-auc:0.899457\n",
            "[62]\tvalidation_0-auc:0.899084\n",
            "[63]\tvalidation_0-auc:0.897074\n",
            "[64]\tvalidation_0-auc:0.896553\n",
            "[65]\tvalidation_0-auc:0.896181\n",
            "[66]\tvalidation_0-auc:0.89566\n",
            "Stopping. Best iteration:\n",
            "[16]\tvalidation_0-auc:0.910624\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.5241379310344828 |  0.4344262295081967 |        1.0         | 0.6057142857142858 |\n",
            "|      GRU 0.15     | 0.5655172413793104 |  0.4523809523809524 | 0.8962264150943396 | 0.6012658227848101 |\n",
            "|    XGBoost 0.15   | 0.5413793103448276 | 0.42780748663101603 | 0.7547169811320755 | 0.5460750853242321 |\n",
            "|    Logreg 0.15    | 0.5241379310344828 | 0.43103448275862066 | 0.9433962264150944 | 0.5917159763313609 |\n",
            "|      SVM 0.15     | 0.5620689655172414 |  0.4507042253521127 | 0.9056603773584906 | 0.6018808777429466 |\n",
            "|   LSTM beta 0.15  | 0.7042801556420234 | 0.48760330578512395 | 0.8082191780821918 | 0.6082474226804123 |\n",
            "|   GRU beta 0.15   | 0.5408560311284046 |  0.3783783783783784 | 0.958904109589041  | 0.5426356589147286 |\n",
            "| XGBoost beta 0.15 | 0.7898832684824902 |  0.5887850467289719 | 0.863013698630137  |        0.7         |\n",
            "|  logreg beta 0.15 | 0.6731517509727627 | 0.46153846153846156 | 0.9041095890410958 | 0.6111111111111112 |\n",
            "|   svm beta 0.15   | 0.8132295719844358 |  0.6288659793814433 | 0.8356164383561644 | 0.7176470588235294 |\n",
            "+-------------------+--------------------+---------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "axbc8LrwHUKw",
        "outputId": "444101d6-95ed-43a0-d4dc-bf4681119577"
      },
      "source": [
        "Result_purging.to_csv('CPRI_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.553648</td>\n",
              "      <td>0.716327</td>\n",
              "      <td>0.649874</td>\n",
              "      <td>0.786585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.493103</td>\n",
              "      <td>0.657143</td>\n",
              "      <td>0.629956</td>\n",
              "      <td>0.871951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.513410</td>\n",
              "      <td>0.679592</td>\n",
              "      <td>0.630588</td>\n",
              "      <td>0.817073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.491694</td>\n",
              "      <td>0.655102</td>\n",
              "      <td>0.636559</td>\n",
              "      <td>0.902439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.512545</td>\n",
              "      <td>0.679592</td>\n",
              "      <td>0.645598</td>\n",
              "      <td>0.871951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.368132</td>\n",
              "      <td>0.608315</td>\n",
              "      <td>0.428115</td>\n",
              "      <td>0.511450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.724289</td>\n",
              "      <td>0.543478</td>\n",
              "      <td>0.572519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.348416</td>\n",
              "      <td>0.566740</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.587786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>0.654267</td>\n",
              "      <td>0.556180</td>\n",
              "      <td>0.755725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.398810</td>\n",
              "      <td>0.638950</td>\n",
              "      <td>0.448161</td>\n",
              "      <td>0.511450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.418803</td>\n",
              "      <td>0.618367</td>\n",
              "      <td>0.511749</td>\n",
              "      <td>0.657718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.441860</td>\n",
              "      <td>0.634694</td>\n",
              "      <td>0.560197</td>\n",
              "      <td>0.765101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.376176</td>\n",
              "      <td>0.534694</td>\n",
              "      <td>0.512821</td>\n",
              "      <td>0.805369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.362025</td>\n",
              "      <td>0.473469</td>\n",
              "      <td>0.525735</td>\n",
              "      <td>0.959732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.387187</td>\n",
              "      <td>0.530612</td>\n",
              "      <td>0.547244</td>\n",
              "      <td>0.932886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.365297</td>\n",
              "      <td>0.617068</td>\n",
              "      <td>0.477612</td>\n",
              "      <td>0.689655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.467626</td>\n",
              "      <td>0.726477</td>\n",
              "      <td>0.509804</td>\n",
              "      <td>0.560345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.421053</td>\n",
              "      <td>0.680525</td>\n",
              "      <td>0.522876</td>\n",
              "      <td>0.689655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.349091</td>\n",
              "      <td>0.564551</td>\n",
              "      <td>0.491049</td>\n",
              "      <td>0.827586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.400990</td>\n",
              "      <td>0.658643</td>\n",
              "      <td>0.509434</td>\n",
              "      <td>0.698276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.385965</td>\n",
              "      <td>0.491837</td>\n",
              "      <td>0.552962</td>\n",
              "      <td>0.974684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.486842</td>\n",
              "      <td>0.669388</td>\n",
              "      <td>0.477419</td>\n",
              "      <td>0.468354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.403175</td>\n",
              "      <td>0.553061</td>\n",
              "      <td>0.536998</td>\n",
              "      <td>0.803797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.396277</td>\n",
              "      <td>0.518367</td>\n",
              "      <td>0.558052</td>\n",
              "      <td>0.943038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.410405</td>\n",
              "      <td>0.551020</td>\n",
              "      <td>0.563492</td>\n",
              "      <td>0.898734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.458824</td>\n",
              "      <td>0.695842</td>\n",
              "      <td>0.528814</td>\n",
              "      <td>0.624000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.385593</td>\n",
              "      <td>0.608315</td>\n",
              "      <td>0.504155</td>\n",
              "      <td>0.728000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.754923</td>\n",
              "      <td>0.582090</td>\n",
              "      <td>0.624000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.390977</td>\n",
              "      <td>0.599562</td>\n",
              "      <td>0.531969</td>\n",
              "      <td>0.832000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.458763</td>\n",
              "      <td>0.691466</td>\n",
              "      <td>0.557994</td>\n",
              "      <td>0.712000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.543860</td>\n",
              "      <td>0.689655</td>\n",
              "      <td>0.673913</td>\n",
              "      <td>0.885714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.537143</td>\n",
              "      <td>0.682759</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.895238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.567742</td>\n",
              "      <td>0.710345</td>\n",
              "      <td>0.676923</td>\n",
              "      <td>0.838095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.513514</td>\n",
              "      <td>0.655172</td>\n",
              "      <td>0.655172</td>\n",
              "      <td>0.904762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.550296</td>\n",
              "      <td>0.696552</td>\n",
              "      <td>0.678832</td>\n",
              "      <td>0.885714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.305882</td>\n",
              "      <td>0.463035</td>\n",
              "      <td>0.429752</td>\n",
              "      <td>0.722222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.682540</td>\n",
              "      <td>0.809339</td>\n",
              "      <td>0.637037</td>\n",
              "      <td>0.597222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.366071</td>\n",
              "      <td>0.603113</td>\n",
              "      <td>0.445652</td>\n",
              "      <td>0.569444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.451613</td>\n",
              "      <td>0.673152</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.777778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.439024</td>\n",
              "      <td>0.680934</td>\n",
              "      <td>0.467532</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.473958</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.610738</td>\n",
              "      <td>0.858491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.482517</td>\n",
              "      <td>0.617241</td>\n",
              "      <td>0.554217</td>\n",
              "      <td>0.650943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.446154</td>\n",
              "      <td>0.562069</td>\n",
              "      <td>0.578073</td>\n",
              "      <td>0.820755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.419753</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.584527</td>\n",
              "      <td>0.962264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.457014</td>\n",
              "      <td>0.568966</td>\n",
              "      <td>0.617737</td>\n",
              "      <td>0.952830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.754864</td>\n",
              "      <td>0.635838</td>\n",
              "      <td>0.753425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.526316</td>\n",
              "      <td>0.739300</td>\n",
              "      <td>0.641711</td>\n",
              "      <td>0.821918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.620000</td>\n",
              "      <td>0.809339</td>\n",
              "      <td>0.716763</td>\n",
              "      <td>0.849315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.465278</td>\n",
              "      <td>0.677043</td>\n",
              "      <td>0.617512</td>\n",
              "      <td>0.917808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.596154</td>\n",
              "      <td>0.793774</td>\n",
              "      <td>0.700565</td>\n",
              "      <td>0.849315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.434426</td>\n",
              "      <td>0.524138</td>\n",
              "      <td>0.605714</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.452381</td>\n",
              "      <td>0.565517</td>\n",
              "      <td>0.601266</td>\n",
              "      <td>0.896226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.427807</td>\n",
              "      <td>0.541379</td>\n",
              "      <td>0.546075</td>\n",
              "      <td>0.754717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.431034</td>\n",
              "      <td>0.524138</td>\n",
              "      <td>0.591716</td>\n",
              "      <td>0.943396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.450704</td>\n",
              "      <td>0.562069</td>\n",
              "      <td>0.601881</td>\n",
              "      <td>0.905660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.487603</td>\n",
              "      <td>0.704280</td>\n",
              "      <td>0.608247</td>\n",
              "      <td>0.808219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.378378</td>\n",
              "      <td>0.540856</td>\n",
              "      <td>0.542636</td>\n",
              "      <td>0.958904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.588785</td>\n",
              "      <td>0.789883</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.863014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.673152</td>\n",
              "      <td>0.611111</td>\n",
              "      <td>0.904110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.628866</td>\n",
              "      <td>0.813230</td>\n",
              "      <td>0.717647</td>\n",
              "      <td>0.835616</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  CPRI  0.553648  0.716327  0.649874  0.786585\n",
              "1            GRU 0.1  CPRI  0.493103  0.657143  0.629956  0.871951\n",
              "2        XGBoost 0.1  CPRI  0.513410  0.679592  0.630588  0.817073\n",
              "3         Logreg 0.1  CPRI  0.491694  0.655102  0.636559  0.902439\n",
              "4            SVM 0.1  CPRI  0.512545  0.679592  0.645598  0.871951\n",
              "5      LSTM beta 0.1  CPRI  0.368132  0.608315  0.428115  0.511450\n",
              "6       GRU beta 0.1  CPRI  0.517241  0.724289  0.543478  0.572519\n",
              "7   XGBoost beta 0.1  CPRI  0.348416  0.566740  0.437500  0.587786\n",
              "8    logreg beta 0.1  CPRI  0.440000  0.654267  0.556180  0.755725\n",
              "9       svm beta 0.1  CPRI  0.398810  0.638950  0.448161  0.511450\n",
              "0           LSTM 0.2  CPRI  0.418803  0.618367  0.511749  0.657718\n",
              "1            GRU 0.2  CPRI  0.441860  0.634694  0.560197  0.765101\n",
              "2        XGBoost 0.2  CPRI  0.376176  0.534694  0.512821  0.805369\n",
              "3         Logreg 0.2  CPRI  0.362025  0.473469  0.525735  0.959732\n",
              "4            SVM 0.2  CPRI  0.387187  0.530612  0.547244  0.932886\n",
              "5      LSTM beta 0.2  CPRI  0.365297  0.617068  0.477612  0.689655\n",
              "6       GRU beta 0.2  CPRI  0.467626  0.726477  0.509804  0.560345\n",
              "7   XGBoost beta 0.2  CPRI  0.421053  0.680525  0.522876  0.689655\n",
              "8    logreg beta 0.2  CPRI  0.349091  0.564551  0.491049  0.827586\n",
              "9       svm beta 0.2  CPRI  0.400990  0.658643  0.509434  0.698276\n",
              "0          LSTM 0.15  CPRI  0.385965  0.491837  0.552962  0.974684\n",
              "1           GRU 0.15  CPRI  0.486842  0.669388  0.477419  0.468354\n",
              "2       XGBoost 0.15  CPRI  0.403175  0.553061  0.536998  0.803797\n",
              "3        Logreg 0.15  CPRI  0.396277  0.518367  0.558052  0.943038\n",
              "4           SVM 0.15  CPRI  0.410405  0.551020  0.563492  0.898734\n",
              "5     LSTM beta 0.15  CPRI  0.458824  0.695842  0.528814  0.624000\n",
              "6      GRU beta 0.15  CPRI  0.385593  0.608315  0.504155  0.728000\n",
              "7  XGBoost beta 0.15  CPRI  0.545455  0.754923  0.582090  0.624000\n",
              "8   logreg beta 0.15  CPRI  0.390977  0.599562  0.531969  0.832000\n",
              "9      svm beta 0.15  CPRI  0.458763  0.691466  0.557994  0.712000\n",
              "0           LSTM 0.1  CPRI  0.543860  0.689655  0.673913  0.885714\n",
              "1            GRU 0.1  CPRI  0.537143  0.682759  0.671429  0.895238\n",
              "2        XGBoost 0.1  CPRI  0.567742  0.710345  0.676923  0.838095\n",
              "3         Logreg 0.1  CPRI  0.513514  0.655172  0.655172  0.904762\n",
              "4            SVM 0.1  CPRI  0.550296  0.696552  0.678832  0.885714\n",
              "5      LSTM beta 0.1  CPRI  0.305882  0.463035  0.429752  0.722222\n",
              "6       GRU beta 0.1  CPRI  0.682540  0.809339  0.637037  0.597222\n",
              "7   XGBoost beta 0.1  CPRI  0.366071  0.603113  0.445652  0.569444\n",
              "8    logreg beta 0.1  CPRI  0.451613  0.673152  0.571429  0.777778\n",
              "9       svm beta 0.1  CPRI  0.439024  0.680934  0.467532  0.500000\n",
              "0           LSTM 0.2  CPRI  0.473958  0.600000  0.610738  0.858491\n",
              "1            GRU 0.2  CPRI  0.482517  0.617241  0.554217  0.650943\n",
              "2        XGBoost 0.2  CPRI  0.446154  0.562069  0.578073  0.820755\n",
              "3         Logreg 0.2  CPRI  0.419753  0.500000  0.584527  0.962264\n",
              "4            SVM 0.2  CPRI  0.457014  0.568966  0.617737  0.952830\n",
              "5      LSTM beta 0.2  CPRI  0.550000  0.754864  0.635838  0.753425\n",
              "6       GRU beta 0.2  CPRI  0.526316  0.739300  0.641711  0.821918\n",
              "7   XGBoost beta 0.2  CPRI  0.620000  0.809339  0.716763  0.849315\n",
              "8    logreg beta 0.2  CPRI  0.465278  0.677043  0.617512  0.917808\n",
              "9       svm beta 0.2  CPRI  0.596154  0.793774  0.700565  0.849315\n",
              "0          LSTM 0.15  CPRI  0.434426  0.524138  0.605714  1.000000\n",
              "1           GRU 0.15  CPRI  0.452381  0.565517  0.601266  0.896226\n",
              "2       XGBoost 0.15  CPRI  0.427807  0.541379  0.546075  0.754717\n",
              "3        Logreg 0.15  CPRI  0.431034  0.524138  0.591716  0.943396\n",
              "4           SVM 0.15  CPRI  0.450704  0.562069  0.601881  0.905660\n",
              "5     LSTM beta 0.15  CPRI  0.487603  0.704280  0.608247  0.808219\n",
              "6      GRU beta 0.15  CPRI  0.378378  0.540856  0.542636  0.958904\n",
              "7  XGBoost beta 0.15  CPRI  0.588785  0.789883  0.700000  0.863014\n",
              "8   logreg beta 0.15  CPRI  0.461538  0.673152  0.611111  0.904110\n",
              "9      svm beta 0.15  CPRI  0.628866  0.813230  0.717647  0.835616"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPLkChLoHUKx"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BFQRf6LSQxc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrOzpv5nVHfS"
      },
      "source": [
        "## FCX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKJb6Ao9VHfY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53a6f633-9c63-41e4-b4aa-87efd012a905"
      },
      "source": [
        "dfs = pd.read_csv(\"FCX.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "# Denoise prices\n",
        "dfs = denoise_data(dfs, '<CLOSE>')\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2768</td>\n",
              "      <td>US1.FCX</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>33.00</td>\n",
              "      <td>33.100</td>\n",
              "      <td>31.8700</td>\n",
              "      <td>32.8400</td>\n",
              "      <td>805504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2767</td>\n",
              "      <td>US1.FCX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>32.69</td>\n",
              "      <td>33.750</td>\n",
              "      <td>32.5300</td>\n",
              "      <td>32.5400</td>\n",
              "      <td>1359849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.FCX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>34.00</td>\n",
              "      <td>34.010</td>\n",
              "      <td>32.9200</td>\n",
              "      <td>33.1000</td>\n",
              "      <td>589350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.FCX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>33.98</td>\n",
              "      <td>34.015</td>\n",
              "      <td>32.8200</td>\n",
              "      <td>33.5700</td>\n",
              "      <td>910310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.FCX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>32.29</td>\n",
              "      <td>33.950</td>\n",
              "      <td>32.2900</td>\n",
              "      <td>33.9000</td>\n",
              "      <td>1099291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.FCX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>92.31</td>\n",
              "      <td>95.910</td>\n",
              "      <td>92.0002</td>\n",
              "      <td>95.5200</td>\n",
              "      <td>12656521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.FCX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>93.63</td>\n",
              "      <td>93.870</td>\n",
              "      <td>89.8500</td>\n",
              "      <td>91.3975</td>\n",
              "      <td>13153770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.FCX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>91.51</td>\n",
              "      <td>94.360</td>\n",
              "      <td>91.4500</td>\n",
              "      <td>93.6100</td>\n",
              "      <td>13105657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.FCX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>88.71</td>\n",
              "      <td>91.800</td>\n",
              "      <td>88.6100</td>\n",
              "      <td>91.1800</td>\n",
              "      <td>12610936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.FCX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>88.44</td>\n",
              "      <td>88.590</td>\n",
              "      <td>86.6700</td>\n",
              "      <td>87.2500</td>\n",
              "      <td>11292152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2769 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...  <HIGH>    <LOW>  <CLOSE>     <VOL>\n",
              "0      2768  US1.FCX     D  20211001  ...  33.100  31.8700  32.8400    805504\n",
              "1      2767  US1.FCX     D  20210930  ...  33.750  32.5300  32.5400   1359849\n",
              "2      2766  US1.FCX     D  20210929  ...  34.010  32.9200  33.1000    589350\n",
              "3      2765  US1.FCX     D  20210928  ...  34.015  32.8200  33.5700    910310\n",
              "4      2764  US1.FCX     D  20210927  ...  33.950  32.2900  33.9000   1099291\n",
              "...     ...      ...   ...       ...  ...     ...      ...      ...       ...\n",
              "2764      4  US1.FCX     D  20101008  ...  95.910  92.0002  95.5200  12656521\n",
              "2765      3  US1.FCX     D  20101007  ...  93.870  89.8500  91.3975  13153770\n",
              "2766      2  US1.FCX     D  20101006  ...  94.360  91.4500  93.6100  13105657\n",
              "2767      1  US1.FCX     D  20101005  ...  91.800  88.6100  91.1800  12610936\n",
              "2768      0  US1.FCX     D  20101004  ...  88.590  86.6700  87.2500  11292152\n",
              "\n",
              "[2769 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVRuZZzqVHfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "140fd34b-cb3f-41b3-b9f8-47b5306585cf"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"f0a65365-f5cf-4654-bfa6-68f975d3084d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"f0a65365-f5cf-4654-bfa6-68f975d3084d\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'f0a65365-f5cf-4654-bfa6-68f975d3084d',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [8.5, 8.76, 8.82, 8.86, 8.88, 9.17, 9.57, 9.64, 9.795, 10.06, 10.01, 10.33, 10.47, 10.45, 10.36, 10.41, 10.47, 10.76, 10.38, 10.08, 10.12, 9.71, 9.42, 9.53, 9.29, 9.075, 9.18, 9.13, 8.95, 8.7, 8.85, 8.835, 9.13, 9.14, 9.19, 9.25, 9.1, 8.98, 9.36, 9.88, 9.51, 9.745, 10.13, 10.09, 10.14, 10.07, 10.3, 10.7, 11.05, 11.46, 11.61, 11.57, 11.43, 11.94, 11.66, 11.49, 11.49, 11.15, 11.05, 11.12, 11.15, 11.015, 11.1, 10.95, 10.87, 11.22, 11.29, 11.3046, 11.44, 11.44, 11.6, 11.56, 11.49, 11.33, 11.34, 11.34, 11.385, 11.19, 11.13, 10.74, 10.655, 10.83, 10.59, 10.625, 10.61, 10.34, 10.19, 10.1, 10.32, 9.91, 9.7, 9.82, 9.98, 10.03, 10.08, 9.95, 10.06, 10.35, 10.2, 10.37, 10.66, 10.86, 10.87, 10.71, 11.37, 11.28, 11.38, 11.44, 11.73, 11.95, 11.51, 11.71, 12.32, 12.43, 12.4963, 12.201, 13.58, 13.46, 13.64, 14.0, 14.18, 14.15, 13.69, 13.7, 13.41, 13.5, 13.385, 13.74, 13.33, 13.57, 13.155, 13.125, 13.1, 12.89, 12.725, 12.72, 12.7, 12.38, 12.18, 12.92, 12.825, 12.68, 12.625, 12.35, 12.323, 12.66, 12.66, 12.36, 12.08, 12.31, 12.54, 12.84, 12.77, 12.86, 12.9, 13.16, 13.18, 13.25, 13.22, 13.01, 13.35, 13.08, 12.28, 12.15, 12.2851, 11.49, 11.53, 11.61, 11.87, 11.91, 11.86, 11.86, 11.51, 11.64, 11.27, 10.45, 10.27, 11.24, 10.69, 12.3, 12.38, 12.55, 12.12, 11.89, 11.57, 11.555, 11.6, 11.51, 11.5, 11.21, 11.0, 10.81, 10.07, 10.37, 10.31, 10.46, 10.675, 10.4, 9.53, 10.05, 10.23, 10.185, 10.6, 10.6, 10.56, 10.92, 11.07, 10.76, 10.69, 10.93, 11.145, 11.49, 12.09, 11.94, 11.82, 11.96, 11.24, 11.09, 10.77, 11.4, 10.88, 11.515, 11.96, 11.96, 11.54, 11.25, 11.17, 11.35, 11.93, 12.31, 12.21, 12.23, 12.24, 12.28, 11.65, 11.415, 11.09, 11.5, 11.25, 10.71, 11.6, 12.0, 12.01, 12.14, 12.4, 12.67, 12.82, 12.73, 13.23, 12.77, 13.185, 13.295, 13.27, 13.63, 13.94, 13.89, 13.7, 13.92, 13.87, 14.01, 14.49, 14.24, 14.6, 14.39, 14.3, 14.01, 13.73, 13.68, 13.58, 13.64, 13.045, 13.2, 13.18, 13.57, 13.68, 13.48, 14.05, 14.15, 14.67, 14.66, 14.73, 14.47, 14.18, 14.53, 14.32, 14.12, 13.96, 13.78, 13.67, 14.8, 14.93, 15.105, 15.31, 15.39, 15.37, 15.4, 15.72, 15.43, 15.58, 16.5, 16.085, 15.99, 15.97, 15.8699, 16.07, 15.82, 15.75, 15.8, 17.09, 16.77, 16.77, 16.8, 17.29, 17.4, 18.11, 18.105, 17.51, 16.98, 17.07, 17.15, 17.27, 16.8, 16.9, 16.25, 15.74, 16.44, 16.35, 16.3, 16.26, 16.805, 16.9, 17.58, 17.82, 17.855, 18.18, 18.03, 17.93, 18.2, 17.66, 17.215, 17.12, 16.9, 17.13, 16.65, 17.095, 17.39, 17.35, 17.16, 16.7, 16.86, 16.77, 16.65, 15.96, 16.2, 16.26, 16.21, 15.67, 15.3, 15.26, 15.42, 15.22, 15.11, 14.93, 15.22, 15.325, 15.63, 15.37, 16.075, 18.8, 19.37, 19.58, 19.17, 18.315, 18.15, 17.865, 17.85, 17.97, 18.19, 17.43, 17.32, 18.105, 17.34, 17.38, 17.14, 17.56, 16.76, 17.24, 17.76, 17.52, 18.06, 19.19, 18.485, 18.055, 18.35, 18.41, 18.66, 18.49, 18.555, 18.43, 17.86, 18.12, 18.695, 18.2, 18.32, 18.52, 18.6, 19.08, 19.54, 19.56, 19.16, 18.73, 18.43, 18.73, 19.12, 19.12, 17.79, 17.58, 17.58, 17.16, 17.87, 18.725, 17.66, 17.97, 19.445, 19.5, 19.13, 19.64, 19.54, 19.83, 19.6, 19.56, 20.0, 19.96, 19.42, 19.76, 19.33, 19.74, 19.88, 19.51, 18.93, 19.81, 19.89, 19.74, 19.465, 19.775, 18.97, 19.26, 18.68, 18.66, 18.105, 18.19, 17.66, 17.33, 17.36, 16.99, 16.185, 16.34, 15.715, 15.03, 14.98, 14.85, 14.35, 14.115, 14.29, 14.12, 13.92, 14.0, 14.21, 14.1, 14.3461, 14.24, 14.15, 13.95, 13.86, 13.615, 13.63, 13.8, 14.43, 14.41, 14.715, 14.87, 14.54, 14.6358, 14.14, 14.23, 14.385, 13.98, 13.95, 14.13, 14.685, 14.71, 15.23, 14.81, 14.825, 14.82, 14.83, 14.925, 15.27, 14.75, 14.52, 14.425, 14.41, 14.33, 14.325, 14.75, 14.53, 14.65, 14.33, 14.035, 14.42, 14.0, 14.0, 13.89, 14.06, 13.98, 14.17, 14.49, 14.125, 13.88, 13.865, 13.93, 14.305, 14.39, 14.23, 15.21, 15.06, 14.78, 15.07, 14.77, 14.57, 15.2243, 15.52, 15.28, 15.48, 15.3, 15.07, 14.72, 14.1484, 14.2, 14.76, 13.965, 14.145, 13.945, 14.155, 14.58, 14.46, 14.69, 14.41, 14.37, 14.38, 14.485, 14.61, 14.595, 14.49, 15.06, 14.87, 12.9522, 13.015, 13.05, 13.12, 13.01, 13.01, 12.6, 12.49, 12.28, 12.585, 12.525, 11.905, 11.895, 12.16, 12.2, 12.0, 12.085, 11.99, 11.725, 11.83, 11.75, 11.45, 11.22, 11.245, 11.59, 11.415, 11.495, 12.085, 12.35, 12.34, 12.35, 12.07, 11.73, 11.84, 11.42, 11.29, 11.46, 11.485, 11.67, 11.67, 11.7, 11.9, 11.89, 11.83, 11.73, 11.29, 11.42, 11.76, 11.75, 11.5, 11.68, 11.72, 11.64, 11.66, 11.8, 11.69, 12.015, 12.715, 12.535, 12.74, 13.02, 13.5, 13.09, 12.21, 12.225, 12.51, 12.355, 12.48, 12.76, 12.71, 12.87, 13.615, 13.52, 13.37, 13.55, 13.45, 13.53, 13.245, 13.36, 13.305, 12.72, 12.57, 12.22, 12.8, 12.84, 12.86, 12.57, 12.74, 12.75, 12.81, 12.89, 12.27, 12.57, 12.375, 12.41, 12.435, 12.68, 12.89, 13.2, 13.3, 13.98, 13.39, 13.26, 13.26, 13.47, 13.73, 14.12, 14.905, 15.07, 15.36, 15.9, 15.95, 15.8, 15.38, 15.53, 15.51, 15.995, 15.82, 16.8, 16.85, 16.65, 16.24, 16.36, 15.85, 16.51, 17.05, 15.71, 15.5, 15.25, 15.23, 15.05, 15.19, 15.27, 15.86, 15.55, 14.68, 14.9, 14.62, 14.82, 13.77, 13.19, 13.479, 13.5648, 13.77, 13.8, 13.8, 14.04, 14.16, 13.55, 13.87, 14.61, 14.65, 15.04, 15.365, 15.747, 15.6, 15.42, 15.884, 15.87, 15.42, 15.03, 15.34, 14.97, 15.78, 15.95, 16.2099, 15.11, 14.51, 13.74, 13.79, 13.8, 14.0, 13.925, 13.93, 13.8, 13.065, 12.1, 11.28, 11.05, 10.73, 10.61, 11.15, 11.19, 10.94, 10.695, 10.575, 10.545, 10.18, 10.03, 10.21, 10.02, 9.73, 9.53, 9.66, 9.645, 10.05, 9.87, 10.2, 10.135, 10.31, 10.675, 10.385, 10.69, 10.87, 10.7, 10.89, 10.21, 10.52, 10.64, 10.97, 10.56, 9.96, 10.0, 9.93, 9.78, 9.8, 10.141, 11.08, 10.285, 10.62, 10.6, 10.715, 10.48, 10.34, 10.28, 10.55, 10.98, 10.89, 11.08, 11.09, 11.98, 11.82, 11.97, 12.22, 11.98, 12.07, 12.18, 11.81, 12.1, 12.02, 12.09, 12.29, 12.23, 12.311, 12.38, 12.4, 12.4799, 12.96, 12.9399, 13.06, 12.68, 12.385, 12.67, 12.87, 12.29, 12.45, 13.15, 13.1, 12.95, 12.95, 12.91, 11.65, 11.18, 10.6617, 10.88, 10.51, 11.36, 11.15, 10.7682, 10.68, 10.13, 10.5701, 11.765, 11.496, 11.62, 11.55, 11.14, 10.78, 11.0, 10.21, 10.41, 10.37, 10.91, 11.5889, 11.23, 11.655, 11.1, 10.66, 10.6, 11.08, 11.14, 11.35, 11.65, 11.1, 11.39, 11.07, 10.98, 10.54, 11.51, 11.04, 10.4, 11.04, 11.615, 10.85, 10.5101, 11.79, 11.28, 11.79, 12.01, 13.56, 14.01, 12.64, 12.65, 11.48, 11.35, 11.66, 11.51, 12.35, 12.01, 11.03, 10.85, 10.73, 10.8199, 10.42, 9.76, 9.32, 8.85, 9.61, 9.34, 9.42, 9.88, 10.33, 10.28, 10.15, 10.15, 10.11, 9.75, 10.99, 10.8, 10.77, 10.89, 10.23, 9.27, 9.97, 9.54, 9.59, 9.14, 8.655, 9.86, 9.73, 9.11, 8.98, 7.76, 7.6275, 7.42, 7.11, 7.19, 7.24, 7.92, 6.92, 7.14, 7.16, 6.37, 5.52, 4.87, 4.98, 5.0, 5.26, 5.67, 5.72, 4.84, 4.35, 4.725, 4.59, 4.41, 4.6589, 4.19, 3.94, 3.93, 4.32, 4.06, 3.96, 4.35, 4.21, 3.73, 4.11, 4.305, 5.405, 5.6, 6.175, 6.71, 6.5463, 6.76, 6.761, 6.97, 6.84, 7.57, 7.4599, 6.42, 6.24, 6.2, 6.12, 6.7, 6.53, 6.47, 6.92, 7.36, 6.99, 6.74, 7.23, 7.84, 7.69, 7.84, 8.33, 8.18, 8.05, 8.11, 8.31, 8.0, 8.25, 8.41, 8.77, 8.4, 8.86, 8.672, 8.77, 9.3001, 9.83, 10.49, 10.75, 11.48, 12.01, 12.4, 11.82, 11.76, 11.62, 11.77, 11.6, 12.0228, 12.13, 12.01, 11.95, 12.24, 12.03, 12.45, 13.0, 13.05, 12.73, 12.94, 13.48, 13.46, 13.015, 11.84, 11.185, 10.63, 9.82, 9.68, 9.115, 8.91, 9.79, 9.99, 9.99, 10.6, 10.53, 10.7399, 12.05, 11.84, 11.29, 11.17, 11.39, 11.28, 10.74, 10.4, 9.7, 10.12, 9.89, 9.76, 10.64, 10.49, 10.2, 7.92, 8.26, 8.67, 9.58, 9.7199, 9.73, 9.92, 10.23, 10.02, 10.07, 10.255, 10.21, 11.66, 10.53, 11.2, 10.93, 11.04, 11.2, 11.75, 11.8532, 12.5, 12.33, 11.36, 12.29, 13.64, 15.06, 15.7, 15.03, 15.88, 16.31, 16.41, 17.11, 16.92, 16.73, 16.775, 16.52, 17.25, 17.84, 18.385, 18.4, 18.62, 19.38, 19.95, 19.74, 20.55, 20.11, 19.41, 19.76, 20.12, 20.02, 19.79, 19.59, 19.83, 19.94, 20.55, 20.02, 19.42, 19.64, 19.53, 19.87, 20.46, 19.34, 19.66, 20.16, 20.14, 20.08, 21.0, 21.36, 21.08, 21.28, 22.14, 22.83, 22.79, 22.6, 22.72, 22.965, 23.29, 22.91, 23.27, 23.34, 23.4, 23.6601, 23.28, 22.9, 22.69, 21.82, 20.82, 20.07, 20.57, 20.24, 20.66, 20.66, 20.83, 20.66, 18.97, 18.27, 18.3, 18.79, 18.83, 18.975, 19.16, 19.01, 18.97, 18.955, 19.38, 18.79, 19.51, 19.1, 19.19, 19.34, 18.4, 17.265, 18.23, 17.42, 17.96, 18.14, 18.89, 18.82, 18.85, 19.615, 19.435, 20.18, 20.88, 20.99, 21.13, 21.635, 21.1, 21.49, 21.23, 20.58, 21.28, 21.1, 21.29, 20.97, 20.3, 19.47, 18.6045, 18.7, 19.51, 18.86, 19.57, 18.29, 18.985, 17.46, 16.81, 16.835, 17.42, 18.3772, 19.54, 19.24, 20.02, 19.84, 19.26, 19.23, 18.32, 18.74, 21.06, 23.06, 23.45, 23.375, 22.84, 22.53, 22.145, 23.55, 23.36, 23.55, 23.26, 23.52, 22.8, 22.83, 22.545, 23.405, 22.82, 22.13, 21.2, 21.0399, 21.79, 22.98, 23.96, 25.13, 25.13, 26.0, 26.43, 26.68, 25.88, 26.1999, 26.99, 29.33, 29.28, 29.1, 29.58, 28.58, 28.08, 28.19, 28.42, 28.31, 27.9099, 28.55, 28.44, 28.0101, 28.33, 27.57, 27.13, 27.79, 27.81, 28.49, 28.09, 28.99, 29.01, 30.27, 30.8, 30.945, 30.9, 31.51, 30.75, 30.3499, 30.24, 30.03, 30.8, 30.72, 30.66, 31.13, 32.29, 31.59, 32.5, 32.31, 32.08, 31.87, 32.66, 32.41, 32.86, 32.66, 33.27, 32.96, 33.17, 34.05, 34.31, 34.51, 34.9, 34.31, 34.25, 34.56, 34.56, 34.42, 34.9, 35.01, 35.21, 35.29, 35.48, 36.37, 36.15, 36.43, 36.38, 36.48, 36.44, 36.67, 36.975, 36.29, 36.36, 36.1, 36.04, 36.4, 36.89, 36.89, 36.49, 36.14, 36.6962, 36.92, 37.24, 36.79, 37.23, 37.89, 37.91, 38.07, 37.97, 37.53, 38.56, 38.72, 38.665, 38.35, 38.51, 38.83, 38.29, 38.69, 38.72, 38.87, 39.05, 38.75, 38.6799, 38.44, 37.843, 36.85, 36.49, 36.06, 35.69, 35.78, 35.65, 36.04, 34.86, 34.63, 34.8, 34.05, 34.01, 33.98, 33.655, 34.09, 34.37, 34.65, 34.94, 34.765, 34.51, 34.08, 34.11, 34.04, 34.275, 33.81, 33.96, 34.44, 34.305, 34.26, 34.44, 35.06, 34.99, 35.22, 35.495, 35.57, 34.9, 33.91, 33.83, 34.0, 33.85, 34.07, 34.68, 34.16, 34.38, 34.44, 33.9, 34.01, 33.95, 33.5, 33.3, 32.97, 33.01, 33.0, 33.02, 33.29, 32.56, 33.19, 33.99, 33.62, 33.08, 33.33, 33.4, 33.33, 32.91, 33.06, 32.86, 32.41, 31.68, 32.08, 31.61, 32.29, 30.89, 31.08, 31.65, 31.5, 31.06, 30.63, 30.78, 30.72, 31.38, 32.17, 33.82, 33.92, 33.51, 32.82, 32.62, 33.46, 33.425, 32.76, 33.21, 33.35, 33.76, 33.17, 33.74, 33.75, 33.17, 32.77, 33.18, 32.23, 32.35, 31.54, 31.08, 30.95, 31.09, 32.4, 32.42, 32.35, 32.565, 32.4, 32.76, 33.67, 34.51, 35.26, 36.18, 36.91, 36.61, 36.11, 35.62, 36.18, 35.72, 36.19, 36.66, 37.02, 37.31, 37.62, 37.7499, 37.68, 37.51, 36.97, 36.31, 35.74, 35.68, 35.2, 35.19, 34.9, 34.59, 34.35, 34.3, 34.46, 34.89, 34.64, 34.25, 34.28, 34.53, 34.21, 34.25, 34.6999, 34.68, 35.33, 35.8, 36.1078, 36.145, 36.18, 36.4, 36.4849, 36.8375, 36.35, 36.32, 35.88, 36.47, 36.59, 35.92, 37.05, 37.18, 37.41, 36.78, 36.7501, 37.27, 37.68, 37.27, 37.45, 37.41, 36.64]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f0a65365-f5cf-4654-bfa6-68f975d3084d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"685da041-ffd5-44f5-8356-4fa1446d0ecc\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"685da041-ffd5-44f5-8356-4fa1446d0ecc\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '685da041-ffd5-44f5-8356-4fa1446d0ecc',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('685da041-ffd5-44f5-8356-4fa1446d0ecc');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYJvaA2uVHfZ"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABW1Wkq-VHfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d2e284-dd6b-4514-9065-82ac5031691b"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.05, .1, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"FCX\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6893 - accuracy: 0.5477 - val_loss: 0.7204 - val_accuracy: 0.3918\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6373 - accuracy: 0.6376 - val_loss: 0.7106 - val_accuracy: 0.5143\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5980 - accuracy: 0.6799 - val_loss: 0.6251 - val_accuracy: 0.6327\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5338 - accuracy: 0.7463 - val_loss: 0.5662 - val_accuracy: 0.7041\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5255 - accuracy: 0.7510 - val_loss: 0.5858 - val_accuracy: 0.6816\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6122 - accuracy: 0.6530 - val_loss: 0.5663 - val_accuracy: 0.7143\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5236 - accuracy: 0.7557 - val_loss: 0.5753 - val_accuracy: 0.6816\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5053 - accuracy: 0.7544 - val_loss: 0.5878 - val_accuracy: 0.6633\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5089 - accuracy: 0.7631 - val_loss: 0.5365 - val_accuracy: 0.7388\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5032 - accuracy: 0.7591 - val_loss: 0.6367 - val_accuracy: 0.6224\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.740754\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.744835\n",
            "[2]\tvalidation_0-auc:0.754046\n",
            "[3]\tvalidation_0-auc:0.755628\n",
            "[4]\tvalidation_0-auc:0.760312\n",
            "[5]\tvalidation_0-auc:0.760696\n",
            "[6]\tvalidation_0-auc:0.773079\n",
            "[7]\tvalidation_0-auc:0.760976\n",
            "[8]\tvalidation_0-auc:0.759586\n",
            "[9]\tvalidation_0-auc:0.759351\n",
            "[10]\tvalidation_0-auc:0.758381\n",
            "[11]\tvalidation_0-auc:0.757507\n",
            "[12]\tvalidation_0-auc:0.758704\n",
            "[13]\tvalidation_0-auc:0.758616\n",
            "[14]\tvalidation_0-auc:0.757987\n",
            "[15]\tvalidation_0-auc:0.758879\n",
            "[16]\tvalidation_0-auc:0.758354\n",
            "[17]\tvalidation_0-auc:0.757856\n",
            "[18]\tvalidation_0-auc:0.757279\n",
            "[19]\tvalidation_0-auc:0.757341\n",
            "[20]\tvalidation_0-auc:0.75298\n",
            "[21]\tvalidation_0-auc:0.753067\n",
            "[22]\tvalidation_0-auc:0.753015\n",
            "[23]\tvalidation_0-auc:0.752945\n",
            "[24]\tvalidation_0-auc:0.757795\n",
            "[25]\tvalidation_0-auc:0.758389\n",
            "[26]\tvalidation_0-auc:0.75818\n",
            "[27]\tvalidation_0-auc:0.756502\n",
            "[28]\tvalidation_0-auc:0.762094\n",
            "[29]\tvalidation_0-auc:0.762872\n",
            "[30]\tvalidation_0-auc:0.7624\n",
            "[31]\tvalidation_0-auc:0.762453\n",
            "[32]\tvalidation_0-auc:0.762418\n",
            "[33]\tvalidation_0-auc:0.762732\n",
            "[34]\tvalidation_0-auc:0.76275\n",
            "[35]\tvalidation_0-auc:0.759683\n",
            "[36]\tvalidation_0-auc:0.759683\n",
            "[37]\tvalidation_0-auc:0.759683\n",
            "[38]\tvalidation_0-auc:0.761535\n",
            "[39]\tvalidation_0-auc:0.762619\n",
            "[40]\tvalidation_0-auc:0.763493\n",
            "[41]\tvalidation_0-auc:0.763528\n",
            "[42]\tvalidation_0-auc:0.763528\n",
            "[43]\tvalidation_0-auc:0.763702\n",
            "[44]\tvalidation_0-auc:0.76386\n",
            "[45]\tvalidation_0-auc:0.763982\n",
            "[46]\tvalidation_0-auc:0.765066\n",
            "[47]\tvalidation_0-auc:0.766656\n",
            "[48]\tvalidation_0-auc:0.767207\n",
            "[49]\tvalidation_0-auc:0.767259\n",
            "[50]\tvalidation_0-auc:0.768483\n",
            "[51]\tvalidation_0-auc:0.768133\n",
            "[52]\tvalidation_0-auc:0.767329\n",
            "[53]\tvalidation_0-auc:0.767941\n",
            "[54]\tvalidation_0-auc:0.768561\n",
            "[55]\tvalidation_0-auc:0.768762\n",
            "[56]\tvalidation_0-auc:0.768587\n",
            "Stopping. Best iteration:\n",
            "[6]\tvalidation_0-auc:0.773079\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6870 - accuracy: 0.5635 - val_loss: 0.7365 - val_accuracy: 0.3917\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6684 - accuracy: 0.5882 - val_loss: 0.6831 - val_accuracy: 0.4551\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5966 - accuracy: 0.6905 - val_loss: 0.6233 - val_accuracy: 0.6171\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5687 - accuracy: 0.7152 - val_loss: 0.6959 - val_accuracy: 0.5536\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5520 - accuracy: 0.7152 - val_loss: 0.6308 - val_accuracy: 0.6193\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6768 - accuracy: 0.5724 - val_loss: 0.6609 - val_accuracy: 0.6389\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5956 - accuracy: 0.6788 - val_loss: 0.6684 - val_accuracy: 0.5383\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5536 - accuracy: 0.7241 - val_loss: 0.6669 - val_accuracy: 0.5646\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5415 - accuracy: 0.7255 - val_loss: 0.6147 - val_accuracy: 0.6149\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5348 - accuracy: 0.7433 - val_loss: 0.6474 - val_accuracy: 0.5886\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.628602\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.689572\n",
            "[2]\tvalidation_0-auc:0.694355\n",
            "[3]\tvalidation_0-auc:0.685463\n",
            "[4]\tvalidation_0-auc:0.678409\n",
            "[5]\tvalidation_0-auc:0.707879\n",
            "[6]\tvalidation_0-auc:0.70589\n",
            "[7]\tvalidation_0-auc:0.708412\n",
            "[8]\tvalidation_0-auc:0.695742\n",
            "[9]\tvalidation_0-auc:0.695089\n",
            "[10]\tvalidation_0-auc:0.702162\n",
            "[11]\tvalidation_0-auc:0.706865\n",
            "[12]\tvalidation_0-auc:0.713155\n",
            "[13]\tvalidation_0-auc:0.720208\n",
            "[14]\tvalidation_0-auc:0.719012\n",
            "[15]\tvalidation_0-auc:0.722941\n",
            "[16]\tvalidation_0-auc:0.729512\n",
            "[17]\tvalidation_0-auc:0.72682\n",
            "[18]\tvalidation_0-auc:0.731623\n",
            "[19]\tvalidation_0-auc:0.727865\n",
            "[20]\tvalidation_0-auc:0.73323\n",
            "[21]\tvalidation_0-auc:0.717817\n",
            "[22]\tvalidation_0-auc:0.720047\n",
            "[23]\tvalidation_0-auc:0.724288\n",
            "[24]\tvalidation_0-auc:0.724288\n",
            "[25]\tvalidation_0-auc:0.725363\n",
            "[26]\tvalidation_0-auc:0.728859\n",
            "[27]\tvalidation_0-auc:0.728739\n",
            "[28]\tvalidation_0-auc:0.731673\n",
            "[29]\tvalidation_0-auc:0.72279\n",
            "[30]\tvalidation_0-auc:0.713135\n",
            "[31]\tvalidation_0-auc:0.70809\n",
            "[32]\tvalidation_0-auc:0.712813\n",
            "[33]\tvalidation_0-auc:0.712662\n",
            "[34]\tvalidation_0-auc:0.712662\n",
            "[35]\tvalidation_0-auc:0.712783\n",
            "[36]\tvalidation_0-auc:0.717787\n",
            "[37]\tvalidation_0-auc:0.725483\n",
            "[38]\tvalidation_0-auc:0.72474\n",
            "[39]\tvalidation_0-auc:0.721565\n",
            "[40]\tvalidation_0-auc:0.721565\n",
            "[41]\tvalidation_0-auc:0.721565\n",
            "[42]\tvalidation_0-auc:0.721565\n",
            "[43]\tvalidation_0-auc:0.717375\n",
            "[44]\tvalidation_0-auc:0.715596\n",
            "[45]\tvalidation_0-auc:0.714833\n",
            "[46]\tvalidation_0-auc:0.714139\n",
            "[47]\tvalidation_0-auc:0.713094\n",
            "[48]\tvalidation_0-auc:0.713938\n",
            "[49]\tvalidation_0-auc:0.713938\n",
            "[50]\tvalidation_0-auc:0.714461\n",
            "[51]\tvalidation_0-auc:0.714461\n",
            "[52]\tvalidation_0-auc:0.714461\n",
            "[53]\tvalidation_0-auc:0.710201\n",
            "[54]\tvalidation_0-auc:0.710201\n",
            "[55]\tvalidation_0-auc:0.708894\n",
            "[56]\tvalidation_0-auc:0.70385\n",
            "[57]\tvalidation_0-auc:0.698987\n",
            "[58]\tvalidation_0-auc:0.696144\n",
            "[59]\tvalidation_0-auc:0.696114\n",
            "[60]\tvalidation_0-auc:0.694184\n",
            "[61]\tvalidation_0-auc:0.693823\n",
            "[62]\tvalidation_0-auc:0.693823\n",
            "[63]\tvalidation_0-auc:0.693823\n",
            "[64]\tvalidation_0-auc:0.693762\n",
            "[65]\tvalidation_0-auc:0.699168\n",
            "[66]\tvalidation_0-auc:0.702223\n",
            "[67]\tvalidation_0-auc:0.702002\n",
            "[68]\tvalidation_0-auc:0.693501\n",
            "[69]\tvalidation_0-auc:0.694375\n",
            "[70]\tvalidation_0-auc:0.691723\n",
            "Stopping. Best iteration:\n",
            "[20]\tvalidation_0-auc:0.73323\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.6816326530612244 | 0.5708661417322834 | 0.7552083333333334 | 0.6502242152466368 |\n",
            "|     GRU 0.1      | 0.6224489795918368 | 0.5103857566765578 | 0.8958333333333334 | 0.6502835538752363 |\n",
            "|   XGBoost 0.1    | 0.6489795918367347 | 0.5331125827814569 | 0.8385416666666666 | 0.6518218623481782 |\n",
            "|    Logreg 0.1    | 0.6551020408163265 | 0.5384615384615384 | 0.8385416666666666 | 0.6558044806517311 |\n",
            "|     SVM 0.1      | 0.6632653061224489 | 0.5454545454545454 |      0.84375       | 0.6625766871165644 |\n",
            "|  LSTM beta 0.1   | 0.6192560175054704 | 0.508833922261484  | 0.8044692737430168 | 0.6233766233766234 |\n",
            "|   GRU beta 0.1   | 0.5886214442013129 | 0.4840989399293286 | 0.7653631284916201 | 0.5930735930735931 |\n",
            "| XGBoost beta 0.1 | 0.5820568927789934 | 0.4794520547945205 | 0.7821229050279329 | 0.5944798301486199 |\n",
            "| logreg beta 0.1  | 0.6214442013129103 | 0.5103448275862069 | 0.8268156424581006 | 0.6311300639658849 |\n",
            "|   svm beta 0.1   | 0.6345733041575492 | 0.5211267605633803 | 0.8268156424581006 | 0.6393088552915767 |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6466 - accuracy: 0.6369 - val_loss: 0.6606 - val_accuracy: 0.5837\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5746 - accuracy: 0.7007 - val_loss: 0.6237 - val_accuracy: 0.6020\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.5729 - accuracy: 0.7020 - val_loss: 0.6466 - val_accuracy: 0.6020\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5562 - accuracy: 0.6899 - val_loss: 0.6513 - val_accuracy: 0.6143\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5565 - accuracy: 0.7047 - val_loss: 0.6644 - val_accuracy: 0.5673\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6257 - accuracy: 0.6537 - val_loss: 0.7340 - val_accuracy: 0.5347\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5582 - accuracy: 0.7034 - val_loss: 0.6223 - val_accuracy: 0.6245\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5318 - accuracy: 0.7336 - val_loss: 0.6284 - val_accuracy: 0.6061\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5324 - accuracy: 0.7201 - val_loss: 0.6151 - val_accuracy: 0.6224\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5357 - accuracy: 0.7248 - val_loss: 0.6404 - val_accuracy: 0.5959\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.659647\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.687099\n",
            "[2]\tvalidation_0-auc:0.685121\n",
            "[3]\tvalidation_0-auc:0.687008\n",
            "[4]\tvalidation_0-auc:0.704519\n",
            "[5]\tvalidation_0-auc:0.701973\n",
            "[6]\tvalidation_0-auc:0.702432\n",
            "[7]\tvalidation_0-auc:0.704335\n",
            "[8]\tvalidation_0-auc:0.704836\n",
            "[9]\tvalidation_0-auc:0.708459\n",
            "[10]\tvalidation_0-auc:0.708225\n",
            "[11]\tvalidation_0-auc:0.708208\n",
            "[12]\tvalidation_0-auc:0.708392\n",
            "[13]\tvalidation_0-auc:0.708225\n",
            "[14]\tvalidation_0-auc:0.709076\n",
            "[15]\tvalidation_0-auc:0.709293\n",
            "[16]\tvalidation_0-auc:0.709143\n",
            "[17]\tvalidation_0-auc:0.709435\n",
            "[18]\tvalidation_0-auc:0.709435\n",
            "[19]\tvalidation_0-auc:0.711956\n",
            "[20]\tvalidation_0-auc:0.712006\n",
            "[21]\tvalidation_0-auc:0.713358\n",
            "[22]\tvalidation_0-auc:0.710387\n",
            "[23]\tvalidation_0-auc:0.709969\n",
            "[24]\tvalidation_0-auc:0.707791\n",
            "[25]\tvalidation_0-auc:0.703451\n",
            "[26]\tvalidation_0-auc:0.705721\n",
            "[27]\tvalidation_0-auc:0.703968\n",
            "[28]\tvalidation_0-auc:0.704093\n",
            "[29]\tvalidation_0-auc:0.705629\n",
            "[30]\tvalidation_0-auc:0.705746\n",
            "[31]\tvalidation_0-auc:0.705746\n",
            "[32]\tvalidation_0-auc:0.705946\n",
            "[33]\tvalidation_0-auc:0.705946\n",
            "[34]\tvalidation_0-auc:0.700212\n",
            "[35]\tvalidation_0-auc:0.700846\n",
            "[36]\tvalidation_0-auc:0.70098\n",
            "[37]\tvalidation_0-auc:0.698317\n",
            "[38]\tvalidation_0-auc:0.699027\n",
            "[39]\tvalidation_0-auc:0.699786\n",
            "[40]\tvalidation_0-auc:0.699786\n",
            "[41]\tvalidation_0-auc:0.699786\n",
            "[42]\tvalidation_0-auc:0.698868\n",
            "[43]\tvalidation_0-auc:0.696898\n",
            "[44]\tvalidation_0-auc:0.695179\n",
            "[45]\tvalidation_0-auc:0.693343\n",
            "[46]\tvalidation_0-auc:0.693159\n",
            "[47]\tvalidation_0-auc:0.693226\n",
            "[48]\tvalidation_0-auc:0.693226\n",
            "[49]\tvalidation_0-auc:0.692717\n",
            "[50]\tvalidation_0-auc:0.693326\n",
            "[51]\tvalidation_0-auc:0.693326\n",
            "[52]\tvalidation_0-auc:0.691815\n",
            "[53]\tvalidation_0-auc:0.691206\n",
            "[54]\tvalidation_0-auc:0.68982\n",
            "[55]\tvalidation_0-auc:0.689002\n",
            "[56]\tvalidation_0-auc:0.688902\n",
            "[57]\tvalidation_0-auc:0.688744\n",
            "[58]\tvalidation_0-auc:0.688727\n",
            "[59]\tvalidation_0-auc:0.68881\n",
            "[60]\tvalidation_0-auc:0.687225\n",
            "[61]\tvalidation_0-auc:0.686749\n",
            "[62]\tvalidation_0-auc:0.686832\n",
            "[63]\tvalidation_0-auc:0.688318\n",
            "[64]\tvalidation_0-auc:0.687867\n",
            "[65]\tvalidation_0-auc:0.686574\n",
            "[66]\tvalidation_0-auc:0.684829\n",
            "[67]\tvalidation_0-auc:0.684529\n",
            "[68]\tvalidation_0-auc:0.684529\n",
            "[69]\tvalidation_0-auc:0.683844\n",
            "[70]\tvalidation_0-auc:0.683327\n",
            "[71]\tvalidation_0-auc:0.683293\n",
            "Stopping. Best iteration:\n",
            "[21]\tvalidation_0-auc:0.713358\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6533 - accuracy: 0.6424 - val_loss: 0.6272 - val_accuracy: 0.7462\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5899 - accuracy: 0.6863 - val_loss: 0.6376 - val_accuracy: 0.5361\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.5568 - accuracy: 0.7056 - val_loss: 0.5845 - val_accuracy: 0.6630\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.5269 - accuracy: 0.7220 - val_loss: 0.5578 - val_accuracy: 0.6783\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5151 - accuracy: 0.7406 - val_loss: 0.5721 - val_accuracy: 0.7309\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5966 - accuracy: 0.6767 - val_loss: 0.6150 - val_accuracy: 0.5667\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5226 - accuracy: 0.7392 - val_loss: 0.5496 - val_accuracy: 0.7330\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5065 - accuracy: 0.7474 - val_loss: 0.7274 - val_accuracy: 0.5580\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5002 - accuracy: 0.7509 - val_loss: 0.5735 - val_accuracy: 0.6455\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5076 - accuracy: 0.7433 - val_loss: 0.5760 - val_accuracy: 0.6477\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.768663\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.688724\n",
            "[2]\tvalidation_0-auc:0.692387\n",
            "[3]\tvalidation_0-auc:0.729097\n",
            "[4]\tvalidation_0-auc:0.705069\n",
            "[5]\tvalidation_0-auc:0.700252\n",
            "[6]\tvalidation_0-auc:0.70804\n",
            "[7]\tvalidation_0-auc:0.690118\n",
            "[8]\tvalidation_0-auc:0.697367\n",
            "[9]\tvalidation_0-auc:0.692435\n",
            "[10]\tvalidation_0-auc:0.674762\n",
            "[11]\tvalidation_0-auc:0.689002\n",
            "[12]\tvalidation_0-auc:0.688541\n",
            "[13]\tvalidation_0-auc:0.67458\n",
            "[14]\tvalidation_0-auc:0.669599\n",
            "[15]\tvalidation_0-auc:0.679743\n",
            "[16]\tvalidation_0-auc:0.692993\n",
            "[17]\tvalidation_0-auc:0.691185\n",
            "[18]\tvalidation_0-auc:0.690916\n",
            "[19]\tvalidation_0-auc:0.687858\n",
            "[20]\tvalidation_0-auc:0.685079\n",
            "[21]\tvalidation_0-auc:0.688897\n",
            "[22]\tvalidation_0-auc:0.68955\n",
            "[23]\tvalidation_0-auc:0.69032\n",
            "[24]\tvalidation_0-auc:0.692589\n",
            "[25]\tvalidation_0-auc:0.692666\n",
            "[26]\tvalidation_0-auc:0.694262\n",
            "[27]\tvalidation_0-auc:0.694627\n",
            "[28]\tvalidation_0-auc:0.689973\n",
            "[29]\tvalidation_0-auc:0.690166\n",
            "[30]\tvalidation_0-auc:0.686022\n",
            "[31]\tvalidation_0-auc:0.68606\n",
            "[32]\tvalidation_0-auc:0.687925\n",
            "[33]\tvalidation_0-auc:0.685118\n",
            "[34]\tvalidation_0-auc:0.685983\n",
            "[35]\tvalidation_0-auc:0.686349\n",
            "[36]\tvalidation_0-auc:0.683185\n",
            "[37]\tvalidation_0-auc:0.680512\n",
            "[38]\tvalidation_0-auc:0.680512\n",
            "[39]\tvalidation_0-auc:0.680512\n",
            "[40]\tvalidation_0-auc:0.681743\n",
            "[41]\tvalidation_0-auc:0.682483\n",
            "[42]\tvalidation_0-auc:0.683964\n",
            "[43]\tvalidation_0-auc:0.685868\n",
            "[44]\tvalidation_0-auc:0.686022\n",
            "[45]\tvalidation_0-auc:0.686445\n",
            "[46]\tvalidation_0-auc:0.681676\n",
            "[47]\tvalidation_0-auc:0.681753\n",
            "[48]\tvalidation_0-auc:0.678791\n",
            "[49]\tvalidation_0-auc:0.67633\n",
            "[50]\tvalidation_0-auc:0.677099\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.768663\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.2     | 0.5673469387755102 | 0.5478260869565217 |      0.984375      | 0.7039106145251397 |\n",
            "|     GRU 0.2      | 0.5959183673469388 | 0.5747422680412371 |     0.87109375     | 0.6925465838509317 |\n",
            "|   XGBoost 0.2    | 0.5897959183673469 | 0.5685785536159601 |      0.890625      | 0.6940639269406392 |\n",
            "|    Logreg 0.2    | 0.5959183673469388 | 0.5736040609137056 |     0.8828125      | 0.6953846153846155 |\n",
            "|     SVM 0.2      | 0.563265306122449  | 0.5462555066079295 |      0.96875       | 0.6985915492957746 |\n",
            "|  LSTM beta 0.2   | 0.7308533916849015 | 0.7362204724409449 | 0.7695473251028807 | 0.7525150905432596 |\n",
            "|   GRU beta 0.2   | 0.6477024070021882 | 0.6120218579234973 | 0.9218106995884774 | 0.7356321839080461 |\n",
            "| XGBoost beta 0.2 | 0.5251641137855579 | 0.5326633165829145 | 0.8724279835390947 | 0.6614664586583464 |\n",
            "| logreg beta 0.2  | 0.6389496717724289 | 0.6077348066298343 | 0.9053497942386831 | 0.7272727272727274 |\n",
            "|   svm beta 0.2   | 0.6608315098468271 | 0.6271676300578035 | 0.8930041152263375 | 0.736842105263158  |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6613 - accuracy: 0.6020 - val_loss: 0.6492 - val_accuracy: 0.6163\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.5958 - accuracy: 0.6960 - val_loss: 0.6342 - val_accuracy: 0.6143\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5777 - accuracy: 0.7067 - val_loss: 0.8095 - val_accuracy: 0.5837\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5679 - accuracy: 0.7228 - val_loss: 0.6305 - val_accuracy: 0.6082\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.5594 - accuracy: 0.7208 - val_loss: 0.6251 - val_accuracy: 0.6347\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6496 - accuracy: 0.6289 - val_loss: 0.6416 - val_accuracy: 0.6102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5634 - accuracy: 0.7181 - val_loss: 0.6193 - val_accuracy: 0.6204\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5361 - accuracy: 0.7376 - val_loss: 0.6154 - val_accuracy: 0.6449\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5274 - accuracy: 0.7423 - val_loss: 0.6128 - val_accuracy: 0.6469\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5251 - accuracy: 0.7389 - val_loss: 0.6251 - val_accuracy: 0.6184\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.701022\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.699845\n",
            "[2]\tvalidation_0-auc:0.697675\n",
            "[3]\tvalidation_0-auc:0.696064\n",
            "[4]\tvalidation_0-auc:0.698242\n",
            "[5]\tvalidation_0-auc:0.699436\n",
            "[6]\tvalidation_0-auc:0.699018\n",
            "[7]\tvalidation_0-auc:0.70007\n",
            "[8]\tvalidation_0-auc:0.701898\n",
            "[9]\tvalidation_0-auc:0.704285\n",
            "[10]\tvalidation_0-auc:0.705596\n",
            "[11]\tvalidation_0-auc:0.704744\n",
            "[12]\tvalidation_0-auc:0.70391\n",
            "[13]\tvalidation_0-auc:0.705879\n",
            "[14]\tvalidation_0-auc:0.707098\n",
            "[15]\tvalidation_0-auc:0.708016\n",
            "[16]\tvalidation_0-auc:0.707924\n",
            "[17]\tvalidation_0-auc:0.708191\n",
            "[18]\tvalidation_0-auc:0.708242\n",
            "[19]\tvalidation_0-auc:0.708191\n",
            "[20]\tvalidation_0-auc:0.7083\n",
            "[21]\tvalidation_0-auc:0.708784\n",
            "[22]\tvalidation_0-auc:0.710395\n",
            "[23]\tvalidation_0-auc:0.710579\n",
            "[24]\tvalidation_0-auc:0.70946\n",
            "[25]\tvalidation_0-auc:0.711013\n",
            "[26]\tvalidation_0-auc:0.711146\n",
            "[27]\tvalidation_0-auc:0.710103\n",
            "[28]\tvalidation_0-auc:0.707933\n",
            "[29]\tvalidation_0-auc:0.708066\n",
            "[30]\tvalidation_0-auc:0.707382\n",
            "[31]\tvalidation_0-auc:0.706923\n",
            "[32]\tvalidation_0-auc:0.706956\n",
            "[33]\tvalidation_0-auc:0.707357\n",
            "[34]\tvalidation_0-auc:0.707732\n",
            "[35]\tvalidation_0-auc:0.708434\n",
            "[36]\tvalidation_0-auc:0.708233\n",
            "[37]\tvalidation_0-auc:0.707816\n",
            "[38]\tvalidation_0-auc:0.708233\n",
            "[39]\tvalidation_0-auc:0.708492\n",
            "[40]\tvalidation_0-auc:0.708459\n",
            "[41]\tvalidation_0-auc:0.708893\n",
            "[42]\tvalidation_0-auc:0.708592\n",
            "[43]\tvalidation_0-auc:0.708792\n",
            "[44]\tvalidation_0-auc:0.708809\n",
            "[45]\tvalidation_0-auc:0.708692\n",
            "[46]\tvalidation_0-auc:0.709302\n",
            "[47]\tvalidation_0-auc:0.709302\n",
            "[48]\tvalidation_0-auc:0.709118\n",
            "[49]\tvalidation_0-auc:0.708976\n",
            "[50]\tvalidation_0-auc:0.708976\n",
            "[51]\tvalidation_0-auc:0.709193\n",
            "[52]\tvalidation_0-auc:0.708692\n",
            "[53]\tvalidation_0-auc:0.708759\n",
            "[54]\tvalidation_0-auc:0.708292\n",
            "[55]\tvalidation_0-auc:0.708292\n",
            "[56]\tvalidation_0-auc:0.708392\n",
            "[57]\tvalidation_0-auc:0.708408\n",
            "[58]\tvalidation_0-auc:0.708325\n",
            "[59]\tvalidation_0-auc:0.708692\n",
            "[60]\tvalidation_0-auc:0.708726\n",
            "[61]\tvalidation_0-auc:0.708709\n",
            "[62]\tvalidation_0-auc:0.708258\n",
            "[63]\tvalidation_0-auc:0.707991\n",
            "[64]\tvalidation_0-auc:0.707874\n",
            "[65]\tvalidation_0-auc:0.707774\n",
            "[66]\tvalidation_0-auc:0.707565\n",
            "[67]\tvalidation_0-auc:0.709619\n",
            "[68]\tvalidation_0-auc:0.709569\n",
            "[69]\tvalidation_0-auc:0.709602\n",
            "[70]\tvalidation_0-auc:0.709059\n",
            "[71]\tvalidation_0-auc:0.709059\n",
            "[72]\tvalidation_0-auc:0.709059\n",
            "[73]\tvalidation_0-auc:0.70911\n",
            "[74]\tvalidation_0-auc:0.709076\n",
            "[75]\tvalidation_0-auc:0.708926\n",
            "[76]\tvalidation_0-auc:0.708976\n",
            "Stopping. Best iteration:\n",
            "[26]\tvalidation_0-auc:0.711146\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6712 - accuracy: 0.5806 - val_loss: 0.6404 - val_accuracy: 0.7287\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6299 - accuracy: 0.6239 - val_loss: 0.6363 - val_accuracy: 0.5317\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.5861 - accuracy: 0.6815 - val_loss: 0.6079 - val_accuracy: 0.5908\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5693 - accuracy: 0.7104 - val_loss: 0.5632 - val_accuracy: 0.7352\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5352 - accuracy: 0.7385 - val_loss: 0.5825 - val_accuracy: 0.6958\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6395 - accuracy: 0.6156 - val_loss: 0.5966 - val_accuracy: 0.6214\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5440 - accuracy: 0.7234 - val_loss: 0.5677 - val_accuracy: 0.6849\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5409 - accuracy: 0.7255 - val_loss: 0.5657 - val_accuracy: 0.7462\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5260 - accuracy: 0.7351 - val_loss: 0.5922 - val_accuracy: 0.6565\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5317 - accuracy: 0.7296 - val_loss: 0.5626 - val_accuracy: 0.7177\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.65334\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.666263\n",
            "[2]\tvalidation_0-auc:0.674157\n",
            "[3]\tvalidation_0-auc:0.679532\n",
            "[4]\tvalidation_0-auc:0.68182\n",
            "[5]\tvalidation_0-auc:0.67582\n",
            "[6]\tvalidation_0-auc:0.667263\n",
            "[7]\tvalidation_0-auc:0.671859\n",
            "[8]\tvalidation_0-auc:0.670359\n",
            "[9]\tvalidation_0-auc:0.677493\n",
            "[10]\tvalidation_0-auc:0.665542\n",
            "[11]\tvalidation_0-auc:0.669388\n",
            "[12]\tvalidation_0-auc:0.669369\n",
            "[13]\tvalidation_0-auc:0.668917\n",
            "[14]\tvalidation_0-auc:0.676301\n",
            "[15]\tvalidation_0-auc:0.676301\n",
            "[16]\tvalidation_0-auc:0.67857\n",
            "[17]\tvalidation_0-auc:0.669493\n",
            "[18]\tvalidation_0-auc:0.670301\n",
            "[19]\tvalidation_0-auc:0.669926\n",
            "[20]\tvalidation_0-auc:0.671801\n",
            "[21]\tvalidation_0-auc:0.675166\n",
            "[22]\tvalidation_0-auc:0.671397\n",
            "[23]\tvalidation_0-auc:0.676628\n",
            "[24]\tvalidation_0-auc:0.677945\n",
            "[25]\tvalidation_0-auc:0.680339\n",
            "[26]\tvalidation_0-auc:0.680339\n",
            "[27]\tvalidation_0-auc:0.681416\n",
            "[28]\tvalidation_0-auc:0.681416\n",
            "[29]\tvalidation_0-auc:0.682128\n",
            "[30]\tvalidation_0-auc:0.679262\n",
            "[31]\tvalidation_0-auc:0.678878\n",
            "[32]\tvalidation_0-auc:0.675551\n",
            "[33]\tvalidation_0-auc:0.676166\n",
            "[34]\tvalidation_0-auc:0.676166\n",
            "[35]\tvalidation_0-auc:0.676109\n",
            "[36]\tvalidation_0-auc:0.682445\n",
            "[37]\tvalidation_0-auc:0.679195\n",
            "[38]\tvalidation_0-auc:0.676359\n",
            "[39]\tvalidation_0-auc:0.673436\n",
            "[40]\tvalidation_0-auc:0.670138\n",
            "[41]\tvalidation_0-auc:0.670138\n",
            "[42]\tvalidation_0-auc:0.670138\n",
            "[43]\tvalidation_0-auc:0.670138\n",
            "[44]\tvalidation_0-auc:0.672945\n",
            "[45]\tvalidation_0-auc:0.672945\n",
            "[46]\tvalidation_0-auc:0.67558\n",
            "[47]\tvalidation_0-auc:0.672205\n",
            "[48]\tvalidation_0-auc:0.673705\n",
            "[49]\tvalidation_0-auc:0.675282\n",
            "[50]\tvalidation_0-auc:0.675282\n",
            "[51]\tvalidation_0-auc:0.675282\n",
            "[52]\tvalidation_0-auc:0.674628\n",
            "[53]\tvalidation_0-auc:0.669763\n",
            "[54]\tvalidation_0-auc:0.671484\n",
            "[55]\tvalidation_0-auc:0.670916\n",
            "[56]\tvalidation_0-auc:0.669205\n",
            "[57]\tvalidation_0-auc:0.670282\n",
            "[58]\tvalidation_0-auc:0.671474\n",
            "[59]\tvalidation_0-auc:0.671416\n",
            "[60]\tvalidation_0-auc:0.672705\n",
            "[61]\tvalidation_0-auc:0.672743\n",
            "[62]\tvalidation_0-auc:0.674388\n",
            "[63]\tvalidation_0-auc:0.674407\n",
            "[64]\tvalidation_0-auc:0.672445\n",
            "[65]\tvalidation_0-auc:0.672272\n",
            "[66]\tvalidation_0-auc:0.669244\n",
            "[67]\tvalidation_0-auc:0.669244\n",
            "[68]\tvalidation_0-auc:0.670378\n",
            "[69]\tvalidation_0-auc:0.665801\n",
            "[70]\tvalidation_0-auc:0.666628\n",
            "[71]\tvalidation_0-auc:0.666282\n",
            "[72]\tvalidation_0-auc:0.666003\n",
            "[73]\tvalidation_0-auc:0.66708\n",
            "[74]\tvalidation_0-auc:0.66708\n",
            "[75]\tvalidation_0-auc:0.66933\n",
            "[76]\tvalidation_0-auc:0.668657\n",
            "[77]\tvalidation_0-auc:0.668311\n",
            "[78]\tvalidation_0-auc:0.665369\n",
            "[79]\tvalidation_0-auc:0.66233\n",
            "[80]\tvalidation_0-auc:0.662407\n",
            "[81]\tvalidation_0-auc:0.662003\n",
            "[82]\tvalidation_0-auc:0.662176\n",
            "[83]\tvalidation_0-auc:0.662176\n",
            "[84]\tvalidation_0-auc:0.661407\n",
            "[85]\tvalidation_0-auc:0.659984\n",
            "[86]\tvalidation_0-auc:0.660003\n",
            "Stopping. Best iteration:\n",
            "[36]\tvalidation_0-auc:0.682445\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.6346938775510204 | 0.7894736842105263 |     0.41015625     | 0.5398457583547558 |\n",
            "|      GRU 0.15     | 0.6183673469387755 | 0.6011730205278593 |     0.80078125     | 0.6867671691792295 |\n",
            "|    XGBoost 0.15   | 0.636734693877551  |        0.65        |     0.66015625     | 0.6550387596899225 |\n",
            "|    Logreg 0.15    | 0.6204081632653061 | 0.6067073170731707 |     0.77734375     | 0.6815068493150686 |\n",
            "|      SVM 0.15     | 0.6612244897959184 | 0.6642335766423357 |     0.7109375      | 0.6867924528301886 |\n",
            "|   LSTM beta 0.15  | 0.6958424507658644 | 0.6925925925925925 | 0.7695473251028807 | 0.7290448343079923 |\n",
            "|   GRU beta 0.15   | 0.7177242888402626 | 0.7261904761904762 | 0.7530864197530864 | 0.7393939393939394 |\n",
            "| XGBoost beta 0.15 | 0.5951859956236324 | 0.6006944444444444 | 0.7119341563786008 | 0.6516007532956686 |\n",
            "|  logreg beta 0.15 | 0.687089715536105  | 0.6623376623376623 | 0.8395061728395061 | 0.7404718693284936 |\n",
            "|   svm beta 0.15   | 0.7396061269146609 | 0.7649572649572649 | 0.7366255144032922 | 0.7505241090146751 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC81KPR2VHfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad6ff538-6c84-4f70-a5f7-d767c7628988"
      },
      "source": [
        "Result_cross.to_csv('FCX_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.570866</td>\n",
              "      <td>0.681633</td>\n",
              "      <td>0.650224</td>\n",
              "      <td>0.755208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.510386</td>\n",
              "      <td>0.622449</td>\n",
              "      <td>0.650284</td>\n",
              "      <td>0.895833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.533113</td>\n",
              "      <td>0.648980</td>\n",
              "      <td>0.651822</td>\n",
              "      <td>0.838542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.655102</td>\n",
              "      <td>0.655804</td>\n",
              "      <td>0.838542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.663265</td>\n",
              "      <td>0.662577</td>\n",
              "      <td>0.843750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.508834</td>\n",
              "      <td>0.619256</td>\n",
              "      <td>0.623377</td>\n",
              "      <td>0.804469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.484099</td>\n",
              "      <td>0.588621</td>\n",
              "      <td>0.593074</td>\n",
              "      <td>0.765363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.479452</td>\n",
              "      <td>0.582057</td>\n",
              "      <td>0.594480</td>\n",
              "      <td>0.782123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.510345</td>\n",
              "      <td>0.621444</td>\n",
              "      <td>0.631130</td>\n",
              "      <td>0.826816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.521127</td>\n",
              "      <td>0.634573</td>\n",
              "      <td>0.639309</td>\n",
              "      <td>0.826816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.547826</td>\n",
              "      <td>0.567347</td>\n",
              "      <td>0.703911</td>\n",
              "      <td>0.984375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.574742</td>\n",
              "      <td>0.595918</td>\n",
              "      <td>0.692547</td>\n",
              "      <td>0.871094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.568579</td>\n",
              "      <td>0.589796</td>\n",
              "      <td>0.694064</td>\n",
              "      <td>0.890625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.573604</td>\n",
              "      <td>0.595918</td>\n",
              "      <td>0.695385</td>\n",
              "      <td>0.882812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.546256</td>\n",
              "      <td>0.563265</td>\n",
              "      <td>0.698592</td>\n",
              "      <td>0.968750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.736220</td>\n",
              "      <td>0.730853</td>\n",
              "      <td>0.752515</td>\n",
              "      <td>0.769547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.612022</td>\n",
              "      <td>0.647702</td>\n",
              "      <td>0.735632</td>\n",
              "      <td>0.921811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.532663</td>\n",
              "      <td>0.525164</td>\n",
              "      <td>0.661466</td>\n",
              "      <td>0.872428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.607735</td>\n",
              "      <td>0.638950</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.905350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.627168</td>\n",
              "      <td>0.660832</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.893004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.789474</td>\n",
              "      <td>0.634694</td>\n",
              "      <td>0.539846</td>\n",
              "      <td>0.410156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.601173</td>\n",
              "      <td>0.618367</td>\n",
              "      <td>0.686767</td>\n",
              "      <td>0.800781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.636735</td>\n",
              "      <td>0.655039</td>\n",
              "      <td>0.660156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.606707</td>\n",
              "      <td>0.620408</td>\n",
              "      <td>0.681507</td>\n",
              "      <td>0.777344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.664234</td>\n",
              "      <td>0.661224</td>\n",
              "      <td>0.686792</td>\n",
              "      <td>0.710938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.692593</td>\n",
              "      <td>0.695842</td>\n",
              "      <td>0.729045</td>\n",
              "      <td>0.769547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.726190</td>\n",
              "      <td>0.717724</td>\n",
              "      <td>0.739394</td>\n",
              "      <td>0.753086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.600694</td>\n",
              "      <td>0.595186</td>\n",
              "      <td>0.651601</td>\n",
              "      <td>0.711934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.662338</td>\n",
              "      <td>0.687090</td>\n",
              "      <td>0.740472</td>\n",
              "      <td>0.839506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.764957</td>\n",
              "      <td>0.739606</td>\n",
              "      <td>0.750524</td>\n",
              "      <td>0.736626</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  FCX  0.570866  0.681633  0.650224  0.755208\n",
              "1            GRU 0.1  FCX  0.510386  0.622449  0.650284  0.895833\n",
              "2        XGBoost 0.1  FCX  0.533113  0.648980  0.651822  0.838542\n",
              "3         Logreg 0.1  FCX  0.538462  0.655102  0.655804  0.838542\n",
              "4            SVM 0.1  FCX  0.545455  0.663265  0.662577  0.843750\n",
              "5      LSTM beta 0.1  FCX  0.508834  0.619256  0.623377  0.804469\n",
              "6       GRU beta 0.1  FCX  0.484099  0.588621  0.593074  0.765363\n",
              "7   XGBoost beta 0.1  FCX  0.479452  0.582057  0.594480  0.782123\n",
              "8    logreg beta 0.1  FCX  0.510345  0.621444  0.631130  0.826816\n",
              "9       svm beta 0.1  FCX  0.521127  0.634573  0.639309  0.826816\n",
              "0           LSTM 0.2  FCX  0.547826  0.567347  0.703911  0.984375\n",
              "1            GRU 0.2  FCX  0.574742  0.595918  0.692547  0.871094\n",
              "2        XGBoost 0.2  FCX  0.568579  0.589796  0.694064  0.890625\n",
              "3         Logreg 0.2  FCX  0.573604  0.595918  0.695385  0.882812\n",
              "4            SVM 0.2  FCX  0.546256  0.563265  0.698592  0.968750\n",
              "5      LSTM beta 0.2  FCX  0.736220  0.730853  0.752515  0.769547\n",
              "6       GRU beta 0.2  FCX  0.612022  0.647702  0.735632  0.921811\n",
              "7   XGBoost beta 0.2  FCX  0.532663  0.525164  0.661466  0.872428\n",
              "8    logreg beta 0.2  FCX  0.607735  0.638950  0.727273  0.905350\n",
              "9       svm beta 0.2  FCX  0.627168  0.660832  0.736842  0.893004\n",
              "0          LSTM 0.15  FCX  0.789474  0.634694  0.539846  0.410156\n",
              "1           GRU 0.15  FCX  0.601173  0.618367  0.686767  0.800781\n",
              "2       XGBoost 0.15  FCX  0.650000  0.636735  0.655039  0.660156\n",
              "3        Logreg 0.15  FCX  0.606707  0.620408  0.681507  0.777344\n",
              "4           SVM 0.15  FCX  0.664234  0.661224  0.686792  0.710938\n",
              "5     LSTM beta 0.15  FCX  0.692593  0.695842  0.729045  0.769547\n",
              "6      GRU beta 0.15  FCX  0.726190  0.717724  0.739394  0.753086\n",
              "7  XGBoost beta 0.15  FCX  0.600694  0.595186  0.651601  0.711934\n",
              "8   logreg beta 0.15  FCX  0.662338  0.687090  0.740472  0.839506\n",
              "9      svm beta 0.15  FCX  0.764957  0.739606  0.750524  0.736626"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orbR1f_1VHfZ"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjZnF0AjVHfZ"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF7y8DU6VHfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea474e86-8917-45e2-b48d-bd485f72d25e"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"FCX\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6878 - accuracy: 0.5483 - val_loss: 0.7010 - val_accuracy: 0.3918\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6269 - accuracy: 0.6718 - val_loss: 0.5939 - val_accuracy: 0.6633\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5853 - accuracy: 0.7074 - val_loss: 0.5955 - val_accuracy: 0.6592\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5343 - accuracy: 0.7403 - val_loss: 0.5591 - val_accuracy: 0.7163\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5134 - accuracy: 0.7557 - val_loss: 0.6455 - val_accuracy: 0.6224\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6332 - accuracy: 0.6430 - val_loss: 0.6509 - val_accuracy: 0.5776\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5276 - accuracy: 0.7342 - val_loss: 0.6214 - val_accuracy: 0.6367\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5149 - accuracy: 0.7537 - val_loss: 0.5878 - val_accuracy: 0.6633\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5120 - accuracy: 0.7584 - val_loss: 0.6281 - val_accuracy: 0.6306\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5163 - accuracy: 0.7409 - val_loss: 0.6397 - val_accuracy: 0.6163\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.740754\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.744835\n",
            "[2]\tvalidation_0-auc:0.754046\n",
            "[3]\tvalidation_0-auc:0.755628\n",
            "[4]\tvalidation_0-auc:0.760312\n",
            "[5]\tvalidation_0-auc:0.760696\n",
            "[6]\tvalidation_0-auc:0.773079\n",
            "[7]\tvalidation_0-auc:0.760976\n",
            "[8]\tvalidation_0-auc:0.759586\n",
            "[9]\tvalidation_0-auc:0.759351\n",
            "[10]\tvalidation_0-auc:0.758381\n",
            "[11]\tvalidation_0-auc:0.757507\n",
            "[12]\tvalidation_0-auc:0.758704\n",
            "[13]\tvalidation_0-auc:0.758616\n",
            "[14]\tvalidation_0-auc:0.757987\n",
            "[15]\tvalidation_0-auc:0.758879\n",
            "[16]\tvalidation_0-auc:0.758354\n",
            "[17]\tvalidation_0-auc:0.757856\n",
            "[18]\tvalidation_0-auc:0.757279\n",
            "[19]\tvalidation_0-auc:0.757341\n",
            "[20]\tvalidation_0-auc:0.75298\n",
            "[21]\tvalidation_0-auc:0.753067\n",
            "[22]\tvalidation_0-auc:0.753015\n",
            "[23]\tvalidation_0-auc:0.752945\n",
            "[24]\tvalidation_0-auc:0.757795\n",
            "[25]\tvalidation_0-auc:0.758389\n",
            "[26]\tvalidation_0-auc:0.75818\n",
            "[27]\tvalidation_0-auc:0.756502\n",
            "[28]\tvalidation_0-auc:0.762094\n",
            "[29]\tvalidation_0-auc:0.762872\n",
            "[30]\tvalidation_0-auc:0.7624\n",
            "[31]\tvalidation_0-auc:0.762453\n",
            "[32]\tvalidation_0-auc:0.762418\n",
            "[33]\tvalidation_0-auc:0.762732\n",
            "[34]\tvalidation_0-auc:0.76275\n",
            "[35]\tvalidation_0-auc:0.759683\n",
            "[36]\tvalidation_0-auc:0.759683\n",
            "[37]\tvalidation_0-auc:0.759683\n",
            "[38]\tvalidation_0-auc:0.761535\n",
            "[39]\tvalidation_0-auc:0.762619\n",
            "[40]\tvalidation_0-auc:0.763493\n",
            "[41]\tvalidation_0-auc:0.763528\n",
            "[42]\tvalidation_0-auc:0.763528\n",
            "[43]\tvalidation_0-auc:0.763702\n",
            "[44]\tvalidation_0-auc:0.76386\n",
            "[45]\tvalidation_0-auc:0.763982\n",
            "[46]\tvalidation_0-auc:0.765066\n",
            "[47]\tvalidation_0-auc:0.766656\n",
            "[48]\tvalidation_0-auc:0.767207\n",
            "[49]\tvalidation_0-auc:0.767259\n",
            "[50]\tvalidation_0-auc:0.768483\n",
            "[51]\tvalidation_0-auc:0.768133\n",
            "[52]\tvalidation_0-auc:0.767329\n",
            "[53]\tvalidation_0-auc:0.767941\n",
            "[54]\tvalidation_0-auc:0.768561\n",
            "[55]\tvalidation_0-auc:0.768762\n",
            "[56]\tvalidation_0-auc:0.768587\n",
            "Stopping. Best iteration:\n",
            "[6]\tvalidation_0-auc:0.773079\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6875 - accuracy: 0.5532 - val_loss: 0.7315 - val_accuracy: 0.3917\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6712 - accuracy: 0.5752 - val_loss: 0.7153 - val_accuracy: 0.3917\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6076 - accuracy: 0.6671 - val_loss: 0.6864 - val_accuracy: 0.5164\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5621 - accuracy: 0.7351 - val_loss: 0.6415 - val_accuracy: 0.6018\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5517 - accuracy: 0.7303 - val_loss: 0.6158 - val_accuracy: 0.6346\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6729 - accuracy: 0.5820 - val_loss: 0.6617 - val_accuracy: 0.5908\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5806 - accuracy: 0.6994 - val_loss: 0.6711 - val_accuracy: 0.5449\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5457 - accuracy: 0.7268 - val_loss: 0.6459 - val_accuracy: 0.5996\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5256 - accuracy: 0.7351 - val_loss: 0.6666 - val_accuracy: 0.5864\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5342 - accuracy: 0.7433 - val_loss: 0.6244 - val_accuracy: 0.6214\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.628602\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.689572\n",
            "[2]\tvalidation_0-auc:0.694355\n",
            "[3]\tvalidation_0-auc:0.685463\n",
            "[4]\tvalidation_0-auc:0.678409\n",
            "[5]\tvalidation_0-auc:0.707879\n",
            "[6]\tvalidation_0-auc:0.70589\n",
            "[7]\tvalidation_0-auc:0.708412\n",
            "[8]\tvalidation_0-auc:0.695742\n",
            "[9]\tvalidation_0-auc:0.695089\n",
            "[10]\tvalidation_0-auc:0.702162\n",
            "[11]\tvalidation_0-auc:0.706865\n",
            "[12]\tvalidation_0-auc:0.713155\n",
            "[13]\tvalidation_0-auc:0.720208\n",
            "[14]\tvalidation_0-auc:0.719012\n",
            "[15]\tvalidation_0-auc:0.722941\n",
            "[16]\tvalidation_0-auc:0.729512\n",
            "[17]\tvalidation_0-auc:0.72682\n",
            "[18]\tvalidation_0-auc:0.731623\n",
            "[19]\tvalidation_0-auc:0.727865\n",
            "[20]\tvalidation_0-auc:0.73323\n",
            "[21]\tvalidation_0-auc:0.717817\n",
            "[22]\tvalidation_0-auc:0.720047\n",
            "[23]\tvalidation_0-auc:0.724288\n",
            "[24]\tvalidation_0-auc:0.724288\n",
            "[25]\tvalidation_0-auc:0.725363\n",
            "[26]\tvalidation_0-auc:0.728859\n",
            "[27]\tvalidation_0-auc:0.728739\n",
            "[28]\tvalidation_0-auc:0.731673\n",
            "[29]\tvalidation_0-auc:0.72279\n",
            "[30]\tvalidation_0-auc:0.713135\n",
            "[31]\tvalidation_0-auc:0.70809\n",
            "[32]\tvalidation_0-auc:0.712813\n",
            "[33]\tvalidation_0-auc:0.712662\n",
            "[34]\tvalidation_0-auc:0.712662\n",
            "[35]\tvalidation_0-auc:0.712783\n",
            "[36]\tvalidation_0-auc:0.717787\n",
            "[37]\tvalidation_0-auc:0.725483\n",
            "[38]\tvalidation_0-auc:0.72474\n",
            "[39]\tvalidation_0-auc:0.721565\n",
            "[40]\tvalidation_0-auc:0.721565\n",
            "[41]\tvalidation_0-auc:0.721565\n",
            "[42]\tvalidation_0-auc:0.721565\n",
            "[43]\tvalidation_0-auc:0.717375\n",
            "[44]\tvalidation_0-auc:0.715596\n",
            "[45]\tvalidation_0-auc:0.714833\n",
            "[46]\tvalidation_0-auc:0.714139\n",
            "[47]\tvalidation_0-auc:0.713094\n",
            "[48]\tvalidation_0-auc:0.713938\n",
            "[49]\tvalidation_0-auc:0.713938\n",
            "[50]\tvalidation_0-auc:0.714461\n",
            "[51]\tvalidation_0-auc:0.714461\n",
            "[52]\tvalidation_0-auc:0.714461\n",
            "[53]\tvalidation_0-auc:0.710201\n",
            "[54]\tvalidation_0-auc:0.710201\n",
            "[55]\tvalidation_0-auc:0.708894\n",
            "[56]\tvalidation_0-auc:0.70385\n",
            "[57]\tvalidation_0-auc:0.698987\n",
            "[58]\tvalidation_0-auc:0.696144\n",
            "[59]\tvalidation_0-auc:0.696114\n",
            "[60]\tvalidation_0-auc:0.694184\n",
            "[61]\tvalidation_0-auc:0.693823\n",
            "[62]\tvalidation_0-auc:0.693823\n",
            "[63]\tvalidation_0-auc:0.693823\n",
            "[64]\tvalidation_0-auc:0.693762\n",
            "[65]\tvalidation_0-auc:0.699168\n",
            "[66]\tvalidation_0-auc:0.702223\n",
            "[67]\tvalidation_0-auc:0.702002\n",
            "[68]\tvalidation_0-auc:0.693501\n",
            "[69]\tvalidation_0-auc:0.694375\n",
            "[70]\tvalidation_0-auc:0.691723\n",
            "Stopping. Best iteration:\n",
            "[20]\tvalidation_0-auc:0.73323\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.6224489795918368 | 0.5106382978723404 |       0.875        | 0.6449136276391555 |\n",
            "|     GRU 0.1      | 0.6163265306122448 | 0.5059523809523809 | 0.8854166666666666 | 0.6439393939393939 |\n",
            "|   XGBoost 0.1    | 0.6489795918367347 | 0.5331125827814569 | 0.8385416666666666 | 0.6518218623481782 |\n",
            "|    Logreg 0.1    | 0.6551020408163265 | 0.5384615384615384 | 0.8385416666666666 | 0.6558044806517311 |\n",
            "|     SVM 0.1      | 0.6632653061224489 | 0.5454545454545454 |      0.84375       | 0.6625766871165644 |\n",
            "|  LSTM beta 0.1   | 0.6345733041575492 | 0.5288461538461539 | 0.6145251396648045 | 0.5684754521963824 |\n",
            "|   GRU beta 0.1   | 0.6214442013129103 | 0.5126050420168067 | 0.6815642458100558 | 0.5851318944844125 |\n",
            "| XGBoost beta 0.1 | 0.5820568927789934 | 0.4794520547945205 | 0.7821229050279329 | 0.5944798301486199 |\n",
            "| logreg beta 0.1  | 0.6214442013129103 | 0.5103448275862069 | 0.8268156424581006 | 0.6311300639658849 |\n",
            "|   svm beta 0.1   | 0.6345733041575492 | 0.5211267605633803 | 0.8268156424581006 | 0.6393088552915767 |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6560 - accuracy: 0.6329 - val_loss: 0.6419 - val_accuracy: 0.6143\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5908 - accuracy: 0.6772 - val_loss: 0.6453 - val_accuracy: 0.5980\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5772 - accuracy: 0.6919 - val_loss: 0.6397 - val_accuracy: 0.6204\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5601 - accuracy: 0.7034 - val_loss: 0.6408 - val_accuracy: 0.5980\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.5561 - accuracy: 0.7087 - val_loss: 0.6523 - val_accuracy: 0.5878\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6146 - accuracy: 0.6584 - val_loss: 0.6328 - val_accuracy: 0.6245\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5485 - accuracy: 0.7228 - val_loss: 0.6387 - val_accuracy: 0.5980\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5317 - accuracy: 0.7336 - val_loss: 0.6624 - val_accuracy: 0.5776\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5356 - accuracy: 0.7282 - val_loss: 0.6965 - val_accuracy: 0.5694\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5290 - accuracy: 0.7262 - val_loss: 0.6682 - val_accuracy: 0.5796\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.659647\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.687099\n",
            "[2]\tvalidation_0-auc:0.685121\n",
            "[3]\tvalidation_0-auc:0.687008\n",
            "[4]\tvalidation_0-auc:0.704519\n",
            "[5]\tvalidation_0-auc:0.701973\n",
            "[6]\tvalidation_0-auc:0.702432\n",
            "[7]\tvalidation_0-auc:0.704335\n",
            "[8]\tvalidation_0-auc:0.704836\n",
            "[9]\tvalidation_0-auc:0.708459\n",
            "[10]\tvalidation_0-auc:0.708225\n",
            "[11]\tvalidation_0-auc:0.708208\n",
            "[12]\tvalidation_0-auc:0.708392\n",
            "[13]\tvalidation_0-auc:0.708225\n",
            "[14]\tvalidation_0-auc:0.709076\n",
            "[15]\tvalidation_0-auc:0.709293\n",
            "[16]\tvalidation_0-auc:0.709143\n",
            "[17]\tvalidation_0-auc:0.709435\n",
            "[18]\tvalidation_0-auc:0.709435\n",
            "[19]\tvalidation_0-auc:0.711956\n",
            "[20]\tvalidation_0-auc:0.712006\n",
            "[21]\tvalidation_0-auc:0.713358\n",
            "[22]\tvalidation_0-auc:0.710387\n",
            "[23]\tvalidation_0-auc:0.709969\n",
            "[24]\tvalidation_0-auc:0.707791\n",
            "[25]\tvalidation_0-auc:0.703451\n",
            "[26]\tvalidation_0-auc:0.705721\n",
            "[27]\tvalidation_0-auc:0.703968\n",
            "[28]\tvalidation_0-auc:0.704093\n",
            "[29]\tvalidation_0-auc:0.705629\n",
            "[30]\tvalidation_0-auc:0.705746\n",
            "[31]\tvalidation_0-auc:0.705746\n",
            "[32]\tvalidation_0-auc:0.705946\n",
            "[33]\tvalidation_0-auc:0.705946\n",
            "[34]\tvalidation_0-auc:0.700212\n",
            "[35]\tvalidation_0-auc:0.700846\n",
            "[36]\tvalidation_0-auc:0.70098\n",
            "[37]\tvalidation_0-auc:0.698317\n",
            "[38]\tvalidation_0-auc:0.699027\n",
            "[39]\tvalidation_0-auc:0.699786\n",
            "[40]\tvalidation_0-auc:0.699786\n",
            "[41]\tvalidation_0-auc:0.699786\n",
            "[42]\tvalidation_0-auc:0.698868\n",
            "[43]\tvalidation_0-auc:0.696898\n",
            "[44]\tvalidation_0-auc:0.695179\n",
            "[45]\tvalidation_0-auc:0.693343\n",
            "[46]\tvalidation_0-auc:0.693159\n",
            "[47]\tvalidation_0-auc:0.693226\n",
            "[48]\tvalidation_0-auc:0.693226\n",
            "[49]\tvalidation_0-auc:0.692717\n",
            "[50]\tvalidation_0-auc:0.693326\n",
            "[51]\tvalidation_0-auc:0.693326\n",
            "[52]\tvalidation_0-auc:0.691815\n",
            "[53]\tvalidation_0-auc:0.691206\n",
            "[54]\tvalidation_0-auc:0.68982\n",
            "[55]\tvalidation_0-auc:0.689002\n",
            "[56]\tvalidation_0-auc:0.688902\n",
            "[57]\tvalidation_0-auc:0.688744\n",
            "[58]\tvalidation_0-auc:0.688727\n",
            "[59]\tvalidation_0-auc:0.68881\n",
            "[60]\tvalidation_0-auc:0.687225\n",
            "[61]\tvalidation_0-auc:0.686749\n",
            "[62]\tvalidation_0-auc:0.686832\n",
            "[63]\tvalidation_0-auc:0.688318\n",
            "[64]\tvalidation_0-auc:0.687867\n",
            "[65]\tvalidation_0-auc:0.686574\n",
            "[66]\tvalidation_0-auc:0.684829\n",
            "[67]\tvalidation_0-auc:0.684529\n",
            "[68]\tvalidation_0-auc:0.684529\n",
            "[69]\tvalidation_0-auc:0.683844\n",
            "[70]\tvalidation_0-auc:0.683327\n",
            "[71]\tvalidation_0-auc:0.683293\n",
            "Stopping. Best iteration:\n",
            "[21]\tvalidation_0-auc:0.713358\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6485 - accuracy: 0.6356 - val_loss: 0.7622 - val_accuracy: 0.5317\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6375 - accuracy: 0.6417 - val_loss: 0.6376 - val_accuracy: 0.5317\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.5916 - accuracy: 0.6809 - val_loss: 0.6086 - val_accuracy: 0.7571\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5666 - accuracy: 0.7186 - val_loss: 0.5885 - val_accuracy: 0.6980\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5236 - accuracy: 0.7474 - val_loss: 0.5826 - val_accuracy: 0.6477\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6047 - accuracy: 0.6733 - val_loss: 0.6377 - val_accuracy: 0.5558\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5147 - accuracy: 0.7447 - val_loss: 0.5661 - val_accuracy: 0.7177\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5142 - accuracy: 0.7467 - val_loss: 0.5565 - val_accuracy: 0.7046\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5081 - accuracy: 0.7461 - val_loss: 0.5962 - val_accuracy: 0.6018\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5072 - accuracy: 0.7426 - val_loss: 0.6091 - val_accuracy: 0.5952\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.768663\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.688724\n",
            "[2]\tvalidation_0-auc:0.692387\n",
            "[3]\tvalidation_0-auc:0.729097\n",
            "[4]\tvalidation_0-auc:0.705069\n",
            "[5]\tvalidation_0-auc:0.700252\n",
            "[6]\tvalidation_0-auc:0.70804\n",
            "[7]\tvalidation_0-auc:0.690118\n",
            "[8]\tvalidation_0-auc:0.697367\n",
            "[9]\tvalidation_0-auc:0.692435\n",
            "[10]\tvalidation_0-auc:0.674762\n",
            "[11]\tvalidation_0-auc:0.689002\n",
            "[12]\tvalidation_0-auc:0.688541\n",
            "[13]\tvalidation_0-auc:0.67458\n",
            "[14]\tvalidation_0-auc:0.669599\n",
            "[15]\tvalidation_0-auc:0.679743\n",
            "[16]\tvalidation_0-auc:0.692993\n",
            "[17]\tvalidation_0-auc:0.691185\n",
            "[18]\tvalidation_0-auc:0.690916\n",
            "[19]\tvalidation_0-auc:0.687858\n",
            "[20]\tvalidation_0-auc:0.685079\n",
            "[21]\tvalidation_0-auc:0.688897\n",
            "[22]\tvalidation_0-auc:0.68955\n",
            "[23]\tvalidation_0-auc:0.69032\n",
            "[24]\tvalidation_0-auc:0.692589\n",
            "[25]\tvalidation_0-auc:0.692666\n",
            "[26]\tvalidation_0-auc:0.694262\n",
            "[27]\tvalidation_0-auc:0.694627\n",
            "[28]\tvalidation_0-auc:0.689973\n",
            "[29]\tvalidation_0-auc:0.690166\n",
            "[30]\tvalidation_0-auc:0.686022\n",
            "[31]\tvalidation_0-auc:0.68606\n",
            "[32]\tvalidation_0-auc:0.687925\n",
            "[33]\tvalidation_0-auc:0.685118\n",
            "[34]\tvalidation_0-auc:0.685983\n",
            "[35]\tvalidation_0-auc:0.686349\n",
            "[36]\tvalidation_0-auc:0.683185\n",
            "[37]\tvalidation_0-auc:0.680512\n",
            "[38]\tvalidation_0-auc:0.680512\n",
            "[39]\tvalidation_0-auc:0.680512\n",
            "[40]\tvalidation_0-auc:0.681743\n",
            "[41]\tvalidation_0-auc:0.682483\n",
            "[42]\tvalidation_0-auc:0.683964\n",
            "[43]\tvalidation_0-auc:0.685868\n",
            "[44]\tvalidation_0-auc:0.686022\n",
            "[45]\tvalidation_0-auc:0.686445\n",
            "[46]\tvalidation_0-auc:0.681676\n",
            "[47]\tvalidation_0-auc:0.681753\n",
            "[48]\tvalidation_0-auc:0.678791\n",
            "[49]\tvalidation_0-auc:0.67633\n",
            "[50]\tvalidation_0-auc:0.677099\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.768663\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.2     | 0.5877551020408164 | 0.5678391959798995 |     0.8828125      | 0.691131498470948  |\n",
            "|     GRU 0.2      | 0.5795918367346938 | 0.5595238095238095 |     0.91796875     | 0.6952662721893491 |\n",
            "|   XGBoost 0.2    | 0.5897959183673469 | 0.5685785536159601 |      0.890625      | 0.6940639269406392 |\n",
            "|    Logreg 0.2    | 0.5959183673469388 | 0.5736040609137056 |     0.8828125      | 0.6953846153846155 |\n",
            "|     SVM 0.2      | 0.563265306122449  | 0.5462555066079295 |      0.96875       | 0.6985915492957746 |\n",
            "|  LSTM beta 0.2   | 0.6477024070021882 | 0.6164772727272727 | 0.8930041152263375 | 0.7294117647058823 |\n",
            "|   GRU beta 0.2   | 0.5951859956236324 |       0.5725       | 0.9423868312757202 | 0.7122861586314152 |\n",
            "| XGBoost beta 0.2 | 0.5251641137855579 | 0.5326633165829145 | 0.8724279835390947 | 0.6614664586583464 |\n",
            "| logreg beta 0.2  | 0.6389496717724289 | 0.6077348066298343 | 0.9053497942386831 | 0.7272727272727274 |\n",
            "|   svm beta 0.2   | 0.6608315098468271 | 0.6271676300578035 | 0.8930041152263375 | 0.736842105263158  |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 19ms/step - loss: 0.6502 - accuracy: 0.5980 - val_loss: 0.6750 - val_accuracy: 0.5592\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5993 - accuracy: 0.6812 - val_loss: 0.6337 - val_accuracy: 0.6286\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.5849 - accuracy: 0.6960 - val_loss: 0.6259 - val_accuracy: 0.6204\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5750 - accuracy: 0.7087 - val_loss: 0.6278 - val_accuracy: 0.6122\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.5555 - accuracy: 0.7154 - val_loss: 0.6296 - val_accuracy: 0.6347\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6373 - accuracy: 0.6356 - val_loss: 0.6705 - val_accuracy: 0.5735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5577 - accuracy: 0.7248 - val_loss: 0.6306 - val_accuracy: 0.6122\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5430 - accuracy: 0.7456 - val_loss: 0.6444 - val_accuracy: 0.6041\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5317 - accuracy: 0.7262 - val_loss: 0.6205 - val_accuracy: 0.6347\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5359 - accuracy: 0.7349 - val_loss: 0.6297 - val_accuracy: 0.6102\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.701022\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.699845\n",
            "[2]\tvalidation_0-auc:0.697675\n",
            "[3]\tvalidation_0-auc:0.696064\n",
            "[4]\tvalidation_0-auc:0.698242\n",
            "[5]\tvalidation_0-auc:0.699436\n",
            "[6]\tvalidation_0-auc:0.699018\n",
            "[7]\tvalidation_0-auc:0.70007\n",
            "[8]\tvalidation_0-auc:0.701898\n",
            "[9]\tvalidation_0-auc:0.704285\n",
            "[10]\tvalidation_0-auc:0.705596\n",
            "[11]\tvalidation_0-auc:0.704744\n",
            "[12]\tvalidation_0-auc:0.70391\n",
            "[13]\tvalidation_0-auc:0.705879\n",
            "[14]\tvalidation_0-auc:0.707098\n",
            "[15]\tvalidation_0-auc:0.708016\n",
            "[16]\tvalidation_0-auc:0.707924\n",
            "[17]\tvalidation_0-auc:0.708191\n",
            "[18]\tvalidation_0-auc:0.708242\n",
            "[19]\tvalidation_0-auc:0.708191\n",
            "[20]\tvalidation_0-auc:0.7083\n",
            "[21]\tvalidation_0-auc:0.708784\n",
            "[22]\tvalidation_0-auc:0.710395\n",
            "[23]\tvalidation_0-auc:0.710579\n",
            "[24]\tvalidation_0-auc:0.70946\n",
            "[25]\tvalidation_0-auc:0.711013\n",
            "[26]\tvalidation_0-auc:0.711146\n",
            "[27]\tvalidation_0-auc:0.710103\n",
            "[28]\tvalidation_0-auc:0.707933\n",
            "[29]\tvalidation_0-auc:0.708066\n",
            "[30]\tvalidation_0-auc:0.707382\n",
            "[31]\tvalidation_0-auc:0.706923\n",
            "[32]\tvalidation_0-auc:0.706956\n",
            "[33]\tvalidation_0-auc:0.707357\n",
            "[34]\tvalidation_0-auc:0.707732\n",
            "[35]\tvalidation_0-auc:0.708434\n",
            "[36]\tvalidation_0-auc:0.708233\n",
            "[37]\tvalidation_0-auc:0.707816\n",
            "[38]\tvalidation_0-auc:0.708233\n",
            "[39]\tvalidation_0-auc:0.708492\n",
            "[40]\tvalidation_0-auc:0.708459\n",
            "[41]\tvalidation_0-auc:0.708893\n",
            "[42]\tvalidation_0-auc:0.708592\n",
            "[43]\tvalidation_0-auc:0.708792\n",
            "[44]\tvalidation_0-auc:0.708809\n",
            "[45]\tvalidation_0-auc:0.708692\n",
            "[46]\tvalidation_0-auc:0.709302\n",
            "[47]\tvalidation_0-auc:0.709302\n",
            "[48]\tvalidation_0-auc:0.709118\n",
            "[49]\tvalidation_0-auc:0.708976\n",
            "[50]\tvalidation_0-auc:0.708976\n",
            "[51]\tvalidation_0-auc:0.709193\n",
            "[52]\tvalidation_0-auc:0.708692\n",
            "[53]\tvalidation_0-auc:0.708759\n",
            "[54]\tvalidation_0-auc:0.708292\n",
            "[55]\tvalidation_0-auc:0.708292\n",
            "[56]\tvalidation_0-auc:0.708392\n",
            "[57]\tvalidation_0-auc:0.708408\n",
            "[58]\tvalidation_0-auc:0.708325\n",
            "[59]\tvalidation_0-auc:0.708692\n",
            "[60]\tvalidation_0-auc:0.708726\n",
            "[61]\tvalidation_0-auc:0.708709\n",
            "[62]\tvalidation_0-auc:0.708258\n",
            "[63]\tvalidation_0-auc:0.707991\n",
            "[64]\tvalidation_0-auc:0.707874\n",
            "[65]\tvalidation_0-auc:0.707774\n",
            "[66]\tvalidation_0-auc:0.707565\n",
            "[67]\tvalidation_0-auc:0.709619\n",
            "[68]\tvalidation_0-auc:0.709569\n",
            "[69]\tvalidation_0-auc:0.709602\n",
            "[70]\tvalidation_0-auc:0.709059\n",
            "[71]\tvalidation_0-auc:0.709059\n",
            "[72]\tvalidation_0-auc:0.709059\n",
            "[73]\tvalidation_0-auc:0.70911\n",
            "[74]\tvalidation_0-auc:0.709076\n",
            "[75]\tvalidation_0-auc:0.708926\n",
            "[76]\tvalidation_0-auc:0.708976\n",
            "Stopping. Best iteration:\n",
            "[26]\tvalidation_0-auc:0.711146\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6686 - accuracy: 0.5889 - val_loss: 0.6660 - val_accuracy: 0.5317\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6329 - accuracy: 0.6259 - val_loss: 0.6475 - val_accuracy: 0.5405\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5844 - accuracy: 0.6987 - val_loss: 0.5659 - val_accuracy: 0.7330\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5561 - accuracy: 0.7090 - val_loss: 0.6044 - val_accuracy: 0.6433\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5417 - accuracy: 0.7344 - val_loss: 0.5922 - val_accuracy: 0.6740\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6238 - accuracy: 0.6404 - val_loss: 0.5753 - val_accuracy: 0.7418\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5473 - accuracy: 0.7303 - val_loss: 0.5869 - val_accuracy: 0.6433\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5287 - accuracy: 0.7330 - val_loss: 0.5681 - val_accuracy: 0.7484\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5209 - accuracy: 0.7358 - val_loss: 0.5631 - val_accuracy: 0.6958\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5284 - accuracy: 0.7392 - val_loss: 0.5664 - val_accuracy: 0.6827\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.65334\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.666263\n",
            "[2]\tvalidation_0-auc:0.674157\n",
            "[3]\tvalidation_0-auc:0.679532\n",
            "[4]\tvalidation_0-auc:0.68182\n",
            "[5]\tvalidation_0-auc:0.67582\n",
            "[6]\tvalidation_0-auc:0.667263\n",
            "[7]\tvalidation_0-auc:0.671859\n",
            "[8]\tvalidation_0-auc:0.670359\n",
            "[9]\tvalidation_0-auc:0.677493\n",
            "[10]\tvalidation_0-auc:0.665542\n",
            "[11]\tvalidation_0-auc:0.669388\n",
            "[12]\tvalidation_0-auc:0.669369\n",
            "[13]\tvalidation_0-auc:0.668917\n",
            "[14]\tvalidation_0-auc:0.676301\n",
            "[15]\tvalidation_0-auc:0.676301\n",
            "[16]\tvalidation_0-auc:0.67857\n",
            "[17]\tvalidation_0-auc:0.669493\n",
            "[18]\tvalidation_0-auc:0.670301\n",
            "[19]\tvalidation_0-auc:0.669926\n",
            "[20]\tvalidation_0-auc:0.671801\n",
            "[21]\tvalidation_0-auc:0.675166\n",
            "[22]\tvalidation_0-auc:0.671397\n",
            "[23]\tvalidation_0-auc:0.676628\n",
            "[24]\tvalidation_0-auc:0.677945\n",
            "[25]\tvalidation_0-auc:0.680339\n",
            "[26]\tvalidation_0-auc:0.680339\n",
            "[27]\tvalidation_0-auc:0.681416\n",
            "[28]\tvalidation_0-auc:0.681416\n",
            "[29]\tvalidation_0-auc:0.682128\n",
            "[30]\tvalidation_0-auc:0.679262\n",
            "[31]\tvalidation_0-auc:0.678878\n",
            "[32]\tvalidation_0-auc:0.675551\n",
            "[33]\tvalidation_0-auc:0.676166\n",
            "[34]\tvalidation_0-auc:0.676166\n",
            "[35]\tvalidation_0-auc:0.676109\n",
            "[36]\tvalidation_0-auc:0.682445\n",
            "[37]\tvalidation_0-auc:0.679195\n",
            "[38]\tvalidation_0-auc:0.676359\n",
            "[39]\tvalidation_0-auc:0.673436\n",
            "[40]\tvalidation_0-auc:0.670138\n",
            "[41]\tvalidation_0-auc:0.670138\n",
            "[42]\tvalidation_0-auc:0.670138\n",
            "[43]\tvalidation_0-auc:0.670138\n",
            "[44]\tvalidation_0-auc:0.672945\n",
            "[45]\tvalidation_0-auc:0.672945\n",
            "[46]\tvalidation_0-auc:0.67558\n",
            "[47]\tvalidation_0-auc:0.672205\n",
            "[48]\tvalidation_0-auc:0.673705\n",
            "[49]\tvalidation_0-auc:0.675282\n",
            "[50]\tvalidation_0-auc:0.675282\n",
            "[51]\tvalidation_0-auc:0.675282\n",
            "[52]\tvalidation_0-auc:0.674628\n",
            "[53]\tvalidation_0-auc:0.669763\n",
            "[54]\tvalidation_0-auc:0.671484\n",
            "[55]\tvalidation_0-auc:0.670916\n",
            "[56]\tvalidation_0-auc:0.669205\n",
            "[57]\tvalidation_0-auc:0.670282\n",
            "[58]\tvalidation_0-auc:0.671474\n",
            "[59]\tvalidation_0-auc:0.671416\n",
            "[60]\tvalidation_0-auc:0.672705\n",
            "[61]\tvalidation_0-auc:0.672743\n",
            "[62]\tvalidation_0-auc:0.674388\n",
            "[63]\tvalidation_0-auc:0.674407\n",
            "[64]\tvalidation_0-auc:0.672445\n",
            "[65]\tvalidation_0-auc:0.672272\n",
            "[66]\tvalidation_0-auc:0.669244\n",
            "[67]\tvalidation_0-auc:0.669244\n",
            "[68]\tvalidation_0-auc:0.670378\n",
            "[69]\tvalidation_0-auc:0.665801\n",
            "[70]\tvalidation_0-auc:0.666628\n",
            "[71]\tvalidation_0-auc:0.666282\n",
            "[72]\tvalidation_0-auc:0.666003\n",
            "[73]\tvalidation_0-auc:0.66708\n",
            "[74]\tvalidation_0-auc:0.66708\n",
            "[75]\tvalidation_0-auc:0.66933\n",
            "[76]\tvalidation_0-auc:0.668657\n",
            "[77]\tvalidation_0-auc:0.668311\n",
            "[78]\tvalidation_0-auc:0.665369\n",
            "[79]\tvalidation_0-auc:0.66233\n",
            "[80]\tvalidation_0-auc:0.662407\n",
            "[81]\tvalidation_0-auc:0.662003\n",
            "[82]\tvalidation_0-auc:0.662176\n",
            "[83]\tvalidation_0-auc:0.662176\n",
            "[84]\tvalidation_0-auc:0.661407\n",
            "[85]\tvalidation_0-auc:0.659984\n",
            "[86]\tvalidation_0-auc:0.660003\n",
            "Stopping. Best iteration:\n",
            "[36]\tvalidation_0-auc:0.682445\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.6346938775510204 | 0.6878048780487804 |     0.55078125     | 0.6117136659436009 |\n",
            "|      GRU 0.15     | 0.610204081632653  | 0.5942028985507246 |     0.80078125     | 0.6821963394342762 |\n",
            "|    XGBoost 0.15   | 0.636734693877551  |        0.65        |     0.66015625     | 0.6550387596899225 |\n",
            "|    Logreg 0.15    | 0.6204081632653061 | 0.6067073170731707 |     0.77734375     | 0.6815068493150686 |\n",
            "|      SVM 0.15     | 0.6612244897959184 | 0.6642335766423357 |     0.7109375      | 0.6867924528301886 |\n",
            "|   LSTM beta 0.15  | 0.6739606126914661 | 0.6654929577464789 | 0.7777777777777778 | 0.7172675521821632 |\n",
            "|   GRU beta 0.15   | 0.6827133479212254 | 0.6590909090909091 | 0.8353909465020576 | 0.7368421052631579 |\n",
            "| XGBoost beta 0.15 | 0.5951859956236324 | 0.6006944444444444 | 0.7119341563786008 | 0.6516007532956686 |\n",
            "|  logreg beta 0.15 | 0.687089715536105  | 0.6623376623376623 | 0.8395061728395061 | 0.7404718693284936 |\n",
            "|   svm beta 0.15   | 0.7396061269146609 | 0.7649572649572649 | 0.7366255144032922 | 0.7505241090146751 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcgFnHPeVHfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "023f5da4-f117-4ffb-a2fb-62a8344da699"
      },
      "source": [
        "Result_purging.to_csv('FCX_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.510638</td>\n",
              "      <td>0.622449</td>\n",
              "      <td>0.644914</td>\n",
              "      <td>0.875000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.505952</td>\n",
              "      <td>0.616327</td>\n",
              "      <td>0.643939</td>\n",
              "      <td>0.885417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.533113</td>\n",
              "      <td>0.648980</td>\n",
              "      <td>0.651822</td>\n",
              "      <td>0.838542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.655102</td>\n",
              "      <td>0.655804</td>\n",
              "      <td>0.838542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.663265</td>\n",
              "      <td>0.662577</td>\n",
              "      <td>0.843750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.528846</td>\n",
              "      <td>0.634573</td>\n",
              "      <td>0.568475</td>\n",
              "      <td>0.614525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.512605</td>\n",
              "      <td>0.621444</td>\n",
              "      <td>0.585132</td>\n",
              "      <td>0.681564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.479452</td>\n",
              "      <td>0.582057</td>\n",
              "      <td>0.594480</td>\n",
              "      <td>0.782123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.510345</td>\n",
              "      <td>0.621444</td>\n",
              "      <td>0.631130</td>\n",
              "      <td>0.826816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.521127</td>\n",
              "      <td>0.634573</td>\n",
              "      <td>0.639309</td>\n",
              "      <td>0.826816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.567839</td>\n",
              "      <td>0.587755</td>\n",
              "      <td>0.691131</td>\n",
              "      <td>0.882812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.559524</td>\n",
              "      <td>0.579592</td>\n",
              "      <td>0.695266</td>\n",
              "      <td>0.917969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.568579</td>\n",
              "      <td>0.589796</td>\n",
              "      <td>0.694064</td>\n",
              "      <td>0.890625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.573604</td>\n",
              "      <td>0.595918</td>\n",
              "      <td>0.695385</td>\n",
              "      <td>0.882812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.546256</td>\n",
              "      <td>0.563265</td>\n",
              "      <td>0.698592</td>\n",
              "      <td>0.968750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.616477</td>\n",
              "      <td>0.647702</td>\n",
              "      <td>0.729412</td>\n",
              "      <td>0.893004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.572500</td>\n",
              "      <td>0.595186</td>\n",
              "      <td>0.712286</td>\n",
              "      <td>0.942387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.532663</td>\n",
              "      <td>0.525164</td>\n",
              "      <td>0.661466</td>\n",
              "      <td>0.872428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.607735</td>\n",
              "      <td>0.638950</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.905350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.627168</td>\n",
              "      <td>0.660832</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.893004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.687805</td>\n",
              "      <td>0.634694</td>\n",
              "      <td>0.611714</td>\n",
              "      <td>0.550781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.594203</td>\n",
              "      <td>0.610204</td>\n",
              "      <td>0.682196</td>\n",
              "      <td>0.800781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.636735</td>\n",
              "      <td>0.655039</td>\n",
              "      <td>0.660156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.606707</td>\n",
              "      <td>0.620408</td>\n",
              "      <td>0.681507</td>\n",
              "      <td>0.777344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.664234</td>\n",
              "      <td>0.661224</td>\n",
              "      <td>0.686792</td>\n",
              "      <td>0.710938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.665493</td>\n",
              "      <td>0.673961</td>\n",
              "      <td>0.717268</td>\n",
              "      <td>0.777778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.659091</td>\n",
              "      <td>0.682713</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.835391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.600694</td>\n",
              "      <td>0.595186</td>\n",
              "      <td>0.651601</td>\n",
              "      <td>0.711934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.662338</td>\n",
              "      <td>0.687090</td>\n",
              "      <td>0.740472</td>\n",
              "      <td>0.839506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.764957</td>\n",
              "      <td>0.739606</td>\n",
              "      <td>0.750524</td>\n",
              "      <td>0.736626</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  FCX  0.510638  0.622449  0.644914  0.875000\n",
              "1            GRU 0.1  FCX  0.505952  0.616327  0.643939  0.885417\n",
              "2        XGBoost 0.1  FCX  0.533113  0.648980  0.651822  0.838542\n",
              "3         Logreg 0.1  FCX  0.538462  0.655102  0.655804  0.838542\n",
              "4            SVM 0.1  FCX  0.545455  0.663265  0.662577  0.843750\n",
              "5      LSTM beta 0.1  FCX  0.528846  0.634573  0.568475  0.614525\n",
              "6       GRU beta 0.1  FCX  0.512605  0.621444  0.585132  0.681564\n",
              "7   XGBoost beta 0.1  FCX  0.479452  0.582057  0.594480  0.782123\n",
              "8    logreg beta 0.1  FCX  0.510345  0.621444  0.631130  0.826816\n",
              "9       svm beta 0.1  FCX  0.521127  0.634573  0.639309  0.826816\n",
              "0           LSTM 0.2  FCX  0.567839  0.587755  0.691131  0.882812\n",
              "1            GRU 0.2  FCX  0.559524  0.579592  0.695266  0.917969\n",
              "2        XGBoost 0.2  FCX  0.568579  0.589796  0.694064  0.890625\n",
              "3         Logreg 0.2  FCX  0.573604  0.595918  0.695385  0.882812\n",
              "4            SVM 0.2  FCX  0.546256  0.563265  0.698592  0.968750\n",
              "5      LSTM beta 0.2  FCX  0.616477  0.647702  0.729412  0.893004\n",
              "6       GRU beta 0.2  FCX  0.572500  0.595186  0.712286  0.942387\n",
              "7   XGBoost beta 0.2  FCX  0.532663  0.525164  0.661466  0.872428\n",
              "8    logreg beta 0.2  FCX  0.607735  0.638950  0.727273  0.905350\n",
              "9       svm beta 0.2  FCX  0.627168  0.660832  0.736842  0.893004\n",
              "0          LSTM 0.15  FCX  0.687805  0.634694  0.611714  0.550781\n",
              "1           GRU 0.15  FCX  0.594203  0.610204  0.682196  0.800781\n",
              "2       XGBoost 0.15  FCX  0.650000  0.636735  0.655039  0.660156\n",
              "3        Logreg 0.15  FCX  0.606707  0.620408  0.681507  0.777344\n",
              "4           SVM 0.15  FCX  0.664234  0.661224  0.686792  0.710938\n",
              "5     LSTM beta 0.15  FCX  0.665493  0.673961  0.717268  0.777778\n",
              "6      GRU beta 0.15  FCX  0.659091  0.682713  0.736842  0.835391\n",
              "7  XGBoost beta 0.15  FCX  0.600694  0.595186  0.651601  0.711934\n",
              "8   logreg beta 0.15  FCX  0.662338  0.687090  0.740472  0.839506\n",
              "9      svm beta 0.15  FCX  0.764957  0.739606  0.750524  0.736626"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzlJgQkNVHfa"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFhdh53cVHfa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRIVh23HVrKZ"
      },
      "source": [
        "## FOX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ8N410FVrKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c336ad-fa30-4167-a048-30a30c491bbc"
      },
      "source": [
        "dfs = pd.read_csv(\"FOX.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "# Denoise prices\n",
        "dfs = denoise_data(dfs, '<CLOSE>')\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2756</td>\n",
              "      <td>US1.FOX</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>37.48</td>\n",
              "      <td>37.92</td>\n",
              "      <td>37.13</td>\n",
              "      <td>37.69</td>\n",
              "      <td>25772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2755</td>\n",
              "      <td>US1.FOX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>37.90</td>\n",
              "      <td>37.99</td>\n",
              "      <td>37.11</td>\n",
              "      <td>37.13</td>\n",
              "      <td>49472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2754</td>\n",
              "      <td>US1.FOX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>36.79</td>\n",
              "      <td>37.32</td>\n",
              "      <td>36.66</td>\n",
              "      <td>37.25</td>\n",
              "      <td>29082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2753</td>\n",
              "      <td>US1.FOX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>36.99</td>\n",
              "      <td>37.35</td>\n",
              "      <td>36.76</td>\n",
              "      <td>36.86</td>\n",
              "      <td>31023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2752</td>\n",
              "      <td>US1.FOX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>36.38</td>\n",
              "      <td>37.19</td>\n",
              "      <td>36.38</td>\n",
              "      <td>36.93</td>\n",
              "      <td>34743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2752</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.FOX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>15.64</td>\n",
              "      <td>15.85</td>\n",
              "      <td>15.54</td>\n",
              "      <td>15.79</td>\n",
              "      <td>1297198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2753</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.FOX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>15.61</td>\n",
              "      <td>15.71</td>\n",
              "      <td>15.33</td>\n",
              "      <td>15.62</td>\n",
              "      <td>2236400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2754</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.FOX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>15.56</td>\n",
              "      <td>15.67</td>\n",
              "      <td>15.41</td>\n",
              "      <td>15.56</td>\n",
              "      <td>2451013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2755</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.FOX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>15.34</td>\n",
              "      <td>15.58</td>\n",
              "      <td>15.24</td>\n",
              "      <td>15.49</td>\n",
              "      <td>2117362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2756</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.FOX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>15.14</td>\n",
              "      <td>15.24</td>\n",
              "      <td>14.90</td>\n",
              "      <td>15.04</td>\n",
              "      <td>1805146</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2757 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...  <HIGH>  <LOW>  <CLOSE>    <VOL>\n",
              "0      2756  US1.FOX     D  20211001  ...   37.92  37.13    37.69    25772\n",
              "1      2755  US1.FOX     D  20210930  ...   37.99  37.11    37.13    49472\n",
              "2      2754  US1.FOX     D  20210929  ...   37.32  36.66    37.25    29082\n",
              "3      2753  US1.FOX     D  20210928  ...   37.35  36.76    36.86    31023\n",
              "4      2752  US1.FOX     D  20210927  ...   37.19  36.38    36.93    34743\n",
              "...     ...      ...   ...       ...  ...     ...    ...      ...      ...\n",
              "2752      4  US1.FOX     D  20101008  ...   15.85  15.54    15.79  1297198\n",
              "2753      3  US1.FOX     D  20101007  ...   15.71  15.33    15.62  2236400\n",
              "2754      2  US1.FOX     D  20101006  ...   15.67  15.41    15.56  2451013\n",
              "2755      1  US1.FOX     D  20101005  ...   15.58  15.24    15.49  2117362\n",
              "2756      0  US1.FOX     D  20101004  ...   15.24  14.90    15.04  1805146\n",
              "\n",
              "[2757 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBQaX_uAVrKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdcd4adb-77f6-4dc2-d1f7-672d99540b4d"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"83bdae3c-30a0-4b4a-88c2-6d09339197d6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"83bdae3c-30a0-4b4a-88c2-6d09339197d6\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '83bdae3c-30a0-4b4a-88c2-6d09339197d6',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [30.04, 30.49, 30.48, 30.4, 30.325, 31.19, 31.53, 31.47, 31.49, 32.0, 31.53, 32.18, 32.09, 31.98, 32.49, 32.815, 33.48, 33.71, 33.69, 33.97, 34.21, 33.99, 33.88, 33.91, 33.2, 32.43, 32.78, 32.44, 31.54, 31.61, 31.69, 31.45, 32.83, 33.14, 33.35, 33.61, 32.98, 32.8, 33.66, 35.005, 34.8, 35.39, 35.0, 36.78, 36.39, 36.0, 36.58, 36.86, 37.2, 37.87, 37.68, 37.48, 37.44, 36.89, 36.51, 36.31, 36.46, 36.6, 36.17, 36.69, 37.03, 36.96, 36.3, 36.12, 36.01, 36.13, 36.57, 36.35, 36.53, 35.96, 36.52, 35.75, 35.31, 35.18, 35.55, 35.87, 35.33, 35.74, 35.86, 35.8, 34.95, 34.94, 34.7, 33.83, 33.75, 33.68, 33.74, 34.02, 34.08, 33.78, 34.71, 34.82, 35.29, 36.17, 36.47, 36.52, 36.67, 37.18, 36.67, 36.83, 36.28, 37.24, 37.46, 37.51, 38.05, 37.97, 36.83, 36.7, 37.31, 37.01, 36.85, 38.75, 38.5, 38.73, 37.89, 37.63, 37.47, 37.6, 37.24, 37.92, 38.21, 37.24, 36.98, 36.95, 36.41, 35.93, 36.87, 36.86, 36.98, 36.02, 35.5, 35.7, 36.15, 35.88, 35.67, 36.2, 36.71, 36.44, 37.46, 38.89, 37.6269, 39.57, 51.05, 51.69, 51.32, 51.08, 50.69, 50.26, 50.22, 50.27, 50.38, 50.3, 50.41, 50.31, 50.15, 50.12, 50.59, 50.41, 50.82, 50.46, 50.37, 50.17, 50.01, 49.63, 49.26, 49.23, 49.23, 49.34, 49.18, 49.14, 49.03, 49.08, 49.09, 49.05, 48.75, 48.8, 48.91, 48.96, 48.64, 48.46, 48.4, 48.37, 48.07, 48.06, 48.33, 48.34, 48.4, 48.48, 48.49, 48.34, 47.93, 47.78, 47.19, 47.47, 47.8, 47.63, 47.75, 47.53, 45.79, 46.57, 47.72, 48.19, 48.49, 48.78, 48.79, 48.89, 48.91, 49.005, 49.03, 48.99, 49.22, 48.93, 49.39, 49.04, 49.3, 49.25, 48.88, 48.54, 48.66, 48.55, 48.31, 48.55, 47.74, 47.83, 47.44, 47.26, 47.57, 47.71, 47.47, 47.13, 46.79, 46.41, 46.08, 46.2, 45.17, 45.3, 45.21, 44.79, 44.78, 44.3, 45.35, 45.44, 45.5, 45.46, 45.45, 45.49, 45.13, 44.93, 44.79, 44.43, 45.25, 45.46, 45.81, 46.24, 46.49, 46.18, 45.83, 45.85, 45.49, 45.27, 44.8, 44.65, 43.98, 44.19, 44.25, 44.17, 44.34, 44.41, 44.37, 44.2, 44.3, 44.47, 44.76, 44.88, 44.74, 44.86, 44.87, 44.73, 45.0, 44.6, 44.61, 44.57, 44.67, 44.76, 44.85, 44.8, 44.83, 45.12, 44.83, 44.96, 44.72, 44.9, 45.02, 44.95, 45.07, 45.26, 44.92, 44.6, 44.53, 44.43, 44.53, 44.7, 45.09, 44.91, 44.915, 45.53, 45.69, 46.21, 46.33, 46.19, 46.42, 47.19, 47.02, 47.4, 49.34, 49.08, 48.44, 48.16, 48.28, 48.67, 49.28, 49.31, 48.31, 47.43, 47.89, 48.28, 47.98, 47.56, 44.28, 44.36, 44.57, 44.41, 43.4, 40.32, 39.92, 39.52, 39.23, 38.94, 38.84, 38.42, 38.26, 38.19, 38.25, 38.26, 38.5, 38.5, 38.25, 37.75, 37.45, 37.27, 37.29, 37.71, 37.35, 37.29, 37.48, 37.46, 37.24, 37.45, 37.49, 37.15, 36.34, 36.13, 36.11, 36.07, 35.99, 35.84, 36.14, 35.505, 36.03, 36.19, 36.68, 36.82, 37.06, 36.51, 36.41, 36.06, 35.66, 35.8, 35.33, 35.34, 35.99, 35.63, 35.61, 35.68, 36.33, 35.5, 36.08, 36.57, 35.45, 36.191, 36.3, 36.18, 36.249, 36.63, 36.7, 37.21, 36.86, 37.52, 37.22, 36.78, 36.52, 37.01, 36.16, 35.92, 35.53, 36.42, 37.15, 38.4, 37.61, 36.73, 36.55, 36.62, 37.05, 36.64, 36.18, 36.2, 35.95, 35.19, 34.05, 35.65, 36.41, 35.57, 36.22, 36.69, 36.49, 37.855, 38.15, 38.31, 37.73, 37.49, 37.18, 36.96, 36.26, 35.74, 36.49, 35.78, 36.36, 35.79, 35.31, 36.18, 35.85, 36.31, 36.19, 35.93, 35.36, 34.12, 34.08, 34.12, 34.16, 34.73, 34.52, 33.86, 34.37, 34.52, 34.22, 34.23, 32.38, 33.6, 33.26, 32.96, 33.77, 32.7, 32.47, 32.295, 31.48, 31.14, 31.31, 30.18, 29.7, 29.6, 29.95, 30.1, 29.9, 30.49, 28.61, 27.95, 27.32, 27.82, 28.06, 28.1, 27.36, 27.04, 26.65, 24.43, 25.15, 25.37, 25.46, 25.75, 25.74, 25.69, 25.435, 25.56, 26.02, 26.84, 26.7, 26.58, 26.23, 26.095, 25.83, 25.48, 25.49, 26.14, 25.98, 26.46, 26.31, 26.45, 26.01, 26.045, 25.79, 26.02, 26.77, 26.43, 26.61, 26.425, 26.33, 26.37, 26.26, 26.03, 26.61, 26.0, 26.14, 25.47, 25.66, 25.38, 25.38, 25.965, 26.09, 26.97, 27.1, 27.0, 26.91, 26.77, 26.71, 26.62, 26.66, 26.97, 26.88, 26.8, 26.89, 27.555, 27.45, 27.76, 27.85, 27.48, 27.52, 27.71, 27.83, 28.05, 28.5, 28.27, 28.92, 28.69, 28.91, 29.23, 27.92, 27.725, 27.51, 27.65, 27.6, 27.46, 27.19, 27.22, 27.53, 27.385, 28.33, 27.72, 27.84, 27.55, 27.52, 27.94, 28.13, 27.86, 27.73, 27.77, 27.22, 27.63, 26.93, 26.63, 26.86, 26.74, 27.42, 27.12, 27.41, 27.13, 27.66, 28.18, 27.69, 27.0, 27.19, 27.21, 27.36, 27.6, 27.54, 26.9, 26.96, 26.94, 27.23, 26.67, 26.53, 27.0, 27.01, 26.77, 26.76, 27.11, 27.66, 27.9, 28.32, 27.67, 27.96, 28.01, 28.59, 28.51, 28.35, 29.85, 29.695, 29.85, 29.93, 30.03, 29.9, 29.74, 30.22, 30.55, 29.81, 30.05, 30.14, 29.86, 30.14, 30.465, 30.59, 30.535, 30.55, 30.84, 31.2, 31.565, 31.78, 31.67, 31.52, 31.82, 31.255, 30.89, 30.74, 30.42, 30.09, 30.09, 30.26, 30.195, 30.31, 29.915, 29.95, 29.98, 30.27, 29.97, 30.1, 30.2, 29.95, 29.7, 29.86, 29.35, 29.995, 30.23, 30.0, 30.15, 30.05, 29.92, 30.08, 30.32, 30.01, 29.48, 29.65, 29.69, 29.81, 30.16, 30.715, 31.09, 31.15, 31.23, 31.01, 30.97, 30.85, 30.59, 29.8, 29.79, 29.5, 29.55, 29.21, 29.52, 29.235, 29.25, 29.08, 29.44, 28.92, 28.77, 28.85, 28.35, 28.24, 27.995, 27.26, 27.48, 27.66, 27.84, 27.695, 27.745, 27.74, 27.59, 27.62, 27.74, 27.385, 27.61, 26.895, 26.2, 27.92, 28.48, 27.95, 27.37, 27.53, 27.36, 28.08, 28.06, 28.35, 28.19, 28.26, 28.28, 28.13, 27.72, 27.705, 27.81, 27.48, 27.39, 27.33, 27.0, 27.16, 27.33, 27.08, 27.18, 26.69, 27.53, 25.765, 26.185, 26.385, 26.24, 26.235, 26.51, 25.71, 25.665, 25.905, 25.355, 25.32, 25.07, 24.8, 24.865, 24.76, 24.77, 24.82, 25.0, 24.67, 24.9, 25.01, 25.15, 25.18, 24.74, 24.62, 24.89, 24.8, 24.31, 24.485, 24.74, 24.3, 24.235, 24.29, 24.28, 24.33, 24.19, 24.2, 24.66, 24.12, 25.04, 24.57, 24.7, 24.75, 24.75, 24.855, 24.87, 25.06, 25.015, 25.11, 25.335, 25.445, 25.7, 25.81, 25.82, 26.19, 26.14, 26.3, 26.58, 26.63, 26.19, 26.175, 26.225, 26.43, 26.21, 27.415, 27.035, 27.13, 27.03, 27.07, 27.42, 27.41, 27.44, 27.76, 27.72, 27.8, 28.45, 28.57, 28.58, 28.61, 28.41, 28.08, 28.08, 28.0, 27.26, 27.12, 26.88, 27.51, 27.25, 27.07, 26.62, 26.26, 26.77, 28.97, 28.68, 28.59, 28.94, 28.695, 29.02, 28.97, 28.98, 29.08, 28.995, 29.41, 29.68, 29.68, 29.73, 29.6, 29.845, 29.69, 29.24, 29.25, 28.97, 29.2, 28.83, 28.44, 28.45, 28.25, 28.48, 28.83, 29.13, 29.24, 29.35, 29.58, 30.02, 29.4, 29.52, 29.81, 29.58, 29.69, 30.25, 30.12, 30.575, 30.79, 30.87, 30.94, 30.74, 30.09, 29.97, 30.04, 30.08, 29.84, 29.79, 29.88, 29.42, 28.69, 28.78, 28.83, 29.37, 28.8, 29.015, 29.12, 28.21, 28.2, 28.175, 27.99, 28.01, 28.0, 28.0, 28.03, 28.2, 28.2, 28.2, 28.11, 28.135, 28.19, 27.99, 28.08, 27.84, 28.14, 27.98, 27.99, 27.67, 28.025, 27.17, 27.21, 27.39, 27.325, 27.03, 27.31, 26.69, 26.41, 26.465, 25.63, 24.67, 24.71, 24.47, 24.21, 24.54, 25.04, 26.24, 26.5, 26.06, 27.11, 27.095, 26.23, 26.585, 26.95, 26.015, 26.41, 25.63, 25.75, 26.58, 26.16, 26.78, 25.82, 26.46, 26.0, 25.93, 25.92, 26.505, 26.59, 26.6, 27.24, 27.35, 27.6, 27.45, 28.1, 27.6, 27.6, 27.57, 27.83, 28.24, 28.5, 28.11, 27.99, 28.16, 29.06, 28.98, 29.5, 30.03, 30.0, 29.71, 29.94, 30.26, 29.97, 29.9, 30.31, 30.25, 30.38, 30.535, 30.31, 30.63, 30.44, 30.6, 30.02, 30.36, 30.34, 30.34, 29.7, 30.09, 29.87, 29.92, 31.495, 31.11, 30.88, 30.64, 30.41, 30.29, 30.58, 30.41, 30.1, 29.55, 29.82, 29.67, 29.66, 29.44, 28.73, 28.95, 28.835, 28.89, 28.9, 28.51, 28.455, 28.685, 28.3, 27.485, 27.06, 25.88, 25.41, 25.985, 26.215, 26.52, 26.57, 26.6, 26.49, 27.04, 27.065, 26.88, 26.67, 26.89, 27.01, 26.98, 27.625, 26.91, 27.18, 27.17, 26.76, 27.65, 28.08, 27.94, 27.49, 26.9, 26.865, 27.92, 28.67, 29.81, 30.06, 30.15, 29.91, 29.65, 29.49, 29.73, 30.15, 30.37, 29.07, 31.18, 33.39, 33.38, 33.53, 33.06, 33.0, 32.73, 32.29, 32.83, 32.49, 32.96, 32.75, 33.14, 33.195, 33.465, 33.18, 33.13, 33.1, 32.53, 31.935, 31.81, 32.46, 32.27, 32.21, 32.45, 32.22, 31.89, 32.79, 32.6, 32.51, 33.07, 32.99, 32.725, 32.78, 32.49, 32.17, 32.26, 32.59, 32.82, 32.85, 32.72, 33.0, 33.11, 33.52, 33.58, 33.35, 33.46, 33.44, 33.77, 34.01, 33.8, 34.1, 34.43, 34.23, 34.19, 33.47, 33.625, 33.06, 32.66, 32.36, 32.51, 32.43, 32.34, 33.02, 33.2, 33.57, 33.65, 33.36, 33.78, 33.79, 33.81, 33.96, 33.65, 33.31, 33.12, 33.11, 32.62, 33.18, 33.18, 33.13, 33.18, 33.37, 33.44, 33.225, 33.3, 33.48, 33.235, 32.69, 32.89, 33.15, 32.74, 33.02, 33.26, 34.13, 34.66, 34.4, 33.59, 33.88, 32.98, 33.33, 32.96, 33.49, 33.17, 33.22, 33.65, 33.6, 34.04, 34.07, 34.62, 34.25, 34.045, 33.8, 33.95, 34.06, 34.4, 34.27, 33.84, 33.62, 33.99, 33.65, 33.31, 33.58, 33.27, 33.01, 32.57, 31.77, 33.42, 33.32, 32.46, 31.86, 32.58, 32.36, 32.99, 33.41, 33.24, 33.38, 32.64, 32.78, 33.58, 33.11, 33.29, 33.57, 33.81, 33.835, 34.11, 33.96, 35.26, 35.76, 36.52, 36.9, 37.18, 37.5, 37.31, 37.33, 37.31, 37.485, 37.05, 36.38, 35.8, 34.81, 35.16, 35.46, 35.75, 35.615, 35.77, 36.19, 36.52, 36.15, 35.66, 35.79, 35.72, 35.361, 34.86, 35.15, 34.45, 33.96, 33.8, 33.66, 33.77, 33.82, 33.7, 33.56, 33.19, 33.41, 33.3, 33.24, 33.95, 33.42, 32.01, 33.12, 33.16, 33.3, 33.11, 33.1, 32.64, 32.23, 32.38, 31.64, 32.54, 31.96, 31.63, 31.56, 30.88, 30.96, 30.73, 31.35, 32.2, 33.11, 32.69, 32.99, 33.03, 32.39, 32.87, 33.31, 33.25, 33.4, 32.76, 33.46, 33.2, 33.5, 34.14, 33.93, 33.62, 33.75, 33.92, 34.08, 34.26, 34.49, 34.65, 34.9, 35.115, 35.14, 35.29, 34.92, 34.43, 34.6, 34.83, 34.51, 34.56, 34.7, 34.645, 34.61, 34.73, 34.87, 34.73, 34.62, 34.25, 34.02, 33.69, 33.545, 33.08, 31.68, 31.02, 31.37, 31.87, 31.66, 31.9557, 32.0, 32.24, 32.55, 32.58, 32.26, 32.73, 32.49, 32.77, 32.47, 32.57, 34.155, 34.42, 34.5, 34.62, 34.84, 34.105, 34.62, 35.15, 34.67, 34.34, 34.22, 34.34, 34.22, 34.09, 33.35, 33.515, 33.58, 34.2, 34.34, 34.38, 34.48, 34.39, 34.305, 35.02, 35.05, 35.37, 35.08, 35.11, 34.52, 34.41, 34.625, 34.46, 34.5, 34.16, 33.81, 34.05, 33.64, 33.72, 33.26, 33.28, 33.21, 33.26, 33.46, 34.07, 34.35, 33.45, 33.49, 31.41, 31.66, 32.0, 31.775, 31.69, 31.32, 31.275, 31.03, 31.79, 32.02, 31.9, 32.0, 31.69, 31.7, 31.32, 30.7725, 31.09, 31.02, 31.54, 32.29, 31.65, 31.19, 32.18, 32.77, 32.52, 32.06, 31.13, 30.73, 30.99, 31.31, 31.63, 31.485, 31.82, 31.79, 32.01, 32.25, 31.8, 31.635, 31.45, 32.06, 32.27, 32.93, 33.14, 33.44, 33.01, 32.74, 32.08, 32.55, 32.11, 31.955, 32.23, 32.31, 32.13, 31.8, 31.89, 32.14, 31.835, 31.61, 31.83, 31.51, 31.44, 31.58, 31.56, 31.155, 31.05, 30.45, 31.25, 31.305, 30.21, 30.68, 30.54, 30.66, 31.04, 31.36, 31.245, 31.45, 31.815, 31.97, 31.8075, 31.925, 32.85, 32.59, 33.8, 34.37, 34.27, 34.66, 35.05, 34.6, 34.45, 34.38, 34.41, 34.14, 33.93, 33.59, 33.235, 32.58, 31.96, 32.01, 32.07, 31.66, 31.75, 32.31, 32.1, 32.16, 31.98, 32.315, 32.74, 32.98, 33.0322, 32.875, 32.68, 32.7, 32.96, 32.89, 32.81, 32.955, 33.41, 33.845, 33.79, 33.35, 32.92, 33.25, 33.6, 32.67, 33.77, 33.87, 34.01, 34.24, 34.0, 33.84, 34.58, 34.62, 34.64, 34.85, 34.5]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('83bdae3c-30a0-4b4a-88c2-6d09339197d6');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"ca2b6955-577d-4460-82c7-1c3fe5c17896\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"ca2b6955-577d-4460-82c7-1c3fe5c17896\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'ca2b6955-577d-4460-82c7-1c3fe5c17896',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ca2b6955-577d-4460-82c7-1c3fe5c17896');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D28g1T1BVrKf"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksGC5RYAVrKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "493a33c5-b36f-4f59-fb87-7a72db3d2c9a"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.05, .1, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"FOX\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6940 - accuracy: 0.5148 - val_loss: 0.7265 - val_accuracy: 0.1469\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6924 - accuracy: 0.5161 - val_loss: 0.6894 - val_accuracy: 0.5531\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6539 - accuracy: 0.6154 - val_loss: 0.5665 - val_accuracy: 0.7918\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6514 - accuracy: 0.6450 - val_loss: 0.6896 - val_accuracy: 0.6224\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6312 - accuracy: 0.6664 - val_loss: 0.7353 - val_accuracy: 0.6510\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6920 - accuracy: 0.5174 - val_loss: 0.7236 - val_accuracy: 0.2102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6400 - accuracy: 0.6624 - val_loss: 0.8450 - val_accuracy: 0.3327\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6117 - accuracy: 0.6758 - val_loss: 0.5557 - val_accuracy: 0.7571\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6114 - accuracy: 0.6859 - val_loss: 0.6684 - val_accuracy: 0.6367\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5926 - accuracy: 0.7101 - val_loss: 0.6350 - val_accuracy: 0.6816\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.697867\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.707486\n",
            "[2]\tvalidation_0-auc:0.722023\n",
            "[3]\tvalidation_0-auc:0.750748\n",
            "[4]\tvalidation_0-auc:0.720195\n",
            "[5]\tvalidation_0-auc:0.720096\n",
            "[6]\tvalidation_0-auc:0.746478\n",
            "[7]\tvalidation_0-auc:0.742507\n",
            "[8]\tvalidation_0-auc:0.741893\n",
            "[9]\tvalidation_0-auc:0.742109\n",
            "[10]\tvalidation_0-auc:0.741145\n",
            "[11]\tvalidation_0-auc:0.744086\n",
            "[12]\tvalidation_0-auc:0.754934\n",
            "[13]\tvalidation_0-auc:0.759652\n",
            "[14]\tvalidation_0-auc:0.761247\n",
            "[15]\tvalidation_0-auc:0.760201\n",
            "[16]\tvalidation_0-auc:0.75927\n",
            "[17]\tvalidation_0-auc:0.760516\n",
            "[18]\tvalidation_0-auc:0.76369\n",
            "[19]\tvalidation_0-auc:0.76359\n",
            "[20]\tvalidation_0-auc:0.762892\n",
            "[21]\tvalidation_0-auc:0.763175\n",
            "[22]\tvalidation_0-auc:0.761945\n",
            "[23]\tvalidation_0-auc:0.758639\n",
            "[24]\tvalidation_0-auc:0.755948\n",
            "[25]\tvalidation_0-auc:0.756911\n",
            "[26]\tvalidation_0-auc:0.757011\n",
            "[27]\tvalidation_0-auc:0.756247\n",
            "[28]\tvalidation_0-auc:0.753921\n",
            "[29]\tvalidation_0-auc:0.754685\n",
            "[30]\tvalidation_0-auc:0.752841\n",
            "[31]\tvalidation_0-auc:0.748937\n",
            "[32]\tvalidation_0-auc:0.749502\n",
            "[33]\tvalidation_0-auc:0.747574\n",
            "[34]\tvalidation_0-auc:0.748505\n",
            "[35]\tvalidation_0-auc:0.747807\n",
            "[36]\tvalidation_0-auc:0.745016\n",
            "[37]\tvalidation_0-auc:0.741826\n",
            "[38]\tvalidation_0-auc:0.737141\n",
            "[39]\tvalidation_0-auc:0.735879\n",
            "[40]\tvalidation_0-auc:0.734981\n",
            "[41]\tvalidation_0-auc:0.734317\n",
            "[42]\tvalidation_0-auc:0.734948\n",
            "[43]\tvalidation_0-auc:0.732024\n",
            "[44]\tvalidation_0-auc:0.729034\n",
            "[45]\tvalidation_0-auc:0.727638\n",
            "[46]\tvalidation_0-auc:0.726974\n",
            "[47]\tvalidation_0-auc:0.729648\n",
            "[48]\tvalidation_0-auc:0.73131\n",
            "[49]\tvalidation_0-auc:0.733137\n",
            "[50]\tvalidation_0-auc:0.731376\n",
            "[51]\tvalidation_0-auc:0.731675\n",
            "[52]\tvalidation_0-auc:0.732838\n",
            "[53]\tvalidation_0-auc:0.735496\n",
            "[54]\tvalidation_0-auc:0.734034\n",
            "[55]\tvalidation_0-auc:0.7344\n",
            "[56]\tvalidation_0-auc:0.73327\n",
            "[57]\tvalidation_0-auc:0.733968\n",
            "[58]\tvalidation_0-auc:0.731509\n",
            "[59]\tvalidation_0-auc:0.734034\n",
            "[60]\tvalidation_0-auc:0.733071\n",
            "[61]\tvalidation_0-auc:0.732041\n",
            "[62]\tvalidation_0-auc:0.732805\n",
            "[63]\tvalidation_0-auc:0.730712\n",
            "[64]\tvalidation_0-auc:0.729748\n",
            "[65]\tvalidation_0-auc:0.728901\n",
            "[66]\tvalidation_0-auc:0.723983\n",
            "[67]\tvalidation_0-auc:0.722986\n",
            "[68]\tvalidation_0-auc:0.722222\n",
            "Stopping. Best iteration:\n",
            "[18]\tvalidation_0-auc:0.76369\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6954 - accuracy: 0.4997 - val_loss: 0.7330 - val_accuracy: 0.0853\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6923 - accuracy: 0.5148 - val_loss: 0.7572 - val_accuracy: 0.0678\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6736 - accuracy: 0.5868 - val_loss: 0.7602 - val_accuracy: 0.3195\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6410 - accuracy: 0.6376 - val_loss: 0.8867 - val_accuracy: 0.2516\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6246 - accuracy: 0.6651 - val_loss: 0.5798 - val_accuracy: 0.7221\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6916 - accuracy: 0.5443 - val_loss: 0.7687 - val_accuracy: 0.0875\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6682 - accuracy: 0.5978 - val_loss: 0.6745 - val_accuracy: 0.6696\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6216 - accuracy: 0.6712 - val_loss: 0.6694 - val_accuracy: 0.6389\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6061 - accuracy: 0.6905 - val_loss: 0.5796 - val_accuracy: 0.7549\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5965 - accuracy: 0.6953 - val_loss: 0.6179 - val_accuracy: 0.7068\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.697246\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.66912\n",
            "[2]\tvalidation_0-auc:0.665593\n",
            "[3]\tvalidation_0-auc:0.678322\n",
            "[4]\tvalidation_0-auc:0.694363\n",
            "[5]\tvalidation_0-auc:0.6874\n",
            "[6]\tvalidation_0-auc:0.697951\n",
            "[7]\tvalidation_0-auc:0.705435\n",
            "[8]\tvalidation_0-auc:0.707797\n",
            "[9]\tvalidation_0-auc:0.71252\n",
            "[10]\tvalidation_0-auc:0.717427\n",
            "[11]\tvalidation_0-auc:0.739326\n",
            "[12]\tvalidation_0-auc:0.739541\n",
            "[13]\tvalidation_0-auc:0.735401\n",
            "[14]\tvalidation_0-auc:0.741903\n",
            "[15]\tvalidation_0-auc:0.75595\n",
            "[16]\tvalidation_0-auc:0.753926\n",
            "[17]\tvalidation_0-auc:0.754754\n",
            "[18]\tvalidation_0-auc:0.752055\n",
            "[19]\tvalidation_0-auc:0.748834\n",
            "[20]\tvalidation_0-auc:0.756196\n",
            "[21]\tvalidation_0-auc:0.756318\n",
            "[22]\tvalidation_0-auc:0.756165\n",
            "[23]\tvalidation_0-auc:0.751564\n",
            "[24]\tvalidation_0-auc:0.752699\n",
            "[25]\tvalidation_0-auc:0.748282\n",
            "[26]\tvalidation_0-auc:0.7427\n",
            "[27]\tvalidation_0-auc:0.751718\n",
            "[28]\tvalidation_0-auc:0.758987\n",
            "[29]\tvalidation_0-auc:0.761072\n",
            "[30]\tvalidation_0-auc:0.769783\n",
            "[31]\tvalidation_0-auc:0.760766\n",
            "[32]\tvalidation_0-auc:0.769906\n",
            "[33]\tvalidation_0-auc:0.768617\n",
            "[34]\tvalidation_0-auc:0.774353\n",
            "[35]\tvalidation_0-auc:0.778524\n",
            "[36]\tvalidation_0-auc:0.776622\n",
            "[37]\tvalidation_0-auc:0.779414\n",
            "[38]\tvalidation_0-auc:0.776653\n",
            "[39]\tvalidation_0-auc:0.77607\n",
            "[40]\tvalidation_0-auc:0.774844\n",
            "[41]\tvalidation_0-auc:0.779444\n",
            "[42]\tvalidation_0-auc:0.779475\n",
            "[43]\tvalidation_0-auc:0.779414\n",
            "[44]\tvalidation_0-auc:0.777389\n",
            "[45]\tvalidation_0-auc:0.775058\n",
            "[46]\tvalidation_0-auc:0.771286\n",
            "[47]\tvalidation_0-auc:0.770304\n",
            "[48]\tvalidation_0-auc:0.769752\n",
            "[49]\tvalidation_0-auc:0.771654\n",
            "[50]\tvalidation_0-auc:0.772083\n",
            "[51]\tvalidation_0-auc:0.776009\n",
            "[52]\tvalidation_0-auc:0.776561\n",
            "[53]\tvalidation_0-auc:0.774537\n",
            "[54]\tvalidation_0-auc:0.770243\n",
            "[55]\tvalidation_0-auc:0.769752\n",
            "[56]\tvalidation_0-auc:0.773157\n",
            "[57]\tvalidation_0-auc:0.774322\n",
            "[58]\tvalidation_0-auc:0.773954\n",
            "[59]\tvalidation_0-auc:0.771132\n",
            "[60]\tvalidation_0-auc:0.770335\n",
            "[61]\tvalidation_0-auc:0.771194\n",
            "[62]\tvalidation_0-auc:0.772175\n",
            "[63]\tvalidation_0-auc:0.771194\n",
            "[64]\tvalidation_0-auc:0.772114\n",
            "[65]\tvalidation_0-auc:0.772237\n",
            "[66]\tvalidation_0-auc:0.772421\n",
            "[67]\tvalidation_0-auc:0.770274\n",
            "[68]\tvalidation_0-auc:0.774445\n",
            "[69]\tvalidation_0-auc:0.769906\n",
            "[70]\tvalidation_0-auc:0.769047\n",
            "[71]\tvalidation_0-auc:0.769353\n",
            "[72]\tvalidation_0-auc:0.766961\n",
            "[73]\tvalidation_0-auc:0.766164\n",
            "[74]\tvalidation_0-auc:0.767145\n",
            "[75]\tvalidation_0-auc:0.770764\n",
            "[76]\tvalidation_0-auc:0.77058\n",
            "[77]\tvalidation_0-auc:0.772237\n",
            "[78]\tvalidation_0-auc:0.774138\n",
            "[79]\tvalidation_0-auc:0.778064\n",
            "[80]\tvalidation_0-auc:0.777573\n",
            "[81]\tvalidation_0-auc:0.775672\n",
            "[82]\tvalidation_0-auc:0.775304\n",
            "[83]\tvalidation_0-auc:0.776592\n",
            "[84]\tvalidation_0-auc:0.774813\n",
            "[85]\tvalidation_0-auc:0.780456\n",
            "[86]\tvalidation_0-auc:0.780272\n",
            "[87]\tvalidation_0-auc:0.779291\n",
            "[88]\tvalidation_0-auc:0.778187\n",
            "[89]\tvalidation_0-auc:0.77972\n",
            "[90]\tvalidation_0-auc:0.780334\n",
            "[91]\tvalidation_0-auc:0.781131\n",
            "[92]\tvalidation_0-auc:0.781683\n",
            "[93]\tvalidation_0-auc:0.78199\n",
            "[94]\tvalidation_0-auc:0.784076\n",
            "[95]\tvalidation_0-auc:0.780947\n",
            "[96]\tvalidation_0-auc:0.779475\n",
            "[97]\tvalidation_0-auc:0.77972\n",
            "[98]\tvalidation_0-auc:0.781929\n",
            "[99]\tvalidation_0-auc:0.782174\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.6510204081632653 |  0.2631578947368421 | 0.7638888888888888 | 0.39145907473309605 |\n",
            "|     GRU 0.1      | 0.6816326530612244 |       0.28125       |        0.75        |  0.4090909090909091 |\n",
            "|   XGBoost 0.1    | 0.6877551020408164 |  0.2786885245901639 | 0.7083333333333334 | 0.39999999999999997 |\n",
            "|    Logreg 0.1    | 0.6346938775510204 | 0.25116279069767444 |        0.75        |  0.3763066202090593 |\n",
            "|     SVM 0.1      | 0.6918367346938775 | 0.29949238578680204 | 0.8194444444444444 |  0.4386617100371747 |\n",
            "|  LSTM beta 0.1   | 0.7221006564551422 | 0.18115942028985507 | 0.6410256410256411 |  0.2824858757062147 |\n",
            "|   GRU beta 0.1   | 0.7067833698030634 |  0.1724137931034483 | 0.6410256410256411 | 0.27173913043478265 |\n",
            "| XGBoost beta 0.1 | 0.6477024070021882 | 0.14942528735632185 | 0.6666666666666666 | 0.24413145539906106 |\n",
            "| logreg beta 0.1  | 0.6608315098468271 |  0.1588235294117647 | 0.6923076923076923 | 0.25837320574162675 |\n",
            "|   svm beta 0.1   | 0.6258205689277899 | 0.16666666666666666 | 0.8461538461538461 | 0.27848101265822783 |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6938 - accuracy: 0.5275 - val_loss: 0.6202 - val_accuracy: 0.9633\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6909 - accuracy: 0.5336 - val_loss: 0.7155 - val_accuracy: 0.0939\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6859 - accuracy: 0.5544 - val_loss: 0.6377 - val_accuracy: 0.7204\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6602 - accuracy: 0.6376 - val_loss: 0.6342 - val_accuracy: 0.6816\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6535 - accuracy: 0.6430 - val_loss: 0.7665 - val_accuracy: 0.4898\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6931 - accuracy: 0.5181 - val_loss: 0.5735 - val_accuracy: 0.9776\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6665 - accuracy: 0.6027 - val_loss: 0.7242 - val_accuracy: 0.5286\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6526 - accuracy: 0.6295 - val_loss: 0.6973 - val_accuracy: 0.5980\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6435 - accuracy: 0.6550 - val_loss: 0.6024 - val_accuracy: 0.7286\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6482 - accuracy: 0.6403 - val_loss: 0.6544 - val_accuracy: 0.6531\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.555851\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.47883\n",
            "[2]\tvalidation_0-auc:0.524521\n",
            "[3]\tvalidation_0-auc:0.525798\n",
            "[4]\tvalidation_0-auc:0.475957\n",
            "[5]\tvalidation_0-auc:0.491489\n",
            "[6]\tvalidation_0-auc:0.483777\n",
            "[7]\tvalidation_0-auc:0.481915\n",
            "[8]\tvalidation_0-auc:0.472979\n",
            "[9]\tvalidation_0-auc:0.46766\n",
            "[10]\tvalidation_0-auc:0.448191\n",
            "[11]\tvalidation_0-auc:0.455957\n",
            "[12]\tvalidation_0-auc:0.455106\n",
            "[13]\tvalidation_0-auc:0.446277\n",
            "[14]\tvalidation_0-auc:0.447128\n",
            "[15]\tvalidation_0-auc:0.441702\n",
            "[16]\tvalidation_0-auc:0.41234\n",
            "[17]\tvalidation_0-auc:0.420638\n",
            "[18]\tvalidation_0-auc:0.422128\n",
            "[19]\tvalidation_0-auc:0.408617\n",
            "[20]\tvalidation_0-auc:0.404149\n",
            "[21]\tvalidation_0-auc:0.396809\n",
            "[22]\tvalidation_0-auc:0.395\n",
            "[23]\tvalidation_0-auc:0.394149\n",
            "[24]\tvalidation_0-auc:0.381809\n",
            "[25]\tvalidation_0-auc:0.383936\n",
            "[26]\tvalidation_0-auc:0.376489\n",
            "[27]\tvalidation_0-auc:0.380851\n",
            "[28]\tvalidation_0-auc:0.383723\n",
            "[29]\tvalidation_0-auc:0.388191\n",
            "[30]\tvalidation_0-auc:0.375851\n",
            "[31]\tvalidation_0-auc:0.38\n",
            "[32]\tvalidation_0-auc:0.377872\n",
            "[33]\tvalidation_0-auc:0.368511\n",
            "[34]\tvalidation_0-auc:0.371808\n",
            "[35]\tvalidation_0-auc:0.367128\n",
            "[36]\tvalidation_0-auc:0.359787\n",
            "[37]\tvalidation_0-auc:0.361809\n",
            "[38]\tvalidation_0-auc:0.36383\n",
            "[39]\tvalidation_0-auc:0.37266\n",
            "[40]\tvalidation_0-auc:0.371383\n",
            "[41]\tvalidation_0-auc:0.367553\n",
            "[42]\tvalidation_0-auc:0.364894\n",
            "[43]\tvalidation_0-auc:0.35883\n",
            "[44]\tvalidation_0-auc:0.363404\n",
            "[45]\tvalidation_0-auc:0.364681\n",
            "[46]\tvalidation_0-auc:0.364043\n",
            "[47]\tvalidation_0-auc:0.361277\n",
            "[48]\tvalidation_0-auc:0.35234\n",
            "[49]\tvalidation_0-auc:0.346064\n",
            "[50]\tvalidation_0-auc:0.344681\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.555851\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6965 - accuracy: 0.4942 - val_loss: 0.7045 - val_accuracy: 0.0438\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6895 - accuracy: 0.5079 - val_loss: 0.7310 - val_accuracy: 0.4836\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6701 - accuracy: 0.6012 - val_loss: 0.7222 - val_accuracy: 0.5886\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6469 - accuracy: 0.6356 - val_loss: 0.7288 - val_accuracy: 0.5383\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6511 - accuracy: 0.6513 - val_loss: 0.5943 - val_accuracy: 0.7374\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6795 - accuracy: 0.5649 - val_loss: 0.5565 - val_accuracy: 0.9190\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6433 - accuracy: 0.6431 - val_loss: 0.5664 - val_accuracy: 0.8249\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6390 - accuracy: 0.6596 - val_loss: 0.6492 - val_accuracy: 0.7002\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6366 - accuracy: 0.6568 - val_loss: 0.6663 - val_accuracy: 0.6630\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6199 - accuracy: 0.6671 - val_loss: 0.6513 - val_accuracy: 0.6565\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.407208\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.320881\n",
            "[2]\tvalidation_0-auc:0.566762\n",
            "[3]\tvalidation_0-auc:0.606007\n",
            "[4]\tvalidation_0-auc:0.661613\n",
            "[5]\tvalidation_0-auc:0.691934\n",
            "[6]\tvalidation_0-auc:0.691247\n",
            "[7]\tvalidation_0-auc:0.701087\n",
            "[8]\tvalidation_0-auc:0.708181\n",
            "[9]\tvalidation_0-auc:0.724313\n",
            "[10]\tvalidation_0-auc:0.726201\n",
            "[11]\tvalidation_0-auc:0.733181\n",
            "[12]\tvalidation_0-auc:0.742105\n",
            "[13]\tvalidation_0-auc:0.757208\n",
            "[14]\tvalidation_0-auc:0.740503\n",
            "[15]\tvalidation_0-auc:0.759725\n",
            "[16]\tvalidation_0-auc:0.767391\n",
            "[17]\tvalidation_0-auc:0.774485\n",
            "[18]\tvalidation_0-auc:0.756064\n",
            "[19]\tvalidation_0-auc:0.768535\n",
            "[20]\tvalidation_0-auc:0.750915\n",
            "[21]\tvalidation_0-auc:0.766705\n",
            "[22]\tvalidation_0-auc:0.775858\n",
            "[23]\tvalidation_0-auc:0.760755\n",
            "[24]\tvalidation_0-auc:0.76762\n",
            "[25]\tvalidation_0-auc:0.757895\n",
            "[26]\tvalidation_0-auc:0.738215\n",
            "[27]\tvalidation_0-auc:0.735355\n",
            "[28]\tvalidation_0-auc:0.74119\n",
            "[29]\tvalidation_0-auc:0.731693\n",
            "[30]\tvalidation_0-auc:0.730549\n",
            "[31]\tvalidation_0-auc:0.70881\n",
            "[32]\tvalidation_0-auc:0.722654\n",
            "[33]\tvalidation_0-auc:0.733524\n",
            "[34]\tvalidation_0-auc:0.739588\n",
            "[35]\tvalidation_0-auc:0.756178\n",
            "[36]\tvalidation_0-auc:0.759611\n",
            "[37]\tvalidation_0-auc:0.763616\n",
            "[38]\tvalidation_0-auc:0.770366\n",
            "[39]\tvalidation_0-auc:0.771053\n",
            "[40]\tvalidation_0-auc:0.774714\n",
            "[41]\tvalidation_0-auc:0.76476\n",
            "[42]\tvalidation_0-auc:0.775744\n",
            "[43]\tvalidation_0-auc:0.774485\n",
            "[44]\tvalidation_0-auc:0.77151\n",
            "[45]\tvalidation_0-auc:0.7746\n",
            "[46]\tvalidation_0-auc:0.783753\n",
            "[47]\tvalidation_0-auc:0.789016\n",
            "[48]\tvalidation_0-auc:0.79714\n",
            "[49]\tvalidation_0-auc:0.797483\n",
            "[50]\tvalidation_0-auc:0.798169\n",
            "[51]\tvalidation_0-auc:0.799542\n",
            "[52]\tvalidation_0-auc:0.804691\n",
            "[53]\tvalidation_0-auc:0.813272\n",
            "[54]\tvalidation_0-auc:0.815904\n",
            "[55]\tvalidation_0-auc:0.8246\n",
            "[56]\tvalidation_0-auc:0.820137\n",
            "[57]\tvalidation_0-auc:0.828947\n",
            "[58]\tvalidation_0-auc:0.828604\n",
            "[59]\tvalidation_0-auc:0.827346\n",
            "[60]\tvalidation_0-auc:0.828261\n",
            "[61]\tvalidation_0-auc:0.825515\n",
            "[62]\tvalidation_0-auc:0.819222\n",
            "[63]\tvalidation_0-auc:0.827803\n",
            "[64]\tvalidation_0-auc:0.824828\n",
            "[65]\tvalidation_0-auc:0.825172\n",
            "[66]\tvalidation_0-auc:0.829062\n",
            "[67]\tvalidation_0-auc:0.846911\n",
            "[68]\tvalidation_0-auc:0.853204\n",
            "[69]\tvalidation_0-auc:0.862815\n",
            "[70]\tvalidation_0-auc:0.861785\n",
            "[71]\tvalidation_0-auc:0.858009\n",
            "[72]\tvalidation_0-auc:0.86476\n",
            "[73]\tvalidation_0-auc:0.863844\n",
            "[74]\tvalidation_0-auc:0.864531\n",
            "[75]\tvalidation_0-auc:0.865103\n",
            "[76]\tvalidation_0-auc:0.865561\n",
            "[77]\tvalidation_0-auc:0.862815\n",
            "[78]\tvalidation_0-auc:0.860526\n",
            "[79]\tvalidation_0-auc:0.852975\n",
            "[80]\tvalidation_0-auc:0.854005\n",
            "[81]\tvalidation_0-auc:0.855835\n",
            "[82]\tvalidation_0-auc:0.860526\n",
            "[83]\tvalidation_0-auc:0.858124\n",
            "[84]\tvalidation_0-auc:0.859497\n",
            "[85]\tvalidation_0-auc:0.860298\n",
            "[86]\tvalidation_0-auc:0.864416\n",
            "[87]\tvalidation_0-auc:0.871625\n",
            "[88]\tvalidation_0-auc:0.869565\n",
            "[89]\tvalidation_0-auc:0.871396\n",
            "[90]\tvalidation_0-auc:0.872197\n",
            "[91]\tvalidation_0-auc:0.869794\n",
            "[92]\tvalidation_0-auc:0.869908\n",
            "[93]\tvalidation_0-auc:0.868993\n",
            "[94]\tvalidation_0-auc:0.869908\n",
            "[95]\tvalidation_0-auc:0.870938\n",
            "[96]\tvalidation_0-auc:0.864416\n",
            "[97]\tvalidation_0-auc:0.871854\n",
            "[98]\tvalidation_0-auc:0.868421\n",
            "[99]\tvalidation_0-auc:0.86476\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+----------------------+--------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision       | Recall |       F1 score      |\n",
            "+------------------+--------------------+----------------------+--------+---------------------+\n",
            "|     LSTM 0.2     | 0.4897959183673469 | 0.047244094488188976 |  0.6   | 0.08759124087591241 |\n",
            "|     GRU 0.2      | 0.6530612244897959 | 0.07386363636363637  |  0.65  |  0.1326530612244898 |\n",
            "|   XGBoost 0.2    | 0.7428571428571429 | 0.07936507936507936  |  0.5   | 0.13698630136986303 |\n",
            "|    Logreg 0.2    | 0.6551020408163265 | 0.07428571428571429  |  0.65  | 0.13333333333333333 |\n",
            "|     SVM 0.2      | 0.6673469387755102 | 0.020134228187919462 |  0.15  | 0.03550295857988166 |\n",
            "|  LSTM beta 0.2   | 0.737417943107221  | 0.07627118644067797  |  0.45  | 0.13043478260869565 |\n",
            "|   GRU beta 0.2   | 0.6564551422319475 | 0.07975460122699386  |  0.65  | 0.14207650273224043 |\n",
            "| XGBoost beta 0.2 | 0.7155361050328227 | 0.10714285714285714  |  0.75  |        0.1875       |\n",
            "| logreg beta 0.2  | 0.7352297592997812 | 0.10236220472440945  |  0.65  |  0.1768707482993197 |\n",
            "|   svm beta 0.2   | 0.6739606126914661 | 0.11834319526627218  |  1.0   | 0.21164021164021163 |\n",
            "+------------------+--------------------+----------------------+--------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6968 - accuracy: 0.5060 - val_loss: 0.7242 - val_accuracy: 0.1102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6937 - accuracy: 0.5101 - val_loss: 0.6826 - val_accuracy: 0.8980\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6784 - accuracy: 0.5631 - val_loss: 0.7482 - val_accuracy: 0.4796\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6574 - accuracy: 0.6309 - val_loss: 0.6773 - val_accuracy: 0.5837\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6348 - accuracy: 0.6624 - val_loss: 0.6810 - val_accuracy: 0.6347\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6891 - accuracy: 0.5342 - val_loss: 0.7273 - val_accuracy: 0.3429\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6515 - accuracy: 0.6430 - val_loss: 0.6410 - val_accuracy: 0.7367\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6345 - accuracy: 0.6604 - val_loss: 0.7435 - val_accuracy: 0.5408\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6235 - accuracy: 0.6591 - val_loss: 0.6221 - val_accuracy: 0.6837\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6198 - accuracy: 0.6611 - val_loss: 0.5682 - val_accuracy: 0.7490\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.645897\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.646704\n",
            "[2]\tvalidation_0-auc:0.649507\n",
            "[3]\tvalidation_0-auc:0.644984\n",
            "[4]\tvalidation_0-auc:0.650548\n",
            "[5]\tvalidation_0-auc:0.65866\n",
            "[6]\tvalidation_0-auc:0.654073\n",
            "[7]\tvalidation_0-auc:0.651567\n",
            "[8]\tvalidation_0-auc:0.646088\n",
            "[9]\tvalidation_0-auc:0.651058\n",
            "[10]\tvalidation_0-auc:0.651822\n",
            "[11]\tvalidation_0-auc:0.645345\n",
            "[12]\tvalidation_0-auc:0.646109\n",
            "[13]\tvalidation_0-auc:0.64732\n",
            "[14]\tvalidation_0-auc:0.646683\n",
            "[15]\tvalidation_0-auc:0.648594\n",
            "[16]\tvalidation_0-auc:0.649168\n",
            "[17]\tvalidation_0-auc:0.644602\n",
            "[18]\tvalidation_0-auc:0.63925\n",
            "[19]\tvalidation_0-auc:0.638783\n",
            "[20]\tvalidation_0-auc:0.635682\n",
            "[21]\tvalidation_0-auc:0.634557\n",
            "[22]\tvalidation_0-auc:0.63067\n",
            "[23]\tvalidation_0-auc:0.632454\n",
            "[24]\tvalidation_0-auc:0.628079\n",
            "[25]\tvalidation_0-auc:0.627548\n",
            "[26]\tvalidation_0-auc:0.624745\n",
            "[27]\tvalidation_0-auc:0.621517\n",
            "[28]\tvalidation_0-auc:0.621262\n",
            "[29]\tvalidation_0-auc:0.622069\n",
            "[30]\tvalidation_0-auc:0.620328\n",
            "[31]\tvalidation_0-auc:0.620477\n",
            "[32]\tvalidation_0-auc:0.623428\n",
            "[33]\tvalidation_0-auc:0.622027\n",
            "[34]\tvalidation_0-auc:0.620413\n",
            "[35]\tvalidation_0-auc:0.616081\n",
            "[36]\tvalidation_0-auc:0.613575\n",
            "[37]\tvalidation_0-auc:0.611961\n",
            "[38]\tvalidation_0-auc:0.610665\n",
            "[39]\tvalidation_0-auc:0.608541\n",
            "[40]\tvalidation_0-auc:0.610453\n",
            "[41]\tvalidation_0-auc:0.609773\n",
            "[42]\tvalidation_0-auc:0.609688\n",
            "[43]\tvalidation_0-auc:0.6068\n",
            "[44]\tvalidation_0-auc:0.602892\n",
            "[45]\tvalidation_0-auc:0.601363\n",
            "[46]\tvalidation_0-auc:0.601172\n",
            "[47]\tvalidation_0-auc:0.602446\n",
            "[48]\tvalidation_0-auc:0.603678\n",
            "[49]\tvalidation_0-auc:0.603126\n",
            "[50]\tvalidation_0-auc:0.606057\n",
            "[51]\tvalidation_0-auc:0.603551\n",
            "[52]\tvalidation_0-auc:0.601894\n",
            "[53]\tvalidation_0-auc:0.603041\n",
            "[54]\tvalidation_0-auc:0.601894\n",
            "[55]\tvalidation_0-auc:0.599558\n",
            "Stopping. Best iteration:\n",
            "[5]\tvalidation_0-auc:0.65866\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6973 - accuracy: 0.4900 - val_loss: 0.6579 - val_accuracy: 0.9540\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6864 - accuracy: 0.5724 - val_loss: 0.5927 - val_accuracy: 0.7877\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6472 - accuracy: 0.6500 - val_loss: 0.8015 - val_accuracy: 0.5098\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6449 - accuracy: 0.6328 - val_loss: 0.6329 - val_accuracy: 0.7155\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6175 - accuracy: 0.6740 - val_loss: 0.7343 - val_accuracy: 0.6018\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6802 - accuracy: 0.5772 - val_loss: 0.7726 - val_accuracy: 0.3239\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6381 - accuracy: 0.6671 - val_loss: 0.6657 - val_accuracy: 0.6718\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6080 - accuracy: 0.6953 - val_loss: 0.6455 - val_accuracy: 0.6127\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5953 - accuracy: 0.7042 - val_loss: 0.6839 - val_accuracy: 0.6521\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5751 - accuracy: 0.7104 - val_loss: 0.6356 - val_accuracy: 0.6718\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.432613\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.539155\n",
            "[2]\tvalidation_0-auc:0.573285\n",
            "[3]\tvalidation_0-auc:0.609436\n",
            "[4]\tvalidation_0-auc:0.578528\n",
            "[5]\tvalidation_0-auc:0.578855\n",
            "[6]\tvalidation_0-auc:0.595347\n",
            "[7]\tvalidation_0-auc:0.613914\n",
            "[8]\tvalidation_0-auc:0.621232\n",
            "[9]\tvalidation_0-auc:0.644932\n",
            "[10]\tvalidation_0-auc:0.623198\n",
            "[11]\tvalidation_0-auc:0.669561\n",
            "[12]\tvalidation_0-auc:0.640181\n",
            "[13]\tvalidation_0-auc:0.619321\n",
            "[14]\tvalidation_0-auc:0.6564\n",
            "[15]\tvalidation_0-auc:0.681575\n",
            "[16]\tvalidation_0-auc:0.662789\n",
            "[17]\tvalidation_0-auc:0.691186\n",
            "[18]\tvalidation_0-auc:0.691295\n",
            "[19]\tvalidation_0-auc:0.698504\n",
            "[20]\tvalidation_0-auc:0.705931\n",
            "[21]\tvalidation_0-auc:0.719037\n",
            "[22]\tvalidation_0-auc:0.725917\n",
            "[23]\tvalidation_0-auc:0.732798\n",
            "[24]\tvalidation_0-auc:0.735965\n",
            "[25]\tvalidation_0-auc:0.733453\n",
            "[26]\tvalidation_0-auc:0.723023\n",
            "[27]\tvalidation_0-auc:0.724661\n",
            "[28]\tvalidation_0-auc:0.714613\n",
            "[29]\tvalidation_0-auc:0.703255\n",
            "[30]\tvalidation_0-auc:0.698558\n",
            "[31]\tvalidation_0-auc:0.69692\n",
            "[32]\tvalidation_0-auc:0.714941\n",
            "[33]\tvalidation_0-auc:0.703146\n",
            "[34]\tvalidation_0-auc:0.701398\n",
            "[35]\tvalidation_0-auc:0.69834\n",
            "[36]\tvalidation_0-auc:0.69976\n",
            "[37]\tvalidation_0-auc:0.710135\n",
            "[38]\tvalidation_0-auc:0.704456\n",
            "[39]\tvalidation_0-auc:0.690804\n",
            "[40]\tvalidation_0-auc:0.70533\n",
            "[41]\tvalidation_0-auc:0.709043\n",
            "[42]\tvalidation_0-auc:0.712101\n",
            "[43]\tvalidation_0-auc:0.719747\n",
            "[44]\tvalidation_0-auc:0.718654\n",
            "[45]\tvalidation_0-auc:0.724771\n",
            "[46]\tvalidation_0-auc:0.721931\n",
            "[47]\tvalidation_0-auc:0.721822\n",
            "[48]\tvalidation_0-auc:0.72346\n",
            "[49]\tvalidation_0-auc:0.725754\n",
            "[50]\tvalidation_0-auc:0.731979\n",
            "[51]\tvalidation_0-auc:0.742901\n",
            "[52]\tvalidation_0-auc:0.754478\n",
            "[53]\tvalidation_0-auc:0.752621\n",
            "[54]\tvalidation_0-auc:0.756225\n",
            "[55]\tvalidation_0-auc:0.756662\n",
            "[56]\tvalidation_0-auc:0.760813\n",
            "[57]\tvalidation_0-auc:0.767693\n",
            "[58]\tvalidation_0-auc:0.766383\n",
            "[59]\tvalidation_0-auc:0.76813\n",
            "[60]\tvalidation_0-auc:0.778397\n",
            "[61]\tvalidation_0-auc:0.783639\n",
            "[62]\tvalidation_0-auc:0.784622\n",
            "[63]\tvalidation_0-auc:0.778615\n",
            "[64]\tvalidation_0-auc:0.783858\n",
            "[65]\tvalidation_0-auc:0.783311\n",
            "[66]\tvalidation_0-auc:0.781673\n",
            "[67]\tvalidation_0-auc:0.782984\n",
            "[68]\tvalidation_0-auc:0.788117\n",
            "[69]\tvalidation_0-auc:0.797182\n",
            "[70]\tvalidation_0-auc:0.793141\n",
            "[71]\tvalidation_0-auc:0.792813\n",
            "[72]\tvalidation_0-auc:0.796745\n",
            "[73]\tvalidation_0-auc:0.79194\n",
            "[74]\tvalidation_0-auc:0.797728\n",
            "[75]\tvalidation_0-auc:0.794561\n",
            "[76]\tvalidation_0-auc:0.793141\n",
            "[77]\tvalidation_0-auc:0.797947\n",
            "[78]\tvalidation_0-auc:0.799585\n",
            "[79]\tvalidation_0-auc:0.797837\n",
            "[80]\tvalidation_0-auc:0.79882\n",
            "[81]\tvalidation_0-auc:0.79882\n",
            "[82]\tvalidation_0-auc:0.799694\n",
            "[83]\tvalidation_0-auc:0.797182\n",
            "[84]\tvalidation_0-auc:0.795544\n",
            "[85]\tvalidation_0-auc:0.795544\n",
            "[86]\tvalidation_0-auc:0.796636\n",
            "[87]\tvalidation_0-auc:0.794233\n",
            "[88]\tvalidation_0-auc:0.789428\n",
            "[89]\tvalidation_0-auc:0.789646\n",
            "[90]\tvalidation_0-auc:0.790301\n",
            "[91]\tvalidation_0-auc:0.787462\n",
            "[92]\tvalidation_0-auc:0.790192\n",
            "[93]\tvalidation_0-auc:0.788663\n",
            "[94]\tvalidation_0-auc:0.784622\n",
            "[95]\tvalidation_0-auc:0.780253\n",
            "[96]\tvalidation_0-auc:0.781673\n",
            "[97]\tvalidation_0-auc:0.782329\n",
            "[98]\tvalidation_0-auc:0.785605\n",
            "[99]\tvalidation_0-auc:0.787025\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+----------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision       |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+----------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.6346938775510204 | 0.18592964824120603  |  0.6851851851851852 | 0.29249011857707513 |\n",
            "|      GRU 0.15     | 0.7489795918367347 |  0.2553191489361702  |  0.6666666666666666 | 0.36923076923076914 |\n",
            "|    XGBoost 0.15   | 0.6428571428571429 | 0.18324607329842932  |  0.6481481481481481 |  0.2857142857142857 |\n",
            "|    Logreg 0.15    | 0.6326530612244898 | 0.19117647058823528  |  0.7222222222222222 |  0.3023255813953488 |\n",
            "|      SVM 0.15     | 0.6775510204081633 | 0.22916666666666666  |  0.8148148148148148 |  0.3577235772357723 |\n",
            "|   LSTM beta 0.15  | 0.6017505470459519 | 0.055248618784530384 | 0.47619047619047616 | 0.09900990099009901 |\n",
            "|   GRU beta 0.15   | 0.6717724288840262 | 0.08917197452229299  |  0.6666666666666666 | 0.15730337078651685 |\n",
            "| XGBoost beta 0.15 | 0.6958424507658644 |  0.0958904109589041  |  0.6666666666666666 | 0.16766467065868262 |\n",
            "|  logreg beta 0.15 | 0.6761487964989059 | 0.09032258064516129  |  0.6666666666666666 |  0.1590909090909091 |\n",
            "|   svm beta 0.15   | 0.6608315098468271 | 0.11931818181818182  |         1.0         |  0.2131979695431472 |\n",
            "+-------------------+--------------------+----------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUtL3PM7VrKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e65ba94-eb5e-49bf-d345-ff89ef75c0d7"
      },
      "source": [
        "Result_cross.to_csv('FOX_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.263158</td>\n",
              "      <td>0.651020</td>\n",
              "      <td>0.391459</td>\n",
              "      <td>0.763889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.281250</td>\n",
              "      <td>0.681633</td>\n",
              "      <td>0.409091</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.278689</td>\n",
              "      <td>0.687755</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.708333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.251163</td>\n",
              "      <td>0.634694</td>\n",
              "      <td>0.376307</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.299492</td>\n",
              "      <td>0.691837</td>\n",
              "      <td>0.438662</td>\n",
              "      <td>0.819444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.181159</td>\n",
              "      <td>0.722101</td>\n",
              "      <td>0.282486</td>\n",
              "      <td>0.641026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.172414</td>\n",
              "      <td>0.706783</td>\n",
              "      <td>0.271739</td>\n",
              "      <td>0.641026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.149425</td>\n",
              "      <td>0.647702</td>\n",
              "      <td>0.244131</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.158824</td>\n",
              "      <td>0.660832</td>\n",
              "      <td>0.258373</td>\n",
              "      <td>0.692308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.625821</td>\n",
              "      <td>0.278481</td>\n",
              "      <td>0.846154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.047244</td>\n",
              "      <td>0.489796</td>\n",
              "      <td>0.087591</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.073864</td>\n",
              "      <td>0.653061</td>\n",
              "      <td>0.132653</td>\n",
              "      <td>0.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.079365</td>\n",
              "      <td>0.742857</td>\n",
              "      <td>0.136986</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.074286</td>\n",
              "      <td>0.655102</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.020134</td>\n",
              "      <td>0.667347</td>\n",
              "      <td>0.035503</td>\n",
              "      <td>0.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.076271</td>\n",
              "      <td>0.737418</td>\n",
              "      <td>0.130435</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.079755</td>\n",
              "      <td>0.656455</td>\n",
              "      <td>0.142077</td>\n",
              "      <td>0.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.107143</td>\n",
              "      <td>0.715536</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.102362</td>\n",
              "      <td>0.735230</td>\n",
              "      <td>0.176871</td>\n",
              "      <td>0.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.118343</td>\n",
              "      <td>0.673961</td>\n",
              "      <td>0.211640</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.185930</td>\n",
              "      <td>0.634694</td>\n",
              "      <td>0.292490</td>\n",
              "      <td>0.685185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.255319</td>\n",
              "      <td>0.748980</td>\n",
              "      <td>0.369231</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.183246</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.648148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.191176</td>\n",
              "      <td>0.632653</td>\n",
              "      <td>0.302326</td>\n",
              "      <td>0.722222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.229167</td>\n",
              "      <td>0.677551</td>\n",
              "      <td>0.357724</td>\n",
              "      <td>0.814815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.055249</td>\n",
              "      <td>0.601751</td>\n",
              "      <td>0.099010</td>\n",
              "      <td>0.476190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.089172</td>\n",
              "      <td>0.671772</td>\n",
              "      <td>0.157303</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.095890</td>\n",
              "      <td>0.695842</td>\n",
              "      <td>0.167665</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.090323</td>\n",
              "      <td>0.676149</td>\n",
              "      <td>0.159091</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.119318</td>\n",
              "      <td>0.660832</td>\n",
              "      <td>0.213198</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  FOX  0.263158  0.651020  0.391459  0.763889\n",
              "1            GRU 0.1  FOX  0.281250  0.681633  0.409091  0.750000\n",
              "2        XGBoost 0.1  FOX  0.278689  0.687755  0.400000  0.708333\n",
              "3         Logreg 0.1  FOX  0.251163  0.634694  0.376307  0.750000\n",
              "4            SVM 0.1  FOX  0.299492  0.691837  0.438662  0.819444\n",
              "5      LSTM beta 0.1  FOX  0.181159  0.722101  0.282486  0.641026\n",
              "6       GRU beta 0.1  FOX  0.172414  0.706783  0.271739  0.641026\n",
              "7   XGBoost beta 0.1  FOX  0.149425  0.647702  0.244131  0.666667\n",
              "8    logreg beta 0.1  FOX  0.158824  0.660832  0.258373  0.692308\n",
              "9       svm beta 0.1  FOX  0.166667  0.625821  0.278481  0.846154\n",
              "0           LSTM 0.2  FOX  0.047244  0.489796  0.087591  0.600000\n",
              "1            GRU 0.2  FOX  0.073864  0.653061  0.132653  0.650000\n",
              "2        XGBoost 0.2  FOX  0.079365  0.742857  0.136986  0.500000\n",
              "3         Logreg 0.2  FOX  0.074286  0.655102  0.133333  0.650000\n",
              "4            SVM 0.2  FOX  0.020134  0.667347  0.035503  0.150000\n",
              "5      LSTM beta 0.2  FOX  0.076271  0.737418  0.130435  0.450000\n",
              "6       GRU beta 0.2  FOX  0.079755  0.656455  0.142077  0.650000\n",
              "7   XGBoost beta 0.2  FOX  0.107143  0.715536  0.187500  0.750000\n",
              "8    logreg beta 0.2  FOX  0.102362  0.735230  0.176871  0.650000\n",
              "9       svm beta 0.2  FOX  0.118343  0.673961  0.211640  1.000000\n",
              "0          LSTM 0.15  FOX  0.185930  0.634694  0.292490  0.685185\n",
              "1           GRU 0.15  FOX  0.255319  0.748980  0.369231  0.666667\n",
              "2       XGBoost 0.15  FOX  0.183246  0.642857  0.285714  0.648148\n",
              "3        Logreg 0.15  FOX  0.191176  0.632653  0.302326  0.722222\n",
              "4           SVM 0.15  FOX  0.229167  0.677551  0.357724  0.814815\n",
              "5     LSTM beta 0.15  FOX  0.055249  0.601751  0.099010  0.476190\n",
              "6      GRU beta 0.15  FOX  0.089172  0.671772  0.157303  0.666667\n",
              "7  XGBoost beta 0.15  FOX  0.095890  0.695842  0.167665  0.666667\n",
              "8   logreg beta 0.15  FOX  0.090323  0.676149  0.159091  0.666667\n",
              "9      svm beta 0.15  FOX  0.119318  0.660832  0.213198  1.000000"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEuqZeD1VrKf"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1Bih0EGVrKf"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umveLR73VrKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e4f46c-6e65-4e3b-c1b4-2b796eb39be2"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"FOX\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6968 - accuracy: 0.4893 - val_loss: 0.7350 - val_accuracy: 0.1408\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6894 - accuracy: 0.5262 - val_loss: 0.8239 - val_accuracy: 0.1327\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6544 - accuracy: 0.6396 - val_loss: 0.5856 - val_accuracy: 0.7735\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6273 - accuracy: 0.6732 - val_loss: 0.7838 - val_accuracy: 0.4959\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6338 - accuracy: 0.6658 - val_loss: 0.6133 - val_accuracy: 0.7041\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6896 - accuracy: 0.5309 - val_loss: 0.7469 - val_accuracy: 0.2000\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6531 - accuracy: 0.6154 - val_loss: 0.8913 - val_accuracy: 0.2490\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6222 - accuracy: 0.6644 - val_loss: 0.7317 - val_accuracy: 0.5388\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6041 - accuracy: 0.6906 - val_loss: 0.6294 - val_accuracy: 0.6980\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5951 - accuracy: 0.6973 - val_loss: 0.7271 - val_accuracy: 0.6224\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.697867\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.707486\n",
            "[2]\tvalidation_0-auc:0.722023\n",
            "[3]\tvalidation_0-auc:0.750748\n",
            "[4]\tvalidation_0-auc:0.720195\n",
            "[5]\tvalidation_0-auc:0.720096\n",
            "[6]\tvalidation_0-auc:0.746478\n",
            "[7]\tvalidation_0-auc:0.742507\n",
            "[8]\tvalidation_0-auc:0.741893\n",
            "[9]\tvalidation_0-auc:0.742109\n",
            "[10]\tvalidation_0-auc:0.741145\n",
            "[11]\tvalidation_0-auc:0.744086\n",
            "[12]\tvalidation_0-auc:0.754934\n",
            "[13]\tvalidation_0-auc:0.759652\n",
            "[14]\tvalidation_0-auc:0.761247\n",
            "[15]\tvalidation_0-auc:0.760201\n",
            "[16]\tvalidation_0-auc:0.75927\n",
            "[17]\tvalidation_0-auc:0.760516\n",
            "[18]\tvalidation_0-auc:0.76369\n",
            "[19]\tvalidation_0-auc:0.76359\n",
            "[20]\tvalidation_0-auc:0.762892\n",
            "[21]\tvalidation_0-auc:0.763175\n",
            "[22]\tvalidation_0-auc:0.761945\n",
            "[23]\tvalidation_0-auc:0.758639\n",
            "[24]\tvalidation_0-auc:0.755948\n",
            "[25]\tvalidation_0-auc:0.756911\n",
            "[26]\tvalidation_0-auc:0.757011\n",
            "[27]\tvalidation_0-auc:0.756247\n",
            "[28]\tvalidation_0-auc:0.753921\n",
            "[29]\tvalidation_0-auc:0.754685\n",
            "[30]\tvalidation_0-auc:0.752841\n",
            "[31]\tvalidation_0-auc:0.748937\n",
            "[32]\tvalidation_0-auc:0.749502\n",
            "[33]\tvalidation_0-auc:0.747574\n",
            "[34]\tvalidation_0-auc:0.748505\n",
            "[35]\tvalidation_0-auc:0.747807\n",
            "[36]\tvalidation_0-auc:0.745016\n",
            "[37]\tvalidation_0-auc:0.741826\n",
            "[38]\tvalidation_0-auc:0.737141\n",
            "[39]\tvalidation_0-auc:0.735879\n",
            "[40]\tvalidation_0-auc:0.734981\n",
            "[41]\tvalidation_0-auc:0.734317\n",
            "[42]\tvalidation_0-auc:0.734948\n",
            "[43]\tvalidation_0-auc:0.732024\n",
            "[44]\tvalidation_0-auc:0.729034\n",
            "[45]\tvalidation_0-auc:0.727638\n",
            "[46]\tvalidation_0-auc:0.726974\n",
            "[47]\tvalidation_0-auc:0.729648\n",
            "[48]\tvalidation_0-auc:0.73131\n",
            "[49]\tvalidation_0-auc:0.733137\n",
            "[50]\tvalidation_0-auc:0.731376\n",
            "[51]\tvalidation_0-auc:0.731675\n",
            "[52]\tvalidation_0-auc:0.732838\n",
            "[53]\tvalidation_0-auc:0.735496\n",
            "[54]\tvalidation_0-auc:0.734034\n",
            "[55]\tvalidation_0-auc:0.7344\n",
            "[56]\tvalidation_0-auc:0.73327\n",
            "[57]\tvalidation_0-auc:0.733968\n",
            "[58]\tvalidation_0-auc:0.731509\n",
            "[59]\tvalidation_0-auc:0.734034\n",
            "[60]\tvalidation_0-auc:0.733071\n",
            "[61]\tvalidation_0-auc:0.732041\n",
            "[62]\tvalidation_0-auc:0.732805\n",
            "[63]\tvalidation_0-auc:0.730712\n",
            "[64]\tvalidation_0-auc:0.729748\n",
            "[65]\tvalidation_0-auc:0.728901\n",
            "[66]\tvalidation_0-auc:0.723983\n",
            "[67]\tvalidation_0-auc:0.722986\n",
            "[68]\tvalidation_0-auc:0.722222\n",
            "Stopping. Best iteration:\n",
            "[18]\tvalidation_0-auc:0.76369\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6946 - accuracy: 0.5168 - val_loss: 0.6576 - val_accuracy: 0.9147\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6909 - accuracy: 0.5251 - val_loss: 0.7427 - val_accuracy: 0.0635\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6720 - accuracy: 0.5992 - val_loss: 0.7575 - val_accuracy: 0.3720\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6398 - accuracy: 0.6644 - val_loss: 0.6265 - val_accuracy: 0.7243\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6328 - accuracy: 0.6541 - val_loss: 0.7363 - val_accuracy: 0.5492\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6854 - accuracy: 0.5511 - val_loss: 0.5899 - val_accuracy: 0.8796\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6538 - accuracy: 0.6095 - val_loss: 0.5880 - val_accuracy: 0.8009\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6227 - accuracy: 0.6726 - val_loss: 0.8424 - val_accuracy: 0.4026\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6050 - accuracy: 0.6891 - val_loss: 0.6661 - val_accuracy: 0.6455\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6004 - accuracy: 0.6973 - val_loss: 0.6063 - val_accuracy: 0.7243\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.697246\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.66912\n",
            "[2]\tvalidation_0-auc:0.665593\n",
            "[3]\tvalidation_0-auc:0.678322\n",
            "[4]\tvalidation_0-auc:0.694363\n",
            "[5]\tvalidation_0-auc:0.6874\n",
            "[6]\tvalidation_0-auc:0.697951\n",
            "[7]\tvalidation_0-auc:0.705435\n",
            "[8]\tvalidation_0-auc:0.707797\n",
            "[9]\tvalidation_0-auc:0.71252\n",
            "[10]\tvalidation_0-auc:0.717427\n",
            "[11]\tvalidation_0-auc:0.739326\n",
            "[12]\tvalidation_0-auc:0.739541\n",
            "[13]\tvalidation_0-auc:0.735401\n",
            "[14]\tvalidation_0-auc:0.741903\n",
            "[15]\tvalidation_0-auc:0.75595\n",
            "[16]\tvalidation_0-auc:0.753926\n",
            "[17]\tvalidation_0-auc:0.754754\n",
            "[18]\tvalidation_0-auc:0.752055\n",
            "[19]\tvalidation_0-auc:0.748834\n",
            "[20]\tvalidation_0-auc:0.756196\n",
            "[21]\tvalidation_0-auc:0.756318\n",
            "[22]\tvalidation_0-auc:0.756165\n",
            "[23]\tvalidation_0-auc:0.751564\n",
            "[24]\tvalidation_0-auc:0.752699\n",
            "[25]\tvalidation_0-auc:0.748282\n",
            "[26]\tvalidation_0-auc:0.7427\n",
            "[27]\tvalidation_0-auc:0.751718\n",
            "[28]\tvalidation_0-auc:0.758987\n",
            "[29]\tvalidation_0-auc:0.761072\n",
            "[30]\tvalidation_0-auc:0.769783\n",
            "[31]\tvalidation_0-auc:0.760766\n",
            "[32]\tvalidation_0-auc:0.769906\n",
            "[33]\tvalidation_0-auc:0.768617\n",
            "[34]\tvalidation_0-auc:0.774353\n",
            "[35]\tvalidation_0-auc:0.778524\n",
            "[36]\tvalidation_0-auc:0.776622\n",
            "[37]\tvalidation_0-auc:0.779414\n",
            "[38]\tvalidation_0-auc:0.776653\n",
            "[39]\tvalidation_0-auc:0.77607\n",
            "[40]\tvalidation_0-auc:0.774844\n",
            "[41]\tvalidation_0-auc:0.779444\n",
            "[42]\tvalidation_0-auc:0.779475\n",
            "[43]\tvalidation_0-auc:0.779414\n",
            "[44]\tvalidation_0-auc:0.777389\n",
            "[45]\tvalidation_0-auc:0.775058\n",
            "[46]\tvalidation_0-auc:0.771286\n",
            "[47]\tvalidation_0-auc:0.770304\n",
            "[48]\tvalidation_0-auc:0.769752\n",
            "[49]\tvalidation_0-auc:0.771654\n",
            "[50]\tvalidation_0-auc:0.772083\n",
            "[51]\tvalidation_0-auc:0.776009\n",
            "[52]\tvalidation_0-auc:0.776561\n",
            "[53]\tvalidation_0-auc:0.774537\n",
            "[54]\tvalidation_0-auc:0.770243\n",
            "[55]\tvalidation_0-auc:0.769752\n",
            "[56]\tvalidation_0-auc:0.773157\n",
            "[57]\tvalidation_0-auc:0.774322\n",
            "[58]\tvalidation_0-auc:0.773954\n",
            "[59]\tvalidation_0-auc:0.771132\n",
            "[60]\tvalidation_0-auc:0.770335\n",
            "[61]\tvalidation_0-auc:0.771194\n",
            "[62]\tvalidation_0-auc:0.772175\n",
            "[63]\tvalidation_0-auc:0.771194\n",
            "[64]\tvalidation_0-auc:0.772114\n",
            "[65]\tvalidation_0-auc:0.772237\n",
            "[66]\tvalidation_0-auc:0.772421\n",
            "[67]\tvalidation_0-auc:0.770274\n",
            "[68]\tvalidation_0-auc:0.774445\n",
            "[69]\tvalidation_0-auc:0.769906\n",
            "[70]\tvalidation_0-auc:0.769047\n",
            "[71]\tvalidation_0-auc:0.769353\n",
            "[72]\tvalidation_0-auc:0.766961\n",
            "[73]\tvalidation_0-auc:0.766164\n",
            "[74]\tvalidation_0-auc:0.767145\n",
            "[75]\tvalidation_0-auc:0.770764\n",
            "[76]\tvalidation_0-auc:0.77058\n",
            "[77]\tvalidation_0-auc:0.772237\n",
            "[78]\tvalidation_0-auc:0.774138\n",
            "[79]\tvalidation_0-auc:0.778064\n",
            "[80]\tvalidation_0-auc:0.777573\n",
            "[81]\tvalidation_0-auc:0.775672\n",
            "[82]\tvalidation_0-auc:0.775304\n",
            "[83]\tvalidation_0-auc:0.776592\n",
            "[84]\tvalidation_0-auc:0.774813\n",
            "[85]\tvalidation_0-auc:0.780456\n",
            "[86]\tvalidation_0-auc:0.780272\n",
            "[87]\tvalidation_0-auc:0.779291\n",
            "[88]\tvalidation_0-auc:0.778187\n",
            "[89]\tvalidation_0-auc:0.77972\n",
            "[90]\tvalidation_0-auc:0.780334\n",
            "[91]\tvalidation_0-auc:0.781131\n",
            "[92]\tvalidation_0-auc:0.781683\n",
            "[93]\tvalidation_0-auc:0.78199\n",
            "[94]\tvalidation_0-auc:0.784076\n",
            "[95]\tvalidation_0-auc:0.780947\n",
            "[96]\tvalidation_0-auc:0.779475\n",
            "[97]\tvalidation_0-auc:0.77972\n",
            "[98]\tvalidation_0-auc:0.781929\n",
            "[99]\tvalidation_0-auc:0.782174\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.7040816326530612 | 0.28402366863905326 | 0.6666666666666666 | 0.39834024896265563 |\n",
            "|     GRU 0.1      | 0.6224489795918368 | 0.24663677130044842 | 0.7638888888888888 |  0.3728813559322034 |\n",
            "|   XGBoost 0.1    | 0.6877551020408164 |  0.2786885245901639 | 0.7083333333333334 | 0.39999999999999997 |\n",
            "|    Logreg 0.1    | 0.6346938775510204 | 0.25116279069767444 |        0.75        |  0.3763066202090593 |\n",
            "|     SVM 0.1      | 0.6918367346938775 | 0.29949238578680204 | 0.8194444444444444 |  0.4386617100371747 |\n",
            "|  LSTM beta 0.1   | 0.5492341356673961 |  0.1187214611872146 | 0.6666666666666666 | 0.20155038759689922 |\n",
            "|   GRU beta 0.1   | 0.7242888402625821 | 0.18705035971223022 | 0.6666666666666666 | 0.29213483146067415 |\n",
            "| XGBoost beta 0.1 | 0.6477024070021882 | 0.14942528735632185 | 0.6666666666666666 | 0.24413145539906106 |\n",
            "| logreg beta 0.1  | 0.6608315098468271 |  0.1588235294117647 | 0.6923076923076923 | 0.25837320574162675 |\n",
            "|   svm beta 0.1   | 0.6258205689277899 | 0.16666666666666666 | 0.8461538461538461 | 0.27848101265822783 |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6957 - accuracy: 0.4899 - val_loss: 0.7192 - val_accuracy: 0.0408\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6951 - accuracy: 0.4953 - val_loss: 0.6540 - val_accuracy: 0.9592\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6943 - accuracy: 0.5000 - val_loss: 0.6944 - val_accuracy: 0.4184\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6806 - accuracy: 0.5792 - val_loss: 0.6979 - val_accuracy: 0.5510\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6700 - accuracy: 0.6074 - val_loss: 0.8706 - val_accuracy: 0.1143\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6881 - accuracy: 0.5369 - val_loss: 0.7488 - val_accuracy: 0.1735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6613 - accuracy: 0.6067 - val_loss: 0.6169 - val_accuracy: 0.7776\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6516 - accuracy: 0.6275 - val_loss: 0.7026 - val_accuracy: 0.6184\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6429 - accuracy: 0.6456 - val_loss: 0.7235 - val_accuracy: 0.5796\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6440 - accuracy: 0.6409 - val_loss: 0.6462 - val_accuracy: 0.6918\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.555851\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.47883\n",
            "[2]\tvalidation_0-auc:0.524521\n",
            "[3]\tvalidation_0-auc:0.525798\n",
            "[4]\tvalidation_0-auc:0.475957\n",
            "[5]\tvalidation_0-auc:0.491489\n",
            "[6]\tvalidation_0-auc:0.483777\n",
            "[7]\tvalidation_0-auc:0.481915\n",
            "[8]\tvalidation_0-auc:0.472979\n",
            "[9]\tvalidation_0-auc:0.46766\n",
            "[10]\tvalidation_0-auc:0.448191\n",
            "[11]\tvalidation_0-auc:0.455957\n",
            "[12]\tvalidation_0-auc:0.455106\n",
            "[13]\tvalidation_0-auc:0.446277\n",
            "[14]\tvalidation_0-auc:0.447128\n",
            "[15]\tvalidation_0-auc:0.441702\n",
            "[16]\tvalidation_0-auc:0.41234\n",
            "[17]\tvalidation_0-auc:0.420638\n",
            "[18]\tvalidation_0-auc:0.422128\n",
            "[19]\tvalidation_0-auc:0.408617\n",
            "[20]\tvalidation_0-auc:0.404149\n",
            "[21]\tvalidation_0-auc:0.396809\n",
            "[22]\tvalidation_0-auc:0.395\n",
            "[23]\tvalidation_0-auc:0.394149\n",
            "[24]\tvalidation_0-auc:0.381809\n",
            "[25]\tvalidation_0-auc:0.383936\n",
            "[26]\tvalidation_0-auc:0.376489\n",
            "[27]\tvalidation_0-auc:0.380851\n",
            "[28]\tvalidation_0-auc:0.383723\n",
            "[29]\tvalidation_0-auc:0.388191\n",
            "[30]\tvalidation_0-auc:0.375851\n",
            "[31]\tvalidation_0-auc:0.38\n",
            "[32]\tvalidation_0-auc:0.377872\n",
            "[33]\tvalidation_0-auc:0.368511\n",
            "[34]\tvalidation_0-auc:0.371808\n",
            "[35]\tvalidation_0-auc:0.367128\n",
            "[36]\tvalidation_0-auc:0.359787\n",
            "[37]\tvalidation_0-auc:0.361809\n",
            "[38]\tvalidation_0-auc:0.36383\n",
            "[39]\tvalidation_0-auc:0.37266\n",
            "[40]\tvalidation_0-auc:0.371383\n",
            "[41]\tvalidation_0-auc:0.367553\n",
            "[42]\tvalidation_0-auc:0.364894\n",
            "[43]\tvalidation_0-auc:0.35883\n",
            "[44]\tvalidation_0-auc:0.363404\n",
            "[45]\tvalidation_0-auc:0.364681\n",
            "[46]\tvalidation_0-auc:0.364043\n",
            "[47]\tvalidation_0-auc:0.361277\n",
            "[48]\tvalidation_0-auc:0.35234\n",
            "[49]\tvalidation_0-auc:0.346064\n",
            "[50]\tvalidation_0-auc:0.344681\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.555851\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6915 - accuracy: 0.5065 - val_loss: 0.6505 - val_accuracy: 0.7440\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6711 - accuracy: 0.6095 - val_loss: 0.6372 - val_accuracy: 0.6586\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6616 - accuracy: 0.6253 - val_loss: 0.5745 - val_accuracy: 0.7309\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6547 - accuracy: 0.6266 - val_loss: 0.7031 - val_accuracy: 0.5974\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6523 - accuracy: 0.6500 - val_loss: 0.6002 - val_accuracy: 0.7287\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6782 - accuracy: 0.5710 - val_loss: 0.6704 - val_accuracy: 0.7112\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6490 - accuracy: 0.6225 - val_loss: 0.7608 - val_accuracy: 0.4967\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6392 - accuracy: 0.6616 - val_loss: 0.6041 - val_accuracy: 0.7702\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6303 - accuracy: 0.6513 - val_loss: 0.6410 - val_accuracy: 0.7243\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6243 - accuracy: 0.6699 - val_loss: 0.6192 - val_accuracy: 0.7462\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.407208\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.320881\n",
            "[2]\tvalidation_0-auc:0.566762\n",
            "[3]\tvalidation_0-auc:0.606007\n",
            "[4]\tvalidation_0-auc:0.661613\n",
            "[5]\tvalidation_0-auc:0.691934\n",
            "[6]\tvalidation_0-auc:0.691247\n",
            "[7]\tvalidation_0-auc:0.701087\n",
            "[8]\tvalidation_0-auc:0.708181\n",
            "[9]\tvalidation_0-auc:0.724313\n",
            "[10]\tvalidation_0-auc:0.726201\n",
            "[11]\tvalidation_0-auc:0.733181\n",
            "[12]\tvalidation_0-auc:0.742105\n",
            "[13]\tvalidation_0-auc:0.757208\n",
            "[14]\tvalidation_0-auc:0.740503\n",
            "[15]\tvalidation_0-auc:0.759725\n",
            "[16]\tvalidation_0-auc:0.767391\n",
            "[17]\tvalidation_0-auc:0.774485\n",
            "[18]\tvalidation_0-auc:0.756064\n",
            "[19]\tvalidation_0-auc:0.768535\n",
            "[20]\tvalidation_0-auc:0.750915\n",
            "[21]\tvalidation_0-auc:0.766705\n",
            "[22]\tvalidation_0-auc:0.775858\n",
            "[23]\tvalidation_0-auc:0.760755\n",
            "[24]\tvalidation_0-auc:0.76762\n",
            "[25]\tvalidation_0-auc:0.757895\n",
            "[26]\tvalidation_0-auc:0.738215\n",
            "[27]\tvalidation_0-auc:0.735355\n",
            "[28]\tvalidation_0-auc:0.74119\n",
            "[29]\tvalidation_0-auc:0.731693\n",
            "[30]\tvalidation_0-auc:0.730549\n",
            "[31]\tvalidation_0-auc:0.70881\n",
            "[32]\tvalidation_0-auc:0.722654\n",
            "[33]\tvalidation_0-auc:0.733524\n",
            "[34]\tvalidation_0-auc:0.739588\n",
            "[35]\tvalidation_0-auc:0.756178\n",
            "[36]\tvalidation_0-auc:0.759611\n",
            "[37]\tvalidation_0-auc:0.763616\n",
            "[38]\tvalidation_0-auc:0.770366\n",
            "[39]\tvalidation_0-auc:0.771053\n",
            "[40]\tvalidation_0-auc:0.774714\n",
            "[41]\tvalidation_0-auc:0.76476\n",
            "[42]\tvalidation_0-auc:0.775744\n",
            "[43]\tvalidation_0-auc:0.774485\n",
            "[44]\tvalidation_0-auc:0.77151\n",
            "[45]\tvalidation_0-auc:0.7746\n",
            "[46]\tvalidation_0-auc:0.783753\n",
            "[47]\tvalidation_0-auc:0.789016\n",
            "[48]\tvalidation_0-auc:0.79714\n",
            "[49]\tvalidation_0-auc:0.797483\n",
            "[50]\tvalidation_0-auc:0.798169\n",
            "[51]\tvalidation_0-auc:0.799542\n",
            "[52]\tvalidation_0-auc:0.804691\n",
            "[53]\tvalidation_0-auc:0.813272\n",
            "[54]\tvalidation_0-auc:0.815904\n",
            "[55]\tvalidation_0-auc:0.8246\n",
            "[56]\tvalidation_0-auc:0.820137\n",
            "[57]\tvalidation_0-auc:0.828947\n",
            "[58]\tvalidation_0-auc:0.828604\n",
            "[59]\tvalidation_0-auc:0.827346\n",
            "[60]\tvalidation_0-auc:0.828261\n",
            "[61]\tvalidation_0-auc:0.825515\n",
            "[62]\tvalidation_0-auc:0.819222\n",
            "[63]\tvalidation_0-auc:0.827803\n",
            "[64]\tvalidation_0-auc:0.824828\n",
            "[65]\tvalidation_0-auc:0.825172\n",
            "[66]\tvalidation_0-auc:0.829062\n",
            "[67]\tvalidation_0-auc:0.846911\n",
            "[68]\tvalidation_0-auc:0.853204\n",
            "[69]\tvalidation_0-auc:0.862815\n",
            "[70]\tvalidation_0-auc:0.861785\n",
            "[71]\tvalidation_0-auc:0.858009\n",
            "[72]\tvalidation_0-auc:0.86476\n",
            "[73]\tvalidation_0-auc:0.863844\n",
            "[74]\tvalidation_0-auc:0.864531\n",
            "[75]\tvalidation_0-auc:0.865103\n",
            "[76]\tvalidation_0-auc:0.865561\n",
            "[77]\tvalidation_0-auc:0.862815\n",
            "[78]\tvalidation_0-auc:0.860526\n",
            "[79]\tvalidation_0-auc:0.852975\n",
            "[80]\tvalidation_0-auc:0.854005\n",
            "[81]\tvalidation_0-auc:0.855835\n",
            "[82]\tvalidation_0-auc:0.860526\n",
            "[83]\tvalidation_0-auc:0.858124\n",
            "[84]\tvalidation_0-auc:0.859497\n",
            "[85]\tvalidation_0-auc:0.860298\n",
            "[86]\tvalidation_0-auc:0.864416\n",
            "[87]\tvalidation_0-auc:0.871625\n",
            "[88]\tvalidation_0-auc:0.869565\n",
            "[89]\tvalidation_0-auc:0.871396\n",
            "[90]\tvalidation_0-auc:0.872197\n",
            "[91]\tvalidation_0-auc:0.869794\n",
            "[92]\tvalidation_0-auc:0.869908\n",
            "[93]\tvalidation_0-auc:0.868993\n",
            "[94]\tvalidation_0-auc:0.869908\n",
            "[95]\tvalidation_0-auc:0.870938\n",
            "[96]\tvalidation_0-auc:0.864416\n",
            "[97]\tvalidation_0-auc:0.871854\n",
            "[98]\tvalidation_0-auc:0.868421\n",
            "[99]\tvalidation_0-auc:0.86476\n",
            "end training. \n",
            "\n",
            "+------------------+---------------------+----------------------+--------+---------------------+\n",
            "|      Model       |       Accuracy      |      Precision       | Recall |       F1 score      |\n",
            "+------------------+---------------------+----------------------+--------+---------------------+\n",
            "|     LSTM 0.2     | 0.11428571428571428 |  0.0273972602739726  |  0.6   | 0.05240174672489083 |\n",
            "|     GRU 0.2      |  0.6918367346938775 | 0.07741935483870968  |  0.6   | 0.13714285714285715 |\n",
            "|   XGBoost 0.2    |  0.7428571428571429 | 0.07936507936507936  |  0.5   | 0.13698630136986303 |\n",
            "|    Logreg 0.2    |  0.6551020408163265 | 0.07428571428571429  |  0.65  | 0.13333333333333333 |\n",
            "|     SVM 0.2      |  0.6673469387755102 | 0.020134228187919462 |  0.15  | 0.03550295857988166 |\n",
            "|  LSTM beta 0.2   |  0.7286652078774617 | 0.07377049180327869  |  0.45  |  0.1267605633802817 |\n",
            "|   GRU beta 0.2   |  0.7461706783369803 |         0.1          |  0.6   | 0.17142857142857143 |\n",
            "| XGBoost beta 0.2 |  0.7155361050328227 | 0.10714285714285714  |  0.75  |        0.1875       |\n",
            "| logreg beta 0.2  |  0.7352297592997812 | 0.10236220472440945  |  0.65  |  0.1768707482993197 |\n",
            "|   svm beta 0.2   |  0.6739606126914661 | 0.11834319526627218  |  1.0   | 0.21164021164021163 |\n",
            "+------------------+---------------------+----------------------+--------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6938 - accuracy: 0.5322 - val_loss: 0.7686 - val_accuracy: 0.1102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6911 - accuracy: 0.5228 - val_loss: 0.7093 - val_accuracy: 0.0939\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6665 - accuracy: 0.5953 - val_loss: 0.6726 - val_accuracy: 0.7122\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6700 - accuracy: 0.5779 - val_loss: 0.6764 - val_accuracy: 0.6429\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6448 - accuracy: 0.6530 - val_loss: 0.8857 - val_accuracy: 0.3306\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 17ms/step - loss: 0.6880 - accuracy: 0.5430 - val_loss: 0.6521 - val_accuracy: 0.8082\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6566 - accuracy: 0.6268 - val_loss: 0.6216 - val_accuracy: 0.7735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6291 - accuracy: 0.6698 - val_loss: 0.6825 - val_accuracy: 0.6347\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6342 - accuracy: 0.6758 - val_loss: 0.7914 - val_accuracy: 0.4367\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6206 - accuracy: 0.6852 - val_loss: 0.7014 - val_accuracy: 0.6102\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.645897\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.646704\n",
            "[2]\tvalidation_0-auc:0.649507\n",
            "[3]\tvalidation_0-auc:0.644984\n",
            "[4]\tvalidation_0-auc:0.650548\n",
            "[5]\tvalidation_0-auc:0.65866\n",
            "[6]\tvalidation_0-auc:0.654073\n",
            "[7]\tvalidation_0-auc:0.651567\n",
            "[8]\tvalidation_0-auc:0.646088\n",
            "[9]\tvalidation_0-auc:0.651058\n",
            "[10]\tvalidation_0-auc:0.651822\n",
            "[11]\tvalidation_0-auc:0.645345\n",
            "[12]\tvalidation_0-auc:0.646109\n",
            "[13]\tvalidation_0-auc:0.64732\n",
            "[14]\tvalidation_0-auc:0.646683\n",
            "[15]\tvalidation_0-auc:0.648594\n",
            "[16]\tvalidation_0-auc:0.649168\n",
            "[17]\tvalidation_0-auc:0.644602\n",
            "[18]\tvalidation_0-auc:0.63925\n",
            "[19]\tvalidation_0-auc:0.638783\n",
            "[20]\tvalidation_0-auc:0.635682\n",
            "[21]\tvalidation_0-auc:0.634557\n",
            "[22]\tvalidation_0-auc:0.63067\n",
            "[23]\tvalidation_0-auc:0.632454\n",
            "[24]\tvalidation_0-auc:0.628079\n",
            "[25]\tvalidation_0-auc:0.627548\n",
            "[26]\tvalidation_0-auc:0.624745\n",
            "[27]\tvalidation_0-auc:0.621517\n",
            "[28]\tvalidation_0-auc:0.621262\n",
            "[29]\tvalidation_0-auc:0.622069\n",
            "[30]\tvalidation_0-auc:0.620328\n",
            "[31]\tvalidation_0-auc:0.620477\n",
            "[32]\tvalidation_0-auc:0.623428\n",
            "[33]\tvalidation_0-auc:0.622027\n",
            "[34]\tvalidation_0-auc:0.620413\n",
            "[35]\tvalidation_0-auc:0.616081\n",
            "[36]\tvalidation_0-auc:0.613575\n",
            "[37]\tvalidation_0-auc:0.611961\n",
            "[38]\tvalidation_0-auc:0.610665\n",
            "[39]\tvalidation_0-auc:0.608541\n",
            "[40]\tvalidation_0-auc:0.610453\n",
            "[41]\tvalidation_0-auc:0.609773\n",
            "[42]\tvalidation_0-auc:0.609688\n",
            "[43]\tvalidation_0-auc:0.6068\n",
            "[44]\tvalidation_0-auc:0.602892\n",
            "[45]\tvalidation_0-auc:0.601363\n",
            "[46]\tvalidation_0-auc:0.601172\n",
            "[47]\tvalidation_0-auc:0.602446\n",
            "[48]\tvalidation_0-auc:0.603678\n",
            "[49]\tvalidation_0-auc:0.603126\n",
            "[50]\tvalidation_0-auc:0.606057\n",
            "[51]\tvalidation_0-auc:0.603551\n",
            "[52]\tvalidation_0-auc:0.601894\n",
            "[53]\tvalidation_0-auc:0.603041\n",
            "[54]\tvalidation_0-auc:0.601894\n",
            "[55]\tvalidation_0-auc:0.599558\n",
            "Stopping. Best iteration:\n",
            "[5]\tvalidation_0-auc:0.65866\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6943 - accuracy: 0.5285 - val_loss: 0.7117 - val_accuracy: 0.0284\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6841 - accuracy: 0.5717 - val_loss: 0.7321 - val_accuracy: 0.5514\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6480 - accuracy: 0.6589 - val_loss: 0.6773 - val_accuracy: 0.6630\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6409 - accuracy: 0.6486 - val_loss: 0.7533 - val_accuracy: 0.4989\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6074 - accuracy: 0.6809 - val_loss: 0.7351 - val_accuracy: 0.5295\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6752 - accuracy: 0.5834 - val_loss: 0.6553 - val_accuracy: 0.7681\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6313 - accuracy: 0.6747 - val_loss: 0.6024 - val_accuracy: 0.7571\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6064 - accuracy: 0.6925 - val_loss: 0.6208 - val_accuracy: 0.7330\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5928 - accuracy: 0.7104 - val_loss: 0.4592 - val_accuracy: 0.8490\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5892 - accuracy: 0.7076 - val_loss: 0.7534 - val_accuracy: 0.5514\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.432613\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.539155\n",
            "[2]\tvalidation_0-auc:0.573285\n",
            "[3]\tvalidation_0-auc:0.609436\n",
            "[4]\tvalidation_0-auc:0.578528\n",
            "[5]\tvalidation_0-auc:0.578855\n",
            "[6]\tvalidation_0-auc:0.595347\n",
            "[7]\tvalidation_0-auc:0.613914\n",
            "[8]\tvalidation_0-auc:0.621232\n",
            "[9]\tvalidation_0-auc:0.644932\n",
            "[10]\tvalidation_0-auc:0.623198\n",
            "[11]\tvalidation_0-auc:0.669561\n",
            "[12]\tvalidation_0-auc:0.640181\n",
            "[13]\tvalidation_0-auc:0.619321\n",
            "[14]\tvalidation_0-auc:0.6564\n",
            "[15]\tvalidation_0-auc:0.681575\n",
            "[16]\tvalidation_0-auc:0.662789\n",
            "[17]\tvalidation_0-auc:0.691186\n",
            "[18]\tvalidation_0-auc:0.691295\n",
            "[19]\tvalidation_0-auc:0.698504\n",
            "[20]\tvalidation_0-auc:0.705931\n",
            "[21]\tvalidation_0-auc:0.719037\n",
            "[22]\tvalidation_0-auc:0.725917\n",
            "[23]\tvalidation_0-auc:0.732798\n",
            "[24]\tvalidation_0-auc:0.735965\n",
            "[25]\tvalidation_0-auc:0.733453\n",
            "[26]\tvalidation_0-auc:0.723023\n",
            "[27]\tvalidation_0-auc:0.724661\n",
            "[28]\tvalidation_0-auc:0.714613\n",
            "[29]\tvalidation_0-auc:0.703255\n",
            "[30]\tvalidation_0-auc:0.698558\n",
            "[31]\tvalidation_0-auc:0.69692\n",
            "[32]\tvalidation_0-auc:0.714941\n",
            "[33]\tvalidation_0-auc:0.703146\n",
            "[34]\tvalidation_0-auc:0.701398\n",
            "[35]\tvalidation_0-auc:0.69834\n",
            "[36]\tvalidation_0-auc:0.69976\n",
            "[37]\tvalidation_0-auc:0.710135\n",
            "[38]\tvalidation_0-auc:0.704456\n",
            "[39]\tvalidation_0-auc:0.690804\n",
            "[40]\tvalidation_0-auc:0.70533\n",
            "[41]\tvalidation_0-auc:0.709043\n",
            "[42]\tvalidation_0-auc:0.712101\n",
            "[43]\tvalidation_0-auc:0.719747\n",
            "[44]\tvalidation_0-auc:0.718654\n",
            "[45]\tvalidation_0-auc:0.724771\n",
            "[46]\tvalidation_0-auc:0.721931\n",
            "[47]\tvalidation_0-auc:0.721822\n",
            "[48]\tvalidation_0-auc:0.72346\n",
            "[49]\tvalidation_0-auc:0.725754\n",
            "[50]\tvalidation_0-auc:0.731979\n",
            "[51]\tvalidation_0-auc:0.742901\n",
            "[52]\tvalidation_0-auc:0.754478\n",
            "[53]\tvalidation_0-auc:0.752621\n",
            "[54]\tvalidation_0-auc:0.756225\n",
            "[55]\tvalidation_0-auc:0.756662\n",
            "[56]\tvalidation_0-auc:0.760813\n",
            "[57]\tvalidation_0-auc:0.767693\n",
            "[58]\tvalidation_0-auc:0.766383\n",
            "[59]\tvalidation_0-auc:0.76813\n",
            "[60]\tvalidation_0-auc:0.778397\n",
            "[61]\tvalidation_0-auc:0.783639\n",
            "[62]\tvalidation_0-auc:0.784622\n",
            "[63]\tvalidation_0-auc:0.778615\n",
            "[64]\tvalidation_0-auc:0.783858\n",
            "[65]\tvalidation_0-auc:0.783311\n",
            "[66]\tvalidation_0-auc:0.781673\n",
            "[67]\tvalidation_0-auc:0.782984\n",
            "[68]\tvalidation_0-auc:0.788117\n",
            "[69]\tvalidation_0-auc:0.797182\n",
            "[70]\tvalidation_0-auc:0.793141\n",
            "[71]\tvalidation_0-auc:0.792813\n",
            "[72]\tvalidation_0-auc:0.796745\n",
            "[73]\tvalidation_0-auc:0.79194\n",
            "[74]\tvalidation_0-auc:0.797728\n",
            "[75]\tvalidation_0-auc:0.794561\n",
            "[76]\tvalidation_0-auc:0.793141\n",
            "[77]\tvalidation_0-auc:0.797947\n",
            "[78]\tvalidation_0-auc:0.799585\n",
            "[79]\tvalidation_0-auc:0.797837\n",
            "[80]\tvalidation_0-auc:0.79882\n",
            "[81]\tvalidation_0-auc:0.79882\n",
            "[82]\tvalidation_0-auc:0.799694\n",
            "[83]\tvalidation_0-auc:0.797182\n",
            "[84]\tvalidation_0-auc:0.795544\n",
            "[85]\tvalidation_0-auc:0.795544\n",
            "[86]\tvalidation_0-auc:0.796636\n",
            "[87]\tvalidation_0-auc:0.794233\n",
            "[88]\tvalidation_0-auc:0.789428\n",
            "[89]\tvalidation_0-auc:0.789646\n",
            "[90]\tvalidation_0-auc:0.790301\n",
            "[91]\tvalidation_0-auc:0.787462\n",
            "[92]\tvalidation_0-auc:0.790192\n",
            "[93]\tvalidation_0-auc:0.788663\n",
            "[94]\tvalidation_0-auc:0.784622\n",
            "[95]\tvalidation_0-auc:0.780253\n",
            "[96]\tvalidation_0-auc:0.781673\n",
            "[97]\tvalidation_0-auc:0.782329\n",
            "[98]\tvalidation_0-auc:0.785605\n",
            "[99]\tvalidation_0-auc:0.787025\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.3306122448979592 | 0.12771739130434784 | 0.8703703703703703 |  0.2227488151658768 |\n",
            "|      GRU 0.15     | 0.610204081632653  |  0.1813953488372093 | 0.7222222222222222 |  0.2899628252788104 |\n",
            "|    XGBoost 0.15   | 0.6428571428571429 | 0.18324607329842932 | 0.6481481481481481 |  0.2857142857142857 |\n",
            "|    Logreg 0.15    | 0.6326530612244898 | 0.19117647058823528 | 0.7222222222222222 |  0.3023255813953488 |\n",
            "|      SVM 0.15     | 0.6775510204081633 | 0.22916666666666666 | 0.8148148148148148 |  0.3577235772357723 |\n",
            "|   LSTM beta 0.15  | 0.5295404814004376 | 0.06306306306306306 | 0.6666666666666666 | 0.11522633744855966 |\n",
            "|   GRU beta 0.15   | 0.5514223194748359 | 0.06190476190476191 | 0.6190476190476191 | 0.11255411255411256 |\n",
            "| XGBoost beta 0.15 | 0.6958424507658644 |  0.0958904109589041 | 0.6666666666666666 | 0.16766467065868262 |\n",
            "|  logreg beta 0.15 | 0.6761487964989059 | 0.09032258064516129 | 0.6666666666666666 |  0.1590909090909091 |\n",
            "|   svm beta 0.15   | 0.6608315098468271 | 0.11931818181818182 |        1.0         |  0.2131979695431472 |\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4ZDRP2MVrKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6874019b-423d-4d04-e8e8-0c25fae13761"
      },
      "source": [
        "Result_purging.to_csv('FOX_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.284024</td>\n",
              "      <td>0.704082</td>\n",
              "      <td>0.398340</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.246637</td>\n",
              "      <td>0.622449</td>\n",
              "      <td>0.372881</td>\n",
              "      <td>0.763889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.278689</td>\n",
              "      <td>0.687755</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.708333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.251163</td>\n",
              "      <td>0.634694</td>\n",
              "      <td>0.376307</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.299492</td>\n",
              "      <td>0.691837</td>\n",
              "      <td>0.438662</td>\n",
              "      <td>0.819444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.118721</td>\n",
              "      <td>0.549234</td>\n",
              "      <td>0.201550</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.187050</td>\n",
              "      <td>0.724289</td>\n",
              "      <td>0.292135</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.149425</td>\n",
              "      <td>0.647702</td>\n",
              "      <td>0.244131</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.158824</td>\n",
              "      <td>0.660832</td>\n",
              "      <td>0.258373</td>\n",
              "      <td>0.692308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.625821</td>\n",
              "      <td>0.278481</td>\n",
              "      <td>0.846154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.027397</td>\n",
              "      <td>0.114286</td>\n",
              "      <td>0.052402</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.077419</td>\n",
              "      <td>0.691837</td>\n",
              "      <td>0.137143</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.079365</td>\n",
              "      <td>0.742857</td>\n",
              "      <td>0.136986</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.074286</td>\n",
              "      <td>0.655102</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.020134</td>\n",
              "      <td>0.667347</td>\n",
              "      <td>0.035503</td>\n",
              "      <td>0.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.073770</td>\n",
              "      <td>0.728665</td>\n",
              "      <td>0.126761</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.746171</td>\n",
              "      <td>0.171429</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.107143</td>\n",
              "      <td>0.715536</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.102362</td>\n",
              "      <td>0.735230</td>\n",
              "      <td>0.176871</td>\n",
              "      <td>0.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.118343</td>\n",
              "      <td>0.673961</td>\n",
              "      <td>0.211640</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.127717</td>\n",
              "      <td>0.330612</td>\n",
              "      <td>0.222749</td>\n",
              "      <td>0.870370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.610204</td>\n",
              "      <td>0.289963</td>\n",
              "      <td>0.722222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.183246</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.648148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.191176</td>\n",
              "      <td>0.632653</td>\n",
              "      <td>0.302326</td>\n",
              "      <td>0.722222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.229167</td>\n",
              "      <td>0.677551</td>\n",
              "      <td>0.357724</td>\n",
              "      <td>0.814815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.063063</td>\n",
              "      <td>0.529540</td>\n",
              "      <td>0.115226</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.061905</td>\n",
              "      <td>0.551422</td>\n",
              "      <td>0.112554</td>\n",
              "      <td>0.619048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.095890</td>\n",
              "      <td>0.695842</td>\n",
              "      <td>0.167665</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.090323</td>\n",
              "      <td>0.676149</td>\n",
              "      <td>0.159091</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.119318</td>\n",
              "      <td>0.660832</td>\n",
              "      <td>0.213198</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  FOX  0.284024  0.704082  0.398340  0.666667\n",
              "1            GRU 0.1  FOX  0.246637  0.622449  0.372881  0.763889\n",
              "2        XGBoost 0.1  FOX  0.278689  0.687755  0.400000  0.708333\n",
              "3         Logreg 0.1  FOX  0.251163  0.634694  0.376307  0.750000\n",
              "4            SVM 0.1  FOX  0.299492  0.691837  0.438662  0.819444\n",
              "5      LSTM beta 0.1  FOX  0.118721  0.549234  0.201550  0.666667\n",
              "6       GRU beta 0.1  FOX  0.187050  0.724289  0.292135  0.666667\n",
              "7   XGBoost beta 0.1  FOX  0.149425  0.647702  0.244131  0.666667\n",
              "8    logreg beta 0.1  FOX  0.158824  0.660832  0.258373  0.692308\n",
              "9       svm beta 0.1  FOX  0.166667  0.625821  0.278481  0.846154\n",
              "0           LSTM 0.2  FOX  0.027397  0.114286  0.052402  0.600000\n",
              "1            GRU 0.2  FOX  0.077419  0.691837  0.137143  0.600000\n",
              "2        XGBoost 0.2  FOX  0.079365  0.742857  0.136986  0.500000\n",
              "3         Logreg 0.2  FOX  0.074286  0.655102  0.133333  0.650000\n",
              "4            SVM 0.2  FOX  0.020134  0.667347  0.035503  0.150000\n",
              "5      LSTM beta 0.2  FOX  0.073770  0.728665  0.126761  0.450000\n",
              "6       GRU beta 0.2  FOX  0.100000  0.746171  0.171429  0.600000\n",
              "7   XGBoost beta 0.2  FOX  0.107143  0.715536  0.187500  0.750000\n",
              "8    logreg beta 0.2  FOX  0.102362  0.735230  0.176871  0.650000\n",
              "9       svm beta 0.2  FOX  0.118343  0.673961  0.211640  1.000000\n",
              "0          LSTM 0.15  FOX  0.127717  0.330612  0.222749  0.870370\n",
              "1           GRU 0.15  FOX  0.181395  0.610204  0.289963  0.722222\n",
              "2       XGBoost 0.15  FOX  0.183246  0.642857  0.285714  0.648148\n",
              "3        Logreg 0.15  FOX  0.191176  0.632653  0.302326  0.722222\n",
              "4           SVM 0.15  FOX  0.229167  0.677551  0.357724  0.814815\n",
              "5     LSTM beta 0.15  FOX  0.063063  0.529540  0.115226  0.666667\n",
              "6      GRU beta 0.15  FOX  0.061905  0.551422  0.112554  0.619048\n",
              "7  XGBoost beta 0.15  FOX  0.095890  0.695842  0.167665  0.666667\n",
              "8   logreg beta 0.15  FOX  0.090323  0.676149  0.159091  0.666667\n",
              "9      svm beta 0.15  FOX  0.119318  0.660832  0.213198  1.000000"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u3EKgRCVrKg"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9pDBGNZVrKg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqt4rK47WILc"
      },
      "source": [
        "## FOXA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixgyea-lWILi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33641b1e-f498-41a5-ab01-c690c17e75ae"
      },
      "source": [
        "dfs = pd.read_csv(\"FOXA.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "# Denoise prices\n",
        "dfs = denoise_data(dfs, '<CLOSE>')\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2768</td>\n",
              "      <td>US1.FOXA</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>40.44</td>\n",
              "      <td>41.065</td>\n",
              "      <td>40.1200</td>\n",
              "      <td>40.79</td>\n",
              "      <td>76891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2767</td>\n",
              "      <td>US1.FOXA</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>40.75</td>\n",
              "      <td>41.060</td>\n",
              "      <td>40.1100</td>\n",
              "      <td>40.11</td>\n",
              "      <td>158785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.FOXA</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>39.81</td>\n",
              "      <td>40.310</td>\n",
              "      <td>39.6500</td>\n",
              "      <td>40.24</td>\n",
              "      <td>91517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.FOXA</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>39.87</td>\n",
              "      <td>40.420</td>\n",
              "      <td>39.7000</td>\n",
              "      <td>39.84</td>\n",
              "      <td>163999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.FOXA</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>39.10</td>\n",
              "      <td>40.370</td>\n",
              "      <td>39.1000</td>\n",
              "      <td>40.11</td>\n",
              "      <td>180216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.FOXA</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>13.68</td>\n",
              "      <td>13.950</td>\n",
              "      <td>13.5675</td>\n",
              "      <td>13.88</td>\n",
              "      <td>20812401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.FOXA</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>13.48</td>\n",
              "      <td>13.750</td>\n",
              "      <td>13.3100</td>\n",
              "      <td>13.64</td>\n",
              "      <td>28318775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.FOXA</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>13.58</td>\n",
              "      <td>13.650</td>\n",
              "      <td>13.3200</td>\n",
              "      <td>13.48</td>\n",
              "      <td>16218875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.FOXA</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>13.28</td>\n",
              "      <td>13.580</td>\n",
              "      <td>13.1700</td>\n",
              "      <td>13.56</td>\n",
              "      <td>17732763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.FOXA</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>13.16</td>\n",
              "      <td>13.250</td>\n",
              "      <td>12.8800</td>\n",
              "      <td>12.97</td>\n",
              "      <td>15096088</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2769 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  <TICKER> <PER>    <DATE>  ...  <HIGH>    <LOW>  <CLOSE>     <VOL>\n",
              "0      2768  US1.FOXA     D  20211001  ...  41.065  40.1200    40.79     76891\n",
              "1      2767  US1.FOXA     D  20210930  ...  41.060  40.1100    40.11    158785\n",
              "2      2766  US1.FOXA     D  20210929  ...  40.310  39.6500    40.24     91517\n",
              "3      2765  US1.FOXA     D  20210928  ...  40.420  39.7000    39.84    163999\n",
              "4      2764  US1.FOXA     D  20210927  ...  40.370  39.1000    40.11    180216\n",
              "...     ...       ...   ...       ...  ...     ...      ...      ...       ...\n",
              "2764      4  US1.FOXA     D  20101008  ...  13.950  13.5675    13.88  20812401\n",
              "2765      3  US1.FOXA     D  20101007  ...  13.750  13.3100    13.64  28318775\n",
              "2766      2  US1.FOXA     D  20101006  ...  13.650  13.3200    13.48  16218875\n",
              "2767      1  US1.FOXA     D  20101005  ...  13.580  13.1700    13.56  17732763\n",
              "2768      0  US1.FOXA     D  20101004  ...  13.250  12.8800    12.97  15096088\n",
              "\n",
              "[2769 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQEWUYhuWILi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df59a0f9-17ff-4a7c-d770-f5abcecdaf02"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"e84bc31c-8410-4550-ac59-53259207602c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"e84bc31c-8410-4550-ac59-53259207602c\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'e84bc31c-8410-4550-ac59-53259207602c',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [30.06, 30.605, 30.57, 30.46, 30.41, 31.25, 31.54, 31.47, 31.455, 32.01, 31.57, 32.26, 32.25, 32.05, 32.65, 32.87, 33.59, 33.82, 33.77, 34.0, 34.29, 34.3, 34.24, 34.25, 33.555, 32.77, 33.14, 32.74, 31.78, 31.94, 32.06, 31.76, 33.05, 33.41, 33.57, 33.88, 32.98, 32.75, 33.69, 35.07, 34.93, 35.41, 35.14, 37.03, 36.45, 36.06, 36.61, 36.91, 37.31, 37.83, 37.5, 37.2, 37.19, 36.61, 36.22, 36.11, 36.44, 36.56, 36.17, 36.67, 37.01, 36.96, 36.29, 36.14, 36.02, 36.13, 36.6, 36.35, 36.59, 35.92, 36.63, 35.99, 35.47, 35.36, 35.8, 36.14, 35.57, 36.0, 36.2, 36.15, 35.24, 35.3, 35.06, 34.17, 34.03, 34.01, 34.12, 34.36, 34.47, 34.22, 35.23, 35.21, 35.8, 36.62, 36.94, 36.91, 37.13, 37.66, 37.14, 37.3, 36.74, 37.72, 37.91, 37.97, 38.56, 38.48, 37.42, 37.28, 37.91, 37.46, 37.27, 39.43, 38.98, 39.21, 38.49, 38.15, 37.96, 38.06, 37.72, 38.44, 38.84, 37.76, 37.53, 37.52, 37.08, 36.49, 37.3, 37.47, 37.69, 36.76, 36.05, 36.29, 36.86, 36.72, 36.39, 36.93, 37.54, 37.45, 38.71, 40.04, 38.59, 40.38, 51.39, 51.95, 51.77, 51.21, 50.95, 50.53, 50.46, 50.58, 50.64, 50.53, 50.66, 50.61, 50.45, 50.42, 50.86, 50.73, 51.16, 50.75, 50.79, 50.525, 50.4, 49.985, 49.65, 49.54, 49.55, 49.68, 49.5, 49.43, 49.26, 49.3, 49.3, 49.23, 49.04, 49.1, 49.23, 49.24, 48.89, 48.64, 48.71, 48.74, 48.405, 48.36, 48.62, 48.68, 48.75, 48.83, 48.78, 48.58, 48.18, 48.07, 47.47, 47.79, 48.12, 47.97, 48.01, 47.84, 46.16, 46.95, 48.01, 48.51, 48.79, 49.0, 49.05, 49.135, 49.14, 49.255, 49.35, 49.26, 49.5, 49.19, 49.675, 49.48, 49.55, 49.47, 49.14, 48.86, 48.94, 48.86, 48.58, 48.91, 48.19, 48.28, 47.93, 47.72, 48.07, 48.14, 47.85, 47.515, 47.17, 46.84, 46.52, 46.56, 45.5, 45.65, 45.5, 45.14, 45.25, 44.705, 45.59, 45.72, 45.76, 45.81, 45.82, 45.86, 45.53, 45.35, 45.12, 44.7, 45.49, 45.72, 46.1, 46.56, 46.82, 46.58, 46.19, 46.34, 45.82, 45.68, 45.21, 45.04, 44.33, 44.59, 44.6, 44.54, 44.71, 44.8, 44.8, 44.63, 44.73, 44.95, 45.23, 45.35, 45.27, 45.32, 45.38, 45.31, 45.61, 45.22, 45.16, 45.15, 45.27, 45.34, 45.36, 45.39, 45.37, 45.71, 45.35, 45.48, 45.31, 45.48, 45.54, 45.46, 45.51, 45.69, 45.42, 45.02, 45.02, 45.01, 45.1, 45.15, 45.42, 45.18, 45.25, 45.92, 46.01, 46.55, 46.68, 46.47, 46.72, 47.56, 47.38, 47.8, 49.77, 49.55, 48.94, 48.73, 48.78, 49.2, 49.76, 49.79, 48.88, 47.68, 48.15, 48.61, 48.35, 48.1, 44.7, 44.55, 44.59, 44.58, 43.69, 40.51, 40.13, 39.9825, 39.65, 39.365, 39.09, 38.64, 38.54, 38.58, 38.65, 38.64, 38.95, 38.92, 38.74, 38.17, 37.92, 37.67, 37.75, 38.16, 37.89, 37.77, 37.94, 37.91, 37.69, 37.99, 38.05, 37.66, 36.88, 36.74, 36.57, 36.57, 36.38, 36.26, 36.59, 36.02, 36.49, 36.69, 37.2, 37.32, 37.54, 37.03, 36.79, 36.48, 36.205, 36.27, 35.7, 35.68, 36.35, 36.14, 36.15, 36.15, 36.65, 35.89, 36.49, 37.03, 36.11, 36.76, 36.87, 36.7, 36.79, 37.14, 37.15, 37.7, 37.4, 37.98, 37.74, 37.19, 37.02, 37.54, 36.65, 36.37, 35.93, 36.83, 37.63, 38.82, 38.04, 37.18, 37.05, 37.13, 37.46, 36.97, 36.55, 36.58, 36.41, 35.69, 34.58, 36.07, 36.8, 36.15, 36.72, 37.01, 36.91, 38.46, 38.74, 38.8, 38.19, 38.01, 37.52, 37.3, 36.66, 36.19, 37.11, 36.3, 36.72, 36.2, 35.56, 36.54, 36.24, 36.75, 36.82, 36.55, 35.87, 34.54, 34.48, 34.56, 34.61, 35.24, 35.12, 34.27, 35.06, 35.2, 34.84, 34.88, 32.75, 34.08, 33.66, 33.3, 34.19, 33.21, 33.0, 33.08, 32.19, 31.93, 32.1, 30.87, 30.39, 30.4, 30.6, 30.88, 30.67, 31.15, 29.33, 28.74, 28.04, 28.55, 28.83, 28.7, 28.125, 27.76, 27.45, 24.96, 25.81, 25.98, 26.145, 26.4, 26.4, 26.32, 26.13, 26.24, 26.7, 27.5, 27.37, 27.2, 26.875, 26.69, 26.52, 26.145, 26.12, 26.77, 26.57, 26.99, 26.95, 27.095, 26.64, 26.6, 26.375, 26.555, 27.33, 26.96, 27.125, 26.88, 26.76, 26.86, 26.7, 26.505, 27.06, 26.51, 26.59, 25.91, 26.055, 25.78, 25.79, 26.43, 26.57, 27.49, 27.6, 27.46, 27.36, 27.26, 27.24, 27.09, 27.155, 27.43, 27.36, 27.27, 27.35, 27.99, 27.86, 28.2, 28.23, 27.88, 27.9, 28.02, 28.19, 28.46, 28.82, 28.67, 29.23, 29.1, 29.37, 29.62, 28.18, 27.94, 27.69, 27.83, 27.8, 27.64, 27.36, 27.38, 27.73, 27.58, 28.65, 28.11, 28.29, 27.9, 27.95, 28.37, 28.52, 28.33, 28.18, 28.11, 27.61, 28.05, 27.19, 26.965, 27.25, 27.17, 27.8, 27.45, 27.74, 27.61, 28.06, 28.75, 28.17, 27.395, 27.58, 27.53, 27.74, 27.93, 27.82, 27.12, 27.045, 27.02, 27.33, 26.87, 26.75, 27.255, 27.11, 26.86, 26.85, 27.33, 27.71, 27.925, 28.51, 27.89, 28.23, 28.4, 29.14, 29.04, 28.88, 30.43, 30.39, 30.535, 30.56, 30.74, 30.615, 30.35, 30.71, 31.07, 30.39, 30.66, 30.73, 30.43, 30.65, 31.15, 31.23, 31.07, 31.12, 31.38, 31.75, 32.15, 32.4, 32.37, 32.12, 32.45, 31.9, 31.52, 31.33, 30.97, 30.56, 30.63, 30.81, 30.74, 30.74, 30.515, 30.5, 30.56, 30.7, 30.355, 30.51, 30.69, 30.46, 30.2, 30.4, 29.92, 30.435, 30.61, 30.35, 30.47, 30.54, 30.35, 30.51, 30.7, 30.31, 29.72, 29.91, 30.12, 30.2, 30.48, 31.06, 31.4, 31.46, 31.62, 31.39, 31.38, 31.295, 31.01, 30.29, 30.27, 30.01, 30.09, 29.81, 30.27, 29.94, 30.03, 29.81, 30.08, 29.68, 29.47, 29.52, 29.0, 29.02, 28.715, 28.04, 28.27, 28.46, 28.53, 28.385, 28.44, 28.44, 28.175, 28.13, 28.03, 28.0, 28.02, 27.03, 26.34, 28.21, 28.64, 28.11, 27.5, 27.56, 27.395, 28.21, 28.12, 28.47, 28.31, 28.36, 28.4, 28.27, 27.8, 27.81, 27.92, 27.54, 27.45, 27.52, 27.13, 27.16, 27.45, 27.08, 27.39, 26.94, 27.72, 25.825, 26.18, 26.27, 26.26, 26.11, 26.4, 25.56, 25.56, 25.83, 25.275, 25.13, 24.96, 24.685, 24.735, 24.57, 24.54, 24.655, 24.71, 24.35, 24.53, 24.65, 24.75, 24.73, 24.22, 24.03, 24.31, 24.21, 23.8, 23.93, 24.3, 24.0, 23.89, 23.94, 23.89, 23.84, 23.7, 23.72, 23.95, 23.57, 24.74, 24.38, 24.42, 24.55, 24.53, 24.54, 24.55, 24.74, 24.55, 24.585, 24.81, 24.89, 25.12, 25.15, 25.075, 25.48, 25.47, 25.845, 26.015, 26.16, 25.58, 25.47, 25.41, 25.63, 25.6, 27.06, 26.7, 26.62, 26.645, 26.75, 26.75, 26.74, 26.85, 27.1, 27.18, 26.995, 27.76, 28.02, 28.03, 28.12, 27.92, 27.83, 27.77, 27.7, 26.93, 26.75, 26.65, 27.26, 27.06, 26.995, 26.65, 26.37, 26.93, 29.2, 28.92, 28.805, 29.2, 28.81, 29.13, 28.96, 28.87, 28.91, 28.93, 29.32, 29.44, 29.31, 29.45, 29.38, 29.55, 29.34, 28.875, 28.93, 28.76, 28.89, 28.58, 28.2, 28.14, 27.92, 28.15, 28.69, 28.93, 29.0, 29.09, 29.41, 29.77, 29.41, 29.51, 29.83, 29.8, 29.96, 30.36, 30.26, 30.7, 30.83, 30.96, 31.06, 30.94, 30.33, 30.09, 30.31, 30.39, 30.0, 29.85, 29.8, 29.295, 28.53, 28.59, 28.55, 28.88, 28.36, 28.585, 28.58, 27.89, 27.91, 27.83, 27.89, 27.74, 27.79, 27.96, 27.97, 28.215, 28.18, 28.13, 27.83, 27.82, 27.91, 27.46, 27.7, 27.45, 27.9, 27.8, 27.84, 27.585, 27.8, 27.015, 27.08, 27.38, 27.32, 26.955, 27.35, 26.62, 26.4, 26.46, 25.61, 24.55, 24.69, 24.33, 24.13, 24.59, 25.065, 26.08, 26.5, 25.99, 26.97, 26.94, 26.13, 26.59, 26.9, 25.99, 26.38, 25.5202, 25.655, 26.37, 26.1, 26.55, 25.875, 26.42, 25.88, 25.9, 25.89, 26.725, 26.58, 26.58, 27.16, 27.29, 27.565, 27.42, 27.43, 27.4, 27.365, 27.06, 27.07, 27.48, 27.89, 27.33, 27.255, 27.455, 28.235, 28.145, 28.725, 29.315, 29.335, 29.02, 29.32, 29.74, 29.53, 29.88, 29.91, 29.86, 29.99, 30.07, 29.95, 30.29, 30.19, 30.31, 29.7, 30.025, 30.01, 30.09, 29.48, 29.835, 29.58, 29.65, 31.29, 30.88, 30.68, 30.41, 30.13, 29.96, 30.23, 30.11, 29.785, 29.25, 29.565, 29.48, 29.47, 29.26, 28.56, 28.695, 28.665, 28.67, 28.58, 28.2, 28.17, 28.35, 28.05, 27.2, 26.97, 25.8, 25.19, 25.735, 25.95, 26.28, 26.355, 26.445, 26.32, 26.78, 26.74, 26.58, 26.33, 26.45, 26.52, 26.48, 27.05, 26.46, 26.87, 26.95, 26.6, 27.39, 27.91, 27.79, 27.34, 26.76, 26.67, 27.94, 28.7, 29.96, 30.2, 30.53, 30.3, 30.12, 30.09, 30.06, 30.67, 30.72, 29.86, 32.07, 34.34, 34.38, 34.49, 34.04, 34.0, 33.63, 33.19, 33.68, 33.23, 33.69, 33.52, 33.845, 33.99, 34.25, 33.92, 33.65, 33.66, 33.1, 32.49, 32.22, 32.68, 32.555, 32.33, 32.6, 32.54, 32.3, 32.995, 32.96, 32.985, 33.42, 33.275, 32.89, 33.02, 32.69, 32.26, 32.48, 32.69, 32.915, 32.95, 32.8, 33.07, 33.31, 33.59, 33.63, 33.4, 33.59, 33.605, 33.96, 34.16, 33.97, 34.265, 34.61, 34.565, 34.36, 33.675, 34.0, 33.32, 32.925, 32.61, 32.8, 32.79, 32.77, 33.67, 33.92, 34.47, 34.5, 34.09, 34.63, 34.53, 34.515, 34.65, 34.35, 34.025, 33.95, 33.93, 33.44, 34.0, 34.04, 34.12, 34.075, 34.33, 34.36, 34.12, 34.24, 34.45, 34.04, 33.58, 33.85, 34.24, 33.7, 33.94, 34.26, 35.07, 35.55, 35.32, 34.46, 34.92, 33.88, 34.36, 34.005, 34.57, 34.19, 34.3, 34.71, 34.5, 34.87, 34.98, 35.64, 35.16, 35.0, 34.81, 34.9, 35.24, 35.36, 35.29, 34.795, 34.67, 35.06, 34.72, 34.41, 34.82, 34.45, 34.04, 33.58, 32.79, 34.65, 34.6, 33.76, 33.17, 33.86, 33.56, 34.165, 34.53, 34.29, 34.55, 33.91, 34.12, 34.94, 34.59, 34.86, 35.13, 35.38, 35.37, 35.68, 35.32, 36.61, 37.09, 37.84, 38.41, 38.67, 39.0, 38.9, 38.8249, 38.75, 38.98, 38.5, 37.75, 37.23, 36.215, 36.5, 36.91, 37.1, 36.99, 37.11, 37.4, 37.81, 37.625, 37.24, 37.19, 37.28, 36.799, 36.31, 36.56, 35.73, 35.205, 35.055, 35.005, 35.32, 35.35, 35.11, 34.99, 34.63, 34.93, 34.74, 34.6, 35.3, 34.83, 33.32, 34.34, 34.48, 34.57, 34.34, 34.31, 33.78, 33.35, 33.54, 32.765, 33.65, 33.05, 32.7, 32.5, 31.78, 31.91, 31.825, 32.43, 33.11, 34.1, 33.65, 34.1, 34.055, 33.4, 33.7, 34.29, 34.15, 34.44, 33.75, 34.49, 34.22, 34.53, 35.16, 34.905, 34.66, 34.745, 34.92, 35.06, 35.3, 35.55, 35.67, 35.88, 36.15, 36.13, 36.305, 35.94, 35.42, 35.6, 35.85, 35.56, 35.58, 35.7, 35.73, 35.68, 35.89, 35.96, 35.87, 35.76, 35.4, 35.12, 34.76, 34.37, 33.96, 32.32, 31.3, 31.51, 31.94, 31.68, 32.0, 32.15, 32.45, 32.815, 32.91, 32.7, 33.34, 32.89, 33.01, 32.78, 33.0, 35.18, 35.54, 35.65, 35.71, 35.91, 35.1, 35.685, 36.21, 35.695, 35.3, 35.15, 35.26, 35.15, 34.88, 34.22, 34.35, 34.53, 35.36, 35.37, 35.51, 35.73, 35.515, 35.41, 36.12, 36.07, 36.21, 36.04, 36.09, 35.5, 35.39, 35.55, 35.41, 35.51, 35.18, 34.78, 34.96, 34.53, 34.55, 34.1, 34.12, 34.06, 34.03, 34.28, 34.9, 35.19, 34.14, 34.21, 32.12, 32.41, 32.82, 32.52, 32.47, 32.02, 32.01, 31.78, 32.655, 32.883, 32.73, 32.81, 32.58, 32.52, 32.27, 31.65, 31.89, 31.92, 32.435, 33.13, 32.36, 31.99, 32.88, 33.74, 33.58, 33.04, 31.96, 31.6, 31.67, 32.075, 32.51, 32.27, 32.65, 32.75, 33.0, 33.04, 32.39, 32.29, 32.11, 32.685, 32.98, 33.7, 33.9, 34.35, 33.93, 33.785, 33.06, 33.54, 33.05, 32.83, 33.2, 33.17, 32.96, 32.67, 32.77, 33.02, 32.58, 32.23, 32.57, 32.15, 32.14, 32.24, 32.14, 31.73, 31.56, 30.89, 31.83, 32.01, 30.735, 31.28, 31.05, 31.27, 31.74, 32.065, 31.92, 32.025, 32.465, 32.62, 32.3, 32.35, 33.47, 33.5, 34.4, 34.96, 34.79, 35.2, 35.63, 35.17, 35.01, 34.965, 34.98, 34.79, 34.6, 34.29, 33.97, 33.46, 32.85, 32.83, 32.92, 32.67, 32.68, 33.17, 33.11, 33.13, 32.75, 32.89, 33.23, 33.54, 33.49, 33.495, 33.2, 33.2, 33.4, 33.26, 33.11, 33.25, 33.76, 34.18, 34.13, 33.69, 33.17, 33.45, 33.83, 32.8, 33.93, 34.125, 34.16, 34.36, 34.09, 33.94, 34.82, 34.85, 34.85, 35.17, 34.78]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e84bc31c-8410-4550-ac59-53259207602c');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"9190afdb-059a-49e1-9f21-a9a630d4523a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"9190afdb-059a-49e1-9f21-a9a630d4523a\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '9190afdb-059a-49e1-9f21-a9a630d4523a',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9190afdb-059a-49e1-9f21-a9a630d4523a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce6zzprTWILi"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCfkaYDhWILi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd81d3d5-06b3-4ba1-8224-1a761f56dbfd"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.05, .1, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"FOXA\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 18ms/step - loss: 0.6935 - accuracy: 0.5336 - val_loss: 0.6800 - val_accuracy: 0.8776\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6735 - accuracy: 0.5899 - val_loss: 0.7676 - val_accuracy: 0.1980\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6342 - accuracy: 0.6423 - val_loss: 0.5570 - val_accuracy: 0.7878\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6346 - accuracy: 0.6631 - val_loss: 0.6406 - val_accuracy: 0.6837\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6171 - accuracy: 0.6624 - val_loss: 0.5673 - val_accuracy: 0.7633\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6896 - accuracy: 0.5221 - val_loss: 0.8035 - val_accuracy: 0.1388\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6544 - accuracy: 0.6174 - val_loss: 0.7432 - val_accuracy: 0.4816\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6171 - accuracy: 0.6839 - val_loss: 0.4671 - val_accuracy: 0.8551\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6190 - accuracy: 0.6752 - val_loss: 0.6396 - val_accuracy: 0.6531\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6039 - accuracy: 0.6879 - val_loss: 0.6468 - val_accuracy: 0.6469\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.834245\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.848881\n",
            "[2]\tvalidation_0-auc:0.847244\n",
            "[3]\tvalidation_0-auc:0.843009\n",
            "[4]\tvalidation_0-auc:0.851739\n",
            "[5]\tvalidation_0-auc:0.855171\n",
            "[6]\tvalidation_0-auc:0.855799\n",
            "[7]\tvalidation_0-auc:0.856792\n",
            "[8]\tvalidation_0-auc:0.85498\n",
            "[9]\tvalidation_0-auc:0.853185\n",
            "[10]\tvalidation_0-auc:0.853882\n",
            "[11]\tvalidation_0-auc:0.850885\n",
            "[12]\tvalidation_0-auc:0.853534\n",
            "[13]\tvalidation_0-auc:0.853081\n",
            "[14]\tvalidation_0-auc:0.855241\n",
            "[15]\tvalidation_0-auc:0.855311\n",
            "[16]\tvalidation_0-auc:0.857245\n",
            "[17]\tvalidation_0-auc:0.861462\n",
            "[18]\tvalidation_0-auc:0.860974\n",
            "[19]\tvalidation_0-auc:0.860782\n",
            "[20]\tvalidation_0-auc:0.859911\n",
            "[21]\tvalidation_0-auc:0.858918\n",
            "[22]\tvalidation_0-auc:0.858743\n",
            "[23]\tvalidation_0-auc:0.858064\n",
            "[24]\tvalidation_0-auc:0.857611\n",
            "[25]\tvalidation_0-auc:0.857924\n",
            "[26]\tvalidation_0-auc:0.85836\n",
            "[27]\tvalidation_0-auc:0.857803\n",
            "[28]\tvalidation_0-auc:0.858621\n",
            "[29]\tvalidation_0-auc:0.858447\n",
            "[30]\tvalidation_0-auc:0.859284\n",
            "[31]\tvalidation_0-auc:0.857977\n",
            "[32]\tvalidation_0-auc:0.85829\n",
            "[33]\tvalidation_0-auc:0.858256\n",
            "[34]\tvalidation_0-auc:0.856652\n",
            "[35]\tvalidation_0-auc:0.856687\n",
            "[36]\tvalidation_0-auc:0.8566\n",
            "[37]\tvalidation_0-auc:0.856513\n",
            "[38]\tvalidation_0-auc:0.856234\n",
            "[39]\tvalidation_0-auc:0.856356\n",
            "[40]\tvalidation_0-auc:0.856635\n",
            "[41]\tvalidation_0-auc:0.856008\n",
            "[42]\tvalidation_0-auc:0.855729\n",
            "[43]\tvalidation_0-auc:0.855729\n",
            "[44]\tvalidation_0-auc:0.854021\n",
            "[45]\tvalidation_0-auc:0.855311\n",
            "[46]\tvalidation_0-auc:0.854474\n",
            "[47]\tvalidation_0-auc:0.854997\n",
            "[48]\tvalidation_0-auc:0.855137\n",
            "[49]\tvalidation_0-auc:0.855171\n",
            "[50]\tvalidation_0-auc:0.85545\n",
            "[51]\tvalidation_0-auc:0.85545\n",
            "[52]\tvalidation_0-auc:0.854962\n",
            "[53]\tvalidation_0-auc:0.855171\n",
            "[54]\tvalidation_0-auc:0.855485\n",
            "[55]\tvalidation_0-auc:0.85444\n",
            "[56]\tvalidation_0-auc:0.854021\n",
            "[57]\tvalidation_0-auc:0.852279\n",
            "[58]\tvalidation_0-auc:0.850362\n",
            "[59]\tvalidation_0-auc:0.851547\n",
            "[60]\tvalidation_0-auc:0.851512\n",
            "[61]\tvalidation_0-auc:0.851478\n",
            "[62]\tvalidation_0-auc:0.851025\n",
            "[63]\tvalidation_0-auc:0.850641\n",
            "[64]\tvalidation_0-auc:0.85099\n",
            "[65]\tvalidation_0-auc:0.850676\n",
            "[66]\tvalidation_0-auc:0.850362\n",
            "[67]\tvalidation_0-auc:0.850397\n",
            "Stopping. Best iteration:\n",
            "[17]\tvalidation_0-auc:0.861462\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6948 - accuracy: 0.5079 - val_loss: 0.7631 - val_accuracy: 0.1138\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6922 - accuracy: 0.5340 - val_loss: 0.6714 - val_accuracy: 0.8753\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6653 - accuracy: 0.6239 - val_loss: 0.7337 - val_accuracy: 0.4201\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6275 - accuracy: 0.6616 - val_loss: 0.6037 - val_accuracy: 0.6893\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6177 - accuracy: 0.6726 - val_loss: 0.7330 - val_accuracy: 0.5383\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6784 - accuracy: 0.5717 - val_loss: 0.6698 - val_accuracy: 0.6149\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6251 - accuracy: 0.6829 - val_loss: 0.6707 - val_accuracy: 0.6039\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6167 - accuracy: 0.6980 - val_loss: 0.7584 - val_accuracy: 0.4661\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6122 - accuracy: 0.6925 - val_loss: 0.6311 - val_accuracy: 0.6805\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5907 - accuracy: 0.7049 - val_loss: 0.6111 - val_accuracy: 0.6543\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.792213\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.786087\n",
            "[2]\tvalidation_0-auc:0.816287\n",
            "[3]\tvalidation_0-auc:0.786491\n",
            "[4]\tvalidation_0-auc:0.786396\n",
            "[5]\tvalidation_0-auc:0.784663\n",
            "[6]\tvalidation_0-auc:0.795014\n",
            "[7]\tvalidation_0-auc:0.801235\n",
            "[8]\tvalidation_0-auc:0.808025\n",
            "[9]\tvalidation_0-auc:0.802683\n",
            "[10]\tvalidation_0-auc:0.802303\n",
            "[11]\tvalidation_0-auc:0.796178\n",
            "[12]\tvalidation_0-auc:0.80254\n",
            "[13]\tvalidation_0-auc:0.798837\n",
            "[14]\tvalidation_0-auc:0.805318\n",
            "[15]\tvalidation_0-auc:0.800997\n",
            "[16]\tvalidation_0-auc:0.800475\n",
            "[17]\tvalidation_0-auc:0.80133\n",
            "[18]\tvalidation_0-auc:0.801852\n",
            "[19]\tvalidation_0-auc:0.796771\n",
            "[20]\tvalidation_0-auc:0.798267\n",
            "[21]\tvalidation_0-auc:0.798552\n",
            "[22]\tvalidation_0-auc:0.796273\n",
            "[23]\tvalidation_0-auc:0.794539\n",
            "[24]\tvalidation_0-auc:0.791263\n",
            "[25]\tvalidation_0-auc:0.793495\n",
            "[26]\tvalidation_0-auc:0.794539\n",
            "[27]\tvalidation_0-auc:0.794658\n",
            "[28]\tvalidation_0-auc:0.794658\n",
            "[29]\tvalidation_0-auc:0.795679\n",
            "[30]\tvalidation_0-auc:0.795916\n",
            "[31]\tvalidation_0-auc:0.79632\n",
            "[32]\tvalidation_0-auc:0.796486\n",
            "[33]\tvalidation_0-auc:0.7967\n",
            "[34]\tvalidation_0-auc:0.797175\n",
            "[35]\tvalidation_0-auc:0.795489\n",
            "[36]\tvalidation_0-auc:0.795394\n",
            "[37]\tvalidation_0-auc:0.795893\n",
            "[38]\tvalidation_0-auc:0.796937\n",
            "[39]\tvalidation_0-auc:0.797317\n",
            "[40]\tvalidation_0-auc:0.797365\n",
            "[41]\tvalidation_0-auc:0.794967\n",
            "[42]\tvalidation_0-auc:0.795726\n",
            "[43]\tvalidation_0-auc:0.796629\n",
            "[44]\tvalidation_0-auc:0.79518\n",
            "[45]\tvalidation_0-auc:0.79518\n",
            "[46]\tvalidation_0-auc:0.795133\n",
            "[47]\tvalidation_0-auc:0.794041\n",
            "[48]\tvalidation_0-auc:0.792141\n",
            "[49]\tvalidation_0-auc:0.792806\n",
            "[50]\tvalidation_0-auc:0.79188\n",
            "[51]\tvalidation_0-auc:0.79378\n",
            "[52]\tvalidation_0-auc:0.795275\n",
            "Stopping. Best iteration:\n",
            "[2]\tvalidation_0-auc:0.816287\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.763265306122449  |         0.34        |        0.75        |  0.4678899082568807 |\n",
            "|     GRU 0.1      | 0.6469387755102041 | 0.26666666666666666 | 0.8823529411764706 |  0.4095563139931741 |\n",
            "|   XGBoost 0.1    | 0.7224489795918367 | 0.31521739130434784 | 0.8529411764705882 | 0.46031746031746035 |\n",
            "|    Logreg 0.1    | 0.6306122448979592 | 0.25957446808510637 | 0.8970588235294118 |  0.4026402640264027 |\n",
            "|     SVM 0.1      | 0.6938775510204082 | 0.29292929292929293 | 0.8529411764705882 | 0.43609022556390975 |\n",
            "|  LSTM beta 0.1   | 0.5382932166301969 | 0.18823529411764706 | 0.9230769230769231 |  0.3127035830618893 |\n",
            "|   GRU beta 0.1   | 0.6542669584245077 |  0.2268041237113402 | 0.8461538461538461 | 0.35772357723577236 |\n",
            "| XGBoost beta 0.1 | 0.6914660831509847 | 0.24277456647398843 | 0.8076923076923077 | 0.37333333333333335 |\n",
            "| logreg beta 0.1  | 0.7089715536105032 |  0.2711864406779661 | 0.9230769230769231 | 0.41921397379912667 |\n",
            "|   svm beta 0.1   | 0.6761487964989059 | 0.23626373626373626 | 0.8269230769230769 |  0.3675213675213675 |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6956 - accuracy: 0.5094 - val_loss: 0.6761 - val_accuracy: 0.9306\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6946 - accuracy: 0.5101 - val_loss: 0.6489 - val_accuracy: 0.9306\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6906 - accuracy: 0.5141 - val_loss: 0.6791 - val_accuracy: 0.7633\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6732 - accuracy: 0.5772 - val_loss: 0.4925 - val_accuracy: 0.8265\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6582 - accuracy: 0.6309 - val_loss: 0.5044 - val_accuracy: 0.8102\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6917 - accuracy: 0.5336 - val_loss: 0.6914 - val_accuracy: 0.5408\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6569 - accuracy: 0.6094 - val_loss: 0.5989 - val_accuracy: 0.7510\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6351 - accuracy: 0.6624 - val_loss: 0.5092 - val_accuracy: 0.8429\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6358 - accuracy: 0.6517 - val_loss: 0.5781 - val_accuracy: 0.7286\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6332 - accuracy: 0.6477 - val_loss: 0.5287 - val_accuracy: 0.7673\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.824207\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.843105\n",
            "[2]\tvalidation_0-auc:0.856037\n",
            "[3]\tvalidation_0-auc:0.865422\n",
            "[4]\tvalidation_0-auc:0.875839\n",
            "[5]\tvalidation_0-auc:0.872872\n",
            "[6]\tvalidation_0-auc:0.869614\n",
            "[7]\tvalidation_0-auc:0.872485\n",
            "[8]\tvalidation_0-auc:0.875419\n",
            "[9]\tvalidation_0-auc:0.87945\n",
            "[10]\tvalidation_0-auc:0.881192\n",
            "[11]\tvalidation_0-auc:0.880128\n",
            "[12]\tvalidation_0-auc:0.882643\n",
            "[13]\tvalidation_0-auc:0.879257\n",
            "[14]\tvalidation_0-auc:0.882482\n",
            "[15]\tvalidation_0-auc:0.881482\n",
            "[16]\tvalidation_0-auc:0.882579\n",
            "[17]\tvalidation_0-auc:0.882417\n",
            "[18]\tvalidation_0-auc:0.883578\n",
            "[19]\tvalidation_0-auc:0.883804\n",
            "[20]\tvalidation_0-auc:0.88532\n",
            "[21]\tvalidation_0-auc:0.886094\n",
            "[22]\tvalidation_0-auc:0.885997\n",
            "[23]\tvalidation_0-auc:0.886094\n",
            "[24]\tvalidation_0-auc:0.889125\n",
            "[25]\tvalidation_0-auc:0.888867\n",
            "[26]\tvalidation_0-auc:0.89048\n",
            "[27]\tvalidation_0-auc:0.892221\n",
            "[28]\tvalidation_0-auc:0.892157\n",
            "[29]\tvalidation_0-auc:0.89364\n",
            "[30]\tvalidation_0-auc:0.892286\n",
            "[31]\tvalidation_0-auc:0.893189\n",
            "[32]\tvalidation_0-auc:0.892608\n",
            "[33]\tvalidation_0-auc:0.892673\n",
            "[34]\tvalidation_0-auc:0.891576\n",
            "[35]\tvalidation_0-auc:0.890738\n",
            "[36]\tvalidation_0-auc:0.891254\n",
            "[37]\tvalidation_0-auc:0.893318\n",
            "[38]\tvalidation_0-auc:0.892673\n",
            "[39]\tvalidation_0-auc:0.892415\n",
            "[40]\tvalidation_0-auc:0.893382\n",
            "[41]\tvalidation_0-auc:0.892737\n",
            "[42]\tvalidation_0-auc:0.890931\n",
            "[43]\tvalidation_0-auc:0.89177\n",
            "[44]\tvalidation_0-auc:0.89048\n",
            "[45]\tvalidation_0-auc:0.889125\n",
            "[46]\tvalidation_0-auc:0.889899\n",
            "[47]\tvalidation_0-auc:0.890738\n",
            "[48]\tvalidation_0-auc:0.890028\n",
            "[49]\tvalidation_0-auc:0.888609\n",
            "[50]\tvalidation_0-auc:0.888996\n",
            "[51]\tvalidation_0-auc:0.889641\n",
            "[52]\tvalidation_0-auc:0.889254\n",
            "[53]\tvalidation_0-auc:0.889706\n",
            "[54]\tvalidation_0-auc:0.889641\n",
            "[55]\tvalidation_0-auc:0.888416\n",
            "[56]\tvalidation_0-auc:0.888867\n",
            "[57]\tvalidation_0-auc:0.887255\n",
            "[58]\tvalidation_0-auc:0.887771\n",
            "[59]\tvalidation_0-auc:0.887835\n",
            "[60]\tvalidation_0-auc:0.886868\n",
            "[61]\tvalidation_0-auc:0.887126\n",
            "[62]\tvalidation_0-auc:0.885578\n",
            "[63]\tvalidation_0-auc:0.885707\n",
            "[64]\tvalidation_0-auc:0.8859\n",
            "[65]\tvalidation_0-auc:0.884804\n",
            "[66]\tvalidation_0-auc:0.882482\n",
            "[67]\tvalidation_0-auc:0.883127\n",
            "[68]\tvalidation_0-auc:0.883127\n",
            "[69]\tvalidation_0-auc:0.883514\n",
            "[70]\tvalidation_0-auc:0.883836\n",
            "[71]\tvalidation_0-auc:0.883385\n",
            "[72]\tvalidation_0-auc:0.883578\n",
            "[73]\tvalidation_0-auc:0.882933\n",
            "[74]\tvalidation_0-auc:0.881127\n",
            "[75]\tvalidation_0-auc:0.881321\n",
            "[76]\tvalidation_0-auc:0.880095\n",
            "[77]\tvalidation_0-auc:0.880353\n",
            "[78]\tvalidation_0-auc:0.880289\n",
            "[79]\tvalidation_0-auc:0.878741\n",
            "Stopping. Best iteration:\n",
            "[29]\tvalidation_0-auc:0.89364\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6932 - accuracy: 0.5216 - val_loss: 0.6723 - val_accuracy: 0.9606\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6719 - accuracy: 0.5964 - val_loss: 0.4886 - val_accuracy: 0.8796\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6613 - accuracy: 0.6239 - val_loss: 0.6602 - val_accuracy: 0.6280\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6474 - accuracy: 0.6328 - val_loss: 0.5115 - val_accuracy: 0.8446\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6201 - accuracy: 0.6740 - val_loss: 0.6009 - val_accuracy: 0.6696\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 17ms/step - loss: 0.6771 - accuracy: 0.5669 - val_loss: 0.5726 - val_accuracy: 0.8884\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6329 - accuracy: 0.6754 - val_loss: 0.5474 - val_accuracy: 0.8337\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6239 - accuracy: 0.6774 - val_loss: 0.7012 - val_accuracy: 0.5558\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6208 - accuracy: 0.6767 - val_loss: 0.6305 - val_accuracy: 0.6652\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6088 - accuracy: 0.6898 - val_loss: 0.4842 - val_accuracy: 0.8009\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.608643\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.633637\n",
            "[2]\tvalidation_0-auc:0.662237\n",
            "[3]\tvalidation_0-auc:0.675588\n",
            "[4]\tvalidation_0-auc:0.66306\n",
            "[5]\tvalidation_0-auc:0.660213\n",
            "[6]\tvalidation_0-auc:0.65262\n",
            "[7]\tvalidation_0-auc:0.652746\n",
            "[8]\tvalidation_0-auc:0.661541\n",
            "[9]\tvalidation_0-auc:0.658631\n",
            "[10]\tvalidation_0-auc:0.655973\n",
            "[11]\tvalidation_0-auc:0.663946\n",
            "[12]\tvalidation_0-auc:0.67426\n",
            "[13]\tvalidation_0-auc:0.66616\n",
            "[14]\tvalidation_0-auc:0.664768\n",
            "[15]\tvalidation_0-auc:0.657049\n",
            "[16]\tvalidation_0-auc:0.650089\n",
            "[17]\tvalidation_0-auc:0.650215\n",
            "[18]\tvalidation_0-auc:0.650974\n",
            "[19]\tvalidation_0-auc:0.648317\n",
            "[20]\tvalidation_0-auc:0.653126\n",
            "[21]\tvalidation_0-auc:0.65224\n",
            "[22]\tvalidation_0-auc:0.647937\n",
            "[23]\tvalidation_0-auc:0.647558\n",
            "[24]\tvalidation_0-auc:0.643824\n",
            "[25]\tvalidation_0-auc:0.649456\n",
            "[26]\tvalidation_0-auc:0.649456\n",
            "[27]\tvalidation_0-auc:0.649456\n",
            "[28]\tvalidation_0-auc:0.649835\n",
            "[29]\tvalidation_0-auc:0.649835\n",
            "[30]\tvalidation_0-auc:0.652556\n",
            "[31]\tvalidation_0-auc:0.650278\n",
            "[32]\tvalidation_0-auc:0.650278\n",
            "[33]\tvalidation_0-auc:0.651038\n",
            "[34]\tvalidation_0-auc:0.654455\n",
            "[35]\tvalidation_0-auc:0.655847\n",
            "[36]\tvalidation_0-auc:0.658125\n",
            "[37]\tvalidation_0-auc:0.664199\n",
            "[38]\tvalidation_0-auc:0.662807\n",
            "[39]\tvalidation_0-auc:0.666287\n",
            "[40]\tvalidation_0-auc:0.665654\n",
            "[41]\tvalidation_0-auc:0.665528\n",
            "[42]\tvalidation_0-auc:0.665591\n",
            "[43]\tvalidation_0-auc:0.665591\n",
            "[44]\tvalidation_0-auc:0.665211\n",
            "[45]\tvalidation_0-auc:0.664199\n",
            "[46]\tvalidation_0-auc:0.664199\n",
            "[47]\tvalidation_0-auc:0.664705\n",
            "[48]\tvalidation_0-auc:0.664705\n",
            "[49]\tvalidation_0-auc:0.663819\n",
            "[50]\tvalidation_0-auc:0.663313\n",
            "[51]\tvalidation_0-auc:0.661415\n",
            "[52]\tvalidation_0-auc:0.661541\n",
            "[53]\tvalidation_0-auc:0.661541\n",
            "Stopping. Best iteration:\n",
            "[3]\tvalidation_0-auc:0.675588\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+----------------------+--------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision       |       Recall       |       F1 score      |\n",
            "+-------------------+--------------------+----------------------+--------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.810204081632653  | 0.22429906542056074  | 0.7058823529411765 |  0.3404255319148936 |\n",
            "|      GRU 0.15     | 0.7673469387755102 | 0.20588235294117646  | 0.8235294117647058 | 0.32941176470588235 |\n",
            "|    XGBoost 0.15   | 0.6408163265306123 |  0.1485148514851485  | 0.8823529411764706 | 0.25423728813559326 |\n",
            "|    Logreg 0.15    | 0.7204081632653061 | 0.18012422360248448  | 0.8529411764705882 | 0.29743589743589743 |\n",
            "|      SVM 0.15     | 0.7040816326530612 | 0.17543859649122806  | 0.8823529411764706 |  0.2926829268292682 |\n",
            "|   LSTM beta 0.15  | 0.6695842450765864 | 0.059602649006622516 |        0.5         | 0.10650887573964497 |\n",
            "|   GRU beta 0.15   | 0.8008752735229759 | 0.08045977011494253  | 0.3888888888888889 | 0.13333333333333333 |\n",
            "| XGBoost beta 0.15 | 0.6148796498905909 | 0.056179775280898875 | 0.5555555555555556 |  0.1020408163265306 |\n",
            "|  logreg beta 0.15 | 0.7636761487964989 | 0.08333333333333333  |        0.5         | 0.14285714285714285 |\n",
            "|   svm beta 0.15   | 0.6739606126914661 | 0.06040268456375839  |        0.5         | 0.10778443113772455 |\n",
            "+-------------------+--------------------+----------------------+--------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzc3B4L-WILk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f2ddd43-eafa-40c4-ff71-2d06a043ebba"
      },
      "source": [
        "Result_cross.to_csv('FOXA_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.763265</td>\n",
              "      <td>0.467890</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.646939</td>\n",
              "      <td>0.409556</td>\n",
              "      <td>0.882353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.315217</td>\n",
              "      <td>0.722449</td>\n",
              "      <td>0.460317</td>\n",
              "      <td>0.852941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.259574</td>\n",
              "      <td>0.630612</td>\n",
              "      <td>0.402640</td>\n",
              "      <td>0.897059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.292929</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>0.436090</td>\n",
              "      <td>0.852941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.188235</td>\n",
              "      <td>0.538293</td>\n",
              "      <td>0.312704</td>\n",
              "      <td>0.923077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.226804</td>\n",
              "      <td>0.654267</td>\n",
              "      <td>0.357724</td>\n",
              "      <td>0.846154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.242775</td>\n",
              "      <td>0.691466</td>\n",
              "      <td>0.373333</td>\n",
              "      <td>0.807692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.271186</td>\n",
              "      <td>0.708972</td>\n",
              "      <td>0.419214</td>\n",
              "      <td>0.923077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.236264</td>\n",
              "      <td>0.676149</td>\n",
              "      <td>0.367521</td>\n",
              "      <td>0.826923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.224299</td>\n",
              "      <td>0.810204</td>\n",
              "      <td>0.340426</td>\n",
              "      <td>0.705882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.205882</td>\n",
              "      <td>0.767347</td>\n",
              "      <td>0.329412</td>\n",
              "      <td>0.823529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.148515</td>\n",
              "      <td>0.640816</td>\n",
              "      <td>0.254237</td>\n",
              "      <td>0.882353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.180124</td>\n",
              "      <td>0.720408</td>\n",
              "      <td>0.297436</td>\n",
              "      <td>0.852941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.175439</td>\n",
              "      <td>0.704082</td>\n",
              "      <td>0.292683</td>\n",
              "      <td>0.882353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.059603</td>\n",
              "      <td>0.669584</td>\n",
              "      <td>0.106509</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.080460</td>\n",
              "      <td>0.800875</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.388889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.056180</td>\n",
              "      <td>0.614880</td>\n",
              "      <td>0.102041</td>\n",
              "      <td>0.555556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.763676</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.060403</td>\n",
              "      <td>0.673961</td>\n",
              "      <td>0.107784</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  FOXA  0.340000  0.763265  0.467890  0.750000\n",
              "1            GRU 0.1  FOXA  0.266667  0.646939  0.409556  0.882353\n",
              "2        XGBoost 0.1  FOXA  0.315217  0.722449  0.460317  0.852941\n",
              "3         Logreg 0.1  FOXA  0.259574  0.630612  0.402640  0.897059\n",
              "4            SVM 0.1  FOXA  0.292929  0.693878  0.436090  0.852941\n",
              "5      LSTM beta 0.1  FOXA  0.188235  0.538293  0.312704  0.923077\n",
              "6       GRU beta 0.1  FOXA  0.226804  0.654267  0.357724  0.846154\n",
              "7   XGBoost beta 0.1  FOXA  0.242775  0.691466  0.373333  0.807692\n",
              "8    logreg beta 0.1  FOXA  0.271186  0.708972  0.419214  0.923077\n",
              "9       svm beta 0.1  FOXA  0.236264  0.676149  0.367521  0.826923\n",
              "0          LSTM 0.15  FOXA  0.224299  0.810204  0.340426  0.705882\n",
              "1           GRU 0.15  FOXA  0.205882  0.767347  0.329412  0.823529\n",
              "2       XGBoost 0.15  FOXA  0.148515  0.640816  0.254237  0.882353\n",
              "3        Logreg 0.15  FOXA  0.180124  0.720408  0.297436  0.852941\n",
              "4           SVM 0.15  FOXA  0.175439  0.704082  0.292683  0.882353\n",
              "5     LSTM beta 0.15  FOXA  0.059603  0.669584  0.106509  0.500000\n",
              "6      GRU beta 0.15  FOXA  0.080460  0.800875  0.133333  0.388889\n",
              "7  XGBoost beta 0.15  FOXA  0.056180  0.614880  0.102041  0.555556\n",
              "8   logreg beta 0.15  FOXA  0.083333  0.763676  0.142857  0.500000\n",
              "9      svm beta 0.15  FOXA  0.060403  0.673961  0.107784  0.500000"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRx5B2JfWILk"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_ZpxDYiWILk"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4MQOtwtWILk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6cf726b-ee6a-4520-e45e-91e8f80d677f"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"FOXA\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6940 - accuracy: 0.5148 - val_loss: 0.7334 - val_accuracy: 0.1388\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6830 - accuracy: 0.5497 - val_loss: 0.6837 - val_accuracy: 0.6204\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6416 - accuracy: 0.6477 - val_loss: 0.6966 - val_accuracy: 0.6286\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6243 - accuracy: 0.6577 - val_loss: 0.6314 - val_accuracy: 0.7102\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6286 - accuracy: 0.6631 - val_loss: 0.6205 - val_accuracy: 0.7143\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6856 - accuracy: 0.5477 - val_loss: 0.5595 - val_accuracy: 0.8816\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6441 - accuracy: 0.6477 - val_loss: 0.6059 - val_accuracy: 0.7082\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6177 - accuracy: 0.6785 - val_loss: 0.6222 - val_accuracy: 0.6878\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6036 - accuracy: 0.6953 - val_loss: 0.6938 - val_accuracy: 0.6122\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6070 - accuracy: 0.6919 - val_loss: 0.5837 - val_accuracy: 0.7367\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.834245\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.848881\n",
            "[2]\tvalidation_0-auc:0.847244\n",
            "[3]\tvalidation_0-auc:0.843009\n",
            "[4]\tvalidation_0-auc:0.851739\n",
            "[5]\tvalidation_0-auc:0.855171\n",
            "[6]\tvalidation_0-auc:0.855799\n",
            "[7]\tvalidation_0-auc:0.856792\n",
            "[8]\tvalidation_0-auc:0.85498\n",
            "[9]\tvalidation_0-auc:0.853185\n",
            "[10]\tvalidation_0-auc:0.853882\n",
            "[11]\tvalidation_0-auc:0.850885\n",
            "[12]\tvalidation_0-auc:0.853534\n",
            "[13]\tvalidation_0-auc:0.853081\n",
            "[14]\tvalidation_0-auc:0.855241\n",
            "[15]\tvalidation_0-auc:0.855311\n",
            "[16]\tvalidation_0-auc:0.857245\n",
            "[17]\tvalidation_0-auc:0.861462\n",
            "[18]\tvalidation_0-auc:0.860974\n",
            "[19]\tvalidation_0-auc:0.860782\n",
            "[20]\tvalidation_0-auc:0.859911\n",
            "[21]\tvalidation_0-auc:0.858918\n",
            "[22]\tvalidation_0-auc:0.858743\n",
            "[23]\tvalidation_0-auc:0.858064\n",
            "[24]\tvalidation_0-auc:0.857611\n",
            "[25]\tvalidation_0-auc:0.857924\n",
            "[26]\tvalidation_0-auc:0.85836\n",
            "[27]\tvalidation_0-auc:0.857803\n",
            "[28]\tvalidation_0-auc:0.858621\n",
            "[29]\tvalidation_0-auc:0.858447\n",
            "[30]\tvalidation_0-auc:0.859284\n",
            "[31]\tvalidation_0-auc:0.857977\n",
            "[32]\tvalidation_0-auc:0.85829\n",
            "[33]\tvalidation_0-auc:0.858256\n",
            "[34]\tvalidation_0-auc:0.856652\n",
            "[35]\tvalidation_0-auc:0.856687\n",
            "[36]\tvalidation_0-auc:0.8566\n",
            "[37]\tvalidation_0-auc:0.856513\n",
            "[38]\tvalidation_0-auc:0.856234\n",
            "[39]\tvalidation_0-auc:0.856356\n",
            "[40]\tvalidation_0-auc:0.856635\n",
            "[41]\tvalidation_0-auc:0.856008\n",
            "[42]\tvalidation_0-auc:0.855729\n",
            "[43]\tvalidation_0-auc:0.855729\n",
            "[44]\tvalidation_0-auc:0.854021\n",
            "[45]\tvalidation_0-auc:0.855311\n",
            "[46]\tvalidation_0-auc:0.854474\n",
            "[47]\tvalidation_0-auc:0.854997\n",
            "[48]\tvalidation_0-auc:0.855137\n",
            "[49]\tvalidation_0-auc:0.855171\n",
            "[50]\tvalidation_0-auc:0.85545\n",
            "[51]\tvalidation_0-auc:0.85545\n",
            "[52]\tvalidation_0-auc:0.854962\n",
            "[53]\tvalidation_0-auc:0.855171\n",
            "[54]\tvalidation_0-auc:0.855485\n",
            "[55]\tvalidation_0-auc:0.85444\n",
            "[56]\tvalidation_0-auc:0.854021\n",
            "[57]\tvalidation_0-auc:0.852279\n",
            "[58]\tvalidation_0-auc:0.850362\n",
            "[59]\tvalidation_0-auc:0.851547\n",
            "[60]\tvalidation_0-auc:0.851512\n",
            "[61]\tvalidation_0-auc:0.851478\n",
            "[62]\tvalidation_0-auc:0.851025\n",
            "[63]\tvalidation_0-auc:0.850641\n",
            "[64]\tvalidation_0-auc:0.85099\n",
            "[65]\tvalidation_0-auc:0.850676\n",
            "[66]\tvalidation_0-auc:0.850362\n",
            "[67]\tvalidation_0-auc:0.850397\n",
            "Stopping. Best iteration:\n",
            "[17]\tvalidation_0-auc:0.861462\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 19ms/step - loss: 0.6930 - accuracy: 0.5264 - val_loss: 0.6889 - val_accuracy: 0.6674\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6831 - accuracy: 0.5566 - val_loss: 0.7265 - val_accuracy: 0.4792\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6633 - accuracy: 0.6342 - val_loss: 0.7032 - val_accuracy: 0.5120\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6426 - accuracy: 0.6568 - val_loss: 0.6898 - val_accuracy: 0.5974\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6216 - accuracy: 0.6699 - val_loss: 0.5190 - val_accuracy: 0.8053\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6853 - accuracy: 0.5463 - val_loss: 0.7593 - val_accuracy: 0.1641\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6405 - accuracy: 0.6555 - val_loss: 0.6959 - val_accuracy: 0.5470\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6245 - accuracy: 0.6822 - val_loss: 0.6733 - val_accuracy: 0.6018\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6002 - accuracy: 0.6747 - val_loss: 0.5653 - val_accuracy: 0.7352\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5878 - accuracy: 0.7111 - val_loss: 0.5208 - val_accuracy: 0.7987\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.792213\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.786087\n",
            "[2]\tvalidation_0-auc:0.816287\n",
            "[3]\tvalidation_0-auc:0.786491\n",
            "[4]\tvalidation_0-auc:0.786396\n",
            "[5]\tvalidation_0-auc:0.784663\n",
            "[6]\tvalidation_0-auc:0.795014\n",
            "[7]\tvalidation_0-auc:0.801235\n",
            "[8]\tvalidation_0-auc:0.808025\n",
            "[9]\tvalidation_0-auc:0.802683\n",
            "[10]\tvalidation_0-auc:0.802303\n",
            "[11]\tvalidation_0-auc:0.796178\n",
            "[12]\tvalidation_0-auc:0.80254\n",
            "[13]\tvalidation_0-auc:0.798837\n",
            "[14]\tvalidation_0-auc:0.805318\n",
            "[15]\tvalidation_0-auc:0.800997\n",
            "[16]\tvalidation_0-auc:0.800475\n",
            "[17]\tvalidation_0-auc:0.80133\n",
            "[18]\tvalidation_0-auc:0.801852\n",
            "[19]\tvalidation_0-auc:0.796771\n",
            "[20]\tvalidation_0-auc:0.798267\n",
            "[21]\tvalidation_0-auc:0.798552\n",
            "[22]\tvalidation_0-auc:0.796273\n",
            "[23]\tvalidation_0-auc:0.794539\n",
            "[24]\tvalidation_0-auc:0.791263\n",
            "[25]\tvalidation_0-auc:0.793495\n",
            "[26]\tvalidation_0-auc:0.794539\n",
            "[27]\tvalidation_0-auc:0.794658\n",
            "[28]\tvalidation_0-auc:0.794658\n",
            "[29]\tvalidation_0-auc:0.795679\n",
            "[30]\tvalidation_0-auc:0.795916\n",
            "[31]\tvalidation_0-auc:0.79632\n",
            "[32]\tvalidation_0-auc:0.796486\n",
            "[33]\tvalidation_0-auc:0.7967\n",
            "[34]\tvalidation_0-auc:0.797175\n",
            "[35]\tvalidation_0-auc:0.795489\n",
            "[36]\tvalidation_0-auc:0.795394\n",
            "[37]\tvalidation_0-auc:0.795893\n",
            "[38]\tvalidation_0-auc:0.796937\n",
            "[39]\tvalidation_0-auc:0.797317\n",
            "[40]\tvalidation_0-auc:0.797365\n",
            "[41]\tvalidation_0-auc:0.794967\n",
            "[42]\tvalidation_0-auc:0.795726\n",
            "[43]\tvalidation_0-auc:0.796629\n",
            "[44]\tvalidation_0-auc:0.79518\n",
            "[45]\tvalidation_0-auc:0.79518\n",
            "[46]\tvalidation_0-auc:0.795133\n",
            "[47]\tvalidation_0-auc:0.794041\n",
            "[48]\tvalidation_0-auc:0.792141\n",
            "[49]\tvalidation_0-auc:0.792806\n",
            "[50]\tvalidation_0-auc:0.79188\n",
            "[51]\tvalidation_0-auc:0.79378\n",
            "[52]\tvalidation_0-auc:0.795275\n",
            "Stopping. Best iteration:\n",
            "[2]\tvalidation_0-auc:0.816287\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.7142857142857143 | 0.29775280898876405 | 0.7794117647058824 |  0.4308943089430894 |\n",
            "|     GRU 0.1      | 0.736734693877551  |  0.327683615819209  | 0.8529411764705882 |  0.473469387755102  |\n",
            "|   XGBoost 0.1    | 0.7224489795918367 | 0.31521739130434784 | 0.8529411764705882 | 0.46031746031746035 |\n",
            "|    Logreg 0.1    | 0.6306122448979592 | 0.25957446808510637 | 0.8970588235294118 |  0.4026402640264027 |\n",
            "|     SVM 0.1      | 0.6938775510204082 | 0.29292929292929293 | 0.8529411764705882 | 0.43609022556390975 |\n",
            "|  LSTM beta 0.1   | 0.8052516411378556 |  0.3418803418803419 | 0.7692307692307693 | 0.47337278106508873 |\n",
            "|   GRU beta 0.1   | 0.7986870897155361 | 0.32456140350877194 | 0.7115384615384616 | 0.44578313253012053 |\n",
            "| XGBoost beta 0.1 | 0.6914660831509847 | 0.24277456647398843 | 0.8076923076923077 | 0.37333333333333335 |\n",
            "| logreg beta 0.1  | 0.7089715536105032 |  0.2711864406779661 | 0.9230769230769231 | 0.41921397379912667 |\n",
            "|   svm beta 0.1   | 0.6761487964989059 | 0.23626373626373626 | 0.8269230769230769 |  0.3675213675213675 |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6948 - accuracy: 0.5020 - val_loss: 0.7211 - val_accuracy: 0.0694\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6821 - accuracy: 0.5671 - val_loss: 0.5609 - val_accuracy: 0.8490\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6511 - accuracy: 0.6235 - val_loss: 0.6390 - val_accuracy: 0.6816\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6459 - accuracy: 0.6517 - val_loss: 0.5611 - val_accuracy: 0.7857\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6454 - accuracy: 0.6416 - val_loss: 0.5606 - val_accuracy: 0.7959\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6906 - accuracy: 0.5383 - val_loss: 0.6905 - val_accuracy: 0.5327\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6510 - accuracy: 0.6268 - val_loss: 0.6026 - val_accuracy: 0.6694\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6324 - accuracy: 0.6591 - val_loss: 0.6028 - val_accuracy: 0.6898\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6248 - accuracy: 0.6678 - val_loss: 0.5525 - val_accuracy: 0.7531\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6214 - accuracy: 0.6671 - val_loss: 0.5756 - val_accuracy: 0.7367\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.824207\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.843105\n",
            "[2]\tvalidation_0-auc:0.856037\n",
            "[3]\tvalidation_0-auc:0.865422\n",
            "[4]\tvalidation_0-auc:0.875839\n",
            "[5]\tvalidation_0-auc:0.872872\n",
            "[6]\tvalidation_0-auc:0.869614\n",
            "[7]\tvalidation_0-auc:0.872485\n",
            "[8]\tvalidation_0-auc:0.875419\n",
            "[9]\tvalidation_0-auc:0.87945\n",
            "[10]\tvalidation_0-auc:0.881192\n",
            "[11]\tvalidation_0-auc:0.880128\n",
            "[12]\tvalidation_0-auc:0.882643\n",
            "[13]\tvalidation_0-auc:0.879257\n",
            "[14]\tvalidation_0-auc:0.882482\n",
            "[15]\tvalidation_0-auc:0.881482\n",
            "[16]\tvalidation_0-auc:0.882579\n",
            "[17]\tvalidation_0-auc:0.882417\n",
            "[18]\tvalidation_0-auc:0.883578\n",
            "[19]\tvalidation_0-auc:0.883804\n",
            "[20]\tvalidation_0-auc:0.88532\n",
            "[21]\tvalidation_0-auc:0.886094\n",
            "[22]\tvalidation_0-auc:0.885997\n",
            "[23]\tvalidation_0-auc:0.886094\n",
            "[24]\tvalidation_0-auc:0.889125\n",
            "[25]\tvalidation_0-auc:0.888867\n",
            "[26]\tvalidation_0-auc:0.89048\n",
            "[27]\tvalidation_0-auc:0.892221\n",
            "[28]\tvalidation_0-auc:0.892157\n",
            "[29]\tvalidation_0-auc:0.89364\n",
            "[30]\tvalidation_0-auc:0.892286\n",
            "[31]\tvalidation_0-auc:0.893189\n",
            "[32]\tvalidation_0-auc:0.892608\n",
            "[33]\tvalidation_0-auc:0.892673\n",
            "[34]\tvalidation_0-auc:0.891576\n",
            "[35]\tvalidation_0-auc:0.890738\n",
            "[36]\tvalidation_0-auc:0.891254\n",
            "[37]\tvalidation_0-auc:0.893318\n",
            "[38]\tvalidation_0-auc:0.892673\n",
            "[39]\tvalidation_0-auc:0.892415\n",
            "[40]\tvalidation_0-auc:0.893382\n",
            "[41]\tvalidation_0-auc:0.892737\n",
            "[42]\tvalidation_0-auc:0.890931\n",
            "[43]\tvalidation_0-auc:0.89177\n",
            "[44]\tvalidation_0-auc:0.89048\n",
            "[45]\tvalidation_0-auc:0.889125\n",
            "[46]\tvalidation_0-auc:0.889899\n",
            "[47]\tvalidation_0-auc:0.890738\n",
            "[48]\tvalidation_0-auc:0.890028\n",
            "[49]\tvalidation_0-auc:0.888609\n",
            "[50]\tvalidation_0-auc:0.888996\n",
            "[51]\tvalidation_0-auc:0.889641\n",
            "[52]\tvalidation_0-auc:0.889254\n",
            "[53]\tvalidation_0-auc:0.889706\n",
            "[54]\tvalidation_0-auc:0.889641\n",
            "[55]\tvalidation_0-auc:0.888416\n",
            "[56]\tvalidation_0-auc:0.888867\n",
            "[57]\tvalidation_0-auc:0.887255\n",
            "[58]\tvalidation_0-auc:0.887771\n",
            "[59]\tvalidation_0-auc:0.887835\n",
            "[60]\tvalidation_0-auc:0.886868\n",
            "[61]\tvalidation_0-auc:0.887126\n",
            "[62]\tvalidation_0-auc:0.885578\n",
            "[63]\tvalidation_0-auc:0.885707\n",
            "[64]\tvalidation_0-auc:0.8859\n",
            "[65]\tvalidation_0-auc:0.884804\n",
            "[66]\tvalidation_0-auc:0.882482\n",
            "[67]\tvalidation_0-auc:0.883127\n",
            "[68]\tvalidation_0-auc:0.883127\n",
            "[69]\tvalidation_0-auc:0.883514\n",
            "[70]\tvalidation_0-auc:0.883836\n",
            "[71]\tvalidation_0-auc:0.883385\n",
            "[72]\tvalidation_0-auc:0.883578\n",
            "[73]\tvalidation_0-auc:0.882933\n",
            "[74]\tvalidation_0-auc:0.881127\n",
            "[75]\tvalidation_0-auc:0.881321\n",
            "[76]\tvalidation_0-auc:0.880095\n",
            "[77]\tvalidation_0-auc:0.880353\n",
            "[78]\tvalidation_0-auc:0.880289\n",
            "[79]\tvalidation_0-auc:0.878741\n",
            "Stopping. Best iteration:\n",
            "[29]\tvalidation_0-auc:0.89364\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6954 - accuracy: 0.5072 - val_loss: 0.6702 - val_accuracy: 0.9606\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6937 - accuracy: 0.5161 - val_loss: 0.6487 - val_accuracy: 0.9606\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6934 - accuracy: 0.5106 - val_loss: 0.6276 - val_accuracy: 0.9606\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6820 - accuracy: 0.5559 - val_loss: 0.4576 - val_accuracy: 0.8621\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6531 - accuracy: 0.6239 - val_loss: 0.6244 - val_accuracy: 0.6630\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6819 - accuracy: 0.5875 - val_loss: 0.6790 - val_accuracy: 0.5908\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6364 - accuracy: 0.6507 - val_loss: 0.5995 - val_accuracy: 0.7309\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6214 - accuracy: 0.6719 - val_loss: 0.6161 - val_accuracy: 0.6871\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6160 - accuracy: 0.6884 - val_loss: 0.6356 - val_accuracy: 0.6521\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6168 - accuracy: 0.6726 - val_loss: 0.4869 - val_accuracy: 0.7746\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.608643\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.633637\n",
            "[2]\tvalidation_0-auc:0.662237\n",
            "[3]\tvalidation_0-auc:0.675588\n",
            "[4]\tvalidation_0-auc:0.66306\n",
            "[5]\tvalidation_0-auc:0.660213\n",
            "[6]\tvalidation_0-auc:0.65262\n",
            "[7]\tvalidation_0-auc:0.652746\n",
            "[8]\tvalidation_0-auc:0.661541\n",
            "[9]\tvalidation_0-auc:0.658631\n",
            "[10]\tvalidation_0-auc:0.655973\n",
            "[11]\tvalidation_0-auc:0.663946\n",
            "[12]\tvalidation_0-auc:0.67426\n",
            "[13]\tvalidation_0-auc:0.66616\n",
            "[14]\tvalidation_0-auc:0.664768\n",
            "[15]\tvalidation_0-auc:0.657049\n",
            "[16]\tvalidation_0-auc:0.650089\n",
            "[17]\tvalidation_0-auc:0.650215\n",
            "[18]\tvalidation_0-auc:0.650974\n",
            "[19]\tvalidation_0-auc:0.648317\n",
            "[20]\tvalidation_0-auc:0.653126\n",
            "[21]\tvalidation_0-auc:0.65224\n",
            "[22]\tvalidation_0-auc:0.647937\n",
            "[23]\tvalidation_0-auc:0.647558\n",
            "[24]\tvalidation_0-auc:0.643824\n",
            "[25]\tvalidation_0-auc:0.649456\n",
            "[26]\tvalidation_0-auc:0.649456\n",
            "[27]\tvalidation_0-auc:0.649456\n",
            "[28]\tvalidation_0-auc:0.649835\n",
            "[29]\tvalidation_0-auc:0.649835\n",
            "[30]\tvalidation_0-auc:0.652556\n",
            "[31]\tvalidation_0-auc:0.650278\n",
            "[32]\tvalidation_0-auc:0.650278\n",
            "[33]\tvalidation_0-auc:0.651038\n",
            "[34]\tvalidation_0-auc:0.654455\n",
            "[35]\tvalidation_0-auc:0.655847\n",
            "[36]\tvalidation_0-auc:0.658125\n",
            "[37]\tvalidation_0-auc:0.664199\n",
            "[38]\tvalidation_0-auc:0.662807\n",
            "[39]\tvalidation_0-auc:0.666287\n",
            "[40]\tvalidation_0-auc:0.665654\n",
            "[41]\tvalidation_0-auc:0.665528\n",
            "[42]\tvalidation_0-auc:0.665591\n",
            "[43]\tvalidation_0-auc:0.665591\n",
            "[44]\tvalidation_0-auc:0.665211\n",
            "[45]\tvalidation_0-auc:0.664199\n",
            "[46]\tvalidation_0-auc:0.664199\n",
            "[47]\tvalidation_0-auc:0.664705\n",
            "[48]\tvalidation_0-auc:0.664705\n",
            "[49]\tvalidation_0-auc:0.663819\n",
            "[50]\tvalidation_0-auc:0.663313\n",
            "[51]\tvalidation_0-auc:0.661415\n",
            "[52]\tvalidation_0-auc:0.661541\n",
            "[53]\tvalidation_0-auc:0.661541\n",
            "Stopping. Best iteration:\n",
            "[3]\tvalidation_0-auc:0.675588\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+----------------------+--------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision       |       Recall       |       F1 score      |\n",
            "+-------------------+--------------------+----------------------+--------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.7959183673469388 | 0.22033898305084745  | 0.7647058823529411 |  0.3421052631578947 |\n",
            "|      GRU 0.15     | 0.736734693877551  |  0.1895424836601307  | 0.8529411764705882 | 0.31016042780748665 |\n",
            "|    XGBoost 0.15   | 0.6408163265306123 |  0.1485148514851485  | 0.8823529411764706 | 0.25423728813559326 |\n",
            "|    Logreg 0.15    | 0.7204081632653061 | 0.18012422360248448  | 0.8529411764705882 | 0.29743589743589743 |\n",
            "|      SVM 0.15     | 0.7040816326530612 | 0.17543859649122806  | 0.8823529411764706 |  0.2926829268292682 |\n",
            "|   LSTM beta 0.15  | 0.6630196936542669 | 0.04054054054054054  | 0.3333333333333333 | 0.07228915662650603 |\n",
            "|   GRU beta 0.15   | 0.774617067833698  |  0.0707070707070707  | 0.3888888888888889 | 0.11965811965811965 |\n",
            "| XGBoost beta 0.15 | 0.6148796498905909 | 0.056179775280898875 | 0.5555555555555556 |  0.1020408163265306 |\n",
            "|  logreg beta 0.15 | 0.7636761487964989 | 0.08333333333333333  |        0.5         | 0.14285714285714285 |\n",
            "|   svm beta 0.15   | 0.6739606126914661 | 0.06040268456375839  |        0.5         | 0.10778443113772455 |\n",
            "+-------------------+--------------------+----------------------+--------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOPYypMLWILk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f492ea9-bc05-48df-f391-3061fb4fc8af"
      },
      "source": [
        "Result_purging.to_csv('FOXA_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.297753</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.430894</td>\n",
              "      <td>0.779412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.327684</td>\n",
              "      <td>0.736735</td>\n",
              "      <td>0.473469</td>\n",
              "      <td>0.852941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.315217</td>\n",
              "      <td>0.722449</td>\n",
              "      <td>0.460317</td>\n",
              "      <td>0.852941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.259574</td>\n",
              "      <td>0.630612</td>\n",
              "      <td>0.402640</td>\n",
              "      <td>0.897059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.292929</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>0.436090</td>\n",
              "      <td>0.852941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.341880</td>\n",
              "      <td>0.805252</td>\n",
              "      <td>0.473373</td>\n",
              "      <td>0.769231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.324561</td>\n",
              "      <td>0.798687</td>\n",
              "      <td>0.445783</td>\n",
              "      <td>0.711538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.242775</td>\n",
              "      <td>0.691466</td>\n",
              "      <td>0.373333</td>\n",
              "      <td>0.807692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.271186</td>\n",
              "      <td>0.708972</td>\n",
              "      <td>0.419214</td>\n",
              "      <td>0.923077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.236264</td>\n",
              "      <td>0.676149</td>\n",
              "      <td>0.367521</td>\n",
              "      <td>0.826923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.220339</td>\n",
              "      <td>0.795918</td>\n",
              "      <td>0.342105</td>\n",
              "      <td>0.764706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.189542</td>\n",
              "      <td>0.736735</td>\n",
              "      <td>0.310160</td>\n",
              "      <td>0.852941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.148515</td>\n",
              "      <td>0.640816</td>\n",
              "      <td>0.254237</td>\n",
              "      <td>0.882353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.180124</td>\n",
              "      <td>0.720408</td>\n",
              "      <td>0.297436</td>\n",
              "      <td>0.852941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.175439</td>\n",
              "      <td>0.704082</td>\n",
              "      <td>0.292683</td>\n",
              "      <td>0.882353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.040541</td>\n",
              "      <td>0.663020</td>\n",
              "      <td>0.072289</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.070707</td>\n",
              "      <td>0.774617</td>\n",
              "      <td>0.119658</td>\n",
              "      <td>0.388889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.056180</td>\n",
              "      <td>0.614880</td>\n",
              "      <td>0.102041</td>\n",
              "      <td>0.555556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.763676</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.060403</td>\n",
              "      <td>0.673961</td>\n",
              "      <td>0.107784</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  FOXA  0.297753  0.714286  0.430894  0.779412\n",
              "1            GRU 0.1  FOXA  0.327684  0.736735  0.473469  0.852941\n",
              "2        XGBoost 0.1  FOXA  0.315217  0.722449  0.460317  0.852941\n",
              "3         Logreg 0.1  FOXA  0.259574  0.630612  0.402640  0.897059\n",
              "4            SVM 0.1  FOXA  0.292929  0.693878  0.436090  0.852941\n",
              "5      LSTM beta 0.1  FOXA  0.341880  0.805252  0.473373  0.769231\n",
              "6       GRU beta 0.1  FOXA  0.324561  0.798687  0.445783  0.711538\n",
              "7   XGBoost beta 0.1  FOXA  0.242775  0.691466  0.373333  0.807692\n",
              "8    logreg beta 0.1  FOXA  0.271186  0.708972  0.419214  0.923077\n",
              "9       svm beta 0.1  FOXA  0.236264  0.676149  0.367521  0.826923\n",
              "0          LSTM 0.15  FOXA  0.220339  0.795918  0.342105  0.764706\n",
              "1           GRU 0.15  FOXA  0.189542  0.736735  0.310160  0.852941\n",
              "2       XGBoost 0.15  FOXA  0.148515  0.640816  0.254237  0.882353\n",
              "3        Logreg 0.15  FOXA  0.180124  0.720408  0.297436  0.852941\n",
              "4           SVM 0.15  FOXA  0.175439  0.704082  0.292683  0.882353\n",
              "5     LSTM beta 0.15  FOXA  0.040541  0.663020  0.072289  0.333333\n",
              "6      GRU beta 0.15  FOXA  0.070707  0.774617  0.119658  0.388889\n",
              "7  XGBoost beta 0.15  FOXA  0.056180  0.614880  0.102041  0.555556\n",
              "8   logreg beta 0.15  FOXA  0.083333  0.763676  0.142857  0.500000\n",
              "9      svm beta 0.15  FOXA  0.060403  0.673961  0.107784  0.500000"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32qoUKvzWILk"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bslUV9G5WILk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KbGsMvQWlIW"
      },
      "source": [
        "## M"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNEWq2i_WlId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d65ef6a-1cec-4599-e7ec-b1a4a347fb13"
      },
      "source": [
        "dfs = pd.read_csv(\"M.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "# Denoise prices\n",
        "dfs = denoise_data(dfs, '<CLOSE>')\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2768</td>\n",
              "      <td>US1.M</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>22.64</td>\n",
              "      <td>23.1400</td>\n",
              "      <td>22.21</td>\n",
              "      <td>22.87</td>\n",
              "      <td>703439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2767</td>\n",
              "      <td>US1.M</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>23.90</td>\n",
              "      <td>23.9000</td>\n",
              "      <td>22.34</td>\n",
              "      <td>22.60</td>\n",
              "      <td>1239730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.M</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>25.28</td>\n",
              "      <td>25.4600</td>\n",
              "      <td>24.67</td>\n",
              "      <td>24.70</td>\n",
              "      <td>386601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.M</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>24.69</td>\n",
              "      <td>25.4500</td>\n",
              "      <td>24.58</td>\n",
              "      <td>25.12</td>\n",
              "      <td>854636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.M</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>23.91</td>\n",
              "      <td>25.1500</td>\n",
              "      <td>23.91</td>\n",
              "      <td>24.33</td>\n",
              "      <td>738222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.M</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>23.93</td>\n",
              "      <td>24.5025</td>\n",
              "      <td>23.83</td>\n",
              "      <td>24.37</td>\n",
              "      <td>8712993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.M</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>23.44</td>\n",
              "      <td>23.9800</td>\n",
              "      <td>23.25</td>\n",
              "      <td>23.85</td>\n",
              "      <td>9219821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.M</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>23.47</td>\n",
              "      <td>23.7300</td>\n",
              "      <td>23.09</td>\n",
              "      <td>23.67</td>\n",
              "      <td>9486082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.M</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>23.27</td>\n",
              "      <td>23.7000</td>\n",
              "      <td>23.08</td>\n",
              "      <td>23.52</td>\n",
              "      <td>10283908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.M</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>22.71</td>\n",
              "      <td>22.7700</td>\n",
              "      <td>22.01</td>\n",
              "      <td>22.76</td>\n",
              "      <td>11287243</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2769 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...   <HIGH>  <LOW>  <CLOSE>     <VOL>\n",
              "0      2768    US1.M     D  20211001  ...  23.1400  22.21    22.87    703439\n",
              "1      2767    US1.M     D  20210930  ...  23.9000  22.34    22.60   1239730\n",
              "2      2766    US1.M     D  20210929  ...  25.4600  24.67    24.70    386601\n",
              "3      2765    US1.M     D  20210928  ...  25.4500  24.58    25.12    854636\n",
              "4      2764    US1.M     D  20210927  ...  25.1500  23.91    24.33    738222\n",
              "...     ...      ...   ...       ...  ...      ...    ...      ...       ...\n",
              "2764      4    US1.M     D  20101008  ...  24.5025  23.83    24.37   8712993\n",
              "2765      3    US1.M     D  20101007  ...  23.9800  23.25    23.85   9219821\n",
              "2766      2    US1.M     D  20101006  ...  23.7300  23.09    23.67   9486082\n",
              "2767      1    US1.M     D  20101005  ...  23.7000  23.08    23.52  10283908\n",
              "2768      0    US1.M     D  20101004  ...  22.7700  22.01    22.76  11287243\n",
              "\n",
              "[2769 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjdiFshSWlId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb254d5-c8bd-4a1d-cf94-4db0c9923a9a"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"4c79765e-3e43-4f5e-bebf-91c990d8a08b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"4c79765e-3e43-4f5e-bebf-91c990d8a08b\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '4c79765e-3e43-4f5e-bebf-91c990d8a08b',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [14.83, 15.02, 14.96, 14.81, 14.66, 15.56, 15.54, 15.29, 15.26, 15.55, 15.27, 15.54, 15.42, 15.72, 16.29, 16.63, 17.415, 17.14, 17.0, 17.835, 17.11, 16.19, 15.39, 15.32, 14.915, 14.4, 14.76, 14.91, 14.63, 14.295, 14.72, 14.95, 15.56, 15.4, 15.38, 16.13, 15.985, 16.155, 16.8, 19.36, 19.27, 19.43, 20.33, 20.46, 20.84, 20.61, 21.29, 21.18, 22.71, 22.53, 22.68, 22.88, 22.78, 23.26, 22.59, 22.09, 22.1, 21.55, 21.5, 22.02, 22.04, 21.91, 21.4, 21.17, 21.235, 21.66, 21.64, 21.29, 21.04, 21.39, 21.47, 21.52, 21.65, 21.3, 21.79, 22.3, 21.91, 22.14, 21.88, 21.725, 21.54, 21.7651, 21.45, 21.92, 21.67, 20.43, 20.49, 21.055, 21.62, 21.15, 20.57, 20.75, 20.95, 21.04, 21.01, 21.27, 21.85, 21.77, 21.56, 21.78, 21.485, 21.695, 21.813, 21.58, 22.47, 22.7, 22.74, 22.79, 23.21, 23.23, 23.44, 23.37, 23.54, 24.09, 24.265, 24.14, 25.06, 24.64, 24.33, 25.08, 24.95, 24.655, 24.575, 24.29, 24.325, 24.76, 25.08, 25.66, 25.5, 25.98, 24.61, 24.67, 24.46, 24.02, 24.2, 24.33, 23.93, 23.86, 23.32, 24.03, 23.63, 23.94, 23.9, 23.705, 23.64, 24.03, 23.83, 23.57, 23.1, 23.3, 24.04, 24.38, 24.35, 24.49, 24.78, 25.32, 24.72, 24.36, 24.06, 24.45, 25.02, 25.34, 24.88, 25.145, 25.21, 24.77, 25.03, 25.135, 25.66, 25.36, 25.94, 25.88, 25.72, 26.285, 25.72, 25.72, 25.73, 25.51, 24.52, 24.705, 24.88, 25.8, 24.75, 24.475, 24.96, 25.085, 25.39, 26.1067, 31.71, 30.82, 29.91, 29.385, 29.76, 30.77, 29.78, 30.02, 30.05, 30.13, 27.8701, 28.21, 29.11, 30.41, 31.045, 30.28, 30.6, 30.8, 32.21, 31.84, 31.89, 31.673, 32.37, 32.43, 34.35, 34.22, 33.74, 34.42, 33.89, 32.55, 32.1, 32.59, 31.95, 33.07, 33.33, 32.26, 33.22, 35.77, 37.04, 37.77, 37.785, 37.18, 37.02, 36.73, 35.54, 34.58, 34.3, 35.72, 33.98, 32.37, 33.08, 32.21, 32.54, 32.89, 32.34, 32.41, 31.84, 33.51, 33.39, 33.38, 32.13, 32.77, 33.06, 33.18, 32.83, 32.87, 33.4, 33.03, 34.72, 34.73, 34.59, 34.89, 34.385, 34.755, 35.69, 35.18, 35.56, 35.535, 35.16, 36.27, 35.74, 36.39, 36.4, 35.98, 35.52, 35.36, 35.79, 36.39, 36.56, 35.97, 36.2, 36.35, 36.27, 36.52, 38.13, 37.56, 38.24, 38.21, 36.03, 35.81, 35.15, 41.87, 40.09, 39.96, 40.55, 39.895, 39.47, 38.78, 38.95, 38.73, 37.94, 39.73, 40.28, 39.48, 39.49, 40.16, 39.35, 39.41, 38.66, 38.95, 37.7, 37.07, 36.99, 36.39, 36.12, 36.44, 36.72, 36.46, 36.89, 36.89, 37.15, 36.52, 37.435, 38.48, 37.89, 37.86, 36.98, 37.44, 39.32, 39.6, 38.88, 38.74, 38.25, 37.56, 38.36, 39.97, 39.75, 39.88, 40.21, 40.08, 40.01, 37.1, 35.56, 34.905, 35.58, 34.83, 34.12, 34.04, 33.48, 33.12, 34.6, 33.98, 33.83, 33.16, 29.95, 29.64, 29.64, 29.41, 30.13, 29.77, 30.03, 31.23, 31.45, 31.33, 30.72, 31.06, 32.19, 31.74, 30.72, 30.5, 30.47, 29.96, 29.415, 29.395, 28.94, 28.75, 28.25, 28.88, 29.06, 29.3, 29.15, 29.795, 30.92, 30.09, 29.0, 29.07, 29.74, 29.0799, 27.86, 27.89, 27.21, 28.27, 28.66, 28.93, 29.3, 28.92, 28.79, 29.23, 29.82, 28.75, 28.85, 28.9, 29.91, 30.37, 30.35, 30.42, 29.24, 29.4, 28.4, 27.43, 26.73, 26.17, 25.57, 25.66, 26.26, 26.48, 25.66, 24.89, 24.2, 24.13, 24.0, 24.76, 24.17, 23.47, 24.88, 25.61, 25.94, 26.24, 27.32, 27.4, 26.62, 27.24, 27.36, 27.26, 27.01, 26.61, 26.24, 25.9, 26.89, 26.29, 25.595, 24.71, 24.43, 24.47, 24.47, 25.34, 26.31, 25.19, 25.71, 25.64, 26.8501, 25.69, 25.57, 25.34, 25.26, 25.85, 24.6, 24.81, 25.85, 25.7, 25.89, 25.8, 25.29, 25.1, 25.21, 25.81, 24.18, 23.8, 23.9999, 22.18, 21.21, 20.97, 20.64, 20.41, 20.81, 20.35, 20.2414, 19.98, 19.71, 19.32, 19.99, 19.5, 17.56, 17.54, 18.15, 18.36, 18.79, 18.95, 18.75, 18.82, 19.68, 21.33, 21.24, 21.42, 21.34, 21.18, 20.2, 20.15, 19.94, 19.89, 20.21, 20.23, 20.49, 20.67, 20.365, 20.805, 21.025, 20.62, 21.04, 20.88, 21.82, 22.02, 22.16, 21.865, 21.81, 21.52, 21.17, 21.47, 21.78, 22.11, 22.565, 21.99, 22.6653, 22.22, 21.49, 21.31, 21.72, 22.17, 21.005, 21.4, 20.76, 20.83, 20.93, 21.15, 21.11, 20.7, 20.5, 20.43, 19.52, 19.49, 19.64, 20.1261, 20.29, 20.34, 20.62, 20.67, 23.02, 23.52, 23.535, 23.32, 23.595, 23.16, 23.97, 23.74, 23.93, 24.2, 23.525, 23.49, 22.69, 23.35, 23.075, 22.98, 22.68, 23.0342, 22.36, 22.11, 21.22, 21.165, 21.1, 22.67, 22.85, 23.52, 23.94, 23.23, 22.94, 23.17, 23.08, 22.4799, 22.26, 22.09, 21.6, 22.11, 22.715, 22.93, 22.72, 22.75, 22.26, 22.66, 22.72, 21.77, 21.81, 21.91, 23.88, 23.8, 24.08, 23.495, 23.61, 23.45, 23.37, 23.19, 23.06, 23.42, 23.02, 22.76, 23.0, 22.83, 23.2, 23.605, 24.34, 29.34, 29.29, 28.66, 28.96, 28.92, 29.17, 29.51, 28.87, 29.21, 29.495, 29.43, 29.27, 29.2, 29.48, 30.01, 29.075, 28.7, 28.82, 29.17, 29.16, 29.69, 29.52, 29.09, 29.4, 28.82, 28.89, 29.63, 29.64, 29.47, 29.28, 28.55, 27.93, 28.18, 28.27, 28.38, 28.41, 29.37, 30.55, 30.41, 30.315, 30.96, 30.9, 31.74, 31.54, 31.77, 30.8, 30.69, 31.76, 33.21, 32.95, 33.2154, 33.57, 33.17, 32.45, 32.35, 32.3, 32.295, 31.81, 32.73, 32.68, 32.57, 32.0, 32.37, 31.98, 31.23, 31.81, 32.6901, 30.725, 29.21, 29.55, 29.52, 29.11, 29.91, 30.16, 29.99, 29.63, 29.71, 29.45, 29.43, 29.89, 29.87, 29.97, 29.96, 30.29, 30.48, 30.8151, 30.85, 35.82, 35.22, 35.82, 36.3, 36.28, 36.52, 36.48, 36.175, 37.52, 37.94, 37.01, 37.465, 40.14, 40.16, 40.7, 40.43, 42.43, 43.07, 43.18, 42.41, 41.89, 42.48, 42.42, 42.2, 42.46, 43.12, 44.19, 44.9, 44.47, 43.2, 43.05, 43.0525, 41.84, 41.385, 41.57, 41.34, 40.5314, 38.36, 37.87, 37.77, 36.85, 36.88, 36.89, 36.86, 36.5, 35.575, 35.27, 35.9, 35.92, 36.63, 36.51, 35.865, 35.58, 35.19, 35.06, 35.57, 36.8, 37.55, 37.235, 37.51, 38.0399, 37.7, 37.36, 36.58, 36.27, 37.04, 36.04, 36.08, 36.6, 36.24, 36.61, 36.25, 36.01, 34.95, 35.21, 35.53, 35.56, 35.215, 34.69, 35.64, 35.48, 36.0, 37.19, 36.16, 36.64, 36.32, 36.17, 38.2, 38.67, 38.56, 39.27, 39.73, 40.01, 39.33, 40.305, 40.23, 40.4, 40.69, 40.48, 39.83, 39.83, 34.04, 33.79, 34.72, 34.09, 33.705, 33.42, 32.75, 35.32, 35.835, 35.87, 35.7, 36.81, 36.45, 35.2711, 35.51, 35.31, 34.87, 35.27, 34.66, 34.82, 35.085, 35.64, 34.86, 34.36, 33.62, 33.61, 33.09, 33.58, 33.59, 33.33, 32.43, 31.7, 32.07, 33.38, 32.79, 33.03, 33.21, 33.23, 32.36, 31.94, 31.31, 31.61, 33.24, 33.62, 34.27, 34.33, 34.28, 34.33, 34.365, 33.03, 33.2, 32.72, 31.56, 31.82, 31.19, 31.3, 31.3, 30.86, 30.08, 30.4, 30.73, 31.21, 31.21, 31.3799, 36.97, 37.75, 37.65, 37.9, 38.68, 39.48, 39.92, 39.59, 40.59, 41.01, 40.54, 40.32, 40.95, 41.55, 41.35, 41.15, 40.73, 40.635, 39.65, 40.33, 40.07, 39.81, 39.67, 40.62, 41.58, 41.46, 42.17, 42.95, 44.07, 44.1, 44.43, 44.39, 43.475, 43.41, 43.77, 44.14, 44.86, 43.97, 43.5, 43.1, 43.87, 44.36, 44.21, 44.31, 44.04, 44.61, 44.59, 44.08, 43.96, 43.68, 43.2, 43.43, 43.38, 42.82, 42.31, 41.04, 40.22, 41.13, 41.095, 40.485, 39.23, 37.88, 38.8, 39.51, 39.64, 40.27, 40.72, 41.63, 40.9, 40.9, 40.385, 39.57, 40.29, 40.19, 39.31, 41.35, 40.61, 39.73, 38.77, 37.87, 37.64, 38.62, 38.61, 38.79, 35.89, 36.9, 36.14, 36.96, 35.81, 34.99, 35.3779, 35.6901, 35.5, 35.51, 36.07, 35.42, 35.02, 34.87, 34.49, 35.87, 35.93, 36.37, 36.76, 37.88, 38.14, 38.44, 38.56, 39.33, 38.97, 38.43, 39.32, 39.09, 40.02, 40.38, 39.645, 40.04, 38.59, 38.53, 38.92, 38.02, 38.62, 39.1, 40.825, 40.41, 47.03, 46.24, 48.9, 50.44, 50.44, 51.17, 51.34, 51.0, 49.71, 49.87, 49.07, 48.66, 47.97, 49.89, 50.34, 50.64, 50.71, 50.465, 50.46, 49.77, 50.26, 50.35, 51.01, 51.46, 51.1, 51.4, 51.68, 51.85, 51.055, 51.33, 50.37, 50.3, 52.51, 52.06, 52.86, 53.42, 53.89, 54.05, 55.99, 57.28, 57.17, 57.86, 58.19, 58.37, 58.86, 59.17, 58.41, 59.28, 58.63, 57.96, 58.6, 59.01, 59.07, 58.71, 56.78, 57.13, 59.21, 61.46, 62.42, 62.64, 62.94, 63.33, 62.98, 64.1, 67.6, 67.37, 66.9, 67.47, 68.9, 68.58, 68.21, 69.05, 69.34, 69.1749, 68.88, 69.97, 69.9401, 71.19, 71.92, 71.66, 72.12, 72.33, 72.81, 72.02, 66.72, 66.69, 66.485, 65.73, 66.2, 68.03, 67.52, 67.44, 67.72, 67.47, 67.08, 69.86, 69.17, 69.9, 70.09, 69.89, 69.86, 70.66, 70.02, 68.86, 68.98, 69.64, 68.95, 68.87, 69.12, 68.96, 69.25, 69.25, 69.71, 68.49, 66.81, 66.94, 68.05, 67.81, 67.33, 68.12, 67.66, 68.21, 69.03, 67.755, 66.52, 63.16, 63.75, 65.29, 66.025, 65.97, 64.73, 63.66, 64.5, 65.31, 65.06, 64.62, 64.79, 65.79, 65.93, 67.02, 67.42, 66.99, 67.04, 66.79, 66.47, 67.66, 67.9, 68.0201, 69.17, 69.17, 68.0, 69.8, 67.96, 68.14, 67.85, 65.45, 64.9, 64.43, 63.65, 63.27, 64.725, 65.35, 65.63, 65.79, 65.05, 64.61, 64.1, 64.12, 63.35, 63.18, 62.34, 63.3, 62.56, 62.99, 63.18, 62.87, 63.47, 63.91, 63.74, 63.57, 63.58, 62.085, 64.14, 63.7, 63.0, 63.49, 63.62, 64.3, 64.64, 64.74, 64.53, 63.67, 63.73, 63.75, 64.87, 66.06, 64.19, 63.89, 65.38, 64.89, 65.53, 66.07, 65.86, 65.505, 64.22, 63.3, 63.18, 62.35, 64.43, 65.59, 66.045, 65.95, 67.81, 67.55, 64.89, 65.15, 65.7, 65.74, 65.27, 65.22, 64.04, 64.6399, 64.03, 63.45, 62.61, 64.08, 62.78, 62.02, 63.15, 62.38, 61.23, 60.62, 61.35, 61.56, 62.33, 62.5, 63.65, 63.25, 63.19, 64.9, 63.51, 63.91, 64.38, 63.31, 62.85, 61.94, 61.46, 61.6, 62.06, 61.59, 61.57, 58.5799, 59.23, 59.89, 59.02, 57.1901, 56.29, 57.24, 57.82, 57.92, 57.45, 57.26, 58.5, 58.95, 58.35, 57.62, 58.38, 57.06, 56.2, 56.58, 56.0, 56.59, 54.99, 56.67, 56.395, 57.98, 57.35, 58.52, 59.61, 58.34, 57.762, 58.17, 58.93, 59.67, 58.82, 59.54, 58.64, 58.93, 60.095, 60.565, 60.63, 60.24, 59.57, 59.57, 59.82, 59.9, 60.42, 60.64, 61.82, 62.42, 61.67, 62.21, 62.27, 62.37, 62.57, 62.59, 62.37, 61.78, 60.71, 60.1, 58.95, 58.06, 57.46, 57.86, 56.48, 59.73, 60.13, 60.2, 58.78, 58.8, 57.63, 58.59, 57.91, 57.79, 58.63, 57.61, 57.57, 57.48, 57.93, 57.46, 56.99, 56.72, 57.11, 56.63, 57.07, 56.71, 57.82, 58.1, 58.81, 59.48, 59.17, 59.5, 60.0899, 58.94, 58.665, 58.01, 58.68, 58.06, 57.99, 58.48, 58.76, 58.14, 58.09, 57.76, 57.97, 57.22, 57.39, 57.28, 57.98, 58.88, 59.23, 59.65, 59.2, 60.02, 59.85, 60.05, 59.88, 59.16, 58.6, 58.36, 58.055, 57.62, 56.79, 56.51, 58.1, 58.07, 57.05, 57.83, 57.84, 57.37, 56.36, 55.96, 55.22, 55.1, 56.665, 57.18, 56.91, 57.42, 58.1, 58.01, 57.58, 58.055, 58.11, 57.52, 57.32, 56.83, 58.67, 57.59, 56.97, 56.89, 58.06, 58.84, 58.39, 58.25, 59.75, 60.6, 60.65, 59.51, 59.31, 58.93, 58.4, 58.09, 58.02, 58.42, 59.06, 58.7, 58.54, 58.89, 58.89, 58.57, 58.21, 58.76, 58.94, 58.13, 58.05, 57.32, 57.32, 57.65, 57.62, 57.85, 57.86, 57.96, 56.24, 53.04, 53.72, 53.45, 53.35, 53.14, 53.38, 53.13, 52.81, 53.56, 52.91, 53.06, 52.55, 51.36, 50.96, 50.9, 53.19, 53.9, 53.39, 54.48, 54.18, 54.41, 55.045, 55.39, 55.56, 56.21, 56.04, 55.81, 55.76, 55.27, 55.84, 55.81, 51.84, 52.18, 53.14, 53.52, 53.4, 53.39, 53.67, 52.7, 52.82, 52.9, 52.85, 52.3, 52.23, 52.15, 51.68, 51.62, 51.4, 51.66, 52.07, 51.78, 52.2, 51.84, 51.55, 52.04, 52.82, 52.44, 53.26, 53.54, 52.93, 52.29, 51.27, 50.82, 50.96, 50.42, 50.48, 51.02, 50.69, 50.699, 46.33, 47.08, 46.18, 45.95, 46.41, 46.14, 46.49, 45.96, 46.12, 46.39, 46.12, 45.45, 44.88, 45.11, 44.54]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4c79765e-3e43-4f5e-bebf-91c990d8a08b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"59e0b4c2-df85-4041-b42b-40e0d3ae7cd7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"59e0b4c2-df85-4041-b42b-40e0d3ae7cd7\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '59e0b4c2-df85-4041-b42b-40e0d3ae7cd7',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('59e0b4c2-df85-4041-b42b-40e0d3ae7cd7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hxgp6en5WlId"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsOy1xB3WlId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb9c47f-9641-4889-823c-f16e8c2c0bfb"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.05, .1, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"M\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6951 - accuracy: 0.5040 - val_loss: 0.6822 - val_accuracy: 0.7367\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6392 - accuracy: 0.6376 - val_loss: 0.7615 - val_accuracy: 0.2898\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6238 - accuracy: 0.6631 - val_loss: 0.5655 - val_accuracy: 0.7245\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5969 - accuracy: 0.6933 - val_loss: 0.6343 - val_accuracy: 0.6490\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5691 - accuracy: 0.7208 - val_loss: 0.5232 - val_accuracy: 0.7327\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6570 - accuracy: 0.6148 - val_loss: 0.5507 - val_accuracy: 0.7469\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5827 - accuracy: 0.7114 - val_loss: 0.6161 - val_accuracy: 0.6653\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5591 - accuracy: 0.7087 - val_loss: 0.5718 - val_accuracy: 0.7245\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5429 - accuracy: 0.7383 - val_loss: 0.5657 - val_accuracy: 0.7122\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5462 - accuracy: 0.7228 - val_loss: 0.5218 - val_accuracy: 0.7347\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.740514\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.747106\n",
            "[2]\tvalidation_0-auc:0.755782\n",
            "[3]\tvalidation_0-auc:0.772638\n",
            "[4]\tvalidation_0-auc:0.780594\n",
            "[5]\tvalidation_0-auc:0.781131\n",
            "[6]\tvalidation_0-auc:0.779821\n",
            "[7]\tvalidation_0-auc:0.779725\n",
            "[8]\tvalidation_0-auc:0.777749\n",
            "[9]\tvalidation_0-auc:0.781432\n",
            "[10]\tvalidation_0-auc:0.781528\n",
            "[11]\tvalidation_0-auc:0.781861\n",
            "[12]\tvalidation_0-auc:0.783944\n",
            "[13]\tvalidation_0-auc:0.78417\n",
            "[14]\tvalidation_0-auc:0.783783\n",
            "[15]\tvalidation_0-auc:0.783182\n",
            "[16]\tvalidation_0-auc:0.782259\n",
            "[17]\tvalidation_0-auc:0.77631\n",
            "[18]\tvalidation_0-auc:0.77558\n",
            "[19]\tvalidation_0-auc:0.78549\n",
            "[20]\tvalidation_0-auc:0.784427\n",
            "[21]\tvalidation_0-auc:0.782516\n",
            "[22]\tvalidation_0-auc:0.784116\n",
            "[23]\tvalidation_0-auc:0.781335\n",
            "[24]\tvalidation_0-auc:0.781206\n",
            "[25]\tvalidation_0-auc:0.77995\n",
            "[26]\tvalidation_0-auc:0.77995\n",
            "[27]\tvalidation_0-auc:0.782581\n",
            "[28]\tvalidation_0-auc:0.782055\n",
            "[29]\tvalidation_0-auc:0.781539\n",
            "[30]\tvalidation_0-auc:0.781539\n",
            "[31]\tvalidation_0-auc:0.782044\n",
            "[32]\tvalidation_0-auc:0.782044\n",
            "[33]\tvalidation_0-auc:0.782581\n",
            "[34]\tvalidation_0-auc:0.781679\n",
            "[35]\tvalidation_0-auc:0.782151\n",
            "[36]\tvalidation_0-auc:0.782248\n",
            "[37]\tvalidation_0-auc:0.782162\n",
            "[38]\tvalidation_0-auc:0.782591\n",
            "[39]\tvalidation_0-auc:0.782591\n",
            "[40]\tvalidation_0-auc:0.782065\n",
            "[41]\tvalidation_0-auc:0.782087\n",
            "[42]\tvalidation_0-auc:0.782087\n",
            "[43]\tvalidation_0-auc:0.781786\n",
            "[44]\tvalidation_0-auc:0.781722\n",
            "[45]\tvalidation_0-auc:0.781722\n",
            "[46]\tvalidation_0-auc:0.782312\n",
            "[47]\tvalidation_0-auc:0.782312\n",
            "[48]\tvalidation_0-auc:0.782795\n",
            "[49]\tvalidation_0-auc:0.783246\n",
            "[50]\tvalidation_0-auc:0.780927\n",
            "[51]\tvalidation_0-auc:0.780927\n",
            "[52]\tvalidation_0-auc:0.781035\n",
            "[53]\tvalidation_0-auc:0.780906\n",
            "[54]\tvalidation_0-auc:0.780691\n",
            "[55]\tvalidation_0-auc:0.780777\n",
            "[56]\tvalidation_0-auc:0.781024\n",
            "[57]\tvalidation_0-auc:0.780874\n",
            "[58]\tvalidation_0-auc:0.780616\n",
            "[59]\tvalidation_0-auc:0.780723\n",
            "[60]\tvalidation_0-auc:0.780723\n",
            "[61]\tvalidation_0-auc:0.780723\n",
            "[62]\tvalidation_0-auc:0.780874\n",
            "[63]\tvalidation_0-auc:0.780874\n",
            "[64]\tvalidation_0-auc:0.780981\n",
            "[65]\tvalidation_0-auc:0.781325\n",
            "[66]\tvalidation_0-auc:0.781303\n",
            "[67]\tvalidation_0-auc:0.780895\n",
            "[68]\tvalidation_0-auc:0.780433\n",
            "[69]\tvalidation_0-auc:0.780895\n",
            "Stopping. Best iteration:\n",
            "[19]\tvalidation_0-auc:0.78549\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6869 - accuracy: 0.5367 - val_loss: 0.6270 - val_accuracy: 0.7177\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6644 - accuracy: 0.6054 - val_loss: 0.8859 - val_accuracy: 0.3392\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6410 - accuracy: 0.6527 - val_loss: 0.6239 - val_accuracy: 0.7505\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5829 - accuracy: 0.6932 - val_loss: 0.5366 - val_accuracy: 0.7571\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5589 - accuracy: 0.7213 - val_loss: 0.5402 - val_accuracy: 0.7637\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6449 - accuracy: 0.6273 - val_loss: 0.5682 - val_accuracy: 0.7440\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5466 - accuracy: 0.7111 - val_loss: 0.5681 - val_accuracy: 0.7243\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5328 - accuracy: 0.7220 - val_loss: 0.5276 - val_accuracy: 0.7396\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5304 - accuracy: 0.7467 - val_loss: 0.5528 - val_accuracy: 0.7352\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5276 - accuracy: 0.7426 - val_loss: 0.5368 - val_accuracy: 0.7527\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.688788\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.68594\n",
            "[2]\tvalidation_0-auc:0.698797\n",
            "[3]\tvalidation_0-auc:0.71488\n",
            "[4]\tvalidation_0-auc:0.710649\n",
            "[5]\tvalidation_0-auc:0.728068\n",
            "[6]\tvalidation_0-auc:0.739034\n",
            "[7]\tvalidation_0-auc:0.750012\n",
            "[8]\tvalidation_0-auc:0.746207\n",
            "[9]\tvalidation_0-auc:0.745592\n",
            "[10]\tvalidation_0-auc:0.749114\n",
            "[11]\tvalidation_0-auc:0.751737\n",
            "[12]\tvalidation_0-auc:0.754148\n",
            "[13]\tvalidation_0-auc:0.759501\n",
            "[14]\tvalidation_0-auc:0.761935\n",
            "[15]\tvalidation_0-auc:0.761935\n",
            "[16]\tvalidation_0-auc:0.762006\n",
            "[17]\tvalidation_0-auc:0.761722\n",
            "[18]\tvalidation_0-auc:0.762881\n",
            "[19]\tvalidation_0-auc:0.762881\n",
            "[20]\tvalidation_0-auc:0.764157\n",
            "[21]\tvalidation_0-auc:0.764109\n",
            "[22]\tvalidation_0-auc:0.767111\n",
            "[23]\tvalidation_0-auc:0.767111\n",
            "[24]\tvalidation_0-auc:0.764145\n",
            "[25]\tvalidation_0-auc:0.764145\n",
            "[26]\tvalidation_0-auc:0.764145\n",
            "[27]\tvalidation_0-auc:0.764913\n",
            "[28]\tvalidation_0-auc:0.762372\n",
            "[29]\tvalidation_0-auc:0.762372\n",
            "[30]\tvalidation_0-auc:0.758473\n",
            "[31]\tvalidation_0-auc:0.758756\n",
            "[32]\tvalidation_0-auc:0.763211\n",
            "[33]\tvalidation_0-auc:0.763211\n",
            "[34]\tvalidation_0-auc:0.761037\n",
            "[35]\tvalidation_0-auc:0.761037\n",
            "[36]\tvalidation_0-auc:0.761203\n",
            "[37]\tvalidation_0-auc:0.761203\n",
            "[38]\tvalidation_0-auc:0.761203\n",
            "[39]\tvalidation_0-auc:0.760293\n",
            "[40]\tvalidation_0-auc:0.760943\n",
            "[41]\tvalidation_0-auc:0.758036\n",
            "[42]\tvalidation_0-auc:0.758036\n",
            "[43]\tvalidation_0-auc:0.755199\n",
            "[44]\tvalidation_0-auc:0.755672\n",
            "[45]\tvalidation_0-auc:0.755672\n",
            "[46]\tvalidation_0-auc:0.755672\n",
            "[47]\tvalidation_0-auc:0.755672\n",
            "[48]\tvalidation_0-auc:0.755743\n",
            "[49]\tvalidation_0-auc:0.753982\n",
            "[50]\tvalidation_0-auc:0.756712\n",
            "[51]\tvalidation_0-auc:0.756476\n",
            "[52]\tvalidation_0-auc:0.753663\n",
            "[53]\tvalidation_0-auc:0.756901\n",
            "[54]\tvalidation_0-auc:0.757752\n",
            "[55]\tvalidation_0-auc:0.757846\n",
            "[56]\tvalidation_0-auc:0.756736\n",
            "[57]\tvalidation_0-auc:0.755105\n",
            "[58]\tvalidation_0-auc:0.753616\n",
            "[59]\tvalidation_0-auc:0.752458\n",
            "[60]\tvalidation_0-auc:0.752458\n",
            "[61]\tvalidation_0-auc:0.751453\n",
            "[62]\tvalidation_0-auc:0.749728\n",
            "[63]\tvalidation_0-auc:0.74961\n",
            "[64]\tvalidation_0-auc:0.749634\n",
            "[65]\tvalidation_0-auc:0.750626\n",
            "[66]\tvalidation_0-auc:0.750626\n",
            "[67]\tvalidation_0-auc:0.749374\n",
            "[68]\tvalidation_0-auc:0.747861\n",
            "[69]\tvalidation_0-auc:0.748121\n",
            "[70]\tvalidation_0-auc:0.748334\n",
            "[71]\tvalidation_0-auc:0.747554\n",
            "[72]\tvalidation_0-auc:0.74675\n",
            "Stopping. Best iteration:\n",
            "[22]\tvalidation_0-auc:0.767111\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.7326530612244898 | 0.49107142857142855 | 0.4263565891472868 | 0.45643153526970953 |\n",
            "|     GRU 0.1      | 0.7346938775510204 |  0.496551724137931  | 0.5581395348837209 |  0.5255474452554745 |\n",
            "|   XGBoost 0.1    | 0.7306122448979592 |  0.4915254237288136 | 0.6744186046511628 |  0.5686274509803922 |\n",
            "|    Logreg 0.1    | 0.7346938775510204 | 0.49710982658959535 | 0.6666666666666666 |  0.5695364238410596 |\n",
            "|     SVM 0.1      | 0.7183673469387755 | 0.47305389221556887 | 0.6124031007751938 |  0.5337837837837838 |\n",
            "|  LSTM beta 0.1   | 0.7636761487964989 |  0.5826771653543307 | 0.5736434108527132 |       0.578125      |\n",
            "|   GRU beta 0.1   | 0.7527352297592997 |  0.5555555555555556 | 0.6201550387596899 |  0.5860805860805861 |\n",
            "| XGBoost beta 0.1 | 0.7396061269146609 |  0.5431034482758621 | 0.4883720930232558 |  0.5142857142857143 |\n",
            "| logreg beta 0.1  | 0.7592997811816192 |  0.5725190839694656 | 0.5813953488372093 |  0.576923076923077  |\n",
            "|   svm beta 0.1   | 0.7352297592997812 |       0.53125       | 0.5271317829457365 |  0.529182879377432  |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6886 - accuracy: 0.5161 - val_loss: 0.5395 - val_accuracy: 0.8673\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6474 - accuracy: 0.6208 - val_loss: 0.6852 - val_accuracy: 0.6041\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6362 - accuracy: 0.6383 - val_loss: 0.6322 - val_accuracy: 0.7082\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6227 - accuracy: 0.6430 - val_loss: 0.6492 - val_accuracy: 0.6633\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6199 - accuracy: 0.6564 - val_loss: 0.6412 - val_accuracy: 0.6776\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6723 - accuracy: 0.5631 - val_loss: 0.8448 - val_accuracy: 0.2265\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6119 - accuracy: 0.6584 - val_loss: 0.8124 - val_accuracy: 0.3510\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6001 - accuracy: 0.6591 - val_loss: 0.6300 - val_accuracy: 0.6653\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6039 - accuracy: 0.6577 - val_loss: 0.5652 - val_accuracy: 0.7510\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5850 - accuracy: 0.6718 - val_loss: 0.7018 - val_accuracy: 0.5612\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.715439\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.729864\n",
            "[2]\tvalidation_0-auc:0.735656\n",
            "[3]\tvalidation_0-auc:0.733756\n",
            "[4]\tvalidation_0-auc:0.738643\n",
            "[5]\tvalidation_0-auc:0.733303\n",
            "[6]\tvalidation_0-auc:0.732054\n",
            "[7]\tvalidation_0-auc:0.733629\n",
            "[8]\tvalidation_0-auc:0.742462\n",
            "[9]\tvalidation_0-auc:0.750968\n",
            "[10]\tvalidation_0-auc:0.748561\n",
            "[11]\tvalidation_0-auc:0.751475\n",
            "[12]\tvalidation_0-auc:0.751674\n",
            "[13]\tvalidation_0-auc:0.74981\n",
            "[14]\tvalidation_0-auc:0.749701\n",
            "[15]\tvalidation_0-auc:0.750443\n",
            "[16]\tvalidation_0-auc:0.75057\n",
            "[17]\tvalidation_0-auc:0.748597\n",
            "[18]\tvalidation_0-auc:0.749701\n",
            "[19]\tvalidation_0-auc:0.747638\n",
            "[20]\tvalidation_0-auc:0.748308\n",
            "[21]\tvalidation_0-auc:0.748181\n",
            "[22]\tvalidation_0-auc:0.747566\n",
            "[23]\tvalidation_0-auc:0.747005\n",
            "[24]\tvalidation_0-auc:0.747584\n",
            "[25]\tvalidation_0-auc:0.746643\n",
            "[26]\tvalidation_0-auc:0.748181\n",
            "[27]\tvalidation_0-auc:0.748181\n",
            "[28]\tvalidation_0-auc:0.748398\n",
            "[29]\tvalidation_0-auc:0.748434\n",
            "[30]\tvalidation_0-auc:0.748434\n",
            "[31]\tvalidation_0-auc:0.748561\n",
            "[32]\tvalidation_0-auc:0.749032\n",
            "[33]\tvalidation_0-auc:0.749032\n",
            "[34]\tvalidation_0-auc:0.75124\n",
            "[35]\tvalidation_0-auc:0.75124\n",
            "[36]\tvalidation_0-auc:0.75124\n",
            "[37]\tvalidation_0-auc:0.75124\n",
            "[38]\tvalidation_0-auc:0.751421\n",
            "[39]\tvalidation_0-auc:0.751023\n",
            "[40]\tvalidation_0-auc:0.748652\n",
            "[41]\tvalidation_0-auc:0.748615\n",
            "[42]\tvalidation_0-auc:0.748615\n",
            "[43]\tvalidation_0-auc:0.748615\n",
            "[44]\tvalidation_0-auc:0.748941\n",
            "[45]\tvalidation_0-auc:0.74876\n",
            "[46]\tvalidation_0-auc:0.74876\n",
            "[47]\tvalidation_0-auc:0.748796\n",
            "[48]\tvalidation_0-auc:0.74876\n",
            "[49]\tvalidation_0-auc:0.748796\n",
            "[50]\tvalidation_0-auc:0.748796\n",
            "[51]\tvalidation_0-auc:0.748724\n",
            "[52]\tvalidation_0-auc:0.748833\n",
            "[53]\tvalidation_0-auc:0.748615\n",
            "[54]\tvalidation_0-auc:0.754643\n",
            "[55]\tvalidation_0-auc:0.754643\n",
            "[56]\tvalidation_0-auc:0.754643\n",
            "[57]\tvalidation_0-auc:0.75095\n",
            "[58]\tvalidation_0-auc:0.75095\n",
            "[59]\tvalidation_0-auc:0.751023\n",
            "[60]\tvalidation_0-auc:0.752959\n",
            "[61]\tvalidation_0-auc:0.752923\n",
            "[62]\tvalidation_0-auc:0.752923\n",
            "[63]\tvalidation_0-auc:0.752923\n",
            "[64]\tvalidation_0-auc:0.747837\n",
            "[65]\tvalidation_0-auc:0.740706\n",
            "[66]\tvalidation_0-auc:0.740995\n",
            "[67]\tvalidation_0-auc:0.740054\n",
            "[68]\tvalidation_0-auc:0.739873\n",
            "[69]\tvalidation_0-auc:0.739873\n",
            "[70]\tvalidation_0-auc:0.739873\n",
            "[71]\tvalidation_0-auc:0.740507\n",
            "[72]\tvalidation_0-auc:0.740398\n",
            "[73]\tvalidation_0-auc:0.741158\n",
            "[74]\tvalidation_0-auc:0.74362\n",
            "[75]\tvalidation_0-auc:0.742751\n",
            "[76]\tvalidation_0-auc:0.740724\n",
            "[77]\tvalidation_0-auc:0.740869\n",
            "[78]\tvalidation_0-auc:0.741086\n",
            "[79]\tvalidation_0-auc:0.741448\n",
            "[80]\tvalidation_0-auc:0.741448\n",
            "[81]\tvalidation_0-auc:0.741448\n",
            "[82]\tvalidation_0-auc:0.741448\n",
            "[83]\tvalidation_0-auc:0.741502\n",
            "[84]\tvalidation_0-auc:0.741792\n",
            "[85]\tvalidation_0-auc:0.740127\n",
            "[86]\tvalidation_0-auc:0.741412\n",
            "[87]\tvalidation_0-auc:0.73743\n",
            "[88]\tvalidation_0-auc:0.738045\n",
            "[89]\tvalidation_0-auc:0.738371\n",
            "[90]\tvalidation_0-auc:0.737864\n",
            "[91]\tvalidation_0-auc:0.737213\n",
            "[92]\tvalidation_0-auc:0.736814\n",
            "[93]\tvalidation_0-auc:0.736561\n",
            "[94]\tvalidation_0-auc:0.736778\n",
            "[95]\tvalidation_0-auc:0.736995\n",
            "[96]\tvalidation_0-auc:0.737502\n",
            "[97]\tvalidation_0-auc:0.737538\n",
            "[98]\tvalidation_0-auc:0.737719\n",
            "[99]\tvalidation_0-auc:0.735837\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6535 - accuracy: 0.5930 - val_loss: 0.5823 - val_accuracy: 0.7724\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6002 - accuracy: 0.6905 - val_loss: 0.4550 - val_accuracy: 0.8862\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5681 - accuracy: 0.7193 - val_loss: 0.4296 - val_accuracy: 0.8993\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5360 - accuracy: 0.7412 - val_loss: 0.5628 - val_accuracy: 0.7681\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5275 - accuracy: 0.7481 - val_loss: 0.4044 - val_accuracy: 0.8928\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6252 - accuracy: 0.6383 - val_loss: 0.6505 - val_accuracy: 0.6061\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5274 - accuracy: 0.7515 - val_loss: 0.4910 - val_accuracy: 0.8140\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5047 - accuracy: 0.7584 - val_loss: 0.4203 - val_accuracy: 0.8796\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4917 - accuracy: 0.7721 - val_loss: 0.4986 - val_accuracy: 0.7987\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5013 - accuracy: 0.7618 - val_loss: 0.3947 - val_accuracy: 0.8928\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.759812\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.735165\n",
            "[2]\tvalidation_0-auc:0.719054\n",
            "[3]\tvalidation_0-auc:0.718093\n",
            "[4]\tvalidation_0-auc:0.716071\n",
            "[5]\tvalidation_0-auc:0.717249\n",
            "[6]\tvalidation_0-auc:0.716385\n",
            "[7]\tvalidation_0-auc:0.718838\n",
            "[8]\tvalidation_0-auc:0.718583\n",
            "[9]\tvalidation_0-auc:0.717249\n",
            "[10]\tvalidation_0-auc:0.723155\n",
            "[11]\tvalidation_0-auc:0.72084\n",
            "[12]\tvalidation_0-auc:0.72029\n",
            "[13]\tvalidation_0-auc:0.723077\n",
            "[14]\tvalidation_0-auc:0.723057\n",
            "[15]\tvalidation_0-auc:0.729101\n",
            "[16]\tvalidation_0-auc:0.72863\n",
            "[17]\tvalidation_0-auc:0.727728\n",
            "[18]\tvalidation_0-auc:0.727688\n",
            "[19]\tvalidation_0-auc:0.727492\n",
            "[20]\tvalidation_0-auc:0.727119\n",
            "[21]\tvalidation_0-auc:0.72916\n",
            "[22]\tvalidation_0-auc:0.727845\n",
            "[23]\tvalidation_0-auc:0.72865\n",
            "[24]\tvalidation_0-auc:0.723214\n",
            "[25]\tvalidation_0-auc:0.723175\n",
            "[26]\tvalidation_0-auc:0.719388\n",
            "[27]\tvalidation_0-auc:0.721291\n",
            "[28]\tvalidation_0-auc:0.721762\n",
            "[29]\tvalidation_0-auc:0.721743\n",
            "[30]\tvalidation_0-auc:0.717327\n",
            "[31]\tvalidation_0-auc:0.717288\n",
            "[32]\tvalidation_0-auc:0.717249\n",
            "[33]\tvalidation_0-auc:0.71717\n",
            "[34]\tvalidation_0-auc:0.716228\n",
            "[35]\tvalidation_0-auc:0.716268\n",
            "[36]\tvalidation_0-auc:0.71458\n",
            "[37]\tvalidation_0-auc:0.714305\n",
            "[38]\tvalidation_0-auc:0.714266\n",
            "[39]\tvalidation_0-auc:0.712068\n",
            "[40]\tvalidation_0-auc:0.711951\n",
            "[41]\tvalidation_0-auc:0.713677\n",
            "[42]\tvalidation_0-auc:0.713677\n",
            "[43]\tvalidation_0-auc:0.710361\n",
            "[44]\tvalidation_0-auc:0.707535\n",
            "[45]\tvalidation_0-auc:0.707457\n",
            "[46]\tvalidation_0-auc:0.705926\n",
            "[47]\tvalidation_0-auc:0.704651\n",
            "[48]\tvalidation_0-auc:0.704768\n",
            "[49]\tvalidation_0-auc:0.704768\n",
            "[50]\tvalidation_0-auc:0.702414\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.759812\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.6775510204081633 | 0.23121387283236994 |  0.6153846153846154 | 0.33613445378151263 |\n",
            "|      GRU 0.15     | 0.5612244897959183 | 0.20238095238095238 |  0.7846153846153846 |  0.3217665615141955 |\n",
            "|    XGBoost 0.15   | 0.7285714285714285 |  0.2638888888888889 |  0.5846153846153846 |  0.3636363636363636 |\n",
            "|    Logreg 0.15    | 0.6857142857142857 | 0.24571428571428572 |  0.6615384615384615 |  0.3583333333333334 |\n",
            "|      SVM 0.15     | 0.7306122448979592 | 0.25547445255474455 |  0.5384615384615384 |  0.3465346534653465 |\n",
            "|   LSTM beta 0.15  | 0.8927789934354485 |  0.7352941176470589 | 0.38461538461538464 |  0.5050505050505051 |\n",
            "|   GRU beta 0.15   | 0.8927789934354485 |  0.7105263157894737 |  0.4153846153846154 |  0.5242718446601942 |\n",
            "| XGBoost beta 0.15 | 0.8840262582056893 |  0.6304347826086957 |  0.4461538461538462 |  0.5225225225225225 |\n",
            "|  logreg beta 0.15 | 0.8161925601750547 | 0.39325842696629215 |  0.5384615384615384 |  0.4545454545454546 |\n",
            "|   svm beta 0.15   | 0.8708971553610503 |  0.5555555555555556 | 0.46153846153846156 |  0.504201680672269  |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROgTBAtbWlId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e5342c5-5857-4c80-a6e3-c03c1aed0dd7"
      },
      "source": [
        "Result_cross.to_csv('M_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.491071</td>\n",
              "      <td>0.732653</td>\n",
              "      <td>0.456432</td>\n",
              "      <td>0.426357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.496552</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.525547</td>\n",
              "      <td>0.558140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.491525</td>\n",
              "      <td>0.730612</td>\n",
              "      <td>0.568627</td>\n",
              "      <td>0.674419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.497110</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.569536</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.473054</td>\n",
              "      <td>0.718367</td>\n",
              "      <td>0.533784</td>\n",
              "      <td>0.612403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.582677</td>\n",
              "      <td>0.763676</td>\n",
              "      <td>0.578125</td>\n",
              "      <td>0.573643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.752735</td>\n",
              "      <td>0.586081</td>\n",
              "      <td>0.620155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.543103</td>\n",
              "      <td>0.739606</td>\n",
              "      <td>0.514286</td>\n",
              "      <td>0.488372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.572519</td>\n",
              "      <td>0.759300</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.581395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.531250</td>\n",
              "      <td>0.735230</td>\n",
              "      <td>0.529183</td>\n",
              "      <td>0.527132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.231214</td>\n",
              "      <td>0.677551</td>\n",
              "      <td>0.336134</td>\n",
              "      <td>0.615385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.202381</td>\n",
              "      <td>0.561224</td>\n",
              "      <td>0.321767</td>\n",
              "      <td>0.784615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>0.728571</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.584615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.245714</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.358333</td>\n",
              "      <td>0.661538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.255474</td>\n",
              "      <td>0.730612</td>\n",
              "      <td>0.346535</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.735294</td>\n",
              "      <td>0.892779</td>\n",
              "      <td>0.505051</td>\n",
              "      <td>0.384615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.710526</td>\n",
              "      <td>0.892779</td>\n",
              "      <td>0.524272</td>\n",
              "      <td>0.415385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.630435</td>\n",
              "      <td>0.884026</td>\n",
              "      <td>0.522523</td>\n",
              "      <td>0.446154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.393258</td>\n",
              "      <td>0.816193</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.870897</td>\n",
              "      <td>0.504202</td>\n",
              "      <td>0.461538</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1    M  0.491071  0.732653  0.456432  0.426357\n",
              "1            GRU 0.1    M  0.496552  0.734694  0.525547  0.558140\n",
              "2        XGBoost 0.1    M  0.491525  0.730612  0.568627  0.674419\n",
              "3         Logreg 0.1    M  0.497110  0.734694  0.569536  0.666667\n",
              "4            SVM 0.1    M  0.473054  0.718367  0.533784  0.612403\n",
              "5      LSTM beta 0.1    M  0.582677  0.763676  0.578125  0.573643\n",
              "6       GRU beta 0.1    M  0.555556  0.752735  0.586081  0.620155\n",
              "7   XGBoost beta 0.1    M  0.543103  0.739606  0.514286  0.488372\n",
              "8    logreg beta 0.1    M  0.572519  0.759300  0.576923  0.581395\n",
              "9       svm beta 0.1    M  0.531250  0.735230  0.529183  0.527132\n",
              "0          LSTM 0.15    M  0.231214  0.677551  0.336134  0.615385\n",
              "1           GRU 0.15    M  0.202381  0.561224  0.321767  0.784615\n",
              "2       XGBoost 0.15    M  0.263889  0.728571  0.363636  0.584615\n",
              "3        Logreg 0.15    M  0.245714  0.685714  0.358333  0.661538\n",
              "4           SVM 0.15    M  0.255474  0.730612  0.346535  0.538462\n",
              "5     LSTM beta 0.15    M  0.735294  0.892779  0.505051  0.384615\n",
              "6      GRU beta 0.15    M  0.710526  0.892779  0.524272  0.415385\n",
              "7  XGBoost beta 0.15    M  0.630435  0.884026  0.522523  0.446154\n",
              "8   logreg beta 0.15    M  0.393258  0.816193  0.454545  0.538462\n",
              "9      svm beta 0.15    M  0.555556  0.870897  0.504202  0.461538"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPk-zjRGWlId"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeySr84fWlId"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhEkpZOEWlIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "071e4f90-30c4-4c8b-d989-b7851f44b837"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"M\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6920 - accuracy: 0.5161 - val_loss: 0.7052 - val_accuracy: 0.2918\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6527 - accuracy: 0.6087 - val_loss: 0.6474 - val_accuracy: 0.7327\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6134 - accuracy: 0.6785 - val_loss: 0.5988 - val_accuracy: 0.7163\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5965 - accuracy: 0.7067 - val_loss: 0.5818 - val_accuracy: 0.7224\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5633 - accuracy: 0.7215 - val_loss: 0.5953 - val_accuracy: 0.7000\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6648 - accuracy: 0.5839 - val_loss: 0.5832 - val_accuracy: 0.7531\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5763 - accuracy: 0.7027 - val_loss: 0.6770 - val_accuracy: 0.5776\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5653 - accuracy: 0.7154 - val_loss: 0.6039 - val_accuracy: 0.6980\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5516 - accuracy: 0.7248 - val_loss: 0.5027 - val_accuracy: 0.7653\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5387 - accuracy: 0.7349 - val_loss: 0.4854 - val_accuracy: 0.7612\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.740514\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.747106\n",
            "[2]\tvalidation_0-auc:0.755782\n",
            "[3]\tvalidation_0-auc:0.772638\n",
            "[4]\tvalidation_0-auc:0.780594\n",
            "[5]\tvalidation_0-auc:0.781131\n",
            "[6]\tvalidation_0-auc:0.779821\n",
            "[7]\tvalidation_0-auc:0.779725\n",
            "[8]\tvalidation_0-auc:0.777749\n",
            "[9]\tvalidation_0-auc:0.781432\n",
            "[10]\tvalidation_0-auc:0.781528\n",
            "[11]\tvalidation_0-auc:0.781861\n",
            "[12]\tvalidation_0-auc:0.783944\n",
            "[13]\tvalidation_0-auc:0.78417\n",
            "[14]\tvalidation_0-auc:0.783783\n",
            "[15]\tvalidation_0-auc:0.783182\n",
            "[16]\tvalidation_0-auc:0.782259\n",
            "[17]\tvalidation_0-auc:0.77631\n",
            "[18]\tvalidation_0-auc:0.77558\n",
            "[19]\tvalidation_0-auc:0.78549\n",
            "[20]\tvalidation_0-auc:0.784427\n",
            "[21]\tvalidation_0-auc:0.782516\n",
            "[22]\tvalidation_0-auc:0.784116\n",
            "[23]\tvalidation_0-auc:0.781335\n",
            "[24]\tvalidation_0-auc:0.781206\n",
            "[25]\tvalidation_0-auc:0.77995\n",
            "[26]\tvalidation_0-auc:0.77995\n",
            "[27]\tvalidation_0-auc:0.782581\n",
            "[28]\tvalidation_0-auc:0.782055\n",
            "[29]\tvalidation_0-auc:0.781539\n",
            "[30]\tvalidation_0-auc:0.781539\n",
            "[31]\tvalidation_0-auc:0.782044\n",
            "[32]\tvalidation_0-auc:0.782044\n",
            "[33]\tvalidation_0-auc:0.782581\n",
            "[34]\tvalidation_0-auc:0.781679\n",
            "[35]\tvalidation_0-auc:0.782151\n",
            "[36]\tvalidation_0-auc:0.782248\n",
            "[37]\tvalidation_0-auc:0.782162\n",
            "[38]\tvalidation_0-auc:0.782591\n",
            "[39]\tvalidation_0-auc:0.782591\n",
            "[40]\tvalidation_0-auc:0.782065\n",
            "[41]\tvalidation_0-auc:0.782087\n",
            "[42]\tvalidation_0-auc:0.782087\n",
            "[43]\tvalidation_0-auc:0.781786\n",
            "[44]\tvalidation_0-auc:0.781722\n",
            "[45]\tvalidation_0-auc:0.781722\n",
            "[46]\tvalidation_0-auc:0.782312\n",
            "[47]\tvalidation_0-auc:0.782312\n",
            "[48]\tvalidation_0-auc:0.782795\n",
            "[49]\tvalidation_0-auc:0.783246\n",
            "[50]\tvalidation_0-auc:0.780927\n",
            "[51]\tvalidation_0-auc:0.780927\n",
            "[52]\tvalidation_0-auc:0.781035\n",
            "[53]\tvalidation_0-auc:0.780906\n",
            "[54]\tvalidation_0-auc:0.780691\n",
            "[55]\tvalidation_0-auc:0.780777\n",
            "[56]\tvalidation_0-auc:0.781024\n",
            "[57]\tvalidation_0-auc:0.780874\n",
            "[58]\tvalidation_0-auc:0.780616\n",
            "[59]\tvalidation_0-auc:0.780723\n",
            "[60]\tvalidation_0-auc:0.780723\n",
            "[61]\tvalidation_0-auc:0.780723\n",
            "[62]\tvalidation_0-auc:0.780874\n",
            "[63]\tvalidation_0-auc:0.780874\n",
            "[64]\tvalidation_0-auc:0.780981\n",
            "[65]\tvalidation_0-auc:0.781325\n",
            "[66]\tvalidation_0-auc:0.781303\n",
            "[67]\tvalidation_0-auc:0.780895\n",
            "[68]\tvalidation_0-auc:0.780433\n",
            "[69]\tvalidation_0-auc:0.780895\n",
            "Stopping. Best iteration:\n",
            "[19]\tvalidation_0-auc:0.78549\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6843 - accuracy: 0.5498 - val_loss: 0.6481 - val_accuracy: 0.7177\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6641 - accuracy: 0.6033 - val_loss: 0.6272 - val_accuracy: 0.7112\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6165 - accuracy: 0.6644 - val_loss: 0.6045 - val_accuracy: 0.7462\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5682 - accuracy: 0.7090 - val_loss: 0.5338 - val_accuracy: 0.7505\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5506 - accuracy: 0.7186 - val_loss: 0.5349 - val_accuracy: 0.7505\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6393 - accuracy: 0.6369 - val_loss: 0.5500 - val_accuracy: 0.7418\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5620 - accuracy: 0.7090 - val_loss: 0.5869 - val_accuracy: 0.7374\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5417 - accuracy: 0.7289 - val_loss: 0.5038 - val_accuracy: 0.7615\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5383 - accuracy: 0.7275 - val_loss: 0.5166 - val_accuracy: 0.7571\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5262 - accuracy: 0.7385 - val_loss: 0.5192 - val_accuracy: 0.7484\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.688788\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.68594\n",
            "[2]\tvalidation_0-auc:0.698797\n",
            "[3]\tvalidation_0-auc:0.71488\n",
            "[4]\tvalidation_0-auc:0.710649\n",
            "[5]\tvalidation_0-auc:0.728068\n",
            "[6]\tvalidation_0-auc:0.739034\n",
            "[7]\tvalidation_0-auc:0.750012\n",
            "[8]\tvalidation_0-auc:0.746207\n",
            "[9]\tvalidation_0-auc:0.745592\n",
            "[10]\tvalidation_0-auc:0.749114\n",
            "[11]\tvalidation_0-auc:0.751737\n",
            "[12]\tvalidation_0-auc:0.754148\n",
            "[13]\tvalidation_0-auc:0.759501\n",
            "[14]\tvalidation_0-auc:0.761935\n",
            "[15]\tvalidation_0-auc:0.761935\n",
            "[16]\tvalidation_0-auc:0.762006\n",
            "[17]\tvalidation_0-auc:0.761722\n",
            "[18]\tvalidation_0-auc:0.762881\n",
            "[19]\tvalidation_0-auc:0.762881\n",
            "[20]\tvalidation_0-auc:0.764157\n",
            "[21]\tvalidation_0-auc:0.764109\n",
            "[22]\tvalidation_0-auc:0.767111\n",
            "[23]\tvalidation_0-auc:0.767111\n",
            "[24]\tvalidation_0-auc:0.764145\n",
            "[25]\tvalidation_0-auc:0.764145\n",
            "[26]\tvalidation_0-auc:0.764145\n",
            "[27]\tvalidation_0-auc:0.764913\n",
            "[28]\tvalidation_0-auc:0.762372\n",
            "[29]\tvalidation_0-auc:0.762372\n",
            "[30]\tvalidation_0-auc:0.758473\n",
            "[31]\tvalidation_0-auc:0.758756\n",
            "[32]\tvalidation_0-auc:0.763211\n",
            "[33]\tvalidation_0-auc:0.763211\n",
            "[34]\tvalidation_0-auc:0.761037\n",
            "[35]\tvalidation_0-auc:0.761037\n",
            "[36]\tvalidation_0-auc:0.761203\n",
            "[37]\tvalidation_0-auc:0.761203\n",
            "[38]\tvalidation_0-auc:0.761203\n",
            "[39]\tvalidation_0-auc:0.760293\n",
            "[40]\tvalidation_0-auc:0.760943\n",
            "[41]\tvalidation_0-auc:0.758036\n",
            "[42]\tvalidation_0-auc:0.758036\n",
            "[43]\tvalidation_0-auc:0.755199\n",
            "[44]\tvalidation_0-auc:0.755672\n",
            "[45]\tvalidation_0-auc:0.755672\n",
            "[46]\tvalidation_0-auc:0.755672\n",
            "[47]\tvalidation_0-auc:0.755672\n",
            "[48]\tvalidation_0-auc:0.755743\n",
            "[49]\tvalidation_0-auc:0.753982\n",
            "[50]\tvalidation_0-auc:0.756712\n",
            "[51]\tvalidation_0-auc:0.756476\n",
            "[52]\tvalidation_0-auc:0.753663\n",
            "[53]\tvalidation_0-auc:0.756901\n",
            "[54]\tvalidation_0-auc:0.757752\n",
            "[55]\tvalidation_0-auc:0.757846\n",
            "[56]\tvalidation_0-auc:0.756736\n",
            "[57]\tvalidation_0-auc:0.755105\n",
            "[58]\tvalidation_0-auc:0.753616\n",
            "[59]\tvalidation_0-auc:0.752458\n",
            "[60]\tvalidation_0-auc:0.752458\n",
            "[61]\tvalidation_0-auc:0.751453\n",
            "[62]\tvalidation_0-auc:0.749728\n",
            "[63]\tvalidation_0-auc:0.74961\n",
            "[64]\tvalidation_0-auc:0.749634\n",
            "[65]\tvalidation_0-auc:0.750626\n",
            "[66]\tvalidation_0-auc:0.750626\n",
            "[67]\tvalidation_0-auc:0.749374\n",
            "[68]\tvalidation_0-auc:0.747861\n",
            "[69]\tvalidation_0-auc:0.748121\n",
            "[70]\tvalidation_0-auc:0.748334\n",
            "[71]\tvalidation_0-auc:0.747554\n",
            "[72]\tvalidation_0-auc:0.74675\n",
            "Stopping. Best iteration:\n",
            "[22]\tvalidation_0-auc:0.767111\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.1     |        0.7         | 0.45263157894736844 | 0.6666666666666666 |  0.5391849529780565 |\n",
            "|     GRU 0.1      | 0.7612244897959184 |  0.5612244897959183 | 0.4263565891472868 |  0.4845814977973568 |\n",
            "|   XGBoost 0.1    | 0.7306122448979592 |  0.4915254237288136 | 0.6744186046511628 |  0.5686274509803922 |\n",
            "|    Logreg 0.1    | 0.7346938775510204 | 0.49710982658959535 | 0.6666666666666666 |  0.5695364238410596 |\n",
            "|     SVM 0.1      | 0.7183673469387755 | 0.47305389221556887 | 0.6124031007751938 |  0.5337837837837838 |\n",
            "|  LSTM beta 0.1   |  0.75054704595186  |  0.5688073394495413 | 0.4806201550387597 |  0.5210084033613446 |\n",
            "|   GRU beta 0.1   | 0.7483588621444202 |        0.5875       | 0.3643410852713178 | 0.44976076555023925 |\n",
            "| XGBoost beta 0.1 | 0.7396061269146609 |  0.5431034482758621 | 0.4883720930232558 |  0.5142857142857143 |\n",
            "| logreg beta 0.1  | 0.7592997811816192 |  0.5725190839694656 | 0.5813953488372093 |  0.576923076923077  |\n",
            "|   svm beta 0.1   | 0.7352297592997812 |       0.53125       | 0.5271317829457365 |  0.529182879377432  |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6956 - accuracy: 0.5121 - val_loss: 0.7101 - val_accuracy: 0.1367\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6679 - accuracy: 0.5893 - val_loss: 0.6479 - val_accuracy: 0.7837\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6372 - accuracy: 0.6362 - val_loss: 0.5952 - val_accuracy: 0.7469\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6446 - accuracy: 0.6309 - val_loss: 0.6291 - val_accuracy: 0.7367\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6316 - accuracy: 0.6389 - val_loss: 0.6163 - val_accuracy: 0.7143\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6651 - accuracy: 0.5886 - val_loss: 0.6984 - val_accuracy: 0.5408\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6245 - accuracy: 0.6436 - val_loss: 0.6265 - val_accuracy: 0.7020\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6002 - accuracy: 0.6611 - val_loss: 0.6078 - val_accuracy: 0.7041\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5871 - accuracy: 0.6698 - val_loss: 0.6772 - val_accuracy: 0.5939\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5988 - accuracy: 0.6624 - val_loss: 0.6327 - val_accuracy: 0.6633\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.715439\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.729864\n",
            "[2]\tvalidation_0-auc:0.735656\n",
            "[3]\tvalidation_0-auc:0.733756\n",
            "[4]\tvalidation_0-auc:0.738643\n",
            "[5]\tvalidation_0-auc:0.733303\n",
            "[6]\tvalidation_0-auc:0.732054\n",
            "[7]\tvalidation_0-auc:0.733629\n",
            "[8]\tvalidation_0-auc:0.742462\n",
            "[9]\tvalidation_0-auc:0.750968\n",
            "[10]\tvalidation_0-auc:0.748561\n",
            "[11]\tvalidation_0-auc:0.751475\n",
            "[12]\tvalidation_0-auc:0.751674\n",
            "[13]\tvalidation_0-auc:0.74981\n",
            "[14]\tvalidation_0-auc:0.749701\n",
            "[15]\tvalidation_0-auc:0.750443\n",
            "[16]\tvalidation_0-auc:0.75057\n",
            "[17]\tvalidation_0-auc:0.748597\n",
            "[18]\tvalidation_0-auc:0.749701\n",
            "[19]\tvalidation_0-auc:0.747638\n",
            "[20]\tvalidation_0-auc:0.748308\n",
            "[21]\tvalidation_0-auc:0.748181\n",
            "[22]\tvalidation_0-auc:0.747566\n",
            "[23]\tvalidation_0-auc:0.747005\n",
            "[24]\tvalidation_0-auc:0.747584\n",
            "[25]\tvalidation_0-auc:0.746643\n",
            "[26]\tvalidation_0-auc:0.748181\n",
            "[27]\tvalidation_0-auc:0.748181\n",
            "[28]\tvalidation_0-auc:0.748398\n",
            "[29]\tvalidation_0-auc:0.748434\n",
            "[30]\tvalidation_0-auc:0.748434\n",
            "[31]\tvalidation_0-auc:0.748561\n",
            "[32]\tvalidation_0-auc:0.749032\n",
            "[33]\tvalidation_0-auc:0.749032\n",
            "[34]\tvalidation_0-auc:0.75124\n",
            "[35]\tvalidation_0-auc:0.75124\n",
            "[36]\tvalidation_0-auc:0.75124\n",
            "[37]\tvalidation_0-auc:0.75124\n",
            "[38]\tvalidation_0-auc:0.751421\n",
            "[39]\tvalidation_0-auc:0.751023\n",
            "[40]\tvalidation_0-auc:0.748652\n",
            "[41]\tvalidation_0-auc:0.748615\n",
            "[42]\tvalidation_0-auc:0.748615\n",
            "[43]\tvalidation_0-auc:0.748615\n",
            "[44]\tvalidation_0-auc:0.748941\n",
            "[45]\tvalidation_0-auc:0.74876\n",
            "[46]\tvalidation_0-auc:0.74876\n",
            "[47]\tvalidation_0-auc:0.748796\n",
            "[48]\tvalidation_0-auc:0.74876\n",
            "[49]\tvalidation_0-auc:0.748796\n",
            "[50]\tvalidation_0-auc:0.748796\n",
            "[51]\tvalidation_0-auc:0.748724\n",
            "[52]\tvalidation_0-auc:0.748833\n",
            "[53]\tvalidation_0-auc:0.748615\n",
            "[54]\tvalidation_0-auc:0.754643\n",
            "[55]\tvalidation_0-auc:0.754643\n",
            "[56]\tvalidation_0-auc:0.754643\n",
            "[57]\tvalidation_0-auc:0.75095\n",
            "[58]\tvalidation_0-auc:0.75095\n",
            "[59]\tvalidation_0-auc:0.751023\n",
            "[60]\tvalidation_0-auc:0.752959\n",
            "[61]\tvalidation_0-auc:0.752923\n",
            "[62]\tvalidation_0-auc:0.752923\n",
            "[63]\tvalidation_0-auc:0.752923\n",
            "[64]\tvalidation_0-auc:0.747837\n",
            "[65]\tvalidation_0-auc:0.740706\n",
            "[66]\tvalidation_0-auc:0.740995\n",
            "[67]\tvalidation_0-auc:0.740054\n",
            "[68]\tvalidation_0-auc:0.739873\n",
            "[69]\tvalidation_0-auc:0.739873\n",
            "[70]\tvalidation_0-auc:0.739873\n",
            "[71]\tvalidation_0-auc:0.740507\n",
            "[72]\tvalidation_0-auc:0.740398\n",
            "[73]\tvalidation_0-auc:0.741158\n",
            "[74]\tvalidation_0-auc:0.74362\n",
            "[75]\tvalidation_0-auc:0.742751\n",
            "[76]\tvalidation_0-auc:0.740724\n",
            "[77]\tvalidation_0-auc:0.740869\n",
            "[78]\tvalidation_0-auc:0.741086\n",
            "[79]\tvalidation_0-auc:0.741448\n",
            "[80]\tvalidation_0-auc:0.741448\n",
            "[81]\tvalidation_0-auc:0.741448\n",
            "[82]\tvalidation_0-auc:0.741448\n",
            "[83]\tvalidation_0-auc:0.741502\n",
            "[84]\tvalidation_0-auc:0.741792\n",
            "[85]\tvalidation_0-auc:0.740127\n",
            "[86]\tvalidation_0-auc:0.741412\n",
            "[87]\tvalidation_0-auc:0.73743\n",
            "[88]\tvalidation_0-auc:0.738045\n",
            "[89]\tvalidation_0-auc:0.738371\n",
            "[90]\tvalidation_0-auc:0.737864\n",
            "[91]\tvalidation_0-auc:0.737213\n",
            "[92]\tvalidation_0-auc:0.736814\n",
            "[93]\tvalidation_0-auc:0.736561\n",
            "[94]\tvalidation_0-auc:0.736778\n",
            "[95]\tvalidation_0-auc:0.736995\n",
            "[96]\tvalidation_0-auc:0.737502\n",
            "[97]\tvalidation_0-auc:0.737538\n",
            "[98]\tvalidation_0-auc:0.737719\n",
            "[99]\tvalidation_0-auc:0.735837\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6707 - accuracy: 0.5813 - val_loss: 0.5567 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6025 - accuracy: 0.6802 - val_loss: 0.5556 - val_accuracy: 0.7527\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5794 - accuracy: 0.7021 - val_loss: 0.4537 - val_accuracy: 0.8906\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5645 - accuracy: 0.7193 - val_loss: 0.4451 - val_accuracy: 0.8665\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5385 - accuracy: 0.7412 - val_loss: 0.6962 - val_accuracy: 0.5208\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6174 - accuracy: 0.6719 - val_loss: 0.3794 - val_accuracy: 0.8578\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5237 - accuracy: 0.7323 - val_loss: 0.4299 - val_accuracy: 0.8862\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4960 - accuracy: 0.7612 - val_loss: 0.4618 - val_accuracy: 0.8271\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4883 - accuracy: 0.7618 - val_loss: 0.4025 - val_accuracy: 0.8818\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4949 - accuracy: 0.7646 - val_loss: 0.4526 - val_accuracy: 0.8446\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.759812\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.735165\n",
            "[2]\tvalidation_0-auc:0.719054\n",
            "[3]\tvalidation_0-auc:0.718093\n",
            "[4]\tvalidation_0-auc:0.716071\n",
            "[5]\tvalidation_0-auc:0.717249\n",
            "[6]\tvalidation_0-auc:0.716385\n",
            "[7]\tvalidation_0-auc:0.718838\n",
            "[8]\tvalidation_0-auc:0.718583\n",
            "[9]\tvalidation_0-auc:0.717249\n",
            "[10]\tvalidation_0-auc:0.723155\n",
            "[11]\tvalidation_0-auc:0.72084\n",
            "[12]\tvalidation_0-auc:0.72029\n",
            "[13]\tvalidation_0-auc:0.723077\n",
            "[14]\tvalidation_0-auc:0.723057\n",
            "[15]\tvalidation_0-auc:0.729101\n",
            "[16]\tvalidation_0-auc:0.72863\n",
            "[17]\tvalidation_0-auc:0.727728\n",
            "[18]\tvalidation_0-auc:0.727688\n",
            "[19]\tvalidation_0-auc:0.727492\n",
            "[20]\tvalidation_0-auc:0.727119\n",
            "[21]\tvalidation_0-auc:0.72916\n",
            "[22]\tvalidation_0-auc:0.727845\n",
            "[23]\tvalidation_0-auc:0.72865\n",
            "[24]\tvalidation_0-auc:0.723214\n",
            "[25]\tvalidation_0-auc:0.723175\n",
            "[26]\tvalidation_0-auc:0.719388\n",
            "[27]\tvalidation_0-auc:0.721291\n",
            "[28]\tvalidation_0-auc:0.721762\n",
            "[29]\tvalidation_0-auc:0.721743\n",
            "[30]\tvalidation_0-auc:0.717327\n",
            "[31]\tvalidation_0-auc:0.717288\n",
            "[32]\tvalidation_0-auc:0.717249\n",
            "[33]\tvalidation_0-auc:0.71717\n",
            "[34]\tvalidation_0-auc:0.716228\n",
            "[35]\tvalidation_0-auc:0.716268\n",
            "[36]\tvalidation_0-auc:0.71458\n",
            "[37]\tvalidation_0-auc:0.714305\n",
            "[38]\tvalidation_0-auc:0.714266\n",
            "[39]\tvalidation_0-auc:0.712068\n",
            "[40]\tvalidation_0-auc:0.711951\n",
            "[41]\tvalidation_0-auc:0.713677\n",
            "[42]\tvalidation_0-auc:0.713677\n",
            "[43]\tvalidation_0-auc:0.710361\n",
            "[44]\tvalidation_0-auc:0.707535\n",
            "[45]\tvalidation_0-auc:0.707457\n",
            "[46]\tvalidation_0-auc:0.705926\n",
            "[47]\tvalidation_0-auc:0.704651\n",
            "[48]\tvalidation_0-auc:0.704768\n",
            "[49]\tvalidation_0-auc:0.704768\n",
            "[50]\tvalidation_0-auc:0.702414\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.759812\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.7142857142857143 | 0.25165562913907286 |  0.5846153846153846 | 0.35185185185185186 |\n",
            "|      GRU 0.15     | 0.6632653061224489 | 0.23118279569892472 |  0.6615384615384615 |  0.3426294820717131 |\n",
            "|    XGBoost 0.15   | 0.7285714285714285 |  0.2638888888888889 |  0.5846153846153846 |  0.3636363636363636 |\n",
            "|    Logreg 0.15    | 0.6857142857142857 | 0.24571428571428572 |  0.6615384615384615 |  0.3583333333333334 |\n",
            "|      SVM 0.15     | 0.7306122448979592 | 0.25547445255474455 |  0.5384615384615384 |  0.3465346534653465 |\n",
            "|   LSTM beta 0.15  | 0.5207877461706784 |      0.19921875     |  0.7846153846153846 |  0.3177570093457944 |\n",
            "|   GRU beta 0.15   | 0.8446389496717724 |  0.4594594594594595 |  0.5230769230769231 | 0.48920863309352514 |\n",
            "| XGBoost beta 0.15 | 0.8840262582056893 |  0.6304347826086957 |  0.4461538461538462 |  0.5225225225225225 |\n",
            "|  logreg beta 0.15 | 0.8161925601750547 | 0.39325842696629215 |  0.5384615384615384 |  0.4545454545454546 |\n",
            "|   svm beta 0.15   | 0.8708971553610503 |  0.5555555555555556 | 0.46153846153846156 |  0.504201680672269  |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXlr0dWRWlIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "004d9151-6de0-42be-affc-a62b9375f394"
      },
      "source": [
        "Result_purging.to_csv('M_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.452632</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.539185</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.561224</td>\n",
              "      <td>0.761224</td>\n",
              "      <td>0.484581</td>\n",
              "      <td>0.426357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.491525</td>\n",
              "      <td>0.730612</td>\n",
              "      <td>0.568627</td>\n",
              "      <td>0.674419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.497110</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.569536</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.473054</td>\n",
              "      <td>0.718367</td>\n",
              "      <td>0.533784</td>\n",
              "      <td>0.612403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.568807</td>\n",
              "      <td>0.750547</td>\n",
              "      <td>0.521008</td>\n",
              "      <td>0.480620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.587500</td>\n",
              "      <td>0.748359</td>\n",
              "      <td>0.449761</td>\n",
              "      <td>0.364341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.543103</td>\n",
              "      <td>0.739606</td>\n",
              "      <td>0.514286</td>\n",
              "      <td>0.488372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.572519</td>\n",
              "      <td>0.759300</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.581395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.531250</td>\n",
              "      <td>0.735230</td>\n",
              "      <td>0.529183</td>\n",
              "      <td>0.527132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.251656</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.351852</td>\n",
              "      <td>0.584615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.231183</td>\n",
              "      <td>0.663265</td>\n",
              "      <td>0.342629</td>\n",
              "      <td>0.661538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>0.728571</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.584615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.245714</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.358333</td>\n",
              "      <td>0.661538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.255474</td>\n",
              "      <td>0.730612</td>\n",
              "      <td>0.346535</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.199219</td>\n",
              "      <td>0.520788</td>\n",
              "      <td>0.317757</td>\n",
              "      <td>0.784615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.459459</td>\n",
              "      <td>0.844639</td>\n",
              "      <td>0.489209</td>\n",
              "      <td>0.523077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.630435</td>\n",
              "      <td>0.884026</td>\n",
              "      <td>0.522523</td>\n",
              "      <td>0.446154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.393258</td>\n",
              "      <td>0.816193</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.870897</td>\n",
              "      <td>0.504202</td>\n",
              "      <td>0.461538</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1    M  0.452632  0.700000  0.539185  0.666667\n",
              "1            GRU 0.1    M  0.561224  0.761224  0.484581  0.426357\n",
              "2        XGBoost 0.1    M  0.491525  0.730612  0.568627  0.674419\n",
              "3         Logreg 0.1    M  0.497110  0.734694  0.569536  0.666667\n",
              "4            SVM 0.1    M  0.473054  0.718367  0.533784  0.612403\n",
              "5      LSTM beta 0.1    M  0.568807  0.750547  0.521008  0.480620\n",
              "6       GRU beta 0.1    M  0.587500  0.748359  0.449761  0.364341\n",
              "7   XGBoost beta 0.1    M  0.543103  0.739606  0.514286  0.488372\n",
              "8    logreg beta 0.1    M  0.572519  0.759300  0.576923  0.581395\n",
              "9       svm beta 0.1    M  0.531250  0.735230  0.529183  0.527132\n",
              "0          LSTM 0.15    M  0.251656  0.714286  0.351852  0.584615\n",
              "1           GRU 0.15    M  0.231183  0.663265  0.342629  0.661538\n",
              "2       XGBoost 0.15    M  0.263889  0.728571  0.363636  0.584615\n",
              "3        Logreg 0.15    M  0.245714  0.685714  0.358333  0.661538\n",
              "4           SVM 0.15    M  0.255474  0.730612  0.346535  0.538462\n",
              "5     LSTM beta 0.15    M  0.199219  0.520788  0.317757  0.784615\n",
              "6      GRU beta 0.15    M  0.459459  0.844639  0.489209  0.523077\n",
              "7  XGBoost beta 0.15    M  0.630435  0.884026  0.522523  0.446154\n",
              "8   logreg beta 0.15    M  0.393258  0.816193  0.454545  0.538462\n",
              "9      svm beta 0.15    M  0.555556  0.870897  0.504202  0.461538"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aItKT2ZuWlIf"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvssar28WlIf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFdNi1zkXF5F"
      },
      "source": [
        "## MRO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu1-MDLoXF5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d0720e6-264b-4edb-9a44-afc66f20b0af"
      },
      "source": [
        "dfs = pd.read_csv(\"MRO.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "# Denoise prices\n",
        "dfs = denoise_data(dfs, '<CLOSE>')\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2768</td>\n",
              "      <td>US1.MRO</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>13.81</td>\n",
              "      <td>14.285</td>\n",
              "      <td>13.790</td>\n",
              "      <td>14.2400</td>\n",
              "      <td>898224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2767</td>\n",
              "      <td>US1.MRO</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>13.62</td>\n",
              "      <td>13.900</td>\n",
              "      <td>13.460</td>\n",
              "      <td>13.6800</td>\n",
              "      <td>814757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.MRO</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>13.76</td>\n",
              "      <td>13.845</td>\n",
              "      <td>13.460</td>\n",
              "      <td>13.7900</td>\n",
              "      <td>733779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.MRO</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>13.99</td>\n",
              "      <td>14.190</td>\n",
              "      <td>13.725</td>\n",
              "      <td>13.7600</td>\n",
              "      <td>1367820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.MRO</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>13.25</td>\n",
              "      <td>13.745</td>\n",
              "      <td>13.245</td>\n",
              "      <td>13.6700</td>\n",
              "      <td>1179235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.MRO</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>34.49</td>\n",
              "      <td>35.530</td>\n",
              "      <td>34.470</td>\n",
              "      <td>35.4701</td>\n",
              "      <td>4593832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.MRO</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>35.28</td>\n",
              "      <td>35.280</td>\n",
              "      <td>34.530</td>\n",
              "      <td>34.8600</td>\n",
              "      <td>4232962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.MRO</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>34.21</td>\n",
              "      <td>35.130</td>\n",
              "      <td>34.080</td>\n",
              "      <td>35.0880</td>\n",
              "      <td>8617466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.MRO</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>33.76</td>\n",
              "      <td>34.500</td>\n",
              "      <td>33.610</td>\n",
              "      <td>34.3200</td>\n",
              "      <td>7447390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.MRO</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>33.59</td>\n",
              "      <td>33.740</td>\n",
              "      <td>33.085</td>\n",
              "      <td>33.3700</td>\n",
              "      <td>4627387</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2769 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...  <HIGH>   <LOW>  <CLOSE>    <VOL>\n",
              "0      2768  US1.MRO     D  20211001  ...  14.285  13.790  14.2400   898224\n",
              "1      2767  US1.MRO     D  20210930  ...  13.900  13.460  13.6800   814757\n",
              "2      2766  US1.MRO     D  20210929  ...  13.845  13.460  13.7900   733779\n",
              "3      2765  US1.MRO     D  20210928  ...  14.190  13.725  13.7600  1367820\n",
              "4      2764  US1.MRO     D  20210927  ...  13.745  13.245  13.6700  1179235\n",
              "...     ...      ...   ...       ...  ...     ...     ...      ...      ...\n",
              "2764      4  US1.MRO     D  20101008  ...  35.530  34.470  35.4701  4593832\n",
              "2765      3  US1.MRO     D  20101007  ...  35.280  34.530  34.8600  4232962\n",
              "2766      2  US1.MRO     D  20101006  ...  35.130  34.080  35.0880  8617466\n",
              "2767      1  US1.MRO     D  20101005  ...  34.500  33.610  34.3200  7447390\n",
              "2768      0  US1.MRO     D  20101004  ...  33.740  33.085  33.3700  4627387\n",
              "\n",
              "[2769 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsO8nD2AXF5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a8cf689-23b5-43dc-d7a7-0a6fc20dd3a9"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"a8f7d284-fb5c-4c29-8e80-4f7ab669c161\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"a8f7d284-fb5c-4c29-8e80-4f7ab669c161\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'a8f7d284-fb5c-4c29-8e80-4f7ab669c161',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [11.09, 11.42, 11.56, 11.66, 11.3, 11.8, 12.28, 12.2, 11.95, 12.33, 12.22, 13.11, 13.18, 13.04, 13.05, 13.06, 14.17, 12.7, 12.58, 13.01, 12.83, 12.89, 12.43, 12.53, 12.1, 11.77, 11.83, 12.19, 11.94, 11.52, 11.64, 11.75, 12.26, 12.42, 12.63, 12.88, 12.46, 12.02, 12.3, 12.95, 12.73, 12.86, 12.86, 12.05, 12.17, 12.5, 13.28, 13.53, 14.06, 14.14, 13.5, 13.75, 13.76, 13.97, 13.75, 13.675, 13.65, 13.39, 13.22, 13.32, 13.47, 13.92, 13.66, 13.9, 13.54, 13.52, 13.755, 13.65, 13.56, 14.26, 14.215, 14.2, 14.39, 13.9, 13.93, 14.04, 14.11, 13.625, 13.67, 13.34, 12.97, 13.2753, 13.0101, 13.41, 13.45, 13.32, 13.26, 13.18, 13.45, 13.33, 13.14, 13.62, 14.08, 14.32, 14.43, 14.57, 15.43, 15.83, 15.54, 15.47, 15.72, 15.44, 15.47, 15.03, 15.265, 15.27, 15.57, 15.5, 15.765, 15.68, 15.51, 16.53, 17.04, 17.14, 17.37, 17.93, 18.15, 18.68, 18.78, 17.62, 17.74, 17.48, 17.38, 17.76, 17.19, 17.46, 17.26, 17.59, 17.7, 17.13, 16.93, 17.13, 17.385, 16.71, 16.82, 17.05, 17.1167, 16.71, 16.68, 17.68, 17.6, 17.12, 17.39, 17.27, 17.26, 17.33, 16.91, 16.56, 16.27, 16.87, 16.86, 17.26, 17.28, 17.09, 16.595, 16.78, 16.52, 16.67, 16.89, 16.68, 17.09, 17.04, 17.08, 16.92, 15.55, 15.45, 15.27, 15.13, 15.27, 15.905, 15.92, 16.15, 16.0, 15.79, 16.01, 15.92, 15.82, 15.9, 15.791, 15.56, 15.67, 16.09, 15.8, 15.55, 15.83, 15.82, 16.1, 16.355, 16.07, 15.48, 15.625, 15.4, 14.62, 14.71, 14.34, 14.08, 14.3, 14.1665, 12.63, 13.46, 13.61, 13.88, 14.235, 14.73, 15.04, 15.61, 15.52, 15.33, 15.3, 16.05, 16.23, 16.83, 17.55, 16.69, 16.55, 16.49, 16.1, 16.24, 15.51, 16.41, 15.93, 17.14, 17.18, 17.15, 16.68, 16.48, 17.27, 17.84, 18.05, 18.49, 18.32, 18.69, 18.21, 18.6, 18.99, 18.68, 18.16, 18.69, 18.79, 18.36, 19.49, 20.46, 20.665, 20.55, 20.88, 21.17, 20.83, 20.51, 19.96, 21.15, 22.94, 22.88, 23.41, 23.71, 23.995, 23.515, 23.86, 23.28, 22.74, 22.44, 22.5, 22.34, 21.85, 21.55, 21.76, 21.5, 20.88, 20.88, 20.67, 20.55, 20.54, 20.35, 20.53, 20.365, 21.21, 21.07, 21.51, 21.64, 21.77, 20.7, 20.86, 20.61, 20.79, 20.87, 20.21, 19.98, 19.81, 19.32, 19.0, 20.45, 20.09, 20.39, 20.085, 20.15, 20.72, 20.61, 20.54, 20.83, 20.32, 21.12, 21.12, 20.83, 21.24, 21.18, 20.84, 20.46, 20.475, 20.545, 20.37, 20.065, 20.04, 21.48, 21.43, 21.29, 21.98, 21.92, 21.09, 20.705, 21.44, 20.65, 20.865, 20.705, 21.22, 20.74, 20.3, 21.48, 19.93, 21.07, 20.52, 20.33, 19.99, 21.12, 21.48, 21.37, 21.48, 20.99, 21.1, 20.7, 20.59, 21.09, 21.52, 21.44, 21.54, 20.683, 20.72, 21.47, 21.63, 21.545, 21.9, 21.41, 21.65, 21.18, 21.28, 21.42, 21.01, 21.13, 21.265, 20.44, 19.77, 19.72, 19.45, 18.31, 18.21, 18.27, 18.12, 18.46, 17.97, 17.71, 17.975, 18.02, 17.98, 18.21, 18.14, 18.2, 18.16, 17.62, 17.52, 17.06, 16.355, 16.25, 16.8, 15.9, 16.02, 15.56, 16.12, 15.32, 15.83, 16.26, 15.76, 15.85, 16.32, 15.28, 14.855, 15.12, 14.915, 14.88, 14.82, 15.08, 15.215, 14.83, 14.8, 15.025, 15.15, 14.86, 14.51, 14.52, 14.995, 15.335, 15.54, 15.18, 15.3, 15.855, 15.635, 16.34, 16.97, 16.07, 16.28, 15.5, 15.69, 16.33, 17.23, 16.78, 17.52, 18.425, 18.18, 18.065, 18.61, 18.84, 18.685, 19.29, 19.215, 19.27, 18.58, 18.69, 18.835, 18.24, 18.82, 18.61, 17.91, 18.05, 17.96, 17.89, 18.12, 17.67, 17.38, 16.93, 17.07, 17.01, 17.26, 16.91, 17.04, 16.33, 16.025, 15.44, 15.005, 15.13, 15.2, 15.18, 15.15, 15.14, 14.74, 14.45, 14.88, 14.91, 15.08, 14.84, 14.74, 14.56, 14.48, 15.15, 14.88, 14.52, 14.47, 15.0599, 14.52, 14.785, 14.845, 15.54, 15.62, 15.78, 15.75, 16.16, 16.34, 15.58, 15.43, 14.89, 14.21, 14.17, 13.81, 13.55, 13.485, 13.72, 13.63, 13.92, 13.74, 13.76, 13.96, 13.85, 13.76, 13.61, 13.675, 13.61, 13.57, 13.55, 13.89, 13.91, 13.75, 13.595, 13.56, 13.525, 13.73, 13.4, 13.19, 12.79, 12.74, 12.42, 12.0, 12.02, 12.0, 11.93, 11.91, 11.595, 11.49, 11.3, 11.78, 11.73, 11.29, 11.2673, 11.12, 10.9, 10.88, 10.92, 11.05, 10.995, 11.01, 10.89, 10.77, 10.8653, 10.82, 11.19, 11.53, 11.6267, 12.11, 12.06, 12.195, 12.36, 12.345, 12.9, 12.52, 12.04, 11.905, 12.22, 12.34, 12.23, 12.165, 12.34, 11.89, 11.84, 12.08, 12.25, 11.68, 11.59, 11.64, 11.46, 11.44, 11.6, 11.53, 11.46, 11.59, 11.74, 12.2, 11.85, 11.7, 11.445, 11.34, 11.52, 11.54, 11.6, 11.62, 12.07, 12.5, 12.52, 12.38, 12.65, 13.01, 12.62, 12.53, 12.07, 12.205, 12.89, 12.79, 12.71, 13.09, 13.02, 13.12, 13.52, 13.5, 14.53, 14.56, 14.44, 14.53, 14.125, 14.34, 14.43, 14.54, 14.32, 14.49, 14.7, 14.4, 14.58, 14.29, 14.18, 14.85, 14.65, 14.83, 14.88, 14.89, 15.32, 15.6, 15.14, 15.15, 15.09, 15.06, 15.74, 16.02, 15.91, 16.42, 16.61, 16.55, 16.27, 16.31, 15.99, 16.13, 15.81, 15.8, 15.855, 15.84, 15.23, 14.74, 14.61, 14.69, 15.01, 15.05, 15.485, 15.455, 15.55, 15.56, 15.32, 15.845, 16.16, 16.065, 14.865, 16.27, 16.46, 16.17, 16.13, 16.46, 16.01, 15.85, 15.73, 15.98, 15.76, 16.38, 16.24, 16.165, 16.29, 16.405, 16.07, 16.18, 16.05, 15.88, 16.02, 16.52, 17.22, 16.67, 16.58, 16.75, 16.52, 17.41, 17.75, 17.75, 17.7, 17.41, 17.6, 17.27, 17.32, 17.45, 17.45, 17.445, 17.69, 17.48, 17.41, 17.88, 18.18, 18.06, 17.755, 17.32, 17.58, 17.85, 18.14, 18.06, 18.035, 18.085, 18.3, 18.51, 18.58, 18.24, 18.21, 18.8, 18.5, 18.25, 18.37, 18.41, 18.35, 18.39, 18.21, 17.91, 18.09, 14.955, 15.55, 16.2, 16.78, 16.54, 16.49, 15.63, 15.46, 15.56, 15.7, 14.93, 14.89, 15.13, 14.88, 14.25, 14.15, 13.69, 14.16, 12.79, 13.27, 13.19, 13.72, 13.955, 14.24, 14.18, 14.58, 14.63, 14.7, 14.65, 14.19, 14.19, 14.58, 14.77, 14.93, 15.07, 15.72, 15.56, 16.07, 16.125, 15.63, 15.97, 15.81, 15.89, 15.17, 14.05, 14.415, 14.51, 14.87, 14.66, 13.87, 14.08, 14.24, 14.26, 14.015, 14.34, 15.47, 15.67, 16.73, 15.68, 15.63, 15.49, 15.1, 15.02, 15.55, 16.0, 15.96, 16.02, 16.07, 16.43, 15.65, 16.8, 16.6761, 15.69, 15.17, 14.895, 14.7, 14.45, 13.99, 14.42, 14.24, 13.87, 13.42, 13.4154, 12.9001, 12.911, 13.64, 13.35, 13.14, 13.79, 13.575, 14.1, 14.41, 14.85, 14.76, 15.18, 15.21, 15.28, 15.28, 15.59, 14.69, 14.92, 14.74, 14.84, 14.72, 15.68, 15.0, 15.13, 14.7701, 13.6556, 14.6, 15.26, 14.74, 15.075, 14.48, 13.16, 12.66, 13.07, 13.23, 13.28, 13.42, 14.26, 14.54, 14.63, 13.96, 13.33, 13.39, 13.22, 13.07, 12.89, 13.18, 13.53, 13.01, 13.2, 13.1, 12.74, 12.65, 12.85, 12.79, 12.29, 12.33, 12.47, 11.93, 11.44, 12.03, 12.06, 12.16, 12.8, 13.54, 14.09, 14.36, 14.83, 14.44, 13.83, 14.33, 13.86, 14.06, 13.94, 13.35, 13.02, 13.11, 12.93, 13.12, 11.66, 11.77, 11.21, 11.01, 10.63, 10.52, 10.5537, 11.125, 10.54, 10.4, 10.16, 10.29, 10.19, 11.3175, 11.44, 11.62, 11.44, 11.51, 11.3, 11.09, 11.15, 10.6, 10.46, 10.12, 11.03, 11.01, 9.98, 9.1189, 7.97, 8.21, 8.02, 7.385, 7.22, 7.02, 7.48, 6.72, 6.93, 7.4, 7.39, 7.48, 7.0699, 7.13, 7.285, 7.81, 8.4599, 9.17, 9.41, 8.74, 9.73, 9.73, 9.17, 8.76, 8.45, 8.1325, 9.02, 8.79, 7.87, 7.68, 8.14, 9.062, 8.54, 9.21, 9.61, 10.34, 10.67, 11.28, 12.76, 12.83, 12.59, 12.39, 13.03, 12.98, 13.93, 13.96, 12.69, 12.53, 12.5, 12.79, 13.77, 14.451, 13.97, 14.3408, 14.75, 14.5, 14.91, 14.7899, 16.11, 16.56, 16.68, 17.62, 17.52, 17.48, 18.1, 18.5, 17.54, 17.43, 17.65, 17.9, 17.72, 17.87, 17.14, 17.21, 17.305, 18.78, 18.37, 18.28, 19.01, 19.39, 19.67, 19.02, 18.37, 17.56, 17.67, 17.12, 17.58, 18.31, 18.49, 18.02, 18.57, 18.29, 19.28, 19.44, 18.9, 18.58, 18.97, 19.59, 20.17, 19.16, 19.17, 18.04, 16.65, 15.31, 15.41, 14.98, 14.76, 15.74, 15.45, 15.29, 15.66, 16.11, 15.81, 16.43, 16.21, 15.46, 15.42, 15.08, 14.98, 14.93, 16.32, 16.37, 16.86, 16.86, 16.85, 17.28, 16.65, 15.98, 14.65, 14.04, 14.39, 15.745, 16.14, 16.25, 17.48, 17.36, 17.4, 18.02, 19.07, 18.89, 18.99, 18.39, 19.46, 19.76, 20.51, 20.57, 21.03, 21.87, 21.76, 21.19, 20.685, 21.41, 22.11, 22.345, 23.16, 23.24, 23.58, 24.28, 24.41, 25.07, 24.75, 24.46, 24.77, 24.74, 25.7, 25.11, 25.72, 25.79, 26.54, 26.686, 27.74, 26.81, 26.59, 26.61, 26.11, 25.91, 26.52, 26.54, 26.74, 26.45, 26.49, 26.92, 27.04, 26.73, 26.58, 26.96, 26.66, 27.09, 27.08, 27.1, 27.19, 27.23, 27.58, 27.67, 28.1, 28.19, 27.45, 27.33, 27.7, 27.68, 27.61, 28.15, 28.4, 28.41, 29.31, 28.92, 29.81, 29.98, 31.09, 30.77, 31.12, 31.19, 30.72, 30.41, 30.33, 30.55, 30.34, 30.135, 30.98, 30.52, 30.71, 30.69, 29.41, 28.9, 29.19, 28.77, 27.95, 28.16, 27.88, 27.08, 26.65, 26.09, 26.44, 25.91, 26.38, 26.42, 25.89, 25.91, 26.17, 25.68, 26.325, 25.57, 26.015, 25.77, 25.61, 25.8, 25.83, 26.29, 26.77, 27.34, 27.61, 27.74, 27.66, 27.86, 27.99, 29.075, 28.85, 28.54, 28.48, 29.03, 29.03, 29.63, 29.49, 28.78, 28.07, 28.37, 28.3, 27.97, 28.37, 27.99, 28.78, 27.905, 26.62, 25.56, 25.54, 26.98, 27.27, 26.53, 26.82, 26.58, 25.93, 26.6, 25.46, 25.97, 25.9, 26.23, 27.66, 27.73, 27.18, 26.84, 27.29, 28.6, 28.29, 28.42, 28.58, 28.25, 28.28, 28.7, 28.28, 28.19, 27.58, 26.68, 25.32, 24.79, 25.42, 26.24, 26.43, 27.78, 27.24, 29.23, 29.84, 30.53, 29.74, 28.98, 28.93, 32.5, 33.12, 33.58, 33.83, 33.28, 32.78, 32.07, 31.8199, 32.37, 31.95, 32.6499, 33.1201, 33.25, 34.4, 34.34, 33.57, 33.37, 34.56, 35.41, 34.12, 34.37, 34.26, 33.01, 34.5, 34.72, 34.05, 34.87, 34.2501, 33.92, 33.49, 32.75, 32.555, 32.93, 34.23, 34.73, 36.16, 35.84, 36.64, 36.65, 36.9, 37.12, 37.6, 38.605, 38.54, 38.14, 38.69, 38.4, 38.73, 39.23, 39.33, 40.34, 40.13, 39.83, 39.59, 40.02, 40.02, 40.07, 40.22, 40.9, 40.54, 41.28, 41.015, 41.7, 40.93, 40.9, 40.67, 40.38, 39.99, 40.28, 39.92, 39.74, 38.925, 39.03, 39.0, 39.15, 39.03, 39.12, 39.08, 38.08, 38.94, 38.45, 39.25, 38.61, 38.76, 39.65, 39.94, 40.24, 40.52, 40.72, 40.49, 40.2, 39.77, 39.8, 39.53, 40.25, 39.32, 39.69, 39.15, 39.38, 39.59, 39.45, 39.58, 40.32, 39.64, 39.77, 39.91, 39.45, 39.62, 39.71, 39.04, 40.16, 39.74, 39.295, 38.93, 38.89, 39.0, 39.07, 39.16, 38.49, 38.16, 37.8301, 37.3, 37.05, 36.66, 36.82, 36.43, 36.65, 36.65, 36.54, 36.29, 36.27, 36.17, 35.96, 35.39, 35.64, 35.84, 36.23, 36.57, 36.49, 36.29, 35.91, 35.69, 35.51, 36.21, 36.45, 36.53, 36.33, 36.16, 36.9, 36.22, 36.07, 36.37, 36.46, 36.38, 36.56, 36.74, 36.45, 35.895, 35.53, 35.07, 35.4052, 35.48, 35.265, 34.89, 35.6, 35.71, 35.39, 35.41, 35.53, 35.21, 34.49, 34.505, 34.56, 34.24, 34.38, 33.81, 33.31, 33.66, 33.17, 33.07, 32.86, 33.46, 33.94, 34.3, 34.25, 33.86, 33.62, 33.86, 33.53, 33.52, 33.33, 33.78, 34.03, 33.88, 33.26, 33.475, 33.35, 33.53, 33.22, 33.26, 33.29, 32.68, 32.26, 32.59, 31.8, 32.1, 32.33, 32.06, 32.79, 33.03, 33.04, 33.37, 32.9, 33.0, 33.72, 34.39, 34.27, 33.97, 34.17, 34.02, 34.56, 34.105, 34.63, 34.5, 34.39, 34.87, 34.42, 34.52, 34.91, 35.31, 35.19, 35.62, 35.27, 35.25, 35.2, 35.19, 34.91, 34.73, 34.85, 35.59, 35.81, 36.18, 35.69, 36.09, 36.41, 36.57, 36.3, 36.74, 36.23, 36.46, 36.14, 36.23, 36.79, 36.96, 37.6, 37.94, 37.26, 36.93, 36.51, 36.72, 36.42, 36.14, 35.81, 36.28, 36.39, 35.74, 36.46, 35.68, 35.51, 35.28, 35.28, 35.75, 35.98, 36.074, 35.6, 35.6, 34.86]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a8f7d284-fb5c-4c29-8e80-4f7ab669c161');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"d8204db7-c0eb-4121-b92c-aca24bab6760\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"d8204db7-c0eb-4121-b92c-aca24bab6760\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'd8204db7-c0eb-4121-b92c-aca24bab6760',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d8204db7-c0eb-4121-b92c-aca24bab6760');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h42TN8jAXF5M"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKJ-yb6wXF5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a11289ca-4725-4386-a178-f4196dcf2bc3"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.05, .1, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"MRO\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6850 - accuracy: 0.5430 - val_loss: 0.8262 - val_accuracy: 0.2102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6308 - accuracy: 0.6631 - val_loss: 0.6113 - val_accuracy: 0.7347\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6138 - accuracy: 0.6805 - val_loss: 0.6257 - val_accuracy: 0.7429\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5819 - accuracy: 0.7275 - val_loss: 0.7716 - val_accuracy: 0.5327\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5402 - accuracy: 0.7369 - val_loss: 0.8533 - val_accuracy: 0.2776\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6339 - accuracy: 0.6342 - val_loss: 0.6148 - val_accuracy: 0.7592\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5380 - accuracy: 0.7349 - val_loss: 0.5665 - val_accuracy: 0.7612\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5168 - accuracy: 0.7483 - val_loss: 0.7915 - val_accuracy: 0.4408\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5217 - accuracy: 0.7369 - val_loss: 0.7049 - val_accuracy: 0.5429\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5014 - accuracy: 0.7497 - val_loss: 0.6577 - val_accuracy: 0.6061\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.764996\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.771995\n",
            "[2]\tvalidation_0-auc:0.780625\n",
            "[3]\tvalidation_0-auc:0.782168\n",
            "[4]\tvalidation_0-auc:0.782795\n",
            "[5]\tvalidation_0-auc:0.783222\n",
            "[6]\tvalidation_0-auc:0.782745\n",
            "[7]\tvalidation_0-auc:0.784125\n",
            "[8]\tvalidation_0-auc:0.784852\n",
            "[9]\tvalidation_0-auc:0.785542\n",
            "[10]\tvalidation_0-auc:0.786872\n",
            "[11]\tvalidation_0-auc:0.786859\n",
            "[12]\tvalidation_0-auc:0.787311\n",
            "[13]\tvalidation_0-auc:0.78839\n",
            "[14]\tvalidation_0-auc:0.789255\n",
            "[15]\tvalidation_0-auc:0.789356\n",
            "[16]\tvalidation_0-auc:0.788352\n",
            "[17]\tvalidation_0-auc:0.788778\n",
            "[18]\tvalidation_0-auc:0.789305\n",
            "[19]\tvalidation_0-auc:0.788289\n",
            "[20]\tvalidation_0-auc:0.788778\n",
            "[21]\tvalidation_0-auc:0.788089\n",
            "[22]\tvalidation_0-auc:0.788465\n",
            "[23]\tvalidation_0-auc:0.788528\n",
            "[24]\tvalidation_0-auc:0.788866\n",
            "[25]\tvalidation_0-auc:0.789017\n",
            "[26]\tvalidation_0-auc:0.789631\n",
            "[27]\tvalidation_0-auc:0.789669\n",
            "[28]\tvalidation_0-auc:0.789669\n",
            "[29]\tvalidation_0-auc:0.789519\n",
            "[30]\tvalidation_0-auc:0.78928\n",
            "[31]\tvalidation_0-auc:0.78928\n",
            "[32]\tvalidation_0-auc:0.789631\n",
            "[33]\tvalidation_0-auc:0.789782\n",
            "[34]\tvalidation_0-auc:0.790409\n",
            "[35]\tvalidation_0-auc:0.790409\n",
            "[36]\tvalidation_0-auc:0.790434\n",
            "[37]\tvalidation_0-auc:0.790434\n",
            "[38]\tvalidation_0-auc:0.790384\n",
            "[39]\tvalidation_0-auc:0.790384\n",
            "[40]\tvalidation_0-auc:0.790384\n",
            "[41]\tvalidation_0-auc:0.790183\n",
            "[42]\tvalidation_0-auc:0.789318\n",
            "[43]\tvalidation_0-auc:0.788904\n",
            "[44]\tvalidation_0-auc:0.788904\n",
            "[45]\tvalidation_0-auc:0.788904\n",
            "[46]\tvalidation_0-auc:0.788904\n",
            "[47]\tvalidation_0-auc:0.788778\n",
            "[48]\tvalidation_0-auc:0.788979\n",
            "[49]\tvalidation_0-auc:0.788979\n",
            "[50]\tvalidation_0-auc:0.789318\n",
            "[51]\tvalidation_0-auc:0.789318\n",
            "[52]\tvalidation_0-auc:0.789318\n",
            "[53]\tvalidation_0-auc:0.789293\n",
            "[54]\tvalidation_0-auc:0.789293\n",
            "[55]\tvalidation_0-auc:0.790974\n",
            "[56]\tvalidation_0-auc:0.791049\n",
            "[57]\tvalidation_0-auc:0.791049\n",
            "[58]\tvalidation_0-auc:0.791224\n",
            "[59]\tvalidation_0-auc:0.791224\n",
            "[60]\tvalidation_0-auc:0.790836\n",
            "[61]\tvalidation_0-auc:0.791613\n",
            "[62]\tvalidation_0-auc:0.791714\n",
            "[63]\tvalidation_0-auc:0.79199\n",
            "[64]\tvalidation_0-auc:0.791613\n",
            "[65]\tvalidation_0-auc:0.791588\n",
            "[66]\tvalidation_0-auc:0.791588\n",
            "[67]\tvalidation_0-auc:0.791011\n",
            "[68]\tvalidation_0-auc:0.790359\n",
            "[69]\tvalidation_0-auc:0.790259\n",
            "[70]\tvalidation_0-auc:0.790259\n",
            "[71]\tvalidation_0-auc:0.790058\n",
            "[72]\tvalidation_0-auc:0.790058\n",
            "[73]\tvalidation_0-auc:0.789983\n",
            "[74]\tvalidation_0-auc:0.790585\n",
            "[75]\tvalidation_0-auc:0.790585\n",
            "[76]\tvalidation_0-auc:0.788766\n",
            "[77]\tvalidation_0-auc:0.789393\n",
            "[78]\tvalidation_0-auc:0.789393\n",
            "[79]\tvalidation_0-auc:0.789318\n",
            "[80]\tvalidation_0-auc:0.78775\n",
            "[81]\tvalidation_0-auc:0.788139\n",
            "[82]\tvalidation_0-auc:0.787863\n",
            "[83]\tvalidation_0-auc:0.787712\n",
            "[84]\tvalidation_0-auc:0.787712\n",
            "[85]\tvalidation_0-auc:0.787712\n",
            "[86]\tvalidation_0-auc:0.786897\n",
            "[87]\tvalidation_0-auc:0.786897\n",
            "[88]\tvalidation_0-auc:0.7877\n",
            "[89]\tvalidation_0-auc:0.7877\n",
            "[90]\tvalidation_0-auc:0.787675\n",
            "[91]\tvalidation_0-auc:0.785919\n",
            "[92]\tvalidation_0-auc:0.785893\n",
            "[93]\tvalidation_0-auc:0.785567\n",
            "[94]\tvalidation_0-auc:0.785756\n",
            "[95]\tvalidation_0-auc:0.784075\n",
            "[96]\tvalidation_0-auc:0.783774\n",
            "[97]\tvalidation_0-auc:0.785128\n",
            "[98]\tvalidation_0-auc:0.78568\n",
            "[99]\tvalidation_0-auc:0.785078\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6825 - accuracy: 0.5491 - val_loss: 0.6756 - val_accuracy: 0.6696\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6484 - accuracy: 0.6088 - val_loss: 0.6730 - val_accuracy: 0.7352\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6079 - accuracy: 0.6760 - val_loss: 0.6442 - val_accuracy: 0.6521\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5606 - accuracy: 0.7069 - val_loss: 0.6157 - val_accuracy: 0.6893\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5488 - accuracy: 0.7289 - val_loss: 0.5509 - val_accuracy: 0.7702\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6672 - accuracy: 0.6033 - val_loss: 0.6150 - val_accuracy: 0.8162\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5754 - accuracy: 0.6911 - val_loss: 0.5685 - val_accuracy: 0.7943\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5477 - accuracy: 0.7268 - val_loss: 0.6001 - val_accuracy: 0.7309\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5306 - accuracy: 0.7509 - val_loss: 0.6470 - val_accuracy: 0.6411\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5202 - accuracy: 0.7385 - val_loss: 0.6583 - val_accuracy: 0.5952\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.706695\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.706695\n",
            "[2]\tvalidation_0-auc:0.764001\n",
            "[3]\tvalidation_0-auc:0.760381\n",
            "[4]\tvalidation_0-auc:0.811626\n",
            "[5]\tvalidation_0-auc:0.81552\n",
            "[6]\tvalidation_0-auc:0.821019\n",
            "[7]\tvalidation_0-auc:0.826395\n",
            "[8]\tvalidation_0-auc:0.825435\n",
            "[9]\tvalidation_0-auc:0.830207\n",
            "[10]\tvalidation_0-auc:0.83997\n",
            "[11]\tvalidation_0-auc:0.827533\n",
            "[12]\tvalidation_0-auc:0.828246\n",
            "[13]\tvalidation_0-auc:0.827067\n",
            "[14]\tvalidation_0-auc:0.829247\n",
            "[15]\tvalidation_0-auc:0.829247\n",
            "[16]\tvalidation_0-auc:0.827738\n",
            "[17]\tvalidation_0-auc:0.824173\n",
            "[18]\tvalidation_0-auc:0.82937\n",
            "[19]\tvalidation_0-auc:0.829233\n",
            "[20]\tvalidation_0-auc:0.828849\n",
            "[21]\tvalidation_0-auc:0.826573\n",
            "[22]\tvalidation_0-auc:0.825311\n",
            "[23]\tvalidation_0-auc:0.825311\n",
            "[24]\tvalidation_0-auc:0.824639\n",
            "[25]\tvalidation_0-auc:0.824173\n",
            "[26]\tvalidation_0-auc:0.826477\n",
            "[27]\tvalidation_0-auc:0.827492\n",
            "[28]\tvalidation_0-auc:0.826614\n",
            "[29]\tvalidation_0-auc:0.827876\n",
            "[30]\tvalidation_0-auc:0.831441\n",
            "[31]\tvalidation_0-auc:0.831441\n",
            "[32]\tvalidation_0-auc:0.831414\n",
            "[33]\tvalidation_0-auc:0.831605\n",
            "[34]\tvalidation_0-auc:0.827615\n",
            "[35]\tvalidation_0-auc:0.827615\n",
            "[36]\tvalidation_0-auc:0.827615\n",
            "[37]\tvalidation_0-auc:0.828026\n",
            "[38]\tvalidation_0-auc:0.824845\n",
            "[39]\tvalidation_0-auc:0.822555\n",
            "[40]\tvalidation_0-auc:0.822089\n",
            "[41]\tvalidation_0-auc:0.823652\n",
            "[42]\tvalidation_0-auc:0.823652\n",
            "[43]\tvalidation_0-auc:0.823364\n",
            "[44]\tvalidation_0-auc:0.822925\n",
            "[45]\tvalidation_0-auc:0.821129\n",
            "[46]\tvalidation_0-auc:0.818894\n",
            "[47]\tvalidation_0-auc:0.818894\n",
            "[48]\tvalidation_0-auc:0.819936\n",
            "[49]\tvalidation_0-auc:0.82165\n",
            "[50]\tvalidation_0-auc:0.82165\n",
            "[51]\tvalidation_0-auc:0.82165\n",
            "[52]\tvalidation_0-auc:0.821239\n",
            "[53]\tvalidation_0-auc:0.821239\n",
            "[54]\tvalidation_0-auc:0.822404\n",
            "[55]\tvalidation_0-auc:0.822404\n",
            "[56]\tvalidation_0-auc:0.821581\n",
            "[57]\tvalidation_0-auc:0.816878\n",
            "[58]\tvalidation_0-auc:0.816713\n",
            "[59]\tvalidation_0-auc:0.816713\n",
            "[60]\tvalidation_0-auc:0.816713\n",
            "Stopping. Best iteration:\n",
            "[10]\tvalidation_0-auc:0.83997\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+---------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |       Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+---------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.27755102040816326 | 0.21923937360178972 | 0.9514563106796117 |  0.3563636363636364 |\n",
            "|     GRU 0.1      |  0.6061224489795919 | 0.32558139534883723 | 0.8155339805825242 | 0.46537396121883656 |\n",
            "|   XGBoost 0.1    |  0.7142857142857143 |  0.4088669950738916 | 0.8058252427184466 |  0.542483660130719  |\n",
            "|    Logreg 0.1    |  0.6183673469387755 |  0.3346456692913386 | 0.8252427184466019 |  0.4761904761904762 |\n",
            "|     SVM 0.1      |  0.6857142857142857 |  0.3813953488372093 | 0.7961165048543689 |  0.5157232704402516 |\n",
            "|  LSTM beta 0.1   |  0.7702407002188184 |  0.4909090909090909 | 0.5242718446601942 |  0.5070422535211268 |\n",
            "|   GRU beta 0.1   |  0.5951859956236324 |  0.3435114503816794 | 0.8737864077669902 |  0.4931506849315068 |\n",
            "| XGBoost beta 0.1 |  0.8358862144420132 |  0.7121212121212122 | 0.4563106796116505 |  0.5562130177514794 |\n",
            "| logreg beta 0.1  |  0.6783369803063457 |  0.397196261682243  | 0.8252427184466019 |  0.5362776025236593 |\n",
            "|   svm beta 0.1   |  0.8183807439824945 |  0.5793650793650794 | 0.7087378640776699 |  0.6375545851528385 |\n",
            "+------------------+---------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 19ms/step - loss: 0.6733 - accuracy: 0.5866 - val_loss: 0.6264 - val_accuracy: 0.7000\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6024 - accuracy: 0.6745 - val_loss: 1.0063 - val_accuracy: 0.2980\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5928 - accuracy: 0.6946 - val_loss: 0.7468 - val_accuracy: 0.4469\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5738 - accuracy: 0.7101 - val_loss: 0.7612 - val_accuracy: 0.5020\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5467 - accuracy: 0.7242 - val_loss: 0.6139 - val_accuracy: 0.7510\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6540 - accuracy: 0.6107 - val_loss: 1.0108 - val_accuracy: 0.2102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5590 - accuracy: 0.7121 - val_loss: 0.6104 - val_accuracy: 0.7245\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5529 - accuracy: 0.7114 - val_loss: 0.5926 - val_accuracy: 0.7367\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5398 - accuracy: 0.7362 - val_loss: 0.6474 - val_accuracy: 0.6551\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5385 - accuracy: 0.7396 - val_loss: 0.6823 - val_accuracy: 0.5898\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.755058\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.755234\n",
            "[2]\tvalidation_0-auc:0.758349\n",
            "[3]\tvalidation_0-auc:0.754449\n",
            "[4]\tvalidation_0-auc:0.754124\n",
            "[5]\tvalidation_0-auc:0.75732\n",
            "[6]\tvalidation_0-auc:0.758471\n",
            "[7]\tvalidation_0-auc:0.75927\n",
            "[8]\tvalidation_0-auc:0.759405\n",
            "[9]\tvalidation_0-auc:0.759405\n",
            "[10]\tvalidation_0-auc:0.758457\n",
            "[11]\tvalidation_0-auc:0.758376\n",
            "[12]\tvalidation_0-auc:0.758349\n",
            "[13]\tvalidation_0-auc:0.758552\n",
            "[14]\tvalidation_0-auc:0.758349\n",
            "[15]\tvalidation_0-auc:0.758295\n",
            "[16]\tvalidation_0-auc:0.758417\n",
            "[17]\tvalidation_0-auc:0.756968\n",
            "[18]\tvalidation_0-auc:0.75713\n",
            "[19]\tvalidation_0-auc:0.757103\n",
            "[20]\tvalidation_0-auc:0.757442\n",
            "[21]\tvalidation_0-auc:0.757618\n",
            "[22]\tvalidation_0-auc:0.757699\n",
            "[23]\tvalidation_0-auc:0.758159\n",
            "[24]\tvalidation_0-auc:0.757577\n",
            "[25]\tvalidation_0-auc:0.758281\n",
            "[26]\tvalidation_0-auc:0.758119\n",
            "[27]\tvalidation_0-auc:0.756805\n",
            "[28]\tvalidation_0-auc:0.756805\n",
            "[29]\tvalidation_0-auc:0.757076\n",
            "[30]\tvalidation_0-auc:0.757238\n",
            "[31]\tvalidation_0-auc:0.756588\n",
            "[32]\tvalidation_0-auc:0.757022\n",
            "[33]\tvalidation_0-auc:0.757293\n",
            "[34]\tvalidation_0-auc:0.757293\n",
            "[35]\tvalidation_0-auc:0.757536\n",
            "[36]\tvalidation_0-auc:0.757401\n",
            "[37]\tvalidation_0-auc:0.756426\n",
            "[38]\tvalidation_0-auc:0.75667\n",
            "[39]\tvalidation_0-auc:0.756683\n",
            "[40]\tvalidation_0-auc:0.756602\n",
            "[41]\tvalidation_0-auc:0.756629\n",
            "[42]\tvalidation_0-auc:0.756629\n",
            "[43]\tvalidation_0-auc:0.756737\n",
            "[44]\tvalidation_0-auc:0.756737\n",
            "[45]\tvalidation_0-auc:0.756764\n",
            "[46]\tvalidation_0-auc:0.756764\n",
            "[47]\tvalidation_0-auc:0.757144\n",
            "[48]\tvalidation_0-auc:0.756683\n",
            "[49]\tvalidation_0-auc:0.755505\n",
            "[50]\tvalidation_0-auc:0.755911\n",
            "[51]\tvalidation_0-auc:0.755911\n",
            "[52]\tvalidation_0-auc:0.755803\n",
            "[53]\tvalidation_0-auc:0.755803\n",
            "[54]\tvalidation_0-auc:0.756439\n",
            "[55]\tvalidation_0-auc:0.757293\n",
            "[56]\tvalidation_0-auc:0.757374\n",
            "[57]\tvalidation_0-auc:0.758335\n",
            "[58]\tvalidation_0-auc:0.758335\n",
            "Stopping. Best iteration:\n",
            "[8]\tvalidation_0-auc:0.759405\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6645 - accuracy: 0.5704 - val_loss: 0.7065 - val_accuracy: 0.3085\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6292 - accuracy: 0.6486 - val_loss: 0.7268 - val_accuracy: 0.4595\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5981 - accuracy: 0.6843 - val_loss: 0.6147 - val_accuracy: 0.8118\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5747 - accuracy: 0.6994 - val_loss: 0.6131 - val_accuracy: 0.7352\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5589 - accuracy: 0.7152 - val_loss: 0.6258 - val_accuracy: 0.7090\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6313 - accuracy: 0.6472 - val_loss: 0.6700 - val_accuracy: 0.6302\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5636 - accuracy: 0.7213 - val_loss: 0.6351 - val_accuracy: 0.6937\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5420 - accuracy: 0.7234 - val_loss: 0.6036 - val_accuracy: 0.7374\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5405 - accuracy: 0.7282 - val_loss: 0.6268 - val_accuracy: 0.6718\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5308 - accuracy: 0.7385 - val_loss: 0.5579 - val_accuracy: 0.7768\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.721951\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.717964\n",
            "[2]\tvalidation_0-auc:0.76186\n",
            "[3]\tvalidation_0-auc:0.770176\n",
            "[4]\tvalidation_0-auc:0.776719\n",
            "[5]\tvalidation_0-auc:0.831118\n",
            "[6]\tvalidation_0-auc:0.830808\n",
            "[7]\tvalidation_0-auc:0.836095\n",
            "[8]\tvalidation_0-auc:0.840896\n",
            "[9]\tvalidation_0-auc:0.839773\n",
            "[10]\tvalidation_0-auc:0.839921\n",
            "[11]\tvalidation_0-auc:0.840334\n",
            "[12]\tvalidation_0-auc:0.840334\n",
            "[13]\tvalidation_0-auc:0.838252\n",
            "[14]\tvalidation_0-auc:0.8371\n",
            "[15]\tvalidation_0-auc:0.835298\n",
            "[16]\tvalidation_0-auc:0.8345\n",
            "[17]\tvalidation_0-auc:0.836657\n",
            "[18]\tvalidation_0-auc:0.839552\n",
            "[19]\tvalidation_0-auc:0.838148\n",
            "[20]\tvalidation_0-auc:0.838267\n",
            "[21]\tvalidation_0-auc:0.840423\n",
            "[22]\tvalidation_0-auc:0.840423\n",
            "[23]\tvalidation_0-auc:0.839478\n",
            "[24]\tvalidation_0-auc:0.839478\n",
            "[25]\tvalidation_0-auc:0.840142\n",
            "[26]\tvalidation_0-auc:0.840142\n",
            "[27]\tvalidation_0-auc:0.839552\n",
            "[28]\tvalidation_0-auc:0.839552\n",
            "[29]\tvalidation_0-auc:0.839581\n",
            "[30]\tvalidation_0-auc:0.839581\n",
            "[31]\tvalidation_0-auc:0.839581\n",
            "[32]\tvalidation_0-auc:0.839581\n",
            "[33]\tvalidation_0-auc:0.839699\n",
            "[34]\tvalidation_0-auc:0.839699\n",
            "[35]\tvalidation_0-auc:0.839699\n",
            "[36]\tvalidation_0-auc:0.842181\n",
            "[37]\tvalidation_0-auc:0.841339\n",
            "[38]\tvalidation_0-auc:0.841339\n",
            "[39]\tvalidation_0-auc:0.8419\n",
            "[40]\tvalidation_0-auc:0.8419\n",
            "[41]\tvalidation_0-auc:0.8419\n",
            "[42]\tvalidation_0-auc:0.839005\n",
            "[43]\tvalidation_0-auc:0.839005\n",
            "[44]\tvalidation_0-auc:0.839094\n",
            "[45]\tvalidation_0-auc:0.839655\n",
            "[46]\tvalidation_0-auc:0.839655\n",
            "[47]\tvalidation_0-auc:0.839301\n",
            "[48]\tvalidation_0-auc:0.84128\n",
            "[49]\tvalidation_0-auc:0.843067\n",
            "[50]\tvalidation_0-auc:0.843067\n",
            "[51]\tvalidation_0-auc:0.840999\n",
            "[52]\tvalidation_0-auc:0.840999\n",
            "[53]\tvalidation_0-auc:0.84097\n",
            "[54]\tvalidation_0-auc:0.842003\n",
            "[55]\tvalidation_0-auc:0.842033\n",
            "[56]\tvalidation_0-auc:0.842033\n",
            "[57]\tvalidation_0-auc:0.842033\n",
            "[58]\tvalidation_0-auc:0.842033\n",
            "[59]\tvalidation_0-auc:0.842033\n",
            "[60]\tvalidation_0-auc:0.841856\n",
            "[61]\tvalidation_0-auc:0.841856\n",
            "[62]\tvalidation_0-auc:0.841915\n",
            "[63]\tvalidation_0-auc:0.841915\n",
            "[64]\tvalidation_0-auc:0.842151\n",
            "[65]\tvalidation_0-auc:0.842151\n",
            "[66]\tvalidation_0-auc:0.842151\n",
            "[67]\tvalidation_0-auc:0.841767\n",
            "[68]\tvalidation_0-auc:0.841501\n",
            "[69]\tvalidation_0-auc:0.841398\n",
            "[70]\tvalidation_0-auc:0.841634\n",
            "[71]\tvalidation_0-auc:0.842284\n",
            "[72]\tvalidation_0-auc:0.842284\n",
            "[73]\tvalidation_0-auc:0.842284\n",
            "[74]\tvalidation_0-auc:0.842934\n",
            "[75]\tvalidation_0-auc:0.842771\n",
            "[76]\tvalidation_0-auc:0.842417\n",
            "[77]\tvalidation_0-auc:0.842476\n",
            "[78]\tvalidation_0-auc:0.842476\n",
            "[79]\tvalidation_0-auc:0.845327\n",
            "[80]\tvalidation_0-auc:0.844544\n",
            "[81]\tvalidation_0-auc:0.844367\n",
            "[82]\tvalidation_0-auc:0.843008\n",
            "[83]\tvalidation_0-auc:0.843008\n",
            "[84]\tvalidation_0-auc:0.842978\n",
            "[85]\tvalidation_0-auc:0.842831\n",
            "[86]\tvalidation_0-auc:0.842949\n",
            "[87]\tvalidation_0-auc:0.842949\n",
            "[88]\tvalidation_0-auc:0.843362\n",
            "[89]\tvalidation_0-auc:0.84351\n",
            "[90]\tvalidation_0-auc:0.84351\n",
            "[91]\tvalidation_0-auc:0.84351\n",
            "[92]\tvalidation_0-auc:0.842712\n",
            "[93]\tvalidation_0-auc:0.842683\n",
            "[94]\tvalidation_0-auc:0.842683\n",
            "[95]\tvalidation_0-auc:0.842506\n",
            "[96]\tvalidation_0-auc:0.843185\n",
            "[97]\tvalidation_0-auc:0.842771\n",
            "[98]\tvalidation_0-auc:0.844308\n",
            "[99]\tvalidation_0-auc:0.844514\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.7510204081632653 |  0.4088050314465409 | 0.6989247311827957 |  0.5158730158730159 |\n",
            "|      GRU 0.15     | 0.5897959183673469 |  0.2923076923076923 | 0.8172043010752689 |  0.4305949008498584 |\n",
            "|    XGBoost 0.15   | 0.6714285714285714 |  0.3411214953271028 | 0.7849462365591398 | 0.47557003257328984 |\n",
            "|    Logreg 0.15    | 0.5551020408163265 | 0.27598566308243727 | 0.8279569892473119 | 0.41397849462365593 |\n",
            "|      SVM 0.15     | 0.6714285714285714 |  0.3380952380952381 | 0.7634408602150538 |  0.4686468646864687 |\n",
            "|   LSTM beta 0.15  | 0.7089715536105032 |  0.3850574712643678 | 0.7204301075268817 |   0.50187265917603  |\n",
            "|   GRU beta 0.15   | 0.7768052516411379 | 0.46938775510204084 | 0.7419354838709677 |  0.5750000000000001 |\n",
            "| XGBoost beta 0.15 | 0.8096280087527352 |  0.5245901639344263 | 0.6881720430107527 |  0.5953488372093024 |\n",
            "|  logreg beta 0.15 | 0.6849015317286652 | 0.37681159420289856 | 0.8387096774193549 |         0.52        |\n",
            "|   svm beta 0.15   | 0.7986870897155361 |  0.5035971223021583 | 0.7526881720430108 |  0.603448275862069  |\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA2z1pK9XF5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7bfb328-87e6-44c6-ddc5-f5fe79038dd2"
      },
      "source": [
        "Result_cross.to_csv('MRO_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.219239</td>\n",
              "      <td>0.277551</td>\n",
              "      <td>0.356364</td>\n",
              "      <td>0.951456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.325581</td>\n",
              "      <td>0.606122</td>\n",
              "      <td>0.465374</td>\n",
              "      <td>0.815534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.408867</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.542484</td>\n",
              "      <td>0.805825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.334646</td>\n",
              "      <td>0.618367</td>\n",
              "      <td>0.476190</td>\n",
              "      <td>0.825243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.381395</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.515723</td>\n",
              "      <td>0.796117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.490909</td>\n",
              "      <td>0.770241</td>\n",
              "      <td>0.507042</td>\n",
              "      <td>0.524272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.343511</td>\n",
              "      <td>0.595186</td>\n",
              "      <td>0.493151</td>\n",
              "      <td>0.873786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.712121</td>\n",
              "      <td>0.835886</td>\n",
              "      <td>0.556213</td>\n",
              "      <td>0.456311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.397196</td>\n",
              "      <td>0.678337</td>\n",
              "      <td>0.536278</td>\n",
              "      <td>0.825243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.579365</td>\n",
              "      <td>0.818381</td>\n",
              "      <td>0.637555</td>\n",
              "      <td>0.708738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.408805</td>\n",
              "      <td>0.751020</td>\n",
              "      <td>0.515873</td>\n",
              "      <td>0.698925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.292308</td>\n",
              "      <td>0.589796</td>\n",
              "      <td>0.430595</td>\n",
              "      <td>0.817204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.341121</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.475570</td>\n",
              "      <td>0.784946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.275986</td>\n",
              "      <td>0.555102</td>\n",
              "      <td>0.413978</td>\n",
              "      <td>0.827957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.338095</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.468647</td>\n",
              "      <td>0.763441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.385057</td>\n",
              "      <td>0.708972</td>\n",
              "      <td>0.501873</td>\n",
              "      <td>0.720430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.469388</td>\n",
              "      <td>0.776805</td>\n",
              "      <td>0.575000</td>\n",
              "      <td>0.741935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.524590</td>\n",
              "      <td>0.809628</td>\n",
              "      <td>0.595349</td>\n",
              "      <td>0.688172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.376812</td>\n",
              "      <td>0.684902</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.838710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.503597</td>\n",
              "      <td>0.798687</td>\n",
              "      <td>0.603448</td>\n",
              "      <td>0.752688</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  MRO  0.219239  0.277551  0.356364  0.951456\n",
              "1            GRU 0.1  MRO  0.325581  0.606122  0.465374  0.815534\n",
              "2        XGBoost 0.1  MRO  0.408867  0.714286  0.542484  0.805825\n",
              "3         Logreg 0.1  MRO  0.334646  0.618367  0.476190  0.825243\n",
              "4            SVM 0.1  MRO  0.381395  0.685714  0.515723  0.796117\n",
              "5      LSTM beta 0.1  MRO  0.490909  0.770241  0.507042  0.524272\n",
              "6       GRU beta 0.1  MRO  0.343511  0.595186  0.493151  0.873786\n",
              "7   XGBoost beta 0.1  MRO  0.712121  0.835886  0.556213  0.456311\n",
              "8    logreg beta 0.1  MRO  0.397196  0.678337  0.536278  0.825243\n",
              "9       svm beta 0.1  MRO  0.579365  0.818381  0.637555  0.708738\n",
              "0          LSTM 0.15  MRO  0.408805  0.751020  0.515873  0.698925\n",
              "1           GRU 0.15  MRO  0.292308  0.589796  0.430595  0.817204\n",
              "2       XGBoost 0.15  MRO  0.341121  0.671429  0.475570  0.784946\n",
              "3        Logreg 0.15  MRO  0.275986  0.555102  0.413978  0.827957\n",
              "4           SVM 0.15  MRO  0.338095  0.671429  0.468647  0.763441\n",
              "5     LSTM beta 0.15  MRO  0.385057  0.708972  0.501873  0.720430\n",
              "6      GRU beta 0.15  MRO  0.469388  0.776805  0.575000  0.741935\n",
              "7  XGBoost beta 0.15  MRO  0.524590  0.809628  0.595349  0.688172\n",
              "8   logreg beta 0.15  MRO  0.376812  0.684902  0.520000  0.838710\n",
              "9      svm beta 0.15  MRO  0.503597  0.798687  0.603448  0.752688"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fwkamDvXF5M"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g3klK47XF5M"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6He4d8kXF5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5badd0a-bb6c-4b96-f730-c65c9a2e0e4c"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"MRO\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6948 - accuracy: 0.5336 - val_loss: 0.7395 - val_accuracy: 0.2102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6674 - accuracy: 0.5685 - val_loss: 0.6596 - val_accuracy: 0.6837\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5986 - accuracy: 0.6980 - val_loss: 0.6967 - val_accuracy: 0.5510\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5491 - accuracy: 0.7342 - val_loss: 0.6012 - val_accuracy: 0.7327\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5178 - accuracy: 0.7430 - val_loss: 0.6752 - val_accuracy: 0.6245\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6302 - accuracy: 0.6403 - val_loss: 0.6930 - val_accuracy: 0.5571\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5483 - accuracy: 0.7208 - val_loss: 0.7283 - val_accuracy: 0.4857\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5151 - accuracy: 0.7450 - val_loss: 0.6022 - val_accuracy: 0.7061\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5064 - accuracy: 0.7430 - val_loss: 0.6170 - val_accuracy: 0.6714\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4886 - accuracy: 0.7631 - val_loss: 0.7076 - val_accuracy: 0.5694\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.764996\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.771995\n",
            "[2]\tvalidation_0-auc:0.780625\n",
            "[3]\tvalidation_0-auc:0.782168\n",
            "[4]\tvalidation_0-auc:0.782795\n",
            "[5]\tvalidation_0-auc:0.783222\n",
            "[6]\tvalidation_0-auc:0.782745\n",
            "[7]\tvalidation_0-auc:0.784125\n",
            "[8]\tvalidation_0-auc:0.784852\n",
            "[9]\tvalidation_0-auc:0.785542\n",
            "[10]\tvalidation_0-auc:0.786872\n",
            "[11]\tvalidation_0-auc:0.786859\n",
            "[12]\tvalidation_0-auc:0.787311\n",
            "[13]\tvalidation_0-auc:0.78839\n",
            "[14]\tvalidation_0-auc:0.789255\n",
            "[15]\tvalidation_0-auc:0.789356\n",
            "[16]\tvalidation_0-auc:0.788352\n",
            "[17]\tvalidation_0-auc:0.788778\n",
            "[18]\tvalidation_0-auc:0.789305\n",
            "[19]\tvalidation_0-auc:0.788289\n",
            "[20]\tvalidation_0-auc:0.788778\n",
            "[21]\tvalidation_0-auc:0.788089\n",
            "[22]\tvalidation_0-auc:0.788465\n",
            "[23]\tvalidation_0-auc:0.788528\n",
            "[24]\tvalidation_0-auc:0.788866\n",
            "[25]\tvalidation_0-auc:0.789017\n",
            "[26]\tvalidation_0-auc:0.789631\n",
            "[27]\tvalidation_0-auc:0.789669\n",
            "[28]\tvalidation_0-auc:0.789669\n",
            "[29]\tvalidation_0-auc:0.789519\n",
            "[30]\tvalidation_0-auc:0.78928\n",
            "[31]\tvalidation_0-auc:0.78928\n",
            "[32]\tvalidation_0-auc:0.789631\n",
            "[33]\tvalidation_0-auc:0.789782\n",
            "[34]\tvalidation_0-auc:0.790409\n",
            "[35]\tvalidation_0-auc:0.790409\n",
            "[36]\tvalidation_0-auc:0.790434\n",
            "[37]\tvalidation_0-auc:0.790434\n",
            "[38]\tvalidation_0-auc:0.790384\n",
            "[39]\tvalidation_0-auc:0.790384\n",
            "[40]\tvalidation_0-auc:0.790384\n",
            "[41]\tvalidation_0-auc:0.790183\n",
            "[42]\tvalidation_0-auc:0.789318\n",
            "[43]\tvalidation_0-auc:0.788904\n",
            "[44]\tvalidation_0-auc:0.788904\n",
            "[45]\tvalidation_0-auc:0.788904\n",
            "[46]\tvalidation_0-auc:0.788904\n",
            "[47]\tvalidation_0-auc:0.788778\n",
            "[48]\tvalidation_0-auc:0.788979\n",
            "[49]\tvalidation_0-auc:0.788979\n",
            "[50]\tvalidation_0-auc:0.789318\n",
            "[51]\tvalidation_0-auc:0.789318\n",
            "[52]\tvalidation_0-auc:0.789318\n",
            "[53]\tvalidation_0-auc:0.789293\n",
            "[54]\tvalidation_0-auc:0.789293\n",
            "[55]\tvalidation_0-auc:0.790974\n",
            "[56]\tvalidation_0-auc:0.791049\n",
            "[57]\tvalidation_0-auc:0.791049\n",
            "[58]\tvalidation_0-auc:0.791224\n",
            "[59]\tvalidation_0-auc:0.791224\n",
            "[60]\tvalidation_0-auc:0.790836\n",
            "[61]\tvalidation_0-auc:0.791613\n",
            "[62]\tvalidation_0-auc:0.791714\n",
            "[63]\tvalidation_0-auc:0.79199\n",
            "[64]\tvalidation_0-auc:0.791613\n",
            "[65]\tvalidation_0-auc:0.791588\n",
            "[66]\tvalidation_0-auc:0.791588\n",
            "[67]\tvalidation_0-auc:0.791011\n",
            "[68]\tvalidation_0-auc:0.790359\n",
            "[69]\tvalidation_0-auc:0.790259\n",
            "[70]\tvalidation_0-auc:0.790259\n",
            "[71]\tvalidation_0-auc:0.790058\n",
            "[72]\tvalidation_0-auc:0.790058\n",
            "[73]\tvalidation_0-auc:0.789983\n",
            "[74]\tvalidation_0-auc:0.790585\n",
            "[75]\tvalidation_0-auc:0.790585\n",
            "[76]\tvalidation_0-auc:0.788766\n",
            "[77]\tvalidation_0-auc:0.789393\n",
            "[78]\tvalidation_0-auc:0.789393\n",
            "[79]\tvalidation_0-auc:0.789318\n",
            "[80]\tvalidation_0-auc:0.78775\n",
            "[81]\tvalidation_0-auc:0.788139\n",
            "[82]\tvalidation_0-auc:0.787863\n",
            "[83]\tvalidation_0-auc:0.787712\n",
            "[84]\tvalidation_0-auc:0.787712\n",
            "[85]\tvalidation_0-auc:0.787712\n",
            "[86]\tvalidation_0-auc:0.786897\n",
            "[87]\tvalidation_0-auc:0.786897\n",
            "[88]\tvalidation_0-auc:0.7877\n",
            "[89]\tvalidation_0-auc:0.7877\n",
            "[90]\tvalidation_0-auc:0.787675\n",
            "[91]\tvalidation_0-auc:0.785919\n",
            "[92]\tvalidation_0-auc:0.785893\n",
            "[93]\tvalidation_0-auc:0.785567\n",
            "[94]\tvalidation_0-auc:0.785756\n",
            "[95]\tvalidation_0-auc:0.784075\n",
            "[96]\tvalidation_0-auc:0.783774\n",
            "[97]\tvalidation_0-auc:0.785128\n",
            "[98]\tvalidation_0-auc:0.78568\n",
            "[99]\tvalidation_0-auc:0.785078\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6808 - accuracy: 0.5587 - val_loss: 0.6121 - val_accuracy: 0.7746\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6651 - accuracy: 0.5889 - val_loss: 0.6746 - val_accuracy: 0.6411\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6055 - accuracy: 0.6905 - val_loss: 0.6078 - val_accuracy: 0.8074\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5606 - accuracy: 0.7042 - val_loss: 0.5055 - val_accuracy: 0.8206\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5524 - accuracy: 0.7316 - val_loss: 0.6963 - val_accuracy: 0.5339\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6652 - accuracy: 0.5951 - val_loss: 0.7251 - val_accuracy: 0.3370\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5762 - accuracy: 0.7152 - val_loss: 0.5495 - val_accuracy: 0.8381\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5489 - accuracy: 0.7261 - val_loss: 0.5647 - val_accuracy: 0.7921\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5524 - accuracy: 0.7227 - val_loss: 0.5412 - val_accuracy: 0.7943\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5379 - accuracy: 0.7412 - val_loss: 0.5884 - val_accuracy: 0.7352\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.706695\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.706695\n",
            "[2]\tvalidation_0-auc:0.764001\n",
            "[3]\tvalidation_0-auc:0.760381\n",
            "[4]\tvalidation_0-auc:0.811626\n",
            "[5]\tvalidation_0-auc:0.81552\n",
            "[6]\tvalidation_0-auc:0.821019\n",
            "[7]\tvalidation_0-auc:0.826395\n",
            "[8]\tvalidation_0-auc:0.825435\n",
            "[9]\tvalidation_0-auc:0.830207\n",
            "[10]\tvalidation_0-auc:0.83997\n",
            "[11]\tvalidation_0-auc:0.827533\n",
            "[12]\tvalidation_0-auc:0.828246\n",
            "[13]\tvalidation_0-auc:0.827067\n",
            "[14]\tvalidation_0-auc:0.829247\n",
            "[15]\tvalidation_0-auc:0.829247\n",
            "[16]\tvalidation_0-auc:0.827738\n",
            "[17]\tvalidation_0-auc:0.824173\n",
            "[18]\tvalidation_0-auc:0.82937\n",
            "[19]\tvalidation_0-auc:0.829233\n",
            "[20]\tvalidation_0-auc:0.828849\n",
            "[21]\tvalidation_0-auc:0.826573\n",
            "[22]\tvalidation_0-auc:0.825311\n",
            "[23]\tvalidation_0-auc:0.825311\n",
            "[24]\tvalidation_0-auc:0.824639\n",
            "[25]\tvalidation_0-auc:0.824173\n",
            "[26]\tvalidation_0-auc:0.826477\n",
            "[27]\tvalidation_0-auc:0.827492\n",
            "[28]\tvalidation_0-auc:0.826614\n",
            "[29]\tvalidation_0-auc:0.827876\n",
            "[30]\tvalidation_0-auc:0.831441\n",
            "[31]\tvalidation_0-auc:0.831441\n",
            "[32]\tvalidation_0-auc:0.831414\n",
            "[33]\tvalidation_0-auc:0.831605\n",
            "[34]\tvalidation_0-auc:0.827615\n",
            "[35]\tvalidation_0-auc:0.827615\n",
            "[36]\tvalidation_0-auc:0.827615\n",
            "[37]\tvalidation_0-auc:0.828026\n",
            "[38]\tvalidation_0-auc:0.824845\n",
            "[39]\tvalidation_0-auc:0.822555\n",
            "[40]\tvalidation_0-auc:0.822089\n",
            "[41]\tvalidation_0-auc:0.823652\n",
            "[42]\tvalidation_0-auc:0.823652\n",
            "[43]\tvalidation_0-auc:0.823364\n",
            "[44]\tvalidation_0-auc:0.822925\n",
            "[45]\tvalidation_0-auc:0.821129\n",
            "[46]\tvalidation_0-auc:0.818894\n",
            "[47]\tvalidation_0-auc:0.818894\n",
            "[48]\tvalidation_0-auc:0.819936\n",
            "[49]\tvalidation_0-auc:0.82165\n",
            "[50]\tvalidation_0-auc:0.82165\n",
            "[51]\tvalidation_0-auc:0.82165\n",
            "[52]\tvalidation_0-auc:0.821239\n",
            "[53]\tvalidation_0-auc:0.821239\n",
            "[54]\tvalidation_0-auc:0.822404\n",
            "[55]\tvalidation_0-auc:0.822404\n",
            "[56]\tvalidation_0-auc:0.821581\n",
            "[57]\tvalidation_0-auc:0.816878\n",
            "[58]\tvalidation_0-auc:0.816713\n",
            "[59]\tvalidation_0-auc:0.816713\n",
            "[60]\tvalidation_0-auc:0.816713\n",
            "Stopping. Best iteration:\n",
            "[10]\tvalidation_0-auc:0.83997\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.6244897959183674 |  0.3424124513618677 | 0.8543689320388349 |  0.4888888888888889 |\n",
            "|     GRU 0.1      | 0.5693877551020409 | 0.30985915492957744 | 0.8543689320388349 |  0.4547803617571059 |\n",
            "|   XGBoost 0.1    | 0.7142857142857143 |  0.4088669950738916 | 0.8058252427184466 |  0.542483660130719  |\n",
            "|    Logreg 0.1    | 0.6183673469387755 |  0.3346456692913386 | 0.8252427184466019 |  0.4761904761904762 |\n",
            "|     SVM 0.1      | 0.6857142857142857 |  0.3813953488372093 | 0.7961165048543689 |  0.5157232704402516 |\n",
            "|  LSTM beta 0.1   | 0.5339168490153173 |  0.3116438356164384 | 0.883495145631068  | 0.46075949367088614 |\n",
            "|   GRU beta 0.1   | 0.7352297592997812 |  0.4476744186046512 | 0.7475728155339806 |         0.56        |\n",
            "| XGBoost beta 0.1 | 0.8358862144420132 |  0.7121212121212122 | 0.4563106796116505 |  0.5562130177514794 |\n",
            "| logreg beta 0.1  | 0.6783369803063457 |  0.397196261682243  | 0.8252427184466019 |  0.5362776025236593 |\n",
            "|   svm beta 0.1   | 0.8183807439824945 |  0.5793650793650794 | 0.7087378640776699 |  0.6375545851528385 |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6883 - accuracy: 0.5691 - val_loss: 1.0549 - val_accuracy: 0.1898\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6761 - accuracy: 0.5960 - val_loss: 0.7541 - val_accuracy: 0.1898\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6437 - accuracy: 0.6168 - val_loss: 0.6012 - val_accuracy: 0.7490\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6091 - accuracy: 0.6799 - val_loss: 0.6415 - val_accuracy: 0.6694\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5978 - accuracy: 0.7007 - val_loss: 0.6806 - val_accuracy: 0.6592\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6529 - accuracy: 0.5966 - val_loss: 0.7422 - val_accuracy: 0.4776\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5683 - accuracy: 0.7242 - val_loss: 0.7000 - val_accuracy: 0.5388\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5628 - accuracy: 0.7060 - val_loss: 0.6405 - val_accuracy: 0.6592\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5438 - accuracy: 0.7275 - val_loss: 0.6650 - val_accuracy: 0.6102\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5330 - accuracy: 0.7430 - val_loss: 0.6427 - val_accuracy: 0.6571\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.755058\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.755234\n",
            "[2]\tvalidation_0-auc:0.758349\n",
            "[3]\tvalidation_0-auc:0.754449\n",
            "[4]\tvalidation_0-auc:0.754124\n",
            "[5]\tvalidation_0-auc:0.75732\n",
            "[6]\tvalidation_0-auc:0.758471\n",
            "[7]\tvalidation_0-auc:0.75927\n",
            "[8]\tvalidation_0-auc:0.759405\n",
            "[9]\tvalidation_0-auc:0.759405\n",
            "[10]\tvalidation_0-auc:0.758457\n",
            "[11]\tvalidation_0-auc:0.758376\n",
            "[12]\tvalidation_0-auc:0.758349\n",
            "[13]\tvalidation_0-auc:0.758552\n",
            "[14]\tvalidation_0-auc:0.758349\n",
            "[15]\tvalidation_0-auc:0.758295\n",
            "[16]\tvalidation_0-auc:0.758417\n",
            "[17]\tvalidation_0-auc:0.756968\n",
            "[18]\tvalidation_0-auc:0.75713\n",
            "[19]\tvalidation_0-auc:0.757103\n",
            "[20]\tvalidation_0-auc:0.757442\n",
            "[21]\tvalidation_0-auc:0.757618\n",
            "[22]\tvalidation_0-auc:0.757699\n",
            "[23]\tvalidation_0-auc:0.758159\n",
            "[24]\tvalidation_0-auc:0.757577\n",
            "[25]\tvalidation_0-auc:0.758281\n",
            "[26]\tvalidation_0-auc:0.758119\n",
            "[27]\tvalidation_0-auc:0.756805\n",
            "[28]\tvalidation_0-auc:0.756805\n",
            "[29]\tvalidation_0-auc:0.757076\n",
            "[30]\tvalidation_0-auc:0.757238\n",
            "[31]\tvalidation_0-auc:0.756588\n",
            "[32]\tvalidation_0-auc:0.757022\n",
            "[33]\tvalidation_0-auc:0.757293\n",
            "[34]\tvalidation_0-auc:0.757293\n",
            "[35]\tvalidation_0-auc:0.757536\n",
            "[36]\tvalidation_0-auc:0.757401\n",
            "[37]\tvalidation_0-auc:0.756426\n",
            "[38]\tvalidation_0-auc:0.75667\n",
            "[39]\tvalidation_0-auc:0.756683\n",
            "[40]\tvalidation_0-auc:0.756602\n",
            "[41]\tvalidation_0-auc:0.756629\n",
            "[42]\tvalidation_0-auc:0.756629\n",
            "[43]\tvalidation_0-auc:0.756737\n",
            "[44]\tvalidation_0-auc:0.756737\n",
            "[45]\tvalidation_0-auc:0.756764\n",
            "[46]\tvalidation_0-auc:0.756764\n",
            "[47]\tvalidation_0-auc:0.757144\n",
            "[48]\tvalidation_0-auc:0.756683\n",
            "[49]\tvalidation_0-auc:0.755505\n",
            "[50]\tvalidation_0-auc:0.755911\n",
            "[51]\tvalidation_0-auc:0.755911\n",
            "[52]\tvalidation_0-auc:0.755803\n",
            "[53]\tvalidation_0-auc:0.755803\n",
            "[54]\tvalidation_0-auc:0.756439\n",
            "[55]\tvalidation_0-auc:0.757293\n",
            "[56]\tvalidation_0-auc:0.757374\n",
            "[57]\tvalidation_0-auc:0.758335\n",
            "[58]\tvalidation_0-auc:0.758335\n",
            "Stopping. Best iteration:\n",
            "[8]\tvalidation_0-auc:0.759405\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6882 - accuracy: 0.5312 - val_loss: 0.7286 - val_accuracy: 0.1969\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6436 - accuracy: 0.6493 - val_loss: 0.7519 - val_accuracy: 0.2035\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6121 - accuracy: 0.6637 - val_loss: 0.7090 - val_accuracy: 0.4726\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5880 - accuracy: 0.6911 - val_loss: 0.5531 - val_accuracy: 0.8468\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5557 - accuracy: 0.7207 - val_loss: 0.6698 - val_accuracy: 0.6171\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6325 - accuracy: 0.6232 - val_loss: 0.8977 - val_accuracy: 0.2144\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5588 - accuracy: 0.7021 - val_loss: 0.6614 - val_accuracy: 0.6193\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5342 - accuracy: 0.7358 - val_loss: 0.5573 - val_accuracy: 0.7921\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5379 - accuracy: 0.7440 - val_loss: 0.5213 - val_accuracy: 0.8293\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5287 - accuracy: 0.7529 - val_loss: 0.5268 - val_accuracy: 0.8184\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.721951\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.717964\n",
            "[2]\tvalidation_0-auc:0.76186\n",
            "[3]\tvalidation_0-auc:0.770176\n",
            "[4]\tvalidation_0-auc:0.776719\n",
            "[5]\tvalidation_0-auc:0.831118\n",
            "[6]\tvalidation_0-auc:0.830808\n",
            "[7]\tvalidation_0-auc:0.836095\n",
            "[8]\tvalidation_0-auc:0.840896\n",
            "[9]\tvalidation_0-auc:0.839773\n",
            "[10]\tvalidation_0-auc:0.839921\n",
            "[11]\tvalidation_0-auc:0.840334\n",
            "[12]\tvalidation_0-auc:0.840334\n",
            "[13]\tvalidation_0-auc:0.838252\n",
            "[14]\tvalidation_0-auc:0.8371\n",
            "[15]\tvalidation_0-auc:0.835298\n",
            "[16]\tvalidation_0-auc:0.8345\n",
            "[17]\tvalidation_0-auc:0.836657\n",
            "[18]\tvalidation_0-auc:0.839552\n",
            "[19]\tvalidation_0-auc:0.838148\n",
            "[20]\tvalidation_0-auc:0.838267\n",
            "[21]\tvalidation_0-auc:0.840423\n",
            "[22]\tvalidation_0-auc:0.840423\n",
            "[23]\tvalidation_0-auc:0.839478\n",
            "[24]\tvalidation_0-auc:0.839478\n",
            "[25]\tvalidation_0-auc:0.840142\n",
            "[26]\tvalidation_0-auc:0.840142\n",
            "[27]\tvalidation_0-auc:0.839552\n",
            "[28]\tvalidation_0-auc:0.839552\n",
            "[29]\tvalidation_0-auc:0.839581\n",
            "[30]\tvalidation_0-auc:0.839581\n",
            "[31]\tvalidation_0-auc:0.839581\n",
            "[32]\tvalidation_0-auc:0.839581\n",
            "[33]\tvalidation_0-auc:0.839699\n",
            "[34]\tvalidation_0-auc:0.839699\n",
            "[35]\tvalidation_0-auc:0.839699\n",
            "[36]\tvalidation_0-auc:0.842181\n",
            "[37]\tvalidation_0-auc:0.841339\n",
            "[38]\tvalidation_0-auc:0.841339\n",
            "[39]\tvalidation_0-auc:0.8419\n",
            "[40]\tvalidation_0-auc:0.8419\n",
            "[41]\tvalidation_0-auc:0.8419\n",
            "[42]\tvalidation_0-auc:0.839005\n",
            "[43]\tvalidation_0-auc:0.839005\n",
            "[44]\tvalidation_0-auc:0.839094\n",
            "[45]\tvalidation_0-auc:0.839655\n",
            "[46]\tvalidation_0-auc:0.839655\n",
            "[47]\tvalidation_0-auc:0.839301\n",
            "[48]\tvalidation_0-auc:0.84128\n",
            "[49]\tvalidation_0-auc:0.843067\n",
            "[50]\tvalidation_0-auc:0.843067\n",
            "[51]\tvalidation_0-auc:0.840999\n",
            "[52]\tvalidation_0-auc:0.840999\n",
            "[53]\tvalidation_0-auc:0.84097\n",
            "[54]\tvalidation_0-auc:0.842003\n",
            "[55]\tvalidation_0-auc:0.842033\n",
            "[56]\tvalidation_0-auc:0.842033\n",
            "[57]\tvalidation_0-auc:0.842033\n",
            "[58]\tvalidation_0-auc:0.842033\n",
            "[59]\tvalidation_0-auc:0.842033\n",
            "[60]\tvalidation_0-auc:0.841856\n",
            "[61]\tvalidation_0-auc:0.841856\n",
            "[62]\tvalidation_0-auc:0.841915\n",
            "[63]\tvalidation_0-auc:0.841915\n",
            "[64]\tvalidation_0-auc:0.842151\n",
            "[65]\tvalidation_0-auc:0.842151\n",
            "[66]\tvalidation_0-auc:0.842151\n",
            "[67]\tvalidation_0-auc:0.841767\n",
            "[68]\tvalidation_0-auc:0.841501\n",
            "[69]\tvalidation_0-auc:0.841398\n",
            "[70]\tvalidation_0-auc:0.841634\n",
            "[71]\tvalidation_0-auc:0.842284\n",
            "[72]\tvalidation_0-auc:0.842284\n",
            "[73]\tvalidation_0-auc:0.842284\n",
            "[74]\tvalidation_0-auc:0.842934\n",
            "[75]\tvalidation_0-auc:0.842771\n",
            "[76]\tvalidation_0-auc:0.842417\n",
            "[77]\tvalidation_0-auc:0.842476\n",
            "[78]\tvalidation_0-auc:0.842476\n",
            "[79]\tvalidation_0-auc:0.845327\n",
            "[80]\tvalidation_0-auc:0.844544\n",
            "[81]\tvalidation_0-auc:0.844367\n",
            "[82]\tvalidation_0-auc:0.843008\n",
            "[83]\tvalidation_0-auc:0.843008\n",
            "[84]\tvalidation_0-auc:0.842978\n",
            "[85]\tvalidation_0-auc:0.842831\n",
            "[86]\tvalidation_0-auc:0.842949\n",
            "[87]\tvalidation_0-auc:0.842949\n",
            "[88]\tvalidation_0-auc:0.843362\n",
            "[89]\tvalidation_0-auc:0.84351\n",
            "[90]\tvalidation_0-auc:0.84351\n",
            "[91]\tvalidation_0-auc:0.84351\n",
            "[92]\tvalidation_0-auc:0.842712\n",
            "[93]\tvalidation_0-auc:0.842683\n",
            "[94]\tvalidation_0-auc:0.842683\n",
            "[95]\tvalidation_0-auc:0.842506\n",
            "[96]\tvalidation_0-auc:0.843185\n",
            "[97]\tvalidation_0-auc:0.842771\n",
            "[98]\tvalidation_0-auc:0.844308\n",
            "[99]\tvalidation_0-auc:0.844514\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.6591836734693878 |  0.3287037037037037 | 0.7634408602150538 |  0.459546925566343  |\n",
            "|      GRU 0.15     | 0.6571428571428571 |  0.3287671232876712 | 0.7741935483870968 |  0.4615384615384615 |\n",
            "|    XGBoost 0.15   | 0.6714285714285714 |  0.3411214953271028 | 0.7849462365591398 | 0.47557003257328984 |\n",
            "|    Logreg 0.15    | 0.5551020408163265 | 0.27598566308243727 | 0.8279569892473119 | 0.41397849462365593 |\n",
            "|      SVM 0.15     | 0.6714285714285714 |  0.3380952380952381 | 0.7634408602150538 |  0.4686468646864687 |\n",
            "|   LSTM beta 0.15  | 0.6170678336980306 |  0.3333333333333333 | 0.8817204301075269 |  0.4837758112094395 |\n",
            "|   GRU beta 0.15   | 0.8183807439824945 |  0.5396825396825397 | 0.7311827956989247 |  0.6210045662100456 |\n",
            "| XGBoost beta 0.15 | 0.8096280087527352 |  0.5245901639344263 | 0.6881720430107527 |  0.5953488372093024 |\n",
            "|  logreg beta 0.15 | 0.6849015317286652 | 0.37681159420289856 | 0.8387096774193549 |         0.52        |\n",
            "|   svm beta 0.15   | 0.7986870897155361 |  0.5035971223021583 | 0.7526881720430108 |  0.603448275862069  |\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-20whT5XF5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12d87a20-2f3c-4f59-d6c6-32aaaf71bf00"
      },
      "source": [
        "Result_purging.to_csv('MRO_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.342412</td>\n",
              "      <td>0.624490</td>\n",
              "      <td>0.488889</td>\n",
              "      <td>0.854369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.309859</td>\n",
              "      <td>0.569388</td>\n",
              "      <td>0.454780</td>\n",
              "      <td>0.854369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.408867</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.542484</td>\n",
              "      <td>0.805825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.334646</td>\n",
              "      <td>0.618367</td>\n",
              "      <td>0.476190</td>\n",
              "      <td>0.825243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.381395</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.515723</td>\n",
              "      <td>0.796117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.311644</td>\n",
              "      <td>0.533917</td>\n",
              "      <td>0.460759</td>\n",
              "      <td>0.883495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.447674</td>\n",
              "      <td>0.735230</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.747573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.712121</td>\n",
              "      <td>0.835886</td>\n",
              "      <td>0.556213</td>\n",
              "      <td>0.456311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.397196</td>\n",
              "      <td>0.678337</td>\n",
              "      <td>0.536278</td>\n",
              "      <td>0.825243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.579365</td>\n",
              "      <td>0.818381</td>\n",
              "      <td>0.637555</td>\n",
              "      <td>0.708738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.328704</td>\n",
              "      <td>0.659184</td>\n",
              "      <td>0.459547</td>\n",
              "      <td>0.763441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.328767</td>\n",
              "      <td>0.657143</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.774194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.341121</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.475570</td>\n",
              "      <td>0.784946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.275986</td>\n",
              "      <td>0.555102</td>\n",
              "      <td>0.413978</td>\n",
              "      <td>0.827957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.338095</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.468647</td>\n",
              "      <td>0.763441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.617068</td>\n",
              "      <td>0.483776</td>\n",
              "      <td>0.881720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.539683</td>\n",
              "      <td>0.818381</td>\n",
              "      <td>0.621005</td>\n",
              "      <td>0.731183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.524590</td>\n",
              "      <td>0.809628</td>\n",
              "      <td>0.595349</td>\n",
              "      <td>0.688172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.376812</td>\n",
              "      <td>0.684902</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.838710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.503597</td>\n",
              "      <td>0.798687</td>\n",
              "      <td>0.603448</td>\n",
              "      <td>0.752688</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  MRO  0.342412  0.624490  0.488889  0.854369\n",
              "1            GRU 0.1  MRO  0.309859  0.569388  0.454780  0.854369\n",
              "2        XGBoost 0.1  MRO  0.408867  0.714286  0.542484  0.805825\n",
              "3         Logreg 0.1  MRO  0.334646  0.618367  0.476190  0.825243\n",
              "4            SVM 0.1  MRO  0.381395  0.685714  0.515723  0.796117\n",
              "5      LSTM beta 0.1  MRO  0.311644  0.533917  0.460759  0.883495\n",
              "6       GRU beta 0.1  MRO  0.447674  0.735230  0.560000  0.747573\n",
              "7   XGBoost beta 0.1  MRO  0.712121  0.835886  0.556213  0.456311\n",
              "8    logreg beta 0.1  MRO  0.397196  0.678337  0.536278  0.825243\n",
              "9       svm beta 0.1  MRO  0.579365  0.818381  0.637555  0.708738\n",
              "0          LSTM 0.15  MRO  0.328704  0.659184  0.459547  0.763441\n",
              "1           GRU 0.15  MRO  0.328767  0.657143  0.461538  0.774194\n",
              "2       XGBoost 0.15  MRO  0.341121  0.671429  0.475570  0.784946\n",
              "3        Logreg 0.15  MRO  0.275986  0.555102  0.413978  0.827957\n",
              "4           SVM 0.15  MRO  0.338095  0.671429  0.468647  0.763441\n",
              "5     LSTM beta 0.15  MRO  0.333333  0.617068  0.483776  0.881720\n",
              "6      GRU beta 0.15  MRO  0.539683  0.818381  0.621005  0.731183\n",
              "7  XGBoost beta 0.15  MRO  0.524590  0.809628  0.595349  0.688172\n",
              "8   logreg beta 0.15  MRO  0.376812  0.684902  0.520000  0.838710\n",
              "9      svm beta 0.15  MRO  0.503597  0.798687  0.603448  0.752688"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnRmoH_8XF5M"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHOt2ASuXF5M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLmeDk6wXhm1"
      },
      "source": [
        "## NKTR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGNI1SJzXhm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6eb49b2-10d1-4ad3-e46d-c654f2a6bec9"
      },
      "source": [
        "dfs = pd.read_csv(\"NKTR.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "# Denoise prices\n",
        "dfs = denoise_data(dfs, '<CLOSE>')\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.NKTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>17.99</td>\n",
              "      <td>18.51</td>\n",
              "      <td>17.950</td>\n",
              "      <td>18.39</td>\n",
              "      <td>34775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.NKTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>18.09</td>\n",
              "      <td>18.28</td>\n",
              "      <td>17.900</td>\n",
              "      <td>17.97</td>\n",
              "      <td>28262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.NKTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>18.43</td>\n",
              "      <td>18.52</td>\n",
              "      <td>17.905</td>\n",
              "      <td>17.92</td>\n",
              "      <td>22410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2763</td>\n",
              "      <td>US1.NKTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>18.66</td>\n",
              "      <td>18.90</td>\n",
              "      <td>18.350</td>\n",
              "      <td>18.40</td>\n",
              "      <td>15735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2762</td>\n",
              "      <td>US1.NKTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>18.59</td>\n",
              "      <td>19.32</td>\n",
              "      <td>18.590</td>\n",
              "      <td>18.84</td>\n",
              "      <td>15873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2762</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.NKTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>14.84</td>\n",
              "      <td>15.83</td>\n",
              "      <td>14.840</td>\n",
              "      <td>15.67</td>\n",
              "      <td>497674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2763</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.NKTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>15.15</td>\n",
              "      <td>15.25</td>\n",
              "      <td>14.800</td>\n",
              "      <td>14.81</td>\n",
              "      <td>236730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.NKTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>14.85</td>\n",
              "      <td>15.15</td>\n",
              "      <td>14.800</td>\n",
              "      <td>15.06</td>\n",
              "      <td>392593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.NKTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>14.75</td>\n",
              "      <td>14.95</td>\n",
              "      <td>14.530</td>\n",
              "      <td>14.93</td>\n",
              "      <td>441063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.NKTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>14.68</td>\n",
              "      <td>14.85</td>\n",
              "      <td>14.350</td>\n",
              "      <td>14.53</td>\n",
              "      <td>419270</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2767 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  <TICKER> <PER>    <DATE>  ...  <HIGH>   <LOW>  <CLOSE>   <VOL>\n",
              "0      2766  US1.NKTR     D  20211001  ...   18.51  17.950    18.39   34775\n",
              "1      2765  US1.NKTR     D  20210930  ...   18.28  17.900    17.97   28262\n",
              "2      2764  US1.NKTR     D  20210929  ...   18.52  17.905    17.92   22410\n",
              "3      2763  US1.NKTR     D  20210928  ...   18.90  18.350    18.40   15735\n",
              "4      2762  US1.NKTR     D  20210927  ...   19.32  18.590    18.84   15873\n",
              "...     ...       ...   ...       ...  ...     ...     ...      ...     ...\n",
              "2762      4  US1.NKTR     D  20101008  ...   15.83  14.840    15.67  497674\n",
              "2763      3  US1.NKTR     D  20101007  ...   15.25  14.800    14.81  236730\n",
              "2764      2  US1.NKTR     D  20101006  ...   15.15  14.800    15.06  392593\n",
              "2765      1  US1.NKTR     D  20101005  ...   14.95  14.530    14.93  441063\n",
              "2766      0  US1.NKTR     D  20101004  ...   14.85  14.350    14.53  419270\n",
              "\n",
              "[2767 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL2xFpRpXhm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e881896-96b8-4c62-a650-6989fb96f2a3"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"cb3a097f-d875-4c5e-bbbc-d71af23443a8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"cb3a097f-d875-4c5e-bbbc-d71af23443a8\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'cb3a097f-d875-4c5e-bbbc-d71af23443a8',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [16.18, 18.66, 18.96, 18.2, 17.3, 18.44, 18.21, 17.9, 17.56, 18.81, 18.915, 19.92, 20.13, 20.21, 20.1, 20.7, 21.22, 20.11, 19.95, 20.0, 19.93, 17.86, 17.0, 17.37, 16.96, 16.93, 17.59, 17.37, 17.46, 16.91, 17.31, 17.0, 16.98, 17.02, 17.12, 17.88, 18.1, 18.21, 18.73, 18.84, 18.5784, 20.92, 29.57, 28.8, 28.76, 28.77, 31.1, 31.21, 28.47, 29.32, 29.26, 31.11, 29.99, 32.12, 32.08, 32.34, 31.98, 32.07, 31.98, 32.49, 33.21, 33.7, 33.53, 34.78, 34.83, 34.18, 35.15, 36.27, 36.13, 35.84, 35.62, 34.69, 33.29, 34.62, 33.89, 34.84, 35.39, 35.34, 35.17, 33.7, 33.4, 34.05, 33.63, 33.27, 33.13, 33.0, 32.99, 33.64, 32.92, 34.05, 31.319, 32.11, 32.55, 32.1, 33.8, 33.07, 33.48, 33.42, 32.14, 32.0, 31.35, 33.91, 33.73, 32.61, 34.78, 35.46, 32.99, 32.09, 32.69, 32.22, 32.14, 31.63, 32.02, 32.33, 32.27, 31.78, 31.15, 31.61, 30.9964, 31.68, 31.19, 33.38, 33.47, 35.17, 35.75, 35.82, 35.46, 35.99, 36.3, 35.3, 35.29, 33.69, 33.24, 33.63, 32.69, 31.78, 32.34, 32.39, 32.14, 33.49, 32.61, 34.41, 34.93, 35.56, 36.32, 37.64, 36.21, 35.44, 35.12, 35.81, 36.38, 38.4, 38.295, 38.15, 40.52, 41.03, 41.04, 40.99, 40.05, 40.26, 43.05, 42.81, 41.69, 42.62, 42.25, 42.25, 46.37, 44.91, 43.84, 44.36, 43.75, 42.5, 42.21, 42.33, 43.1, 42.49, 42.99, 43.91, 43.22, 43.94, 44.74, 46.2201, 45.42, 44.62, 44.53, 41.51, 41.01, 41.47, 41.18, 40.7, 36.62, 33.92, 31.57, 34.21, 32.87, 33.63, 33.17, 32.78, 30.43, 30.61, 32.64, 34.27, 35.74, 34.17, 36.51, 37.99, 36.97, 36.43, 36.63, 37.0, 38.04, 38.01, 40.55, 40.4, 39.27, 38.59, 37.89, 39.01, 37.16, 37.35, 38.14, 38.01, 38.22, 38.2, 37.63, 37.35, 36.98, 34.03, 35.33, 37.96, 36.07, 40.0, 39.04, 40.71, 38.67, 37.5, 36.57, 36.79, 36.77, 36.61, 39.495, 39.89, 48.23, 49.56, 50.59, 50.46, 49.06, 48.91, 47.9, 48.63, 51.97, 51.75, 52.38, 52.11, 53.78, 55.33, 56.66, 60.96, 60.91, 59.52, 61.15, 60.86, 59.01, 58.44, 56.68, 58.32, 58.31, 59.25, 61.91, 62.83, 64.41, 66.58, 65.0, 66.0, 67.56, 68.49, 66.5, 66.0, 66.75, 66.11, 65.2265, 63.88, 63.51, 63.09, 61.95, 60.6, 60.03, 59.95, 58.61, 60.39, 59.32, 59.76, 59.37, 55.96, 54.61, 55.39, 53.93, 54.29, 52.71, 52.6, 51.04, 50.52, 50.16, 49.83, 46.46, 49.37, 48.72, 49.09, 47.89, 48.37, 47.8, 47.79, 47.67, 46.77, 47.13, 47.55, 47.66, 47.54, 48.1, 48.83, 48.84, 48.2, 46.24, 49.81, 51.27, 52.85, 55.86, 58.17, 57.35, 58.41, 57.86, 56.4, 53.47, 53.57, 53.02, 54.13, 52.97, 59.96, 55.97, 52.57, 90.33, 80.29, 80.6, 77.83, 79.96, 80.58, 82.8, 81.95, 83.46, 85.3, 79.23, 85.77, 83.4, 80.93, 79.43, 77.29, 77.82, 78.11, 78.76, 77.2, 82.75, 85.31, 84.12, 83.66, 83.66, 84.19, 82.59, 84.51, 85.45, 86.2, 90.13, 92.53, 93.71, 93.49, 100.5, 103.73, 102.77, 102.74, 104.44, 93.86, 101.44, 101.87, 99.36, 98.77, 106.26, 105.03, 104.51, 107.025, 103.0, 104.02, 106.9, 107.2, 105.01, 103.0, 102.0, 104.51, 103.18, 103.06, 108.35, 108.43, 101.31, 98.09, 102.04, 102.83, 84.56, 86.56, 90.06, 90.56, 89.61, 88.2, 86.44, 83.45, 83.45, 82.13, 84.0, 75.67, 78.71, 74.62, 74.65, 76.57, 78.72, 82.43, 88.01, 88.74, 83.62, 82.45, 82.4, 79.02, 78.92, 76.53, 77.78, 76.78, 71.58, 70.17, 69.4, 69.73, 71.99, 69.88, 68.9, 68.03, 57.43, 57.76, 58.34, 57.85, 57.65, 59.71, 60.5, 59.03, 60.0, 58.0, 57.41, 57.92, 57.05, 56.9, 56.91, 55.06, 56.62, 54.89, 55.2, 53.96, 54.83, 49.67, 51.91, 50.72, 53.34, 53.97, 54.15, 52.11, 51.975, 52.3858, 49.75, 49.97, 46.4, 44.99, 44.92, 43.07, 39.56, 37.1, 32.5, 31.75, 30.3, 26.84, 24.27, 23.75, 23.541, 23.37, 24.09, 23.52, 23.61, 23.65, 24.17, 24.15, 24.56, 24.16, 23.68, 24.32, 24.0, 23.67, 23.21, 23.02, 23.07, 23.26, 23.88, 24.81, 24.68, 24.91, 24.49, 24.77, 24.0, 23.68, 23.11, 22.26, 22.93, 22.44, 21.88, 21.38, 21.735, 22.62, 22.05, 22.0, 21.765, 21.88, 21.83, 22.27, 22.485, 21.66, 21.745, 21.63, 21.045, 19.525, 19.2, 19.13, 18.67, 18.68, 18.33, 18.61, 17.79, 18.04, 18.425, 19.52, 19.5, 19.28, 18.26, 17.95, 18.68, 19.63, 20.27, 20.03, 20.045, 20.49, 20.64, 21.83, 22.92, 22.61, 23.1, 23.22, 22.94, 22.24, 22.18, 21.66, 21.61, 20.66, 20.67, 20.81, 20.85, 20.97, 19.515, 19.78, 19.52, 20.06, 19.76, 19.54, 19.34, 19.51, 18.82, 19.735, 19.98, 19.5, 19.16, 18.16, 18.385, 18.2, 18.1, 18.44, 18.26, 18.04, 19.03, 18.66, 18.84, 19.68, 19.66, 20.51, 19.99, 19.88, 19.01, 20.78, 21.5, 21.1, 19.76, 19.28, 18.94, 19.53, 18.9, 19.53, 19.81, 19.71, 19.41, 18.975, 17.54, 17.925, 18.64, 18.58, 18.39, 18.555, 18.77, 18.97, 18.94, 18.79, 19.395, 18.86, 18.37, 19.14, 18.6, 18.585, 18.87, 18.725, 18.22, 18.52, 18.495, 20.12, 21.04, 21.07, 22.36, 22.5589, 23.465, 23.25, 24.2, 23.93, 23.38, 22.43, 22.54, 22.89, 21.84, 22.12, 15.5, 15.71, 15.69, 15.405, 15.695, 15.345, 15.32, 15.25, 14.26, 14.38, 14.63, 14.8, 13.01, 13.08, 13.69, 13.45, 13.2, 13.21, 13.4, 13.09, 12.97, 13.69, 13.375, 13.25, 13.29, 13.7, 13.15, 13.17, 13.45, 12.89, 12.39, 12.63, 12.105, 11.75, 12.37, 12.26, 12.41, 12.13, 12.13, 12.01, 12.24, 12.51, 12.31, 12.93, 12.81, 13.1, 13.51, 13.63, 13.76, 13.19, 13.16, 12.69, 12.28, 12.19, 12.18, 12.66, 12.885, 12.32, 12.34, 12.79, 12.98, 12.9, 12.585, 12.18, 12.23, 12.56, 13.28, 12.95, 12.33, 12.79, 12.59, 12.37, 12.09, 12.28, 12.57, 12.74, 13.48, 13.255, 13.11, 13.5, 13.93, 13.98, 13.91, 13.93, 14.41, 14.41, 13.68, 13.59, 13.07, 12.8, 12.3, 11.85, 12.44, 12.5, 12.4, 12.01, 12.26, 12.61, 12.77, 13.09, 13.34, 13.75, 13.345, 14.07, 15.66, 15.89, 16.1, 16.22, 16.64, 17.21, 16.81, 16.99, 17.06, 16.93, 17.48, 17.18, 16.96, 17.67, 17.34, 18.02, 18.75, 19.51, 19.14, 18.97, 18.99, 19.07, 19.25, 19.09, 18.68, 19.32, 18.66, 19.615, 19.67, 18.94, 18.45, 18.1, 17.86, 17.52, 17.32, 17.43, 17.34, 17.59, 18.14, 17.83, 17.26, 17.72, 17.65, 17.52, 17.63, 17.15, 17.21, 17.04, 17.375, 17.25, 17.58, 17.47, 17.52, 17.18, 17.76, 17.285, 16.1, 15.17, 15.15, 15.31, 15.2, 15.38, 15.285, 14.96, 15.39, 15.455, 15.62, 15.77, 15.69, 15.06, 14.91, 14.29, 14.21, 14.08, 14.39, 14.22, 13.97, 13.755, 13.38, 14.39, 15.54, 15.1, 14.98, 15.21, 14.85, 15.02, 15.05, 15.05, 14.98, 15.8, 16.28, 16.01, 16.02, 16.24, 15.83, 15.95, 15.525, 15.43, 15.17, 14.9, 14.79, 14.53, 14.07, 13.85, 13.39, 13.645, 13.34, 13.91, 13.32, 13.09, 13.3, 13.715, 13.58, 13.38, 13.64, 14.015, 15.5901, 15.97, 15.68, 16.13, 16.25, 16.14, 16.21, 16.22, 15.76, 15.375, 15.56, 15.75, 15.13, 15.18, 15.27, 14.84, 15.3, 15.49, 15.15, 15.31, 14.37, 14.27, 14.44, 13.76, 13.56, 13.345, 12.61, 12.71, 12.61, 12.97, 12.64, 12.6, 12.32, 12.3, 12.77, 13.06, 12.86, 12.035, 11.66, 11.91, 12.06, 11.35, 11.45, 11.15, 11.48, 11.18, 11.27, 11.29, 11.35, 10.99, 11.51, 11.42, 11.34, 11.94, 11.54, 11.53, 11.33, 11.17, 11.355, 11.17, 11.93, 11.68, 12.2, 12.15, 13.33, 13.64, 13.71, 14.42, 15.25, 14.93, 14.76, 14.27, 14.58, 14.03, 14.06, 13.765, 13.19, 14.275, 13.85, 14.73, 14.99, 15.62, 16.03, 16.21, 16.83, 17.16, 17.405, 17.16, 17.07, 16.82, 17.02, 16.52, 15.86, 15.91, 16.13, 15.55, 15.17, 15.45, 15.84, 15.67, 15.31, 15.37, 15.9, 15.21, 15.71, 15.81, 15.66, 15.21, 15.65, 15.06, 14.92, 15.49, 14.945, 14.99, 14.37, 13.725, 13.43, 12.43, 12.89, 13.87, 12.77, 12.89, 12.31, 12.35, 12.5, 12.24, 11.865, 11.91, 12.29, 11.42, 11.34, 11.37, 11.11, 11.11, 11.16, 11.3, 10.86, 10.86, 9.98, 10.21, 11.09, 11.36, 10.78, 11.15, 10.3, 11.41, 11.17, 10.71, 10.96, 10.55, 11.23, 11.94, 13.04, 12.88, 13.045, 13.74, 13.91, 13.76, 13.22, 12.8, 12.53, 12.24, 11.45, 11.34, 11.82, 11.27, 11.12, 11.4, 10.75, 11.04, 10.87, 10.83, 10.08, 9.83, 9.5, 10.27, 10.31, 10.81, 11.09, 11.44, 11.22, 10.99, 10.66, 10.73, 11.46, 11.345, 11.73, 12.3, 12.43, 12.58, 12.62, 12.34, 12.16, 12.88, 12.42, 12.77, 12.72, 12.79, 12.61, 12.455, 12.47, 12.62, 12.285, 12.11, 11.815, 11.395, 11.17, 11.1, 11.67, 11.78, 11.895, 12.12, 12.505, 12.36, 12.97, 13.83, 13.45, 12.57, 12.465, 11.82, 11.8, 11.5, 11.51, 11.19, 11.42, 11.49, 11.65, 11.405, 11.77, 11.685, 11.44, 11.74, 11.78, 11.48, 11.5, 11.51, 11.55, 11.54, 11.87, 11.575, 11.59, 11.53, 11.32, 11.12, 10.93, 10.92, 11.28, 11.47, 11.36, 11.315, 11.41, 11.17, 11.34, 11.47, 9.51, 10.135, 10.34, 10.55, 10.77, 11.06, 10.89, 10.74, 10.96, 11.98, 12.13, 12.1, 11.77, 11.365, 11.21, 11.17, 11.15, 10.72, 10.71, 11.0, 10.81, 11.0, 11.31, 11.09, 10.92, 11.025, 11.51, 11.62, 12.02, 11.885, 12.505, 14.13, 14.06, 13.89, 13.5, 13.05, 12.66, 12.79, 13.02, 13.3, 13.4, 12.95, 13.02, 13.05, 12.98, 13.23, 13.48, 13.585, 13.6, 13.65, 13.6, 13.64, 13.54, 13.69, 13.69, 13.77, 13.85, 13.96, 14.47, 14.13, 14.55, 14.595, 14.63, 15.06, 14.4901, 14.98, 14.75, 14.62, 14.6, 14.51, 14.86, 15.24, 14.6, 15.61, 15.36, 15.68, 15.39, 15.7, 15.19, 14.67, 14.99, 15.17, 15.5, 15.36, 15.58, 15.22, 15.0218, 14.81, 15.36, 15.69, 15.62, 15.21, 14.47, 14.5, 15.3, 15.6, 15.625, 16.31, 15.91, 16.24, 15.96, 15.76, 16.67, 17.05, 16.668, 16.7, 15.93, 14.98, 14.67, 14.315, 13.76, 14.09, 13.75, 13.625, 13.85, 13.97, 13.88, 13.79, 13.35, 13.57, 13.6, 13.81, 13.82, 13.78, 14.02, 13.89, 13.94, 13.485, 13.15, 13.245, 12.66, 12.63, 12.86, 12.4, 13.275, 12.605, 12.53, 12.535, 12.66, 12.68, 12.89, 12.47, 12.45, 12.57, 12.25, 12.07, 12.07, 12.59, 12.71, 12.6, 12.96, 12.71, 12.53, 12.82, 13.36, 13.71, 13.58, 13.53, 13.83, 13.82, 13.98, 13.56, 14.02, 13.9, 13.73, 14.12, 14.48, 14.26, 14.3, 14.28, 14.405, 14.23, 13.44, 13.32, 13.39, 13.595, 13.54, 13.42, 13.51, 13.2597, 12.88, 12.98, 12.64, 12.43, 12.54, 12.07, 11.83, 11.815, 10.55, 11.01, 11.0, 10.91, 10.99, 11.02, 11.01, 11.02, 10.95, 11.11, 11.07, 11.66, 11.745, 12.14, 12.58, 12.29, 12.13, 12.155, 12.46, 13.25, 13.28, 13.47, 12.82, 12.84, 12.99, 12.94, 12.83, 13.18, 13.24, 13.5, 13.89, 13.84, 14.31, 13.89, 12.54, 12.24, 11.185, 11.45, 11.125, 11.01, 11.53, 11.73, 12.05, 11.72, 11.75, 11.69, 11.675, 11.015, 10.94, 11.035, 10.95, 11.03, 11.11, 11.1, 11.21, 10.93, 10.88, 11.24, 11.21, 11.72, 11.47, 11.645, 11.78, 11.48, 11.19, 11.23, 11.88, 11.97, 12.25, 11.05, 10.97, 11.05, 10.89, 10.59, 10.53, 11.02, 11.76, 10.93, 11.15, 11.11, 11.73, 12.57, 12.64, 12.11, 11.76, 12.16, 12.3, 12.71, 12.75, 13.44, 14.5, 14.42, 14.37, 13.17, 13.43, 13.29, 13.36, 13.5, 13.6, 14.03, 14.25, 14.97, 14.78, 14.18, 12.83, 13.07, 13.7, 13.4, 13.63, 13.65, 13.74, 13.53, 13.88, 13.44, 13.62, 13.49, 13.37, 13.27, 12.8, 12.01, 12.43, 12.7, 12.86, 13.61, 13.99, 13.29, 13.37, 12.74, 12.92, 13.99, 13.11, 13.52, 13.04, 13.265, 13.16, 13.13, 12.385, 12.79, 12.41, 12.17, 11.88, 11.68, 11.97, 11.87, 11.34, 11.57, 11.39, 11.74, 11.48, 11.22, 11.19, 10.76, 10.98, 10.84, 10.76, 10.53, 10.75, 10.54, 11.235, 11.38, 11.54, 11.47, 11.655, 11.84, 12.02, 12.52, 12.33, 11.79, 11.84, 11.8, 11.59, 11.075, 11.01, 11.01, 11.36, 11.06, 11.23, 10.89, 10.83, 10.83, 8.96, 9.36, 9.58, 9.51, 9.42, 9.51, 9.6, 10.22, 10.27, 10.38, 10.72, 10.52, 10.49, 10.26]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('cb3a097f-d875-4c5e-bbbc-d71af23443a8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"433a60dd-478d-4aa3-b575-882574bbe634\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"433a60dd-478d-4aa3-b575-882574bbe634\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '433a60dd-478d-4aa3-b575-882574bbe634',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('433a60dd-478d-4aa3-b575-882574bbe634');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2SxfGOyXhm7"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovHB4COzXhm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e73cbc1-6705-4d38-e178-a59fc29be97b"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.05, .1, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"NKTR\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6824 - accuracy: 0.5570 - val_loss: 0.6777 - val_accuracy: 0.5327\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6161 - accuracy: 0.6772 - val_loss: 0.5930 - val_accuracy: 0.6796\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5747 - accuracy: 0.7128 - val_loss: 0.5350 - val_accuracy: 0.7653\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5313 - accuracy: 0.7470 - val_loss: 0.4949 - val_accuracy: 0.7755\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5228 - accuracy: 0.7550 - val_loss: 0.4781 - val_accuracy: 0.7735\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6420 - accuracy: 0.6248 - val_loss: 0.5402 - val_accuracy: 0.7490\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5243 - accuracy: 0.7537 - val_loss: 0.4787 - val_accuracy: 0.7816\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4960 - accuracy: 0.7638 - val_loss: 0.4914 - val_accuracy: 0.7714\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4881 - accuracy: 0.7691 - val_loss: 0.4761 - val_accuracy: 0.7776\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4827 - accuracy: 0.7678 - val_loss: 0.4900 - val_accuracy: 0.7673\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.820074\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.820299\n",
            "[2]\tvalidation_0-auc:0.827489\n",
            "[3]\tvalidation_0-auc:0.835666\n",
            "[4]\tvalidation_0-auc:0.840621\n",
            "[5]\tvalidation_0-auc:0.839868\n",
            "[6]\tvalidation_0-auc:0.838594\n",
            "[7]\tvalidation_0-auc:0.841011\n",
            "[8]\tvalidation_0-auc:0.841817\n",
            "[9]\tvalidation_0-auc:0.842943\n",
            "[10]\tvalidation_0-auc:0.843385\n",
            "[11]\tvalidation_0-auc:0.843515\n",
            "[12]\tvalidation_0-auc:0.844181\n",
            "[13]\tvalidation_0-auc:0.844415\n",
            "[14]\tvalidation_0-auc:0.845169\n",
            "[15]\tvalidation_0-auc:0.845412\n",
            "[16]\tvalidation_0-auc:0.845879\n",
            "[17]\tvalidation_0-auc:0.844961\n",
            "[18]\tvalidation_0-auc:0.844693\n",
            "[19]\tvalidation_0-auc:0.845386\n",
            "[20]\tvalidation_0-auc:0.845282\n",
            "[21]\tvalidation_0-auc:0.845784\n",
            "[22]\tvalidation_0-auc:0.844814\n",
            "[23]\tvalidation_0-auc:0.845619\n",
            "[24]\tvalidation_0-auc:0.845498\n",
            "[25]\tvalidation_0-auc:0.845758\n",
            "[26]\tvalidation_0-auc:0.845992\n",
            "[27]\tvalidation_0-auc:0.846616\n",
            "[28]\tvalidation_0-auc:0.845905\n",
            "[29]\tvalidation_0-auc:0.843757\n",
            "[30]\tvalidation_0-auc:0.842614\n",
            "[31]\tvalidation_0-auc:0.843021\n",
            "[32]\tvalidation_0-auc:0.843393\n",
            "[33]\tvalidation_0-auc:0.843289\n",
            "[34]\tvalidation_0-auc:0.84335\n",
            "[35]\tvalidation_0-auc:0.843532\n",
            "[36]\tvalidation_0-auc:0.843133\n",
            "[37]\tvalidation_0-auc:0.842406\n",
            "[38]\tvalidation_0-auc:0.841912\n",
            "[39]\tvalidation_0-auc:0.841306\n",
            "[40]\tvalidation_0-auc:0.84147\n",
            "[41]\tvalidation_0-auc:0.841531\n",
            "[42]\tvalidation_0-auc:0.842033\n",
            "[43]\tvalidation_0-auc:0.841583\n",
            "[44]\tvalidation_0-auc:0.842172\n",
            "[45]\tvalidation_0-auc:0.842562\n",
            "[46]\tvalidation_0-auc:0.842388\n",
            "[47]\tvalidation_0-auc:0.842302\n",
            "[48]\tvalidation_0-auc:0.841999\n",
            "[49]\tvalidation_0-auc:0.841141\n",
            "[50]\tvalidation_0-auc:0.840933\n",
            "[51]\tvalidation_0-auc:0.840708\n",
            "[52]\tvalidation_0-auc:0.84037\n",
            "[53]\tvalidation_0-auc:0.840301\n",
            "[54]\tvalidation_0-auc:0.840145\n",
            "[55]\tvalidation_0-auc:0.839885\n",
            "[56]\tvalidation_0-auc:0.839816\n",
            "[57]\tvalidation_0-auc:0.839331\n",
            "[58]\tvalidation_0-auc:0.83992\n",
            "[59]\tvalidation_0-auc:0.839746\n",
            "[60]\tvalidation_0-auc:0.839123\n",
            "[61]\tvalidation_0-auc:0.838742\n",
            "[62]\tvalidation_0-auc:0.83862\n",
            "[63]\tvalidation_0-auc:0.838395\n",
            "[64]\tvalidation_0-auc:0.838542\n",
            "[65]\tvalidation_0-auc:0.838525\n",
            "[66]\tvalidation_0-auc:0.837901\n",
            "[67]\tvalidation_0-auc:0.837641\n",
            "[68]\tvalidation_0-auc:0.837226\n",
            "[69]\tvalidation_0-auc:0.837434\n",
            "[70]\tvalidation_0-auc:0.836637\n",
            "[71]\tvalidation_0-auc:0.837087\n",
            "[72]\tvalidation_0-auc:0.836758\n",
            "[73]\tvalidation_0-auc:0.836567\n",
            "[74]\tvalidation_0-auc:0.836359\n",
            "[75]\tvalidation_0-auc:0.835701\n",
            "[76]\tvalidation_0-auc:0.834506\n",
            "[77]\tvalidation_0-auc:0.834523\n",
            "Stopping. Best iteration:\n",
            "[27]\tvalidation_0-auc:0.846616\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6821 - accuracy: 0.5607 - val_loss: 0.6858 - val_accuracy: 0.5580\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6552 - accuracy: 0.6060 - val_loss: 0.6653 - val_accuracy: 0.6214\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6155 - accuracy: 0.6616 - val_loss: 0.5820 - val_accuracy: 0.7112\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5701 - accuracy: 0.7193 - val_loss: 0.5670 - val_accuracy: 0.7352\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5386 - accuracy: 0.7467 - val_loss: 0.5527 - val_accuracy: 0.7199\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6459 - accuracy: 0.6177 - val_loss: 0.5949 - val_accuracy: 0.6718\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5535 - accuracy: 0.7261 - val_loss: 0.5618 - val_accuracy: 0.7199\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5333 - accuracy: 0.7385 - val_loss: 0.5526 - val_accuracy: 0.7177\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5131 - accuracy: 0.7570 - val_loss: 0.5819 - val_accuracy: 0.7155\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5102 - accuracy: 0.7714 - val_loss: 0.5506 - val_accuracy: 0.7396\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.699837\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.699971\n",
            "[2]\tvalidation_0-auc:0.724162\n",
            "[3]\tvalidation_0-auc:0.725356\n",
            "[4]\tvalidation_0-auc:0.725573\n",
            "[5]\tvalidation_0-auc:0.727766\n",
            "[6]\tvalidation_0-auc:0.733368\n",
            "[7]\tvalidation_0-auc:0.74103\n",
            "[8]\tvalidation_0-auc:0.743677\n",
            "[9]\tvalidation_0-auc:0.743656\n",
            "[10]\tvalidation_0-auc:0.748785\n",
            "[11]\tvalidation_0-auc:0.751493\n",
            "[12]\tvalidation_0-auc:0.747745\n",
            "[13]\tvalidation_0-auc:0.74827\n",
            "[14]\tvalidation_0-auc:0.746385\n",
            "[15]\tvalidation_0-auc:0.746303\n",
            "[16]\tvalidation_0-auc:0.745211\n",
            "[17]\tvalidation_0-auc:0.747971\n",
            "[18]\tvalidation_0-auc:0.747992\n",
            "[19]\tvalidation_0-auc:0.751617\n",
            "[20]\tvalidation_0-auc:0.747724\n",
            "[21]\tvalidation_0-auc:0.748115\n",
            "[22]\tvalidation_0-auc:0.75069\n",
            "[23]\tvalidation_0-auc:0.752647\n",
            "[24]\tvalidation_0-auc:0.756663\n",
            "[25]\tvalidation_0-auc:0.756714\n",
            "[26]\tvalidation_0-auc:0.75622\n",
            "[27]\tvalidation_0-auc:0.755108\n",
            "[28]\tvalidation_0-auc:0.756941\n",
            "[29]\tvalidation_0-auc:0.758259\n",
            "[30]\tvalidation_0-auc:0.757126\n",
            "[31]\tvalidation_0-auc:0.756539\n",
            "[32]\tvalidation_0-auc:0.756169\n",
            "[33]\tvalidation_0-auc:0.756086\n",
            "[34]\tvalidation_0-auc:0.756035\n",
            "[35]\tvalidation_0-auc:0.758239\n",
            "[36]\tvalidation_0-auc:0.758053\n",
            "[37]\tvalidation_0-auc:0.758578\n",
            "[38]\tvalidation_0-auc:0.759155\n",
            "[39]\tvalidation_0-auc:0.758723\n",
            "[40]\tvalidation_0-auc:0.761915\n",
            "[41]\tvalidation_0-auc:0.762502\n",
            "[42]\tvalidation_0-auc:0.762749\n",
            "[43]\tvalidation_0-auc:0.762111\n",
            "[44]\tvalidation_0-auc:0.762646\n",
            "[45]\tvalidation_0-auc:0.761791\n",
            "[46]\tvalidation_0-auc:0.76348\n",
            "[47]\tvalidation_0-auc:0.765437\n",
            "[48]\tvalidation_0-auc:0.764181\n",
            "[49]\tvalidation_0-auc:0.763604\n",
            "[50]\tvalidation_0-auc:0.762986\n",
            "[51]\tvalidation_0-auc:0.76313\n",
            "[52]\tvalidation_0-auc:0.763449\n",
            "[53]\tvalidation_0-auc:0.764356\n",
            "[54]\tvalidation_0-auc:0.764624\n",
            "[55]\tvalidation_0-auc:0.764294\n",
            "[56]\tvalidation_0-auc:0.765715\n",
            "[57]\tvalidation_0-auc:0.765736\n",
            "[58]\tvalidation_0-auc:0.765715\n",
            "[59]\tvalidation_0-auc:0.764912\n",
            "[60]\tvalidation_0-auc:0.766415\n",
            "[61]\tvalidation_0-auc:0.766354\n",
            "[62]\tvalidation_0-auc:0.767651\n",
            "[63]\tvalidation_0-auc:0.76831\n",
            "[64]\tvalidation_0-auc:0.768599\n",
            "[65]\tvalidation_0-auc:0.770061\n",
            "[66]\tvalidation_0-auc:0.770514\n",
            "[67]\tvalidation_0-auc:0.76969\n",
            "[68]\tvalidation_0-auc:0.769917\n",
            "[69]\tvalidation_0-auc:0.770391\n",
            "[70]\tvalidation_0-auc:0.770329\n",
            "[71]\tvalidation_0-auc:0.770308\n",
            "[72]\tvalidation_0-auc:0.770452\n",
            "[73]\tvalidation_0-auc:0.770452\n",
            "[74]\tvalidation_0-auc:0.769567\n",
            "[75]\tvalidation_0-auc:0.768619\n",
            "[76]\tvalidation_0-auc:0.768146\n",
            "[77]\tvalidation_0-auc:0.768104\n",
            "[78]\tvalidation_0-auc:0.768599\n",
            "[79]\tvalidation_0-auc:0.770308\n",
            "[80]\tvalidation_0-auc:0.76864\n",
            "[81]\tvalidation_0-auc:0.768588\n",
            "[82]\tvalidation_0-auc:0.768259\n",
            "[83]\tvalidation_0-auc:0.768279\n",
            "[84]\tvalidation_0-auc:0.769227\n",
            "[85]\tvalidation_0-auc:0.768835\n",
            "[86]\tvalidation_0-auc:0.769701\n",
            "[87]\tvalidation_0-auc:0.770483\n",
            "[88]\tvalidation_0-auc:0.770772\n",
            "[89]\tvalidation_0-auc:0.77106\n",
            "[90]\tvalidation_0-auc:0.771719\n",
            "[91]\tvalidation_0-auc:0.771348\n",
            "[92]\tvalidation_0-auc:0.77141\n",
            "[93]\tvalidation_0-auc:0.771101\n",
            "[94]\tvalidation_0-auc:0.771122\n",
            "[95]\tvalidation_0-auc:0.771019\n",
            "[96]\tvalidation_0-auc:0.77141\n",
            "[97]\tvalidation_0-auc:0.772049\n",
            "[98]\tvalidation_0-auc:0.772543\n",
            "[99]\tvalidation_0-auc:0.772512\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.773469387755102  | 0.7193877551020408 | 0.7157360406091371 | 0.717557251908397  |\n",
            "|     GRU 0.1      | 0.7673469387755102 | 0.6751054852320675 | 0.8121827411167513 | 0.7373271889400922 |\n",
            "|   XGBoost 0.1    | 0.7816326530612245 | 0.7586206896551724 | 0.6700507614213198 | 0.7115902964959568 |\n",
            "|    Logreg 0.1    | 0.7877551020408163 | 0.7961783439490446 | 0.6345177664974619 | 0.7062146892655368 |\n",
            "|     SVM 0.1      | 0.7755102040816326 | 0.7636363636363637 | 0.6395939086294417 | 0.696132596685083  |\n",
            "|  LSTM beta 0.1   | 0.7199124726477024 | 0.6754385964912281 | 0.4583333333333333 | 0.5460992907801417 |\n",
            "|   GRU beta 0.1   | 0.7396061269146609 | 0.6467065868263473 | 0.6428571428571429 | 0.6447761194029851 |\n",
            "| XGBoost beta 0.1 | 0.7286652078774617 |       0.625        | 0.6547619047619048 | 0.6395348837209303 |\n",
            "| logreg beta 0.1  | 0.7199124726477024 | 0.644927536231884  | 0.5297619047619048 | 0.5816993464052287 |\n",
            "|   svm beta 0.1   | 0.7111597374179431 | 0.618421052631579  | 0.5595238095238095 |       0.5875       |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6927 - accuracy: 0.5389 - val_loss: 0.7016 - val_accuracy: 0.3184\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6381 - accuracy: 0.6463 - val_loss: 0.5936 - val_accuracy: 0.7510\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5953 - accuracy: 0.6899 - val_loss: 0.5915 - val_accuracy: 0.6796\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5870 - accuracy: 0.6926 - val_loss: 0.5606 - val_accuracy: 0.7510\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5857 - accuracy: 0.6879 - val_loss: 0.5626 - val_accuracy: 0.7163\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6390 - accuracy: 0.6201 - val_loss: 0.5433 - val_accuracy: 0.7531\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5747 - accuracy: 0.6980 - val_loss: 0.5865 - val_accuracy: 0.6959\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5713 - accuracy: 0.7034 - val_loss: 0.5252 - val_accuracy: 0.7571\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5620 - accuracy: 0.7007 - val_loss: 0.5381 - val_accuracy: 0.7429\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5564 - accuracy: 0.7168 - val_loss: 0.5740 - val_accuracy: 0.7143\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.786342\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.797189\n",
            "[2]\tvalidation_0-auc:0.793366\n",
            "[3]\tvalidation_0-auc:0.795845\n",
            "[4]\tvalidation_0-auc:0.795322\n",
            "[5]\tvalidation_0-auc:0.797643\n",
            "[6]\tvalidation_0-auc:0.796833\n",
            "[7]\tvalidation_0-auc:0.796764\n",
            "[8]\tvalidation_0-auc:0.798364\n",
            "[9]\tvalidation_0-auc:0.794768\n",
            "[10]\tvalidation_0-auc:0.796349\n",
            "[11]\tvalidation_0-auc:0.797139\n",
            "[12]\tvalidation_0-auc:0.799036\n",
            "[13]\tvalidation_0-auc:0.799105\n",
            "[14]\tvalidation_0-auc:0.798038\n",
            "[15]\tvalidation_0-auc:0.797771\n",
            "[16]\tvalidation_0-auc:0.797643\n",
            "[17]\tvalidation_0-auc:0.796537\n",
            "[18]\tvalidation_0-auc:0.796645\n",
            "[19]\tvalidation_0-auc:0.796744\n",
            "[20]\tvalidation_0-auc:0.79626\n",
            "[21]\tvalidation_0-auc:0.796063\n",
            "[22]\tvalidation_0-auc:0.796112\n",
            "[23]\tvalidation_0-auc:0.795658\n",
            "[24]\tvalidation_0-auc:0.795835\n",
            "[25]\tvalidation_0-auc:0.79546\n",
            "[26]\tvalidation_0-auc:0.794186\n",
            "[27]\tvalidation_0-auc:0.79466\n",
            "[28]\tvalidation_0-auc:0.794601\n",
            "[29]\tvalidation_0-auc:0.794136\n",
            "[30]\tvalidation_0-auc:0.795401\n",
            "[31]\tvalidation_0-auc:0.795006\n",
            "[32]\tvalidation_0-auc:0.79461\n",
            "[33]\tvalidation_0-auc:0.794037\n",
            "[34]\tvalidation_0-auc:0.793741\n",
            "[35]\tvalidation_0-auc:0.793563\n",
            "[36]\tvalidation_0-auc:0.793415\n",
            "[37]\tvalidation_0-auc:0.793079\n",
            "[38]\tvalidation_0-auc:0.792704\n",
            "[39]\tvalidation_0-auc:0.792684\n",
            "[40]\tvalidation_0-auc:0.79142\n",
            "[41]\tvalidation_0-auc:0.791025\n",
            "[42]\tvalidation_0-auc:0.791805\n",
            "[43]\tvalidation_0-auc:0.791706\n",
            "[44]\tvalidation_0-auc:0.791686\n",
            "[45]\tvalidation_0-auc:0.792368\n",
            "[46]\tvalidation_0-auc:0.791736\n",
            "[47]\tvalidation_0-auc:0.791341\n",
            "[48]\tvalidation_0-auc:0.791025\n",
            "[49]\tvalidation_0-auc:0.790955\n",
            "[50]\tvalidation_0-auc:0.790955\n",
            "[51]\tvalidation_0-auc:0.790501\n",
            "[52]\tvalidation_0-auc:0.789553\n",
            "[53]\tvalidation_0-auc:0.789533\n",
            "[54]\tvalidation_0-auc:0.789335\n",
            "[55]\tvalidation_0-auc:0.789335\n",
            "[56]\tvalidation_0-auc:0.789049\n",
            "[57]\tvalidation_0-auc:0.789424\n",
            "[58]\tvalidation_0-auc:0.789088\n",
            "[59]\tvalidation_0-auc:0.788219\n",
            "[60]\tvalidation_0-auc:0.788753\n",
            "[61]\tvalidation_0-auc:0.78814\n",
            "[62]\tvalidation_0-auc:0.788476\n",
            "[63]\tvalidation_0-auc:0.788101\n",
            "Stopping. Best iteration:\n",
            "[13]\tvalidation_0-auc:0.799105\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6493 - accuracy: 0.6012 - val_loss: 0.6604 - val_accuracy: 0.6586\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6023 - accuracy: 0.6767 - val_loss: 0.6494 - val_accuracy: 0.6521\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5471 - accuracy: 0.7213 - val_loss: 0.6049 - val_accuracy: 0.7068\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5375 - accuracy: 0.7440 - val_loss: 0.6135 - val_accuracy: 0.6761\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5321 - accuracy: 0.7406 - val_loss: 0.7089 - val_accuracy: 0.6346\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6227 - accuracy: 0.6452 - val_loss: 0.6328 - val_accuracy: 0.6565\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5314 - accuracy: 0.7392 - val_loss: 0.6462 - val_accuracy: 0.6674\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5245 - accuracy: 0.7358 - val_loss: 0.7094 - val_accuracy: 0.6368\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5213 - accuracy: 0.7454 - val_loss: 0.6978 - val_accuracy: 0.6324\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5168 - accuracy: 0.7358 - val_loss: 0.6071 - val_accuracy: 0.6893\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.697168\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.689734\n",
            "[2]\tvalidation_0-auc:0.695652\n",
            "[3]\tvalidation_0-auc:0.709177\n",
            "[4]\tvalidation_0-auc:0.708306\n",
            "[5]\tvalidation_0-auc:0.707063\n",
            "[6]\tvalidation_0-auc:0.709674\n",
            "[7]\tvalidation_0-auc:0.713179\n",
            "[8]\tvalidation_0-auc:0.72259\n",
            "[9]\tvalidation_0-auc:0.727202\n",
            "[10]\tvalidation_0-auc:0.728942\n",
            "[11]\tvalidation_0-auc:0.729464\n",
            "[12]\tvalidation_0-auc:0.729054\n",
            "[13]\tvalidation_0-auc:0.731465\n",
            "[14]\tvalidation_0-auc:0.730595\n",
            "[15]\tvalidation_0-auc:0.730123\n",
            "[16]\tvalidation_0-auc:0.72888\n",
            "[17]\tvalidation_0-auc:0.729588\n",
            "[18]\tvalidation_0-auc:0.731192\n",
            "[19]\tvalidation_0-auc:0.729812\n",
            "[20]\tvalidation_0-auc:0.728967\n",
            "[21]\tvalidation_0-auc:0.730123\n",
            "[22]\tvalidation_0-auc:0.733044\n",
            "[23]\tvalidation_0-auc:0.732746\n",
            "[24]\tvalidation_0-auc:0.732447\n",
            "[25]\tvalidation_0-auc:0.732162\n",
            "[26]\tvalidation_0-auc:0.734063\n",
            "[27]\tvalidation_0-auc:0.734474\n",
            "[28]\tvalidation_0-auc:0.733753\n",
            "[29]\tvalidation_0-auc:0.732833\n",
            "[30]\tvalidation_0-auc:0.733181\n",
            "[31]\tvalidation_0-auc:0.733653\n",
            "[32]\tvalidation_0-auc:0.734126\n",
            "[33]\tvalidation_0-auc:0.73517\n",
            "[34]\tvalidation_0-auc:0.735791\n",
            "[35]\tvalidation_0-auc:0.735294\n",
            "[36]\tvalidation_0-auc:0.736972\n",
            "[37]\tvalidation_0-auc:0.736624\n",
            "[38]\tvalidation_0-auc:0.735605\n",
            "[39]\tvalidation_0-auc:0.736376\n",
            "[40]\tvalidation_0-auc:0.735729\n",
            "[41]\tvalidation_0-auc:0.73461\n",
            "[42]\tvalidation_0-auc:0.734834\n",
            "[43]\tvalidation_0-auc:0.735456\n",
            "[44]\tvalidation_0-auc:0.736649\n",
            "[45]\tvalidation_0-auc:0.736226\n",
            "[46]\tvalidation_0-auc:0.737022\n",
            "[47]\tvalidation_0-auc:0.736624\n",
            "[48]\tvalidation_0-auc:0.737644\n",
            "[49]\tvalidation_0-auc:0.736947\n",
            "[50]\tvalidation_0-auc:0.73788\n",
            "[51]\tvalidation_0-auc:0.73972\n",
            "[52]\tvalidation_0-auc:0.740117\n",
            "[53]\tvalidation_0-auc:0.739844\n",
            "[54]\tvalidation_0-auc:0.738825\n",
            "[55]\tvalidation_0-auc:0.738999\n",
            "[56]\tvalidation_0-auc:0.740615\n",
            "[57]\tvalidation_0-auc:0.740739\n",
            "[58]\tvalidation_0-auc:0.740565\n",
            "[59]\tvalidation_0-auc:0.740664\n",
            "[60]\tvalidation_0-auc:0.739943\n",
            "[61]\tvalidation_0-auc:0.738054\n",
            "[62]\tvalidation_0-auc:0.738327\n",
            "[63]\tvalidation_0-auc:0.738476\n",
            "[64]\tvalidation_0-auc:0.738626\n",
            "[65]\tvalidation_0-auc:0.739123\n",
            "[66]\tvalidation_0-auc:0.738551\n",
            "[67]\tvalidation_0-auc:0.738775\n",
            "[68]\tvalidation_0-auc:0.737979\n",
            "[69]\tvalidation_0-auc:0.739794\n",
            "[70]\tvalidation_0-auc:0.740291\n",
            "[71]\tvalidation_0-auc:0.741684\n",
            "[72]\tvalidation_0-auc:0.742628\n",
            "[73]\tvalidation_0-auc:0.741485\n",
            "[74]\tvalidation_0-auc:0.741907\n",
            "[75]\tvalidation_0-auc:0.739869\n",
            "[76]\tvalidation_0-auc:0.739446\n",
            "[77]\tvalidation_0-auc:0.739123\n",
            "[78]\tvalidation_0-auc:0.740341\n",
            "[79]\tvalidation_0-auc:0.740739\n",
            "[80]\tvalidation_0-auc:0.74049\n",
            "[81]\tvalidation_0-auc:0.740963\n",
            "[82]\tvalidation_0-auc:0.741261\n",
            "[83]\tvalidation_0-auc:0.740739\n",
            "[84]\tvalidation_0-auc:0.740813\n",
            "[85]\tvalidation_0-auc:0.742032\n",
            "[86]\tvalidation_0-auc:0.741584\n",
            "[87]\tvalidation_0-auc:0.740938\n",
            "[88]\tvalidation_0-auc:0.741087\n",
            "[89]\tvalidation_0-auc:0.741162\n",
            "[90]\tvalidation_0-auc:0.740167\n",
            "[91]\tvalidation_0-auc:0.740863\n",
            "[92]\tvalidation_0-auc:0.740789\n",
            "[93]\tvalidation_0-auc:0.740615\n",
            "[94]\tvalidation_0-auc:0.739297\n",
            "[95]\tvalidation_0-auc:0.738029\n",
            "[96]\tvalidation_0-auc:0.738327\n",
            "[97]\tvalidation_0-auc:0.738302\n",
            "[98]\tvalidation_0-auc:0.738278\n",
            "[99]\tvalidation_0-auc:0.738029\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|     LSTM 0.2     | 0.7163265306122449 |  0.5211267605633803 |        0.75        | 0.6149584487534626 |\n",
            "|     GRU 0.2      | 0.7142857142857143 |  0.5173913043478261 | 0.8040540540540541 | 0.6296296296296297 |\n",
            "|   XGBoost 0.2    | 0.7551020408163265 |  0.5760869565217391 | 0.7162162162162162 | 0.6385542168674698 |\n",
            "|    Logreg 0.2    | 0.736734693877551  |  0.5458937198067633 | 0.7635135135135135 | 0.6366197183098592 |\n",
            "|     SVM 0.2      | 0.753061224489796  |  0.5685279187817259 | 0.7567567567567568 | 0.6492753623188405 |\n",
            "|  LSTM beta 0.2   | 0.6345733041575492 | 0.38571428571428573 | 0.680672268907563  | 0.4924012158054712 |\n",
            "|   GRU beta 0.2   | 0.6892778993435449 | 0.43352601156069365 | 0.6302521008403361 | 0.5136986301369864 |\n",
            "| XGBoost beta 0.2 | 0.649890590809628  | 0.40375586854460094 | 0.7226890756302521 | 0.5180722891566265 |\n",
            "| logreg beta 0.2  | 0.6892778993435449 |  0.4327485380116959 | 0.6218487394957983 | 0.5103448275862069 |\n",
            "|   svm beta 0.2   | 0.6389496717724289 | 0.38613861386138615 | 0.6554621848739496 | 0.4859813084112149 |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6653 - accuracy: 0.5738 - val_loss: 0.5993 - val_accuracy: 0.7469\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6102 - accuracy: 0.6779 - val_loss: 0.5565 - val_accuracy: 0.7408\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5926 - accuracy: 0.6886 - val_loss: 0.5522 - val_accuracy: 0.7571\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5711 - accuracy: 0.6926 - val_loss: 0.5343 - val_accuracy: 0.7776\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5482 - accuracy: 0.7188 - val_loss: 0.5419 - val_accuracy: 0.7551\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 18ms/step - loss: 0.6342 - accuracy: 0.6329 - val_loss: 0.5684 - val_accuracy: 0.7143\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5438 - accuracy: 0.7289 - val_loss: 0.5330 - val_accuracy: 0.7408\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5382 - accuracy: 0.7242 - val_loss: 0.5278 - val_accuracy: 0.7388\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5318 - accuracy: 0.7342 - val_loss: 0.6054 - val_accuracy: 0.6694\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5209 - accuracy: 0.7537 - val_loss: 0.5048 - val_accuracy: 0.7612\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.808397\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.80781\n",
            "[2]\tvalidation_0-auc:0.808262\n",
            "[3]\tvalidation_0-auc:0.810076\n",
            "[4]\tvalidation_0-auc:0.816005\n",
            "[5]\tvalidation_0-auc:0.818541\n",
            "[6]\tvalidation_0-auc:0.81874\n",
            "[7]\tvalidation_0-auc:0.821447\n",
            "[8]\tvalidation_0-auc:0.821492\n",
            "[9]\tvalidation_0-auc:0.820752\n",
            "[10]\tvalidation_0-auc:0.822115\n",
            "[11]\tvalidation_0-auc:0.822702\n",
            "[12]\tvalidation_0-auc:0.822973\n",
            "[13]\tvalidation_0-auc:0.823947\n",
            "[14]\tvalidation_0-auc:0.825364\n",
            "[15]\tvalidation_0-auc:0.825815\n",
            "[16]\tvalidation_0-auc:0.826835\n",
            "[17]\tvalidation_0-auc:0.826799\n",
            "[18]\tvalidation_0-auc:0.827034\n",
            "[19]\tvalidation_0-auc:0.827828\n",
            "[20]\tvalidation_0-auc:0.828415\n",
            "[21]\tvalidation_0-auc:0.828397\n",
            "[22]\tvalidation_0-auc:0.829119\n",
            "[23]\tvalidation_0-auc:0.828965\n",
            "[24]\tvalidation_0-auc:0.829904\n",
            "[25]\tvalidation_0-auc:0.829046\n",
            "[26]\tvalidation_0-auc:0.82929\n",
            "[27]\tvalidation_0-auc:0.827503\n",
            "[28]\tvalidation_0-auc:0.828234\n",
            "[29]\tvalidation_0-auc:0.828126\n",
            "[30]\tvalidation_0-auc:0.827954\n",
            "[31]\tvalidation_0-auc:0.827223\n",
            "[32]\tvalidation_0-auc:0.827566\n",
            "[33]\tvalidation_0-auc:0.827684\n",
            "[34]\tvalidation_0-auc:0.827729\n",
            "[35]\tvalidation_0-auc:0.827458\n",
            "[36]\tvalidation_0-auc:0.826664\n",
            "[37]\tvalidation_0-auc:0.825978\n",
            "[38]\tvalidation_0-auc:0.826501\n",
            "[39]\tvalidation_0-auc:0.825743\n",
            "[40]\tvalidation_0-auc:0.825689\n",
            "[41]\tvalidation_0-auc:0.826014\n",
            "[42]\tvalidation_0-auc:0.825382\n",
            "[43]\tvalidation_0-auc:0.824588\n",
            "[44]\tvalidation_0-auc:0.824768\n",
            "[45]\tvalidation_0-auc:0.824732\n",
            "[46]\tvalidation_0-auc:0.824155\n",
            "[47]\tvalidation_0-auc:0.823451\n",
            "[48]\tvalidation_0-auc:0.823487\n",
            "[49]\tvalidation_0-auc:0.822973\n",
            "[50]\tvalidation_0-auc:0.823063\n",
            "[51]\tvalidation_0-auc:0.822991\n",
            "[52]\tvalidation_0-auc:0.822359\n",
            "[53]\tvalidation_0-auc:0.823162\n",
            "[54]\tvalidation_0-auc:0.823397\n",
            "[55]\tvalidation_0-auc:0.822945\n",
            "[56]\tvalidation_0-auc:0.823649\n",
            "[57]\tvalidation_0-auc:0.822512\n",
            "[58]\tvalidation_0-auc:0.822684\n",
            "[59]\tvalidation_0-auc:0.8229\n",
            "[60]\tvalidation_0-auc:0.823063\n",
            "[61]\tvalidation_0-auc:0.822828\n",
            "[62]\tvalidation_0-auc:0.822467\n",
            "[63]\tvalidation_0-auc:0.822404\n",
            "[64]\tvalidation_0-auc:0.822945\n",
            "[65]\tvalidation_0-auc:0.823126\n",
            "[66]\tvalidation_0-auc:0.823027\n",
            "[67]\tvalidation_0-auc:0.823171\n",
            "[68]\tvalidation_0-auc:0.822846\n",
            "[69]\tvalidation_0-auc:0.823009\n",
            "[70]\tvalidation_0-auc:0.82263\n",
            "[71]\tvalidation_0-auc:0.823207\n",
            "[72]\tvalidation_0-auc:0.823009\n",
            "[73]\tvalidation_0-auc:0.823424\n",
            "[74]\tvalidation_0-auc:0.823478\n",
            "Stopping. Best iteration:\n",
            "[24]\tvalidation_0-auc:0.829904\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6834 - accuracy: 0.5525 - val_loss: 0.7191 - val_accuracy: 0.5624\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6779 - accuracy: 0.5827 - val_loss: 0.6469 - val_accuracy: 0.6871\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6072 - accuracy: 0.6815 - val_loss: 0.5650 - val_accuracy: 0.7330\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5461 - accuracy: 0.7316 - val_loss: 0.6816 - val_accuracy: 0.6258\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5118 - accuracy: 0.7461 - val_loss: 0.6224 - val_accuracy: 0.7046\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6341 - accuracy: 0.6266 - val_loss: 0.6132 - val_accuracy: 0.6565\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5339 - accuracy: 0.7426 - val_loss: 0.6459 - val_accuracy: 0.6630\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5211 - accuracy: 0.7481 - val_loss: 0.6694 - val_accuracy: 0.6630\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5108 - accuracy: 0.7522 - val_loss: 0.6196 - val_accuracy: 0.6674\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5003 - accuracy: 0.7509 - val_loss: 0.6863 - val_accuracy: 0.6499\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.688019\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.722208\n",
            "[2]\tvalidation_0-auc:0.712346\n",
            "[3]\tvalidation_0-auc:0.693705\n",
            "[4]\tvalidation_0-auc:0.702353\n",
            "[5]\tvalidation_0-auc:0.707076\n",
            "[6]\tvalidation_0-auc:0.711744\n",
            "[7]\tvalidation_0-auc:0.697652\n",
            "[8]\tvalidation_0-auc:0.70654\n",
            "[9]\tvalidation_0-auc:0.708486\n",
            "[10]\tvalidation_0-auc:0.71122\n",
            "[11]\tvalidation_0-auc:0.717277\n",
            "[12]\tvalidation_0-auc:0.713395\n",
            "[13]\tvalidation_0-auc:0.71017\n",
            "[14]\tvalidation_0-auc:0.709995\n",
            "[15]\tvalidation_0-auc:0.707459\n",
            "[16]\tvalidation_0-auc:0.708388\n",
            "[17]\tvalidation_0-auc:0.707655\n",
            "[18]\tvalidation_0-auc:0.711602\n",
            "[19]\tvalidation_0-auc:0.712554\n",
            "[20]\tvalidation_0-auc:0.711898\n",
            "[21]\tvalidation_0-auc:0.711657\n",
            "[22]\tvalidation_0-auc:0.712991\n",
            "[23]\tvalidation_0-auc:0.713833\n",
            "[24]\tvalidation_0-auc:0.71403\n",
            "[25]\tvalidation_0-auc:0.715855\n",
            "[26]\tvalidation_0-auc:0.71544\n",
            "[27]\tvalidation_0-auc:0.717386\n",
            "[28]\tvalidation_0-auc:0.717474\n",
            "[29]\tvalidation_0-auc:0.718611\n",
            "[30]\tvalidation_0-auc:0.719267\n",
            "[31]\tvalidation_0-auc:0.719868\n",
            "[32]\tvalidation_0-auc:0.719704\n",
            "[33]\tvalidation_0-auc:0.71895\n",
            "[34]\tvalidation_0-auc:0.721049\n",
            "[35]\tvalidation_0-auc:0.722885\n",
            "[36]\tvalidation_0-auc:0.722361\n",
            "[37]\tvalidation_0-auc:0.725225\n",
            "[38]\tvalidation_0-auc:0.72516\n",
            "[39]\tvalidation_0-auc:0.723126\n",
            "[40]\tvalidation_0-auc:0.725925\n",
            "[41]\tvalidation_0-auc:0.72575\n",
            "[42]\tvalidation_0-auc:0.726504\n",
            "[43]\tvalidation_0-auc:0.726297\n",
            "[44]\tvalidation_0-auc:0.726319\n",
            "[45]\tvalidation_0-auc:0.725182\n",
            "[46]\tvalidation_0-auc:0.72516\n",
            "[47]\tvalidation_0-auc:0.725739\n",
            "[48]\tvalidation_0-auc:0.725433\n",
            "[49]\tvalidation_0-auc:0.725739\n",
            "[50]\tvalidation_0-auc:0.727204\n",
            "[51]\tvalidation_0-auc:0.727816\n",
            "[52]\tvalidation_0-auc:0.728735\n",
            "[53]\tvalidation_0-auc:0.728604\n",
            "[54]\tvalidation_0-auc:0.729773\n",
            "[55]\tvalidation_0-auc:0.72962\n",
            "[56]\tvalidation_0-auc:0.729314\n",
            "[57]\tvalidation_0-auc:0.729686\n",
            "[58]\tvalidation_0-auc:0.730408\n",
            "[59]\tvalidation_0-auc:0.730484\n",
            "[60]\tvalidation_0-auc:0.730375\n",
            "[61]\tvalidation_0-auc:0.729981\n",
            "[62]\tvalidation_0-auc:0.727795\n",
            "[63]\tvalidation_0-auc:0.72751\n",
            "[64]\tvalidation_0-auc:0.728604\n",
            "[65]\tvalidation_0-auc:0.729019\n",
            "[66]\tvalidation_0-auc:0.729675\n",
            "[67]\tvalidation_0-auc:0.730112\n",
            "[68]\tvalidation_0-auc:0.730069\n",
            "[69]\tvalidation_0-auc:0.730987\n",
            "[70]\tvalidation_0-auc:0.731206\n",
            "[71]\tvalidation_0-auc:0.732059\n",
            "[72]\tvalidation_0-auc:0.733567\n",
            "[73]\tvalidation_0-auc:0.732999\n",
            "[74]\tvalidation_0-auc:0.732999\n",
            "[75]\tvalidation_0-auc:0.733611\n",
            "[76]\tvalidation_0-auc:0.734923\n",
            "[77]\tvalidation_0-auc:0.734857\n",
            "[78]\tvalidation_0-auc:0.734639\n",
            "[79]\tvalidation_0-auc:0.734267\n",
            "[80]\tvalidation_0-auc:0.735295\n",
            "[81]\tvalidation_0-auc:0.735273\n",
            "[82]\tvalidation_0-auc:0.735732\n",
            "[83]\tvalidation_0-auc:0.735667\n",
            "[84]\tvalidation_0-auc:0.734879\n",
            "[85]\tvalidation_0-auc:0.734201\n",
            "[86]\tvalidation_0-auc:0.733677\n",
            "[87]\tvalidation_0-auc:0.733436\n",
            "[88]\tvalidation_0-auc:0.733589\n",
            "[89]\tvalidation_0-auc:0.733655\n",
            "[90]\tvalidation_0-auc:0.732999\n",
            "[91]\tvalidation_0-auc:0.733196\n",
            "[92]\tvalidation_0-auc:0.733611\n",
            "[93]\tvalidation_0-auc:0.732102\n",
            "[94]\tvalidation_0-auc:0.732299\n",
            "[95]\tvalidation_0-auc:0.732255\n",
            "[96]\tvalidation_0-auc:0.731523\n",
            "[97]\tvalidation_0-auc:0.732441\n",
            "[98]\tvalidation_0-auc:0.732179\n",
            "[99]\tvalidation_0-auc:0.730561\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.7551020408163265 |  0.6476683937823834 | 0.7062146892655368 | 0.6756756756756755 |\n",
            "|      GRU 0.15     | 0.7612244897959184 |         0.65        | 0.7344632768361582 | 0.6896551724137931 |\n",
            "|    XGBoost 0.15   | 0.7714285714285715 |  0.6701570680628273 | 0.7231638418079096 | 0.6956521739130435 |\n",
            "|    Logreg 0.15    | 0.7551020408163265 |  0.6338028169014085 | 0.7627118644067796 | 0.6923076923076923 |\n",
            "|      SVM 0.15     | 0.773469387755102  |        0.665        | 0.751412429378531  | 0.7055702917771882 |\n",
            "|   LSTM beta 0.15  | 0.7045951859956237 |  0.5333333333333333 | 0.7027027027027027 | 0.6064139941690962 |\n",
            "|   GRU beta 0.15   | 0.649890590809628  | 0.47391304347826085 | 0.7364864864864865 | 0.5767195767195767 |\n",
            "| XGBoost beta 0.15 | 0.6630196936542669 |  0.4861111111111111 | 0.7094594594594594 | 0.5769230769230769 |\n",
            "|  logreg beta 0.15 | 0.6958424507658644 |  0.5243243243243243 | 0.6554054054054054 | 0.5825825825825827 |\n",
            "|   svm beta 0.15   | 0.6783369803063457 |  0.5024875621890548 | 0.6824324324324325 | 0.5787965616045846 |\n",
            "+-------------------+--------------------+---------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6vXIbh7Xhm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6e3658a-9fde-4d00-bcdf-422ae19885a5"
      },
      "source": [
        "Result_cross.to_csv('NKTR_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.719388</td>\n",
              "      <td>0.773469</td>\n",
              "      <td>0.717557</td>\n",
              "      <td>0.715736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.675105</td>\n",
              "      <td>0.767347</td>\n",
              "      <td>0.737327</td>\n",
              "      <td>0.812183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.758621</td>\n",
              "      <td>0.781633</td>\n",
              "      <td>0.711590</td>\n",
              "      <td>0.670051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.796178</td>\n",
              "      <td>0.787755</td>\n",
              "      <td>0.706215</td>\n",
              "      <td>0.634518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.763636</td>\n",
              "      <td>0.775510</td>\n",
              "      <td>0.696133</td>\n",
              "      <td>0.639594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.675439</td>\n",
              "      <td>0.719912</td>\n",
              "      <td>0.546099</td>\n",
              "      <td>0.458333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.646707</td>\n",
              "      <td>0.739606</td>\n",
              "      <td>0.644776</td>\n",
              "      <td>0.642857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.728665</td>\n",
              "      <td>0.639535</td>\n",
              "      <td>0.654762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.644928</td>\n",
              "      <td>0.719912</td>\n",
              "      <td>0.581699</td>\n",
              "      <td>0.529762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.618421</td>\n",
              "      <td>0.711160</td>\n",
              "      <td>0.587500</td>\n",
              "      <td>0.559524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.521127</td>\n",
              "      <td>0.716327</td>\n",
              "      <td>0.614958</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.517391</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.629630</td>\n",
              "      <td>0.804054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.576087</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.638554</td>\n",
              "      <td>0.716216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.545894</td>\n",
              "      <td>0.736735</td>\n",
              "      <td>0.636620</td>\n",
              "      <td>0.763514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.568528</td>\n",
              "      <td>0.753061</td>\n",
              "      <td>0.649275</td>\n",
              "      <td>0.756757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.385714</td>\n",
              "      <td>0.634573</td>\n",
              "      <td>0.492401</td>\n",
              "      <td>0.680672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.433526</td>\n",
              "      <td>0.689278</td>\n",
              "      <td>0.513699</td>\n",
              "      <td>0.630252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.403756</td>\n",
              "      <td>0.649891</td>\n",
              "      <td>0.518072</td>\n",
              "      <td>0.722689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.432749</td>\n",
              "      <td>0.689278</td>\n",
              "      <td>0.510345</td>\n",
              "      <td>0.621849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.386139</td>\n",
              "      <td>0.638950</td>\n",
              "      <td>0.485981</td>\n",
              "      <td>0.655462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.647668</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.675676</td>\n",
              "      <td>0.706215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.761224</td>\n",
              "      <td>0.689655</td>\n",
              "      <td>0.734463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.670157</td>\n",
              "      <td>0.771429</td>\n",
              "      <td>0.695652</td>\n",
              "      <td>0.723164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.633803</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.762712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.665000</td>\n",
              "      <td>0.773469</td>\n",
              "      <td>0.705570</td>\n",
              "      <td>0.751412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.704595</td>\n",
              "      <td>0.606414</td>\n",
              "      <td>0.702703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.473913</td>\n",
              "      <td>0.649891</td>\n",
              "      <td>0.576720</td>\n",
              "      <td>0.736486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.486111</td>\n",
              "      <td>0.663020</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.709459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.524324</td>\n",
              "      <td>0.695842</td>\n",
              "      <td>0.582583</td>\n",
              "      <td>0.655405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.502488</td>\n",
              "      <td>0.678337</td>\n",
              "      <td>0.578797</td>\n",
              "      <td>0.682432</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  NKTR  0.719388  0.773469  0.717557  0.715736\n",
              "1            GRU 0.1  NKTR  0.675105  0.767347  0.737327  0.812183\n",
              "2        XGBoost 0.1  NKTR  0.758621  0.781633  0.711590  0.670051\n",
              "3         Logreg 0.1  NKTR  0.796178  0.787755  0.706215  0.634518\n",
              "4            SVM 0.1  NKTR  0.763636  0.775510  0.696133  0.639594\n",
              "5      LSTM beta 0.1  NKTR  0.675439  0.719912  0.546099  0.458333\n",
              "6       GRU beta 0.1  NKTR  0.646707  0.739606  0.644776  0.642857\n",
              "7   XGBoost beta 0.1  NKTR  0.625000  0.728665  0.639535  0.654762\n",
              "8    logreg beta 0.1  NKTR  0.644928  0.719912  0.581699  0.529762\n",
              "9       svm beta 0.1  NKTR  0.618421  0.711160  0.587500  0.559524\n",
              "0           LSTM 0.2  NKTR  0.521127  0.716327  0.614958  0.750000\n",
              "1            GRU 0.2  NKTR  0.517391  0.714286  0.629630  0.804054\n",
              "2        XGBoost 0.2  NKTR  0.576087  0.755102  0.638554  0.716216\n",
              "3         Logreg 0.2  NKTR  0.545894  0.736735  0.636620  0.763514\n",
              "4            SVM 0.2  NKTR  0.568528  0.753061  0.649275  0.756757\n",
              "5      LSTM beta 0.2  NKTR  0.385714  0.634573  0.492401  0.680672\n",
              "6       GRU beta 0.2  NKTR  0.433526  0.689278  0.513699  0.630252\n",
              "7   XGBoost beta 0.2  NKTR  0.403756  0.649891  0.518072  0.722689\n",
              "8    logreg beta 0.2  NKTR  0.432749  0.689278  0.510345  0.621849\n",
              "9       svm beta 0.2  NKTR  0.386139  0.638950  0.485981  0.655462\n",
              "0          LSTM 0.15  NKTR  0.647668  0.755102  0.675676  0.706215\n",
              "1           GRU 0.15  NKTR  0.650000  0.761224  0.689655  0.734463\n",
              "2       XGBoost 0.15  NKTR  0.670157  0.771429  0.695652  0.723164\n",
              "3        Logreg 0.15  NKTR  0.633803  0.755102  0.692308  0.762712\n",
              "4           SVM 0.15  NKTR  0.665000  0.773469  0.705570  0.751412\n",
              "5     LSTM beta 0.15  NKTR  0.533333  0.704595  0.606414  0.702703\n",
              "6      GRU beta 0.15  NKTR  0.473913  0.649891  0.576720  0.736486\n",
              "7  XGBoost beta 0.15  NKTR  0.486111  0.663020  0.576923  0.709459\n",
              "8   logreg beta 0.15  NKTR  0.524324  0.695842  0.582583  0.655405\n",
              "9      svm beta 0.15  NKTR  0.502488  0.678337  0.578797  0.682432"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nyXK2FzXhm7"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdr43dCDXhm7"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4bGvgBuXhm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f01719f-7517-4868-9236-34648f98973d"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"NKTR\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6870 - accuracy: 0.5483 - val_loss: 0.6636 - val_accuracy: 0.6041\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6231 - accuracy: 0.6624 - val_loss: 0.5894 - val_accuracy: 0.7286\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5580 - accuracy: 0.7302 - val_loss: 0.5166 - val_accuracy: 0.7653\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5140 - accuracy: 0.7584 - val_loss: 0.4839 - val_accuracy: 0.7735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5217 - accuracy: 0.7584 - val_loss: 0.4849 - val_accuracy: 0.7633\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6212 - accuracy: 0.6604 - val_loss: 0.5433 - val_accuracy: 0.7551\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5380 - accuracy: 0.7483 - val_loss: 0.4937 - val_accuracy: 0.7755\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5059 - accuracy: 0.7644 - val_loss: 0.4875 - val_accuracy: 0.7776\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5092 - accuracy: 0.7651 - val_loss: 0.4758 - val_accuracy: 0.7735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5034 - accuracy: 0.7544 - val_loss: 0.4754 - val_accuracy: 0.7714\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.820074\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.820299\n",
            "[2]\tvalidation_0-auc:0.827489\n",
            "[3]\tvalidation_0-auc:0.835666\n",
            "[4]\tvalidation_0-auc:0.840621\n",
            "[5]\tvalidation_0-auc:0.839868\n",
            "[6]\tvalidation_0-auc:0.838594\n",
            "[7]\tvalidation_0-auc:0.841011\n",
            "[8]\tvalidation_0-auc:0.841817\n",
            "[9]\tvalidation_0-auc:0.842943\n",
            "[10]\tvalidation_0-auc:0.843385\n",
            "[11]\tvalidation_0-auc:0.843515\n",
            "[12]\tvalidation_0-auc:0.844181\n",
            "[13]\tvalidation_0-auc:0.844415\n",
            "[14]\tvalidation_0-auc:0.845169\n",
            "[15]\tvalidation_0-auc:0.845412\n",
            "[16]\tvalidation_0-auc:0.845879\n",
            "[17]\tvalidation_0-auc:0.844961\n",
            "[18]\tvalidation_0-auc:0.844693\n",
            "[19]\tvalidation_0-auc:0.845386\n",
            "[20]\tvalidation_0-auc:0.845282\n",
            "[21]\tvalidation_0-auc:0.845784\n",
            "[22]\tvalidation_0-auc:0.844814\n",
            "[23]\tvalidation_0-auc:0.845619\n",
            "[24]\tvalidation_0-auc:0.845498\n",
            "[25]\tvalidation_0-auc:0.845758\n",
            "[26]\tvalidation_0-auc:0.845992\n",
            "[27]\tvalidation_0-auc:0.846616\n",
            "[28]\tvalidation_0-auc:0.845905\n",
            "[29]\tvalidation_0-auc:0.843757\n",
            "[30]\tvalidation_0-auc:0.842614\n",
            "[31]\tvalidation_0-auc:0.843021\n",
            "[32]\tvalidation_0-auc:0.843393\n",
            "[33]\tvalidation_0-auc:0.843289\n",
            "[34]\tvalidation_0-auc:0.84335\n",
            "[35]\tvalidation_0-auc:0.843532\n",
            "[36]\tvalidation_0-auc:0.843133\n",
            "[37]\tvalidation_0-auc:0.842406\n",
            "[38]\tvalidation_0-auc:0.841912\n",
            "[39]\tvalidation_0-auc:0.841306\n",
            "[40]\tvalidation_0-auc:0.84147\n",
            "[41]\tvalidation_0-auc:0.841531\n",
            "[42]\tvalidation_0-auc:0.842033\n",
            "[43]\tvalidation_0-auc:0.841583\n",
            "[44]\tvalidation_0-auc:0.842172\n",
            "[45]\tvalidation_0-auc:0.842562\n",
            "[46]\tvalidation_0-auc:0.842388\n",
            "[47]\tvalidation_0-auc:0.842302\n",
            "[48]\tvalidation_0-auc:0.841999\n",
            "[49]\tvalidation_0-auc:0.841141\n",
            "[50]\tvalidation_0-auc:0.840933\n",
            "[51]\tvalidation_0-auc:0.840708\n",
            "[52]\tvalidation_0-auc:0.84037\n",
            "[53]\tvalidation_0-auc:0.840301\n",
            "[54]\tvalidation_0-auc:0.840145\n",
            "[55]\tvalidation_0-auc:0.839885\n",
            "[56]\tvalidation_0-auc:0.839816\n",
            "[57]\tvalidation_0-auc:0.839331\n",
            "[58]\tvalidation_0-auc:0.83992\n",
            "[59]\tvalidation_0-auc:0.839746\n",
            "[60]\tvalidation_0-auc:0.839123\n",
            "[61]\tvalidation_0-auc:0.838742\n",
            "[62]\tvalidation_0-auc:0.83862\n",
            "[63]\tvalidation_0-auc:0.838395\n",
            "[64]\tvalidation_0-auc:0.838542\n",
            "[65]\tvalidation_0-auc:0.838525\n",
            "[66]\tvalidation_0-auc:0.837901\n",
            "[67]\tvalidation_0-auc:0.837641\n",
            "[68]\tvalidation_0-auc:0.837226\n",
            "[69]\tvalidation_0-auc:0.837434\n",
            "[70]\tvalidation_0-auc:0.836637\n",
            "[71]\tvalidation_0-auc:0.837087\n",
            "[72]\tvalidation_0-auc:0.836758\n",
            "[73]\tvalidation_0-auc:0.836567\n",
            "[74]\tvalidation_0-auc:0.836359\n",
            "[75]\tvalidation_0-auc:0.835701\n",
            "[76]\tvalidation_0-auc:0.834506\n",
            "[77]\tvalidation_0-auc:0.834523\n",
            "Stopping. Best iteration:\n",
            "[27]\tvalidation_0-auc:0.846616\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6824 - accuracy: 0.5710 - val_loss: 0.6853 - val_accuracy: 0.5886\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6812 - accuracy: 0.5697 - val_loss: 0.6718 - val_accuracy: 0.6061\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6555 - accuracy: 0.6239 - val_loss: 0.6347 - val_accuracy: 0.6674\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5977 - accuracy: 0.6898 - val_loss: 0.5495 - val_accuracy: 0.7265\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5508 - accuracy: 0.7008 - val_loss: 0.5717 - val_accuracy: 0.7155\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6479 - accuracy: 0.6259 - val_loss: 0.6387 - val_accuracy: 0.6193\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5577 - accuracy: 0.7035 - val_loss: 0.5417 - val_accuracy: 0.7177\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5282 - accuracy: 0.7461 - val_loss: 0.5395 - val_accuracy: 0.7374\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5076 - accuracy: 0.7618 - val_loss: 0.5224 - val_accuracy: 0.7637\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5140 - accuracy: 0.7474 - val_loss: 0.5359 - val_accuracy: 0.7309\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.699837\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.699971\n",
            "[2]\tvalidation_0-auc:0.724162\n",
            "[3]\tvalidation_0-auc:0.725356\n",
            "[4]\tvalidation_0-auc:0.725573\n",
            "[5]\tvalidation_0-auc:0.727766\n",
            "[6]\tvalidation_0-auc:0.733368\n",
            "[7]\tvalidation_0-auc:0.74103\n",
            "[8]\tvalidation_0-auc:0.743677\n",
            "[9]\tvalidation_0-auc:0.743656\n",
            "[10]\tvalidation_0-auc:0.748785\n",
            "[11]\tvalidation_0-auc:0.751493\n",
            "[12]\tvalidation_0-auc:0.747745\n",
            "[13]\tvalidation_0-auc:0.74827\n",
            "[14]\tvalidation_0-auc:0.746385\n",
            "[15]\tvalidation_0-auc:0.746303\n",
            "[16]\tvalidation_0-auc:0.745211\n",
            "[17]\tvalidation_0-auc:0.747971\n",
            "[18]\tvalidation_0-auc:0.747992\n",
            "[19]\tvalidation_0-auc:0.751617\n",
            "[20]\tvalidation_0-auc:0.747724\n",
            "[21]\tvalidation_0-auc:0.748115\n",
            "[22]\tvalidation_0-auc:0.75069\n",
            "[23]\tvalidation_0-auc:0.752647\n",
            "[24]\tvalidation_0-auc:0.756663\n",
            "[25]\tvalidation_0-auc:0.756714\n",
            "[26]\tvalidation_0-auc:0.75622\n",
            "[27]\tvalidation_0-auc:0.755108\n",
            "[28]\tvalidation_0-auc:0.756941\n",
            "[29]\tvalidation_0-auc:0.758259\n",
            "[30]\tvalidation_0-auc:0.757126\n",
            "[31]\tvalidation_0-auc:0.756539\n",
            "[32]\tvalidation_0-auc:0.756169\n",
            "[33]\tvalidation_0-auc:0.756086\n",
            "[34]\tvalidation_0-auc:0.756035\n",
            "[35]\tvalidation_0-auc:0.758239\n",
            "[36]\tvalidation_0-auc:0.758053\n",
            "[37]\tvalidation_0-auc:0.758578\n",
            "[38]\tvalidation_0-auc:0.759155\n",
            "[39]\tvalidation_0-auc:0.758723\n",
            "[40]\tvalidation_0-auc:0.761915\n",
            "[41]\tvalidation_0-auc:0.762502\n",
            "[42]\tvalidation_0-auc:0.762749\n",
            "[43]\tvalidation_0-auc:0.762111\n",
            "[44]\tvalidation_0-auc:0.762646\n",
            "[45]\tvalidation_0-auc:0.761791\n",
            "[46]\tvalidation_0-auc:0.76348\n",
            "[47]\tvalidation_0-auc:0.765437\n",
            "[48]\tvalidation_0-auc:0.764181\n",
            "[49]\tvalidation_0-auc:0.763604\n",
            "[50]\tvalidation_0-auc:0.762986\n",
            "[51]\tvalidation_0-auc:0.76313\n",
            "[52]\tvalidation_0-auc:0.763449\n",
            "[53]\tvalidation_0-auc:0.764356\n",
            "[54]\tvalidation_0-auc:0.764624\n",
            "[55]\tvalidation_0-auc:0.764294\n",
            "[56]\tvalidation_0-auc:0.765715\n",
            "[57]\tvalidation_0-auc:0.765736\n",
            "[58]\tvalidation_0-auc:0.765715\n",
            "[59]\tvalidation_0-auc:0.764912\n",
            "[60]\tvalidation_0-auc:0.766415\n",
            "[61]\tvalidation_0-auc:0.766354\n",
            "[62]\tvalidation_0-auc:0.767651\n",
            "[63]\tvalidation_0-auc:0.76831\n",
            "[64]\tvalidation_0-auc:0.768599\n",
            "[65]\tvalidation_0-auc:0.770061\n",
            "[66]\tvalidation_0-auc:0.770514\n",
            "[67]\tvalidation_0-auc:0.76969\n",
            "[68]\tvalidation_0-auc:0.769917\n",
            "[69]\tvalidation_0-auc:0.770391\n",
            "[70]\tvalidation_0-auc:0.770329\n",
            "[71]\tvalidation_0-auc:0.770308\n",
            "[72]\tvalidation_0-auc:0.770452\n",
            "[73]\tvalidation_0-auc:0.770452\n",
            "[74]\tvalidation_0-auc:0.769567\n",
            "[75]\tvalidation_0-auc:0.768619\n",
            "[76]\tvalidation_0-auc:0.768146\n",
            "[77]\tvalidation_0-auc:0.768104\n",
            "[78]\tvalidation_0-auc:0.768599\n",
            "[79]\tvalidation_0-auc:0.770308\n",
            "[80]\tvalidation_0-auc:0.76864\n",
            "[81]\tvalidation_0-auc:0.768588\n",
            "[82]\tvalidation_0-auc:0.768259\n",
            "[83]\tvalidation_0-auc:0.768279\n",
            "[84]\tvalidation_0-auc:0.769227\n",
            "[85]\tvalidation_0-auc:0.768835\n",
            "[86]\tvalidation_0-auc:0.769701\n",
            "[87]\tvalidation_0-auc:0.770483\n",
            "[88]\tvalidation_0-auc:0.770772\n",
            "[89]\tvalidation_0-auc:0.77106\n",
            "[90]\tvalidation_0-auc:0.771719\n",
            "[91]\tvalidation_0-auc:0.771348\n",
            "[92]\tvalidation_0-auc:0.77141\n",
            "[93]\tvalidation_0-auc:0.771101\n",
            "[94]\tvalidation_0-auc:0.771122\n",
            "[95]\tvalidation_0-auc:0.771019\n",
            "[96]\tvalidation_0-auc:0.77141\n",
            "[97]\tvalidation_0-auc:0.772049\n",
            "[98]\tvalidation_0-auc:0.772543\n",
            "[99]\tvalidation_0-auc:0.772512\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.763265306122449  | 0.6901408450704225 | 0.7461928934010152 | 0.7170731707317074 |\n",
            "|     GRU 0.1      | 0.7714285714285715 | 0.7272727272727273 | 0.6903553299492385 | 0.7083333333333331 |\n",
            "|   XGBoost 0.1    | 0.7816326530612245 | 0.7586206896551724 | 0.6700507614213198 | 0.7115902964959568 |\n",
            "|    Logreg 0.1    | 0.7877551020408163 | 0.7961783439490446 | 0.6345177664974619 | 0.7062146892655368 |\n",
            "|     SVM 0.1      | 0.7755102040816326 | 0.7636363636363637 | 0.6395939086294417 | 0.696132596685083  |\n",
            "|  LSTM beta 0.1   | 0.7155361050328227 | 0.6130952380952381 | 0.6130952380952381 | 0.6130952380952381 |\n",
            "|   GRU beta 0.1   | 0.7308533916849015 | 0.6470588235294118 | 0.5892857142857143 | 0.6168224299065421 |\n",
            "| XGBoost beta 0.1 | 0.7286652078774617 |       0.625        | 0.6547619047619048 | 0.6395348837209303 |\n",
            "| logreg beta 0.1  | 0.7199124726477024 | 0.644927536231884  | 0.5297619047619048 | 0.5816993464052287 |\n",
            "|   svm beta 0.1   | 0.7111597374179431 | 0.618421052631579  | 0.5595238095238095 |       0.5875       |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6752 - accuracy: 0.5685 - val_loss: 0.8413 - val_accuracy: 0.4082\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6199 - accuracy: 0.6564 - val_loss: 0.6053 - val_accuracy: 0.7510\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6032 - accuracy: 0.6785 - val_loss: 0.6040 - val_accuracy: 0.6633\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5870 - accuracy: 0.7034 - val_loss: 0.5934 - val_accuracy: 0.6857\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5889 - accuracy: 0.7054 - val_loss: 0.6343 - val_accuracy: 0.6306\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6517 - accuracy: 0.6054 - val_loss: 0.6146 - val_accuracy: 0.6571\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5854 - accuracy: 0.7000 - val_loss: 0.6178 - val_accuracy: 0.6612\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5549 - accuracy: 0.7081 - val_loss: 0.5683 - val_accuracy: 0.7204\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5639 - accuracy: 0.7013 - val_loss: 0.6013 - val_accuracy: 0.6796\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5677 - accuracy: 0.7141 - val_loss: 0.5800 - val_accuracy: 0.7102\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.786342\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.797189\n",
            "[2]\tvalidation_0-auc:0.793366\n",
            "[3]\tvalidation_0-auc:0.795845\n",
            "[4]\tvalidation_0-auc:0.795322\n",
            "[5]\tvalidation_0-auc:0.797643\n",
            "[6]\tvalidation_0-auc:0.796833\n",
            "[7]\tvalidation_0-auc:0.796764\n",
            "[8]\tvalidation_0-auc:0.798364\n",
            "[9]\tvalidation_0-auc:0.794768\n",
            "[10]\tvalidation_0-auc:0.796349\n",
            "[11]\tvalidation_0-auc:0.797139\n",
            "[12]\tvalidation_0-auc:0.799036\n",
            "[13]\tvalidation_0-auc:0.799105\n",
            "[14]\tvalidation_0-auc:0.798038\n",
            "[15]\tvalidation_0-auc:0.797771\n",
            "[16]\tvalidation_0-auc:0.797643\n",
            "[17]\tvalidation_0-auc:0.796537\n",
            "[18]\tvalidation_0-auc:0.796645\n",
            "[19]\tvalidation_0-auc:0.796744\n",
            "[20]\tvalidation_0-auc:0.79626\n",
            "[21]\tvalidation_0-auc:0.796063\n",
            "[22]\tvalidation_0-auc:0.796112\n",
            "[23]\tvalidation_0-auc:0.795658\n",
            "[24]\tvalidation_0-auc:0.795835\n",
            "[25]\tvalidation_0-auc:0.79546\n",
            "[26]\tvalidation_0-auc:0.794186\n",
            "[27]\tvalidation_0-auc:0.79466\n",
            "[28]\tvalidation_0-auc:0.794601\n",
            "[29]\tvalidation_0-auc:0.794136\n",
            "[30]\tvalidation_0-auc:0.795401\n",
            "[31]\tvalidation_0-auc:0.795006\n",
            "[32]\tvalidation_0-auc:0.79461\n",
            "[33]\tvalidation_0-auc:0.794037\n",
            "[34]\tvalidation_0-auc:0.793741\n",
            "[35]\tvalidation_0-auc:0.793563\n",
            "[36]\tvalidation_0-auc:0.793415\n",
            "[37]\tvalidation_0-auc:0.793079\n",
            "[38]\tvalidation_0-auc:0.792704\n",
            "[39]\tvalidation_0-auc:0.792684\n",
            "[40]\tvalidation_0-auc:0.79142\n",
            "[41]\tvalidation_0-auc:0.791025\n",
            "[42]\tvalidation_0-auc:0.791805\n",
            "[43]\tvalidation_0-auc:0.791706\n",
            "[44]\tvalidation_0-auc:0.791686\n",
            "[45]\tvalidation_0-auc:0.792368\n",
            "[46]\tvalidation_0-auc:0.791736\n",
            "[47]\tvalidation_0-auc:0.791341\n",
            "[48]\tvalidation_0-auc:0.791025\n",
            "[49]\tvalidation_0-auc:0.790955\n",
            "[50]\tvalidation_0-auc:0.790955\n",
            "[51]\tvalidation_0-auc:0.790501\n",
            "[52]\tvalidation_0-auc:0.789553\n",
            "[53]\tvalidation_0-auc:0.789533\n",
            "[54]\tvalidation_0-auc:0.789335\n",
            "[55]\tvalidation_0-auc:0.789335\n",
            "[56]\tvalidation_0-auc:0.789049\n",
            "[57]\tvalidation_0-auc:0.789424\n",
            "[58]\tvalidation_0-auc:0.789088\n",
            "[59]\tvalidation_0-auc:0.788219\n",
            "[60]\tvalidation_0-auc:0.788753\n",
            "[61]\tvalidation_0-auc:0.78814\n",
            "[62]\tvalidation_0-auc:0.788476\n",
            "[63]\tvalidation_0-auc:0.788101\n",
            "Stopping. Best iteration:\n",
            "[13]\tvalidation_0-auc:0.799105\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6617 - accuracy: 0.5875 - val_loss: 0.6295 - val_accuracy: 0.7462\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6079 - accuracy: 0.6555 - val_loss: 0.5884 - val_accuracy: 0.7396\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5795 - accuracy: 0.7042 - val_loss: 0.7090 - val_accuracy: 0.6127\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5451 - accuracy: 0.7152 - val_loss: 0.6262 - val_accuracy: 0.6608\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5312 - accuracy: 0.7502 - val_loss: 0.6173 - val_accuracy: 0.6718\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6234 - accuracy: 0.6644 - val_loss: 0.6093 - val_accuracy: 0.6696\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5262 - accuracy: 0.7234 - val_loss: 0.6121 - val_accuracy: 0.6980\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5360 - accuracy: 0.7296 - val_loss: 0.6510 - val_accuracy: 0.6674\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5146 - accuracy: 0.7447 - val_loss: 0.7266 - val_accuracy: 0.6105\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5254 - accuracy: 0.7330 - val_loss: 0.5924 - val_accuracy: 0.6958\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.697168\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.689734\n",
            "[2]\tvalidation_0-auc:0.695652\n",
            "[3]\tvalidation_0-auc:0.709177\n",
            "[4]\tvalidation_0-auc:0.708306\n",
            "[5]\tvalidation_0-auc:0.707063\n",
            "[6]\tvalidation_0-auc:0.709674\n",
            "[7]\tvalidation_0-auc:0.713179\n",
            "[8]\tvalidation_0-auc:0.72259\n",
            "[9]\tvalidation_0-auc:0.727202\n",
            "[10]\tvalidation_0-auc:0.728942\n",
            "[11]\tvalidation_0-auc:0.729464\n",
            "[12]\tvalidation_0-auc:0.729054\n",
            "[13]\tvalidation_0-auc:0.731465\n",
            "[14]\tvalidation_0-auc:0.730595\n",
            "[15]\tvalidation_0-auc:0.730123\n",
            "[16]\tvalidation_0-auc:0.72888\n",
            "[17]\tvalidation_0-auc:0.729588\n",
            "[18]\tvalidation_0-auc:0.731192\n",
            "[19]\tvalidation_0-auc:0.729812\n",
            "[20]\tvalidation_0-auc:0.728967\n",
            "[21]\tvalidation_0-auc:0.730123\n",
            "[22]\tvalidation_0-auc:0.733044\n",
            "[23]\tvalidation_0-auc:0.732746\n",
            "[24]\tvalidation_0-auc:0.732447\n",
            "[25]\tvalidation_0-auc:0.732162\n",
            "[26]\tvalidation_0-auc:0.734063\n",
            "[27]\tvalidation_0-auc:0.734474\n",
            "[28]\tvalidation_0-auc:0.733753\n",
            "[29]\tvalidation_0-auc:0.732833\n",
            "[30]\tvalidation_0-auc:0.733181\n",
            "[31]\tvalidation_0-auc:0.733653\n",
            "[32]\tvalidation_0-auc:0.734126\n",
            "[33]\tvalidation_0-auc:0.73517\n",
            "[34]\tvalidation_0-auc:0.735791\n",
            "[35]\tvalidation_0-auc:0.735294\n",
            "[36]\tvalidation_0-auc:0.736972\n",
            "[37]\tvalidation_0-auc:0.736624\n",
            "[38]\tvalidation_0-auc:0.735605\n",
            "[39]\tvalidation_0-auc:0.736376\n",
            "[40]\tvalidation_0-auc:0.735729\n",
            "[41]\tvalidation_0-auc:0.73461\n",
            "[42]\tvalidation_0-auc:0.734834\n",
            "[43]\tvalidation_0-auc:0.735456\n",
            "[44]\tvalidation_0-auc:0.736649\n",
            "[45]\tvalidation_0-auc:0.736226\n",
            "[46]\tvalidation_0-auc:0.737022\n",
            "[47]\tvalidation_0-auc:0.736624\n",
            "[48]\tvalidation_0-auc:0.737644\n",
            "[49]\tvalidation_0-auc:0.736947\n",
            "[50]\tvalidation_0-auc:0.73788\n",
            "[51]\tvalidation_0-auc:0.73972\n",
            "[52]\tvalidation_0-auc:0.740117\n",
            "[53]\tvalidation_0-auc:0.739844\n",
            "[54]\tvalidation_0-auc:0.738825\n",
            "[55]\tvalidation_0-auc:0.738999\n",
            "[56]\tvalidation_0-auc:0.740615\n",
            "[57]\tvalidation_0-auc:0.740739\n",
            "[58]\tvalidation_0-auc:0.740565\n",
            "[59]\tvalidation_0-auc:0.740664\n",
            "[60]\tvalidation_0-auc:0.739943\n",
            "[61]\tvalidation_0-auc:0.738054\n",
            "[62]\tvalidation_0-auc:0.738327\n",
            "[63]\tvalidation_0-auc:0.738476\n",
            "[64]\tvalidation_0-auc:0.738626\n",
            "[65]\tvalidation_0-auc:0.739123\n",
            "[66]\tvalidation_0-auc:0.738551\n",
            "[67]\tvalidation_0-auc:0.738775\n",
            "[68]\tvalidation_0-auc:0.737979\n",
            "[69]\tvalidation_0-auc:0.739794\n",
            "[70]\tvalidation_0-auc:0.740291\n",
            "[71]\tvalidation_0-auc:0.741684\n",
            "[72]\tvalidation_0-auc:0.742628\n",
            "[73]\tvalidation_0-auc:0.741485\n",
            "[74]\tvalidation_0-auc:0.741907\n",
            "[75]\tvalidation_0-auc:0.739869\n",
            "[76]\tvalidation_0-auc:0.739446\n",
            "[77]\tvalidation_0-auc:0.739123\n",
            "[78]\tvalidation_0-auc:0.740341\n",
            "[79]\tvalidation_0-auc:0.740739\n",
            "[80]\tvalidation_0-auc:0.74049\n",
            "[81]\tvalidation_0-auc:0.740963\n",
            "[82]\tvalidation_0-auc:0.741261\n",
            "[83]\tvalidation_0-auc:0.740739\n",
            "[84]\tvalidation_0-auc:0.740813\n",
            "[85]\tvalidation_0-auc:0.742032\n",
            "[86]\tvalidation_0-auc:0.741584\n",
            "[87]\tvalidation_0-auc:0.740938\n",
            "[88]\tvalidation_0-auc:0.741087\n",
            "[89]\tvalidation_0-auc:0.741162\n",
            "[90]\tvalidation_0-auc:0.740167\n",
            "[91]\tvalidation_0-auc:0.740863\n",
            "[92]\tvalidation_0-auc:0.740789\n",
            "[93]\tvalidation_0-auc:0.740615\n",
            "[94]\tvalidation_0-auc:0.739297\n",
            "[95]\tvalidation_0-auc:0.738029\n",
            "[96]\tvalidation_0-auc:0.738327\n",
            "[97]\tvalidation_0-auc:0.738302\n",
            "[98]\tvalidation_0-auc:0.738278\n",
            "[99]\tvalidation_0-auc:0.738029\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|     LSTM 0.2     | 0.6306122448979592 | 0.44086021505376344 | 0.831081081081081  | 0.5761124121779859 |\n",
            "|     GRU 0.2      | 0.710204081632653  |  0.5128205128205128 | 0.8108108108108109 | 0.6282722513089004 |\n",
            "|   XGBoost 0.2    | 0.7551020408163265 |  0.5760869565217391 | 0.7162162162162162 | 0.6385542168674698 |\n",
            "|    Logreg 0.2    | 0.736734693877551  |  0.5458937198067633 | 0.7635135135135135 | 0.6366197183098592 |\n",
            "|     SVM 0.2      | 0.753061224489796  |  0.5685279187817259 | 0.7567567567567568 | 0.6492753623188405 |\n",
            "|  LSTM beta 0.2   | 0.6717724288840262 |  0.4205128205128205 | 0.6890756302521008 | 0.5222929936305732 |\n",
            "|   GRU beta 0.2   | 0.6958424507658644 | 0.44047619047619047 | 0.6218487394957983 | 0.5156794425087108 |\n",
            "| XGBoost beta 0.2 | 0.649890590809628  | 0.40375586854460094 | 0.7226890756302521 | 0.5180722891566265 |\n",
            "| logreg beta 0.2  | 0.6892778993435449 |  0.4327485380116959 | 0.6218487394957983 | 0.5103448275862069 |\n",
            "|   svm beta 0.2   | 0.6389496717724289 | 0.38613861386138615 | 0.6554621848739496 | 0.4859813084112149 |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6575 - accuracy: 0.5973 - val_loss: 0.6928 - val_accuracy: 0.5286\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5983 - accuracy: 0.6872 - val_loss: 0.5617 - val_accuracy: 0.7551\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6005 - accuracy: 0.6899 - val_loss: 0.5506 - val_accuracy: 0.7510\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5710 - accuracy: 0.7195 - val_loss: 0.5610 - val_accuracy: 0.7184\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5640 - accuracy: 0.7295 - val_loss: 0.5343 - val_accuracy: 0.7551\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6500 - accuracy: 0.5960 - val_loss: 0.6376 - val_accuracy: 0.6408\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5513 - accuracy: 0.7242 - val_loss: 0.5114 - val_accuracy: 0.7633\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5390 - accuracy: 0.7329 - val_loss: 0.5502 - val_accuracy: 0.7102\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5139 - accuracy: 0.7396 - val_loss: 0.5090 - val_accuracy: 0.7449\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5234 - accuracy: 0.7497 - val_loss: 0.4971 - val_accuracy: 0.7755\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.808397\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.80781\n",
            "[2]\tvalidation_0-auc:0.808262\n",
            "[3]\tvalidation_0-auc:0.810076\n",
            "[4]\tvalidation_0-auc:0.816005\n",
            "[5]\tvalidation_0-auc:0.818541\n",
            "[6]\tvalidation_0-auc:0.81874\n",
            "[7]\tvalidation_0-auc:0.821447\n",
            "[8]\tvalidation_0-auc:0.821492\n",
            "[9]\tvalidation_0-auc:0.820752\n",
            "[10]\tvalidation_0-auc:0.822115\n",
            "[11]\tvalidation_0-auc:0.822702\n",
            "[12]\tvalidation_0-auc:0.822973\n",
            "[13]\tvalidation_0-auc:0.823947\n",
            "[14]\tvalidation_0-auc:0.825364\n",
            "[15]\tvalidation_0-auc:0.825815\n",
            "[16]\tvalidation_0-auc:0.826835\n",
            "[17]\tvalidation_0-auc:0.826799\n",
            "[18]\tvalidation_0-auc:0.827034\n",
            "[19]\tvalidation_0-auc:0.827828\n",
            "[20]\tvalidation_0-auc:0.828415\n",
            "[21]\tvalidation_0-auc:0.828397\n",
            "[22]\tvalidation_0-auc:0.829119\n",
            "[23]\tvalidation_0-auc:0.828965\n",
            "[24]\tvalidation_0-auc:0.829904\n",
            "[25]\tvalidation_0-auc:0.829046\n",
            "[26]\tvalidation_0-auc:0.82929\n",
            "[27]\tvalidation_0-auc:0.827503\n",
            "[28]\tvalidation_0-auc:0.828234\n",
            "[29]\tvalidation_0-auc:0.828126\n",
            "[30]\tvalidation_0-auc:0.827954\n",
            "[31]\tvalidation_0-auc:0.827223\n",
            "[32]\tvalidation_0-auc:0.827566\n",
            "[33]\tvalidation_0-auc:0.827684\n",
            "[34]\tvalidation_0-auc:0.827729\n",
            "[35]\tvalidation_0-auc:0.827458\n",
            "[36]\tvalidation_0-auc:0.826664\n",
            "[37]\tvalidation_0-auc:0.825978\n",
            "[38]\tvalidation_0-auc:0.826501\n",
            "[39]\tvalidation_0-auc:0.825743\n",
            "[40]\tvalidation_0-auc:0.825689\n",
            "[41]\tvalidation_0-auc:0.826014\n",
            "[42]\tvalidation_0-auc:0.825382\n",
            "[43]\tvalidation_0-auc:0.824588\n",
            "[44]\tvalidation_0-auc:0.824768\n",
            "[45]\tvalidation_0-auc:0.824732\n",
            "[46]\tvalidation_0-auc:0.824155\n",
            "[47]\tvalidation_0-auc:0.823451\n",
            "[48]\tvalidation_0-auc:0.823487\n",
            "[49]\tvalidation_0-auc:0.822973\n",
            "[50]\tvalidation_0-auc:0.823063\n",
            "[51]\tvalidation_0-auc:0.822991\n",
            "[52]\tvalidation_0-auc:0.822359\n",
            "[53]\tvalidation_0-auc:0.823162\n",
            "[54]\tvalidation_0-auc:0.823397\n",
            "[55]\tvalidation_0-auc:0.822945\n",
            "[56]\tvalidation_0-auc:0.823649\n",
            "[57]\tvalidation_0-auc:0.822512\n",
            "[58]\tvalidation_0-auc:0.822684\n",
            "[59]\tvalidation_0-auc:0.8229\n",
            "[60]\tvalidation_0-auc:0.823063\n",
            "[61]\tvalidation_0-auc:0.822828\n",
            "[62]\tvalidation_0-auc:0.822467\n",
            "[63]\tvalidation_0-auc:0.822404\n",
            "[64]\tvalidation_0-auc:0.822945\n",
            "[65]\tvalidation_0-auc:0.823126\n",
            "[66]\tvalidation_0-auc:0.823027\n",
            "[67]\tvalidation_0-auc:0.823171\n",
            "[68]\tvalidation_0-auc:0.822846\n",
            "[69]\tvalidation_0-auc:0.823009\n",
            "[70]\tvalidation_0-auc:0.82263\n",
            "[71]\tvalidation_0-auc:0.823207\n",
            "[72]\tvalidation_0-auc:0.823009\n",
            "[73]\tvalidation_0-auc:0.823424\n",
            "[74]\tvalidation_0-auc:0.823478\n",
            "Stopping. Best iteration:\n",
            "[24]\tvalidation_0-auc:0.829904\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6844 - accuracy: 0.5477 - val_loss: 0.7292 - val_accuracy: 0.3589\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6705 - accuracy: 0.5834 - val_loss: 0.7014 - val_accuracy: 0.5952\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6163 - accuracy: 0.6637 - val_loss: 0.6114 - val_accuracy: 0.6827\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5447 - accuracy: 0.7310 - val_loss: 0.5991 - val_accuracy: 0.6937\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5185 - accuracy: 0.7358 - val_loss: 0.5691 - val_accuracy: 0.7287\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6189 - accuracy: 0.6472 - val_loss: 0.6199 - val_accuracy: 0.6630\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5245 - accuracy: 0.7289 - val_loss: 0.5737 - val_accuracy: 0.7024\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5269 - accuracy: 0.7323 - val_loss: 0.5791 - val_accuracy: 0.6915\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5180 - accuracy: 0.7515 - val_loss: 0.6077 - val_accuracy: 0.6958\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5055 - accuracy: 0.7488 - val_loss: 0.5937 - val_accuracy: 0.7133\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.688019\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.722208\n",
            "[2]\tvalidation_0-auc:0.712346\n",
            "[3]\tvalidation_0-auc:0.693705\n",
            "[4]\tvalidation_0-auc:0.702353\n",
            "[5]\tvalidation_0-auc:0.707076\n",
            "[6]\tvalidation_0-auc:0.711744\n",
            "[7]\tvalidation_0-auc:0.697652\n",
            "[8]\tvalidation_0-auc:0.70654\n",
            "[9]\tvalidation_0-auc:0.708486\n",
            "[10]\tvalidation_0-auc:0.71122\n",
            "[11]\tvalidation_0-auc:0.717277\n",
            "[12]\tvalidation_0-auc:0.713395\n",
            "[13]\tvalidation_0-auc:0.71017\n",
            "[14]\tvalidation_0-auc:0.709995\n",
            "[15]\tvalidation_0-auc:0.707459\n",
            "[16]\tvalidation_0-auc:0.708388\n",
            "[17]\tvalidation_0-auc:0.707655\n",
            "[18]\tvalidation_0-auc:0.711602\n",
            "[19]\tvalidation_0-auc:0.712554\n",
            "[20]\tvalidation_0-auc:0.711898\n",
            "[21]\tvalidation_0-auc:0.711657\n",
            "[22]\tvalidation_0-auc:0.712991\n",
            "[23]\tvalidation_0-auc:0.713833\n",
            "[24]\tvalidation_0-auc:0.71403\n",
            "[25]\tvalidation_0-auc:0.715855\n",
            "[26]\tvalidation_0-auc:0.71544\n",
            "[27]\tvalidation_0-auc:0.717386\n",
            "[28]\tvalidation_0-auc:0.717474\n",
            "[29]\tvalidation_0-auc:0.718611\n",
            "[30]\tvalidation_0-auc:0.719267\n",
            "[31]\tvalidation_0-auc:0.719868\n",
            "[32]\tvalidation_0-auc:0.719704\n",
            "[33]\tvalidation_0-auc:0.71895\n",
            "[34]\tvalidation_0-auc:0.721049\n",
            "[35]\tvalidation_0-auc:0.722885\n",
            "[36]\tvalidation_0-auc:0.722361\n",
            "[37]\tvalidation_0-auc:0.725225\n",
            "[38]\tvalidation_0-auc:0.72516\n",
            "[39]\tvalidation_0-auc:0.723126\n",
            "[40]\tvalidation_0-auc:0.725925\n",
            "[41]\tvalidation_0-auc:0.72575\n",
            "[42]\tvalidation_0-auc:0.726504\n",
            "[43]\tvalidation_0-auc:0.726297\n",
            "[44]\tvalidation_0-auc:0.726319\n",
            "[45]\tvalidation_0-auc:0.725182\n",
            "[46]\tvalidation_0-auc:0.72516\n",
            "[47]\tvalidation_0-auc:0.725739\n",
            "[48]\tvalidation_0-auc:0.725433\n",
            "[49]\tvalidation_0-auc:0.725739\n",
            "[50]\tvalidation_0-auc:0.727204\n",
            "[51]\tvalidation_0-auc:0.727816\n",
            "[52]\tvalidation_0-auc:0.728735\n",
            "[53]\tvalidation_0-auc:0.728604\n",
            "[54]\tvalidation_0-auc:0.729773\n",
            "[55]\tvalidation_0-auc:0.72962\n",
            "[56]\tvalidation_0-auc:0.729314\n",
            "[57]\tvalidation_0-auc:0.729686\n",
            "[58]\tvalidation_0-auc:0.730408\n",
            "[59]\tvalidation_0-auc:0.730484\n",
            "[60]\tvalidation_0-auc:0.730375\n",
            "[61]\tvalidation_0-auc:0.729981\n",
            "[62]\tvalidation_0-auc:0.727795\n",
            "[63]\tvalidation_0-auc:0.72751\n",
            "[64]\tvalidation_0-auc:0.728604\n",
            "[65]\tvalidation_0-auc:0.729019\n",
            "[66]\tvalidation_0-auc:0.729675\n",
            "[67]\tvalidation_0-auc:0.730112\n",
            "[68]\tvalidation_0-auc:0.730069\n",
            "[69]\tvalidation_0-auc:0.730987\n",
            "[70]\tvalidation_0-auc:0.731206\n",
            "[71]\tvalidation_0-auc:0.732059\n",
            "[72]\tvalidation_0-auc:0.733567\n",
            "[73]\tvalidation_0-auc:0.732999\n",
            "[74]\tvalidation_0-auc:0.732999\n",
            "[75]\tvalidation_0-auc:0.733611\n",
            "[76]\tvalidation_0-auc:0.734923\n",
            "[77]\tvalidation_0-auc:0.734857\n",
            "[78]\tvalidation_0-auc:0.734639\n",
            "[79]\tvalidation_0-auc:0.734267\n",
            "[80]\tvalidation_0-auc:0.735295\n",
            "[81]\tvalidation_0-auc:0.735273\n",
            "[82]\tvalidation_0-auc:0.735732\n",
            "[83]\tvalidation_0-auc:0.735667\n",
            "[84]\tvalidation_0-auc:0.734879\n",
            "[85]\tvalidation_0-auc:0.734201\n",
            "[86]\tvalidation_0-auc:0.733677\n",
            "[87]\tvalidation_0-auc:0.733436\n",
            "[88]\tvalidation_0-auc:0.733589\n",
            "[89]\tvalidation_0-auc:0.733655\n",
            "[90]\tvalidation_0-auc:0.732999\n",
            "[91]\tvalidation_0-auc:0.733196\n",
            "[92]\tvalidation_0-auc:0.733611\n",
            "[93]\tvalidation_0-auc:0.732102\n",
            "[94]\tvalidation_0-auc:0.732299\n",
            "[95]\tvalidation_0-auc:0.732255\n",
            "[96]\tvalidation_0-auc:0.731523\n",
            "[97]\tvalidation_0-auc:0.732441\n",
            "[98]\tvalidation_0-auc:0.732179\n",
            "[99]\tvalidation_0-auc:0.730561\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.7551020408163265 | 0.6492146596858639 | 0.7005649717514124 | 0.6739130434782609 |\n",
            "|      GRU 0.15     | 0.7755102040816326 | 0.6830601092896175 | 0.7062146892655368 | 0.6944444444444444 |\n",
            "|    XGBoost 0.15   | 0.7714285714285715 | 0.6701570680628273 | 0.7231638418079096 | 0.6956521739130435 |\n",
            "|    Logreg 0.15    | 0.7551020408163265 | 0.6338028169014085 | 0.7627118644067796 | 0.6923076923076923 |\n",
            "|      SVM 0.15     | 0.773469387755102  |       0.665        | 0.751412429378531  | 0.7055702917771882 |\n",
            "|   LSTM beta 0.15  | 0.7286652078774617 | 0.5652173913043478 | 0.7027027027027027 | 0.6265060240963856 |\n",
            "|   GRU beta 0.15   | 0.7133479212253829 | 0.5435897435897435 | 0.7162162162162162 | 0.6180758017492711 |\n",
            "| XGBoost beta 0.15 | 0.6630196936542669 | 0.4861111111111111 | 0.7094594594594594 | 0.5769230769230769 |\n",
            "|  logreg beta 0.15 | 0.6958424507658644 | 0.5243243243243243 | 0.6554054054054054 | 0.5825825825825827 |\n",
            "|   svm beta 0.15   | 0.6783369803063457 | 0.5024875621890548 | 0.6824324324324325 | 0.5787965616045846 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGQlms8BXhm8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9683fa6e-d2e4-4cfd-f49d-af24229cd84f"
      },
      "source": [
        "Result_purging.to_csv('NKTR_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.690141</td>\n",
              "      <td>0.763265</td>\n",
              "      <td>0.717073</td>\n",
              "      <td>0.746193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.771429</td>\n",
              "      <td>0.708333</td>\n",
              "      <td>0.690355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.758621</td>\n",
              "      <td>0.781633</td>\n",
              "      <td>0.711590</td>\n",
              "      <td>0.670051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.796178</td>\n",
              "      <td>0.787755</td>\n",
              "      <td>0.706215</td>\n",
              "      <td>0.634518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.763636</td>\n",
              "      <td>0.775510</td>\n",
              "      <td>0.696133</td>\n",
              "      <td>0.639594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.613095</td>\n",
              "      <td>0.715536</td>\n",
              "      <td>0.613095</td>\n",
              "      <td>0.613095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.730853</td>\n",
              "      <td>0.616822</td>\n",
              "      <td>0.589286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.728665</td>\n",
              "      <td>0.639535</td>\n",
              "      <td>0.654762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.644928</td>\n",
              "      <td>0.719912</td>\n",
              "      <td>0.581699</td>\n",
              "      <td>0.529762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.618421</td>\n",
              "      <td>0.711160</td>\n",
              "      <td>0.587500</td>\n",
              "      <td>0.559524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.440860</td>\n",
              "      <td>0.630612</td>\n",
              "      <td>0.576112</td>\n",
              "      <td>0.831081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.512821</td>\n",
              "      <td>0.710204</td>\n",
              "      <td>0.628272</td>\n",
              "      <td>0.810811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.576087</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.638554</td>\n",
              "      <td>0.716216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.545894</td>\n",
              "      <td>0.736735</td>\n",
              "      <td>0.636620</td>\n",
              "      <td>0.763514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.568528</td>\n",
              "      <td>0.753061</td>\n",
              "      <td>0.649275</td>\n",
              "      <td>0.756757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.420513</td>\n",
              "      <td>0.671772</td>\n",
              "      <td>0.522293</td>\n",
              "      <td>0.689076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.440476</td>\n",
              "      <td>0.695842</td>\n",
              "      <td>0.515679</td>\n",
              "      <td>0.621849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.403756</td>\n",
              "      <td>0.649891</td>\n",
              "      <td>0.518072</td>\n",
              "      <td>0.722689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.432749</td>\n",
              "      <td>0.689278</td>\n",
              "      <td>0.510345</td>\n",
              "      <td>0.621849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.386139</td>\n",
              "      <td>0.638950</td>\n",
              "      <td>0.485981</td>\n",
              "      <td>0.655462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.649215</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.673913</td>\n",
              "      <td>0.700565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.683060</td>\n",
              "      <td>0.775510</td>\n",
              "      <td>0.694444</td>\n",
              "      <td>0.706215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.670157</td>\n",
              "      <td>0.771429</td>\n",
              "      <td>0.695652</td>\n",
              "      <td>0.723164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.633803</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.762712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.665000</td>\n",
              "      <td>0.773469</td>\n",
              "      <td>0.705570</td>\n",
              "      <td>0.751412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.565217</td>\n",
              "      <td>0.728665</td>\n",
              "      <td>0.626506</td>\n",
              "      <td>0.702703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.543590</td>\n",
              "      <td>0.713348</td>\n",
              "      <td>0.618076</td>\n",
              "      <td>0.716216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.486111</td>\n",
              "      <td>0.663020</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.709459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.524324</td>\n",
              "      <td>0.695842</td>\n",
              "      <td>0.582583</td>\n",
              "      <td>0.655405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.502488</td>\n",
              "      <td>0.678337</td>\n",
              "      <td>0.578797</td>\n",
              "      <td>0.682432</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  NKTR  0.690141  0.763265  0.717073  0.746193\n",
              "1            GRU 0.1  NKTR  0.727273  0.771429  0.708333  0.690355\n",
              "2        XGBoost 0.1  NKTR  0.758621  0.781633  0.711590  0.670051\n",
              "3         Logreg 0.1  NKTR  0.796178  0.787755  0.706215  0.634518\n",
              "4            SVM 0.1  NKTR  0.763636  0.775510  0.696133  0.639594\n",
              "5      LSTM beta 0.1  NKTR  0.613095  0.715536  0.613095  0.613095\n",
              "6       GRU beta 0.1  NKTR  0.647059  0.730853  0.616822  0.589286\n",
              "7   XGBoost beta 0.1  NKTR  0.625000  0.728665  0.639535  0.654762\n",
              "8    logreg beta 0.1  NKTR  0.644928  0.719912  0.581699  0.529762\n",
              "9       svm beta 0.1  NKTR  0.618421  0.711160  0.587500  0.559524\n",
              "0           LSTM 0.2  NKTR  0.440860  0.630612  0.576112  0.831081\n",
              "1            GRU 0.2  NKTR  0.512821  0.710204  0.628272  0.810811\n",
              "2        XGBoost 0.2  NKTR  0.576087  0.755102  0.638554  0.716216\n",
              "3         Logreg 0.2  NKTR  0.545894  0.736735  0.636620  0.763514\n",
              "4            SVM 0.2  NKTR  0.568528  0.753061  0.649275  0.756757\n",
              "5      LSTM beta 0.2  NKTR  0.420513  0.671772  0.522293  0.689076\n",
              "6       GRU beta 0.2  NKTR  0.440476  0.695842  0.515679  0.621849\n",
              "7   XGBoost beta 0.2  NKTR  0.403756  0.649891  0.518072  0.722689\n",
              "8    logreg beta 0.2  NKTR  0.432749  0.689278  0.510345  0.621849\n",
              "9       svm beta 0.2  NKTR  0.386139  0.638950  0.485981  0.655462\n",
              "0          LSTM 0.15  NKTR  0.649215  0.755102  0.673913  0.700565\n",
              "1           GRU 0.15  NKTR  0.683060  0.775510  0.694444  0.706215\n",
              "2       XGBoost 0.15  NKTR  0.670157  0.771429  0.695652  0.723164\n",
              "3        Logreg 0.15  NKTR  0.633803  0.755102  0.692308  0.762712\n",
              "4           SVM 0.15  NKTR  0.665000  0.773469  0.705570  0.751412\n",
              "5     LSTM beta 0.15  NKTR  0.565217  0.728665  0.626506  0.702703\n",
              "6      GRU beta 0.15  NKTR  0.543590  0.713348  0.618076  0.716216\n",
              "7  XGBoost beta 0.15  NKTR  0.486111  0.663020  0.576923  0.709459\n",
              "8   logreg beta 0.15  NKTR  0.524324  0.695842  0.582583  0.655405\n",
              "9      svm beta 0.15  NKTR  0.502488  0.678337  0.578797  0.682432"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ryC_d4SXhm8"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXdmJJvjXhm8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmO44zEWX4gq"
      },
      "source": [
        "## NVDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-EAGlliX4gw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc15293b-c464-4050-ac4f-3a5b8c31ce5b"
      },
      "source": [
        "dfs = pd.read_csv(\"NVDA.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "# Denoise prices\n",
        "dfs = denoise_data(dfs, '<CLOSE>')\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.NVDA</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>207.70</td>\n",
              "      <td>208.485</td>\n",
              "      <td>202.04</td>\n",
              "      <td>207.23</td>\n",
              "      <td>657406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.NVDA</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>207.89</td>\n",
              "      <td>210.650</td>\n",
              "      <td>206.89</td>\n",
              "      <td>207.12</td>\n",
              "      <td>537697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.NVDA</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>209.33</td>\n",
              "      <td>210.160</td>\n",
              "      <td>204.68</td>\n",
              "      <td>205.22</td>\n",
              "      <td>606902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2763</td>\n",
              "      <td>US1.NVDA</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>212.01</td>\n",
              "      <td>214.030</td>\n",
              "      <td>206.51</td>\n",
              "      <td>207.02</td>\n",
              "      <td>835341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2762</td>\n",
              "      <td>US1.NVDA</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>217.16</td>\n",
              "      <td>217.750</td>\n",
              "      <td>213.28</td>\n",
              "      <td>216.57</td>\n",
              "      <td>625939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2762</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.NVDA</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>10.65</td>\n",
              "      <td>10.950</td>\n",
              "      <td>10.51</td>\n",
              "      <td>10.86</td>\n",
              "      <td>16998198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2763</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.NVDA</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>10.82</td>\n",
              "      <td>10.840</td>\n",
              "      <td>10.38</td>\n",
              "      <td>10.70</td>\n",
              "      <td>18732301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.NVDA</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>11.32</td>\n",
              "      <td>11.370</td>\n",
              "      <td>10.67</td>\n",
              "      <td>10.78</td>\n",
              "      <td>25753399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.NVDA</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>11.48</td>\n",
              "      <td>11.500</td>\n",
              "      <td>11.29</td>\n",
              "      <td>11.32</td>\n",
              "      <td>18266877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.NVDA</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>11.24</td>\n",
              "      <td>11.430</td>\n",
              "      <td>11.01</td>\n",
              "      <td>11.23</td>\n",
              "      <td>18184874</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2767 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  <TICKER> <PER>    <DATE>  ...   <HIGH>   <LOW>  <CLOSE>     <VOL>\n",
              "0      2766  US1.NVDA     D  20211001  ...  208.485  202.04   207.23    657406\n",
              "1      2765  US1.NVDA     D  20210930  ...  210.650  206.89   207.12    537697\n",
              "2      2764  US1.NVDA     D  20210929  ...  210.160  204.68   205.22    606902\n",
              "3      2763  US1.NVDA     D  20210928  ...  214.030  206.51   207.02    835341\n",
              "4      2762  US1.NVDA     D  20210927  ...  217.750  213.28   216.57    625939\n",
              "...     ...       ...   ...       ...  ...      ...     ...      ...       ...\n",
              "2762      4  US1.NVDA     D  20101008  ...   10.950   10.51    10.86  16998198\n",
              "2763      3  US1.NVDA     D  20101007  ...   10.840   10.38    10.70  18732301\n",
              "2764      2  US1.NVDA     D  20101006  ...   11.370   10.67    10.78  25753399\n",
              "2765      1  US1.NVDA     D  20101005  ...   11.500   11.29    11.32  18266877\n",
              "2766      0  US1.NVDA     D  20101004  ...   11.430   11.01    11.23  18184874\n",
              "\n",
              "[2767 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkX_oC6eX4gx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9205ed-c1fc-43e5-f191-c036feaf4475"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"00cd1d2d-13d8-4391-aafb-279f178593e7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"00cd1d2d-13d8-4391-aafb-279f178593e7\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '00cd1d2d-13d8-4391-aafb-279f178593e7',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [182.04, 181.32, 173.12, 173.94, 174.07, 171.79, 177.37, 178.27, 172.56, 174.84, 172.62, 176.91, 179.99, 181.02, 180.2, 181.92, 184.26, 184.41, 183.24, 180.44, 178.62, 179.76, 168.76, 164.16, 167.51, 166.99, 161.21, 161.81, 165.46, 162.52, 171.46, 171.24, 167.84, 170.77, 159.49, 148.7975, 150.07, 156.08, 151.45, 154.18, 158.31, 153.97, 152.34, 150.73, 161.17, 164.92, 168.72, 175.46, 174.82, 175.04, 173.32, 178.7, 175.71, 171.31, 168.44, 170.18, 169.69, 167.07, 167.31, 167.6, 166.28, 160.06, 157.32, 157.26, 160.33, 162.4963, 162.2201, 166.2, 164.16, 163.23, 159.25, 151.48, 152.66, 151.76, 154.1, 153.11, 152.89, 145.02, 144.58, 148.32, 146.165, 150.76, 148.45, 145.5, 143.79, 141.33, 143.02, 133.78, 135.46, 139.11, 140.31, 143.3, 145.13, 147.37, 152.2, 155.06, 151.67, 156.535, 160.24, 159.58, 162.03, 158.4, 168.84, 170.2, 173.88, 173.2, 179.88, 183.0, 183.19, 180.47, 181.09, 179.36, 178.09, 186.84, 191.17, 190.67, 188.45, 186.3, 187.27, 188.28, 184.7, 190.0, 191.51, 192.149, 189.28, 191.78, 190.95, 188.285, 188.62, 183.0, 182.31, 179.58, 177.22, 176.59, 176.95, 173.78, 177.5, 183.9, 174.4, 175.74, 168.97, 169.82, 165.56, 168.62, 162.51, 161.13, 150.67, 149.25, 152.04, 156.51, 156.8, 156.44, 154.29, 155.4, 157.14, 158.69, 159.22, 155.81, 158.57, 156.63, 157.3, 154.53, 152.87, 151.14, 146.47, 148.22, 147.42, 153.0, 149.95, 149.19, 144.73, 143.75, 137.46, 131.55, 137.96, 160.15, 157.83, 149.3, 148.84, 156.93, 151.72, 148.84, 149.86, 150.38, 148.8, 145.17, 142.69, 139.82, 143.4, 136.19, 127.99, 136.22, 133.5, 133.65, 131.62, 133.12, 126.8898, 129.58, 135.16, 138.49, 146.94, 143.61, 146.42, 148.88, 148.85, 148.21, 151.86, 147.6, 158.3, 157.16, 170.05, 163.67, 157.37, 160.08, 153.73, 153.05, 145.16, 144.71, 149.08, 144.7, 164.38, 202.49, 197.2, 199.33, 189.44, 205.6103, 205.98, 213.78, 211.06, 211.77, 214.87, 218.06, 210.88, 203.01, 185.61, 198.29, 207.89, 199.32, 221.05, 231.14, 229.12, 239.57, 243.06, 245.59, 235.38, 246.57, 235.12, 245.71, 265.53, 265.75, 269.86, 279.28, 286.73, 286.52, 289.37, 281.12, 267.4, 266.92, 268.4, 265.71, 263.44, 266.27, 271.89, 271.015, 273.93, 276.45, 271.34, 268.2, 272.8, 274.74, 271.88, 272.75, 278.44, 283.7, 280.71, 277.8, 278.49, 274.4, 275.89, 272.28, 266.87, 262.83, 253.32, 247.85, 244.9, 257.53, 259.02, 261.46, 256.12, 254.79, 256.45, 258.4, 256.91, 254.09, 252.12, 250.62, 246.57, 244.83, 244.14, 251.99, 254.88, 251.89, 248.7, 249.38, 250.89, 252.01, 251.69, 253.68, 248.2, 249.32, 251.23, 247.47, 253.25, 249.25, 247.32, 242.74, 237.1, 242.24, 236.9, 240.86, 235.59, 241.97, 239.16, 250.76, 257.19, 262.29, 260.17, 265.08, 265.38, 266.9, 262.37, 262.73, 260.6, 262.17, 262.9, 265.27, 265.091, 264.85, 257.59, 252.22, 252.99, 248.59, 249.29, 247.74, 247.57, 242.7, 244.26, 245.91, 247.71, 246.1, 245.51, 255.37, 254.53, 260.24, 255.87, 250.3969, 248.69, 239.08, 232.99, 226.31, 227.1, 224.97, 226.34, 225.15, 216.66, 221.24, 223.88, 228.71, 229.05, 236.36, 237.55, 231.47, 231.49, 234.65, 226.24, 227.91, 215.41, 214.17, 221.36, 226.18, 225.35, 221.05, 232.31, 221.31, 225.51, 244.39, 232.925, 241.85, 248.54, 249.58, 240.99, 250.48, 249.35, 248.74, 247.71, 249.76, 245.33, 241.17, 241.84, 242.16, 235.6399, 236.54, 232.18, 242.01, 246.06, 246.5, 245.93, 242.15, 241.51, 249.07, 243.84, 246.5, 241.42, 232.69, 228.02, 231.46, 217.73, 228.81, 225.56, 213.67, 233.51, 240.48, 245.83, 242.7, 246.78, 243.27, 236.34, 235.84, 238.94, 233.67, 230.08, 224.44, 224.7, 220.17, 222.69, 224.05, 223.66, 221.95, 221.98, 215.28, 213.59, 212.5, 199.3, 193.55, 197.34, 197.1199, 197.46, 195.26, 195.88, 196.7501, 196.1, 197.96, 191.6, 186.47, 186.22, 190.81, 194.66, 191.48, 191.98, 189.246, 187.7, 186.6577, 197.68, 200.6, 196.37, 210.71, 214.13, 216.94, 214.93, 216.01, 214.03, 211.31, 211.59, 209.98, 214.18, 212.58, 216.14, 205.1592, 209.1, 212.01, 209.62, 208.68, 205.911, 207.2, 206.75, 203.76, 201.75, 195.59, 193.64, 198.67, 196.62, 196.86, 197.79, 197.54, 197.72, 197.91, 194.59, 191.03, 190.93, 188.94, 185.38, 181.3, 180.72, 180.83, 179.36, 178.93, 178.67, 175.66, 175.73, 171.96, 170.95, 178.97, 180.76, 185.83, 187.36, 187.59, 180.1, 169.39, 170.35, 169.61, 168.93, 163.68, 166.59, 165.8, 165.92, 170.39, 169.44, 165.65, 164.7, 164.96, 163.81, 165.2, 165.78, 162.5, 159.14, 161.58, 161.47, 165.13, 166.96, 168.45, 155.85, 164.76, 172.1, 170.28, 172.32, 167.163, 166.48, 164.27, 164.4826, 162.51, 164.4, 161.74, 167.21, 165.35, 166.17, 168.03, 167.51, 165.1, 165.99, 164.26, 164.935, 160.63, 162.54, 155.88, 153.71, 146.76, 143.49, 143.03, 139.2, 144.57, 146.71, 151.75, 146.6, 152.14, 153.84, 158.37, 159.47, 157.1, 157.33, 151.5, 152.37, 151.72, 151.39, 149.97, 149.59, 159.94, 149.12, 147.35, 148.01, 143.54, 144.35, 144.35, 144.87, 141.83, 138.26, 138.57, 137.02, 138.9, 136.01, 133.06, 127.78, 136.84, 134.31, 127.89, 126.43, 121.3, 102.89, 102.77, 103.86, 103.85, 104.25, 103.48, 106.65, 104.3, 105.63, 104.02, 104.73, 102.95, 101.68, 101.25, 99.68, 99.28, 99.23, 95.49, 97.29, 98.12, 97.77, 100.34, 100.76, 100.01, 100.78, 108.38, 108.93, 109.41, 107.32, 107.7, 108.24, 107.47, 107.1, 108.07, 105.91, 109.45, 106.1, 103.81, 102.55, 101.75, 101.85, 99.1, 98.54, 98.55, 98.73, 97.66, 98.43, 99.0, 102.78, 101.48, 104.41, 101.3925, 100.48, 110.76, 111.07, 107.23, 107.24, 108.98, 108.78, 108.37, 113.63, 116.45, 118.63, 119.13, 117.31, 114.38, 115.39, 113.95, 109.17, 110.01, 111.77, 109.65, 107.79, 107.31, 105.09, 104.01, 105.15, 102.96, 101.09, 103.43, 103.45, 105.16, 106.47, 107.29, 103.13, 101.74, 104.39, 102.0, 106.74, 111.41, 109.25, 117.32, 109.78, 107.11, 105.83, 105.15, 101.65, 100.4, 98.72, 96.46, 91.17, 89.59, 91.82, 93.47, 95.07, 93.39, 91.88, 88.45, 87.64, 92.21, 93.25, 94.11, 94.05, 93.96, 93.65, 92.98, 93.36, 92.39, 91.64, 86.19, 83.66, 87.97, 67.77, 69.95, 71.16, 71.26, 67.57, 67.97, 68.76, 69.07, 71.16, 70.56, 70.68, 72.16, 71.87, 70.71, 67.54, 67.73, 66.48, 66.6, 65.61, 65.99, 65.35, 66.43, 66.12, 67.0911, 66.84, 67.34, 68.23, 68.3, 68.4595, 68.51, 67.408, 66.78, 66.54, 64.34, 64.97, 65.02, 64.85, 63.1, 63.67, 62.85, 62.69, 60.4, 59.8864, 60.75, 59.53, 62.64, 62.2, 63.12, 62.53, 63.14, 61.34, 61.64, 61.99, 62.03, 61.53, 61.94, 62.91, 62.52, 62.25, 62.1, 61.13, 62.6, 62.975, 63.03, 59.73, 58.51, 58.89, 58.74, 58.2, 57.23, 56.18, 56.04, 56.73, 57.1, 56.16, 56.06, 56.63, 55.68, 54.67, 53.22, 54.22, 53.51, 52.98, 52.69, 53.32, 52.78, 52.8, 52.0211, 50.85, 48.88, 47.66, 47.35, 46.64, 47.01, 46.62, 45.89, 45.24, 45.72, 48.49, 47.23, 47.26, 47.55, 46.73, 47.55, 47.36, 46.88, 46.8, 46.2, 47.38, 46.18, 46.34, 46.24, 46.48, 47.14, 46.81, 46.73, 45.9, 45.65, 45.175, 45.35, 44.4, 44.33, 43.56, 43.36, 42.28, 42.19, 40.97, 35.56, 36.06, 35.96, 35.28, 35.33, 34.96, 34.76, 35.57, 35.99, 35.53, 36.2, 37.3, 36.48, 36.45, 36.28, 36.41, 36.45, 36.31, 36.97, 37.13, 36.84, 36.74, 35.84, 35.87, 35.66, 35.43, 35.8, 35.75, 35.82, 36.15, 35.62, 35.76, 35.39, 34.84, 34.48, 34.43, 33.85, 33.905, 33.82, 32.82, 33.11, 32.14, 32.295, 32.22, 31.68, 31.73, 31.75, 32.34, 32.55, 32.65, 32.95, 32.75, 31.36, 31.68, 31.88, 31.815, 31.6, 31.52, 30.44, 30.04, 27.679, 26.99, 25.71, 25.3, 25.45, 25.49, 25.21, 26.43, 28.21, 28.19, 28.06, 29.29, 29.29, 28.05, 28.36, 28.7, 28.41, 28.45, 27.79, 27.47, 27.33, 27.11, 28.67, 29.27, 30.18, 29.68, 29.63, 30.27, 31.53, 32.88, 32.37, 32.96, 33.39, 33.675, 33.14, 33.17, 33.06, 32.93, 32.895, 32.16, 32.66, 33.16, 32.97, 32.575, 32.48, 32.99, 32.71, 33.55, 33.12, 33.74, 32.435, 32.5, 32.75, 31.71, 31.39, 31.14, 31.16, 30.91, 31.39, 31.12, 31.03, 30.41, 30.4, 29.81, 30.4, 30.51, 30.81, 31.41, 31.54, 27.7, 28.03, 28.49, 28.7, 28.37, 27.68, 28.67, 28.44, 28.47, 28.59, 28.39, 27.41, 27.77, 27.82, 27.86, 27.43, 27.37, 26.41, 26.345, 26.07, 26.18, 26.02, 25.75, 25.41, 24.8, 24.17, 24.64, 23.71, 23.3, 23.61, 23.44, 23.0, 22.9, 23.53, 23.3, 23.31, 23.11, 22.94, 22.69, 22.65, 22.58, 22.23, 22.69, 21.75, 22.275, 22.22, 21.565, 22.47, 22.73, 22.64, 21.82, 20.28, 20.71, 21.48, 22.155, 22.99, 23.08, 23.39, 23.545, 23.52, 23.72, 23.66, 23.78, 22.98, 20.48, 20.58, 20.3708, 20.29, 19.95, 20.01, 19.98, 19.735, 19.32, 19.42, 19.65, 19.41, 19.79, 19.7, 20.08, 20.18, 19.75, 19.89, 19.88, 19.75, 19.41, 19.65, 19.785, 20.17, 20.41, 20.4, 20.12, 20.12, 20.74, 21.17, 21.01, 21.215, 21.77, 21.86, 21.93, 21.58, 21.325, 21.065, 21.1, 21.7, 21.455, 21.85, 21.75, 22.265, 22.09, 21.7, 21.93, 22.38, 22.14, 22.15, 21.84, 20.73, 20.855, 20.9, 21.04, 21.035, 21.27, 21.295, 21.29, 20.95, 20.83, 20.63, 20.83, 22.5, 22.09, 22.035, 22.615, 22.75, 22.19, 22.14, 22.3, 22.1975, 22.03, 22.23, 22.31, 22.055, 22.09, 22.195, 22.5, 22.62, 22.37, 22.55, 22.76, 22.52, 22.03, 21.86, 21.67, 21.06, 21.01, 20.93, 21.47, 21.38, 20.97, 21.03, 22.395, 22.71, 23.49, 23.21, 22.88, 23.25, 22.97, 22.7, 22.66, 22.875, 22.99, 22.61, 22.55, 22.865, 22.445, 22.18, 22.6, 22.0501, 22.2, 22.14, 22.31, 22.16, 22.33, 22.18, 22.115, 22.38, 22.305, 22.3, 20.81, 20.94, 20.39, 20.4, 20.49, 20.17, 20.105, 19.615, 19.21, 19.78, 19.31, 19.59, 20.61, 20.72, 20.64, 20.3, 20.02, 19.96, 19.6, 19.74, 19.66, 19.69, 19.94, 19.86, 19.14, 19.18, 19.8, 20.125, 20.05, 20.36, 20.565, 20.59, 20.6363, 20.64, 20.78, 20.42, 20.22, 20.15, 19.34, 19.56, 19.635, 20.27, 20.265, 20.735, 20.805, 21.07, 20.96, 21.13, 20.61, 20.58, 20.9672, 20.91, 20.57, 20.58, 20.46, 20.35, 20.005, 20.17, 19.7, 19.785, 19.56, 19.64, 19.77, 20.015, 19.79, 20.21, 20.13, 20.14, 19.88, 19.53, 18.7, 18.81, 18.93, 18.49, 18.475, 18.285, 17.895, 18.325, 17.57, 17.42, 17.45, 17.435, 17.18, 16.785, 16.85, 17.92, 18.26, 17.925, 18.12, 18.23, 18.19, 18.27, 18.455, 18.515, 18.54, 18.51, 18.92, 18.805, 18.91, 19.06, 19.445, 19.15, 19.135, 18.86, 19.13, 19.41, 19.61, 19.53, 19.79, 19.96, 20.03, 19.68, 19.49, 19.44, 19.39, 19.235, 19.455, 19.1, 19.085, 19.06, 19.25, 19.37, 19.295, 19.04, 18.795, 19.0, 18.895, 18.905, 18.99, 17.46, 17.65, 17.67, 17.64, 17.68, 17.5, 18.08, 17.78, 17.715, 17.79, 18.1, 18.09, 18.47, 18.55, 18.44, 19.3, 19.35, 19.38, 19.29, 19.05, 19.005, 19.12, 18.545, 18.7, 18.8738, 18.685, 18.74, 18.53, 18.39, 18.37, 18.6, 18.43, 18.71, 18.925, 19.13, 19.585, 19.6, 19.49, 19.53, 19.52, 19.41, 19.145, 19.05, 19.03, 18.95, 18.88, 18.86, 18.94, 19.005, 18.945, 18.98, 18.82, 18.48, 18.32, 18.25, 18.24, 18.54, 17.98, 18.0, 18.11, 18.27, 18.565, 18.04, 18.495, 18.27, 18.26, 18.62, 18.42, 18.57, 18.46, 18.68, 18.65, 18.725, 19.25, 19.09, 18.88, 18.71, 18.56, 18.48, 18.445, 18.32, 18.12, 18.385, 18.84, 18.86, 18.21, 18.15, 18.72, 18.53, 18.65, 17.92, 17.89, 17.795, 18.03, 18.45, 18.46, 18.51, 18.575, 18.555, 18.23, 17.81, 17.8, 17.74, 18.315, 18.28, 18.085, 18.355, 18.39, 18.64, 18.48, 18.29, 18.375, 18.49, 18.71, 18.745, 18.88, 18.64, 18.77, 18.145, 17.895, 17.915, 17.35, 16.82, 16.245, 15.91, 15.86, 15.64, 15.44, 15.58, 15.49, 15.695, 15.71, 15.46, 15.6, 15.46, 15.57, 15.97, 16.03, 16.045, 15.99, 16.06, 16.01, 15.84, 15.36, 15.73, 15.755, 16.37, 16.13, 15.87, 15.655, 15.86, 16.01, 15.96, 15.76, 15.67, 15.8232, 15.79, 15.69, 15.38, 15.31, 15.115, 15.025, 15.02, 15.105, 15.42, 15.56, 15.21, 15.46, 15.71, 15.96, 15.73, 15.75, 15.6022, 15.7, 15.63, 15.5, 15.19, 15.34, 15.21, 15.44, 15.78, 16.17, 16.22, 16.15, 15.72, 15.69, 15.561, 14.545, 14.89, 14.79, 14.81, 15.25, 15.18, 15.22, 15.225, 15.21, 15.235, 15.38, 15.51, 15.78, 15.85]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('00cd1d2d-13d8-4391-aafb-279f178593e7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"e6ab3729-1e29-4b13-a08c-61ae7e0b8972\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"e6ab3729-1e29-4b13-a08c-61ae7e0b8972\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'e6ab3729-1e29-4b13-a08c-61ae7e0b8972',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e6ab3729-1e29-4b13-a08c-61ae7e0b8972');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V3uEQQwX4gx"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJJWuxWFX4gx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59625dbe-0aac-45f4-d9cb-8b6253b7f1ed"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.05, .1, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"NVDA\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6014 - accuracy: 0.7195 - val_loss: 0.4851 - val_accuracy: 0.8122\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5460 - accuracy: 0.7396 - val_loss: 0.4376 - val_accuracy: 0.8327\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5096 - accuracy: 0.7758 - val_loss: 0.4309 - val_accuracy: 0.8184\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4935 - accuracy: 0.7812 - val_loss: 0.5178 - val_accuracy: 0.7694\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4941 - accuracy: 0.7832 - val_loss: 0.4486 - val_accuracy: 0.8388\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.5831 - accuracy: 0.7228 - val_loss: 0.4851 - val_accuracy: 0.8122\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4794 - accuracy: 0.7926 - val_loss: 0.4257 - val_accuracy: 0.8347\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4482 - accuracy: 0.8087 - val_loss: 0.4356 - val_accuracy: 0.8449\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4473 - accuracy: 0.8047 - val_loss: 0.4374 - val_accuracy: 0.8429\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4432 - accuracy: 0.8181 - val_loss: 0.4231 - val_accuracy: 0.8469\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.711165\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.710782\n",
            "[2]\tvalidation_0-auc:0.724806\n",
            "[3]\tvalidation_0-auc:0.726417\n",
            "[4]\tvalidation_0-auc:0.729353\n",
            "[5]\tvalidation_0-auc:0.735703\n",
            "[6]\tvalidation_0-auc:0.731702\n",
            "[7]\tvalidation_0-auc:0.732357\n",
            "[8]\tvalidation_0-auc:0.735498\n",
            "[9]\tvalidation_0-auc:0.740469\n",
            "[10]\tvalidation_0-auc:0.733177\n",
            "[11]\tvalidation_0-auc:0.733832\n",
            "[12]\tvalidation_0-auc:0.738871\n",
            "[13]\tvalidation_0-auc:0.732699\n",
            "[14]\tvalidation_0-auc:0.732494\n",
            "[15]\tvalidation_0-auc:0.732221\n",
            "[16]\tvalidation_0-auc:0.736072\n",
            "[17]\tvalidation_0-auc:0.735198\n",
            "[18]\tvalidation_0-auc:0.731224\n",
            "[19]\tvalidation_0-auc:0.730309\n",
            "[20]\tvalidation_0-auc:0.730528\n",
            "[21]\tvalidation_0-auc:0.730459\n",
            "[22]\tvalidation_0-auc:0.729613\n",
            "[23]\tvalidation_0-auc:0.731552\n",
            "[24]\tvalidation_0-auc:0.730541\n",
            "[25]\tvalidation_0-auc:0.730446\n",
            "[26]\tvalidation_0-auc:0.72583\n",
            "[27]\tvalidation_0-auc:0.725858\n",
            "[28]\tvalidation_0-auc:0.722307\n",
            "[29]\tvalidation_0-auc:0.721911\n",
            "[30]\tvalidation_0-auc:0.72314\n",
            "[31]\tvalidation_0-auc:0.724943\n",
            "[32]\tvalidation_0-auc:0.724751\n",
            "[33]\tvalidation_0-auc:0.724424\n",
            "[34]\tvalidation_0-auc:0.724751\n",
            "[35]\tvalidation_0-auc:0.72467\n",
            "[36]\tvalidation_0-auc:0.725421\n",
            "[37]\tvalidation_0-auc:0.72527\n",
            "[38]\tvalidation_0-auc:0.725107\n",
            "[39]\tvalidation_0-auc:0.721119\n",
            "[40]\tvalidation_0-auc:0.721228\n",
            "[41]\tvalidation_0-auc:0.721228\n",
            "[42]\tvalidation_0-auc:0.721952\n",
            "[43]\tvalidation_0-auc:0.722853\n",
            "[44]\tvalidation_0-auc:0.723058\n",
            "[45]\tvalidation_0-auc:0.722498\n",
            "[46]\tvalidation_0-auc:0.722744\n",
            "[47]\tvalidation_0-auc:0.722758\n",
            "[48]\tvalidation_0-auc:0.722758\n",
            "[49]\tvalidation_0-auc:0.723782\n",
            "[50]\tvalidation_0-auc:0.722881\n",
            "[51]\tvalidation_0-auc:0.722881\n",
            "[52]\tvalidation_0-auc:0.722881\n",
            "[53]\tvalidation_0-auc:0.723768\n",
            "[54]\tvalidation_0-auc:0.722034\n",
            "[55]\tvalidation_0-auc:0.722908\n",
            "[56]\tvalidation_0-auc:0.722143\n",
            "[57]\tvalidation_0-auc:0.722908\n",
            "[58]\tvalidation_0-auc:0.723318\n",
            "[59]\tvalidation_0-auc:0.722785\n",
            "Stopping. Best iteration:\n",
            "[9]\tvalidation_0-auc:0.740469\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6123 - accuracy: 0.7117 - val_loss: 0.5036 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5953 - accuracy: 0.7145 - val_loss: 0.4568 - val_accuracy: 0.8556\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5779 - accuracy: 0.7165 - val_loss: 0.4353 - val_accuracy: 0.8556\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5351 - accuracy: 0.7406 - val_loss: 0.4070 - val_accuracy: 0.8796\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5017 - accuracy: 0.7639 - val_loss: 0.3720 - val_accuracy: 0.8775\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.5914 - accuracy: 0.7165 - val_loss: 0.4277 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5174 - accuracy: 0.7563 - val_loss: 0.3956 - val_accuracy: 0.8709\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4937 - accuracy: 0.7701 - val_loss: 0.3811 - val_accuracy: 0.9015\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4557 - accuracy: 0.8016 - val_loss: 0.3893 - val_accuracy: 0.8884\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4723 - accuracy: 0.7927 - val_loss: 0.4074 - val_accuracy: 0.8972\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.789816\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.738065\n",
            "[2]\tvalidation_0-auc:0.720763\n",
            "[3]\tvalidation_0-auc:0.720646\n",
            "[4]\tvalidation_0-auc:0.740739\n",
            "[5]\tvalidation_0-auc:0.741417\n",
            "[6]\tvalidation_0-auc:0.770984\n",
            "[7]\tvalidation_0-auc:0.748392\n",
            "[8]\tvalidation_0-auc:0.746764\n",
            "[9]\tvalidation_0-auc:0.739518\n",
            "[10]\tvalidation_0-auc:0.725548\n",
            "[11]\tvalidation_0-auc:0.740952\n",
            "[12]\tvalidation_0-auc:0.74752\n",
            "[13]\tvalidation_0-auc:0.739382\n",
            "[14]\tvalidation_0-auc:0.735701\n",
            "[15]\tvalidation_0-auc:0.73357\n",
            "[16]\tvalidation_0-auc:0.747888\n",
            "[17]\tvalidation_0-auc:0.732717\n",
            "[18]\tvalidation_0-auc:0.737096\n",
            "[19]\tvalidation_0-auc:0.737406\n",
            "[20]\tvalidation_0-auc:0.746086\n",
            "[21]\tvalidation_0-auc:0.75124\n",
            "[22]\tvalidation_0-auc:0.750368\n",
            "[23]\tvalidation_0-auc:0.756413\n",
            "[24]\tvalidation_0-auc:0.755987\n",
            "[25]\tvalidation_0-auc:0.756336\n",
            "[26]\tvalidation_0-auc:0.759668\n",
            "[27]\tvalidation_0-auc:0.759668\n",
            "[28]\tvalidation_0-auc:0.75777\n",
            "[29]\tvalidation_0-auc:0.75777\n",
            "[30]\tvalidation_0-auc:0.760405\n",
            "[31]\tvalidation_0-auc:0.754069\n",
            "[32]\tvalidation_0-auc:0.75279\n",
            "[33]\tvalidation_0-auc:0.757886\n",
            "[34]\tvalidation_0-auc:0.757886\n",
            "[35]\tvalidation_0-auc:0.757886\n",
            "[36]\tvalidation_0-auc:0.757498\n",
            "[37]\tvalidation_0-auc:0.756336\n",
            "[38]\tvalidation_0-auc:0.757808\n",
            "[39]\tvalidation_0-auc:0.758312\n",
            "[40]\tvalidation_0-auc:0.752887\n",
            "[41]\tvalidation_0-auc:0.749884\n",
            "[42]\tvalidation_0-auc:0.756646\n",
            "[43]\tvalidation_0-auc:0.755754\n",
            "[44]\tvalidation_0-auc:0.755754\n",
            "[45]\tvalidation_0-auc:0.762652\n",
            "[46]\tvalidation_0-auc:0.762652\n",
            "[47]\tvalidation_0-auc:0.768348\n",
            "[48]\tvalidation_0-auc:0.767244\n",
            "[49]\tvalidation_0-auc:0.766895\n",
            "[50]\tvalidation_0-auc:0.766314\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.789816\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.8387755102040816 |        0.76        | 0.20652173913043478 |  0.3247863247863248 |\n",
            "|     GRU 0.1      | 0.8469387755102041 | 0.7428571428571429 |  0.2826086956521739 |  0.4094488188976378 |\n",
            "|   XGBoost 0.1    | 0.8306122448979592 | 0.5714285714285714 |  0.391304347826087  |  0.4645161290322581 |\n",
            "|    Logreg 0.1    | 0.8448979591836735 |        0.7         | 0.30434782608695654 | 0.42424242424242425 |\n",
            "|     SVM 0.1      | 0.8448979591836735 | 0.6904761904761905 | 0.31521739130434784 | 0.43283582089552236 |\n",
            "|  LSTM beta 0.1   | 0.8774617067833698 | 0.9166666666666666 | 0.16666666666666666 | 0.28205128205128205 |\n",
            "|   GRU beta 0.1   | 0.8971553610503282 | 0.7435897435897436 |  0.4393939393939394 |  0.5523809523809524 |\n",
            "| XGBoost beta 0.1 | 0.838074398249453  |        0.3         | 0.09090909090909091 | 0.13953488372093023 |\n",
            "| logreg beta 0.1  | 0.9015317286652079 | 0.7837837837837838 |  0.4393939393939394 |  0.5631067961165048 |\n",
            "|   svm beta 0.1   | 0.8862144420131292 | 0.6666666666666666 | 0.42424242424242425 |  0.5185185185185185 |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.5190 - accuracy: 0.7926 - val_loss: 0.4249 - val_accuracy: 0.8653\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4798 - accuracy: 0.8000 - val_loss: 0.3531 - val_accuracy: 0.8694\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4570 - accuracy: 0.8101 - val_loss: 0.3710 - val_accuracy: 0.8653\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4476 - accuracy: 0.8121 - val_loss: 0.3643 - val_accuracy: 0.8816\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4454 - accuracy: 0.8208 - val_loss: 0.3673 - val_accuracy: 0.8714\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.5279 - accuracy: 0.7933 - val_loss: 0.4223 - val_accuracy: 0.8653\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4683 - accuracy: 0.8074 - val_loss: 0.3874 - val_accuracy: 0.8735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4426 - accuracy: 0.8235 - val_loss: 0.3639 - val_accuracy: 0.8735\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4245 - accuracy: 0.8336 - val_loss: 0.3686 - val_accuracy: 0.8735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4219 - accuracy: 0.8322 - val_loss: 0.3590 - val_accuracy: 0.8673\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.710674\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.730685\n",
            "[2]\tvalidation_0-auc:0.730685\n",
            "[3]\tvalidation_0-auc:0.748785\n",
            "[4]\tvalidation_0-auc:0.74925\n",
            "[5]\tvalidation_0-auc:0.756754\n",
            "[6]\tvalidation_0-auc:0.748571\n",
            "[7]\tvalidation_0-auc:0.75209\n",
            "[8]\tvalidation_0-auc:0.752484\n",
            "[9]\tvalidation_0-auc:0.750607\n",
            "[10]\tvalidation_0-auc:0.749428\n",
            "[11]\tvalidation_0-auc:0.736081\n",
            "[12]\tvalidation_0-auc:0.743729\n",
            "[13]\tvalidation_0-auc:0.744425\n",
            "[14]\tvalidation_0-auc:0.742353\n",
            "[15]\tvalidation_0-auc:0.743175\n",
            "[16]\tvalidation_0-auc:0.745497\n",
            "[17]\tvalidation_0-auc:0.745265\n",
            "[18]\tvalidation_0-auc:0.746498\n",
            "[19]\tvalidation_0-auc:0.746373\n",
            "[20]\tvalidation_0-auc:0.745426\n",
            "[21]\tvalidation_0-auc:0.744836\n",
            "[22]\tvalidation_0-auc:0.747088\n",
            "[23]\tvalidation_0-auc:0.747016\n",
            "[24]\tvalidation_0-auc:0.745748\n",
            "[25]\tvalidation_0-auc:0.745247\n",
            "[26]\tvalidation_0-auc:0.74539\n",
            "[27]\tvalidation_0-auc:0.740066\n",
            "[28]\tvalidation_0-auc:0.73869\n",
            "[29]\tvalidation_0-auc:0.738618\n",
            "[30]\tvalidation_0-auc:0.736224\n",
            "[31]\tvalidation_0-auc:0.735903\n",
            "[32]\tvalidation_0-auc:0.736046\n",
            "[33]\tvalidation_0-auc:0.736189\n",
            "[34]\tvalidation_0-auc:0.731418\n",
            "[35]\tvalidation_0-auc:0.726629\n",
            "[36]\tvalidation_0-auc:0.726004\n",
            "[37]\tvalidation_0-auc:0.729506\n",
            "[38]\tvalidation_0-auc:0.728613\n",
            "[39]\tvalidation_0-auc:0.72704\n",
            "[40]\tvalidation_0-auc:0.726719\n",
            "[41]\tvalidation_0-auc:0.723485\n",
            "[42]\tvalidation_0-auc:0.723378\n",
            "[43]\tvalidation_0-auc:0.724092\n",
            "[44]\tvalidation_0-auc:0.723842\n",
            "[45]\tvalidation_0-auc:0.719054\n",
            "[46]\tvalidation_0-auc:0.717982\n",
            "[47]\tvalidation_0-auc:0.716785\n",
            "[48]\tvalidation_0-auc:0.716356\n",
            "[49]\tvalidation_0-auc:0.715266\n",
            "[50]\tvalidation_0-auc:0.715552\n",
            "[51]\tvalidation_0-auc:0.714623\n",
            "[52]\tvalidation_0-auc:0.713336\n",
            "[53]\tvalidation_0-auc:0.713211\n",
            "[54]\tvalidation_0-auc:0.71364\n",
            "[55]\tvalidation_0-auc:0.713908\n",
            "Stopping. Best iteration:\n",
            "[5]\tvalidation_0-auc:0.756754\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.5366 - accuracy: 0.7893 - val_loss: 0.4115 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5079 - accuracy: 0.7927 - val_loss: 0.3710 - val_accuracy: 0.8556\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4671 - accuracy: 0.8037 - val_loss: 0.3669 - val_accuracy: 0.8556\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.4387 - accuracy: 0.8140 - val_loss: 0.3424 - val_accuracy: 0.8556\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4431 - accuracy: 0.8181 - val_loss: 0.3483 - val_accuracy: 0.8556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5156 - accuracy: 0.7920 - val_loss: 0.3779 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4444 - accuracy: 0.8202 - val_loss: 0.3459 - val_accuracy: 0.8731\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4267 - accuracy: 0.8250 - val_loss: 0.3456 - val_accuracy: 0.8731\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4185 - accuracy: 0.8257 - val_loss: 0.3817 - val_accuracy: 0.8972\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4139 - accuracy: 0.8284 - val_loss: 0.3618 - val_accuracy: 0.8709\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.84831\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.850868\n",
            "[2]\tvalidation_0-auc:0.84862\n",
            "[3]\tvalidation_0-auc:0.844397\n",
            "[4]\tvalidation_0-auc:0.882624\n",
            "[5]\tvalidation_0-auc:0.898958\n",
            "[6]\tvalidation_0-auc:0.890801\n",
            "[7]\tvalidation_0-auc:0.890839\n",
            "[8]\tvalidation_0-auc:0.889793\n",
            "[9]\tvalidation_0-auc:0.895451\n",
            "[10]\tvalidation_0-auc:0.898744\n",
            "[11]\tvalidation_0-auc:0.897563\n",
            "[12]\tvalidation_0-auc:0.897834\n",
            "[13]\tvalidation_0-auc:0.89795\n",
            "[14]\tvalidation_0-auc:0.897291\n",
            "[15]\tvalidation_0-auc:0.896691\n",
            "[16]\tvalidation_0-auc:0.895199\n",
            "[17]\tvalidation_0-auc:0.892041\n",
            "[18]\tvalidation_0-auc:0.890471\n",
            "[19]\tvalidation_0-auc:0.892409\n",
            "[20]\tvalidation_0-auc:0.890626\n",
            "[21]\tvalidation_0-auc:0.890859\n",
            "[22]\tvalidation_0-auc:0.88896\n",
            "[23]\tvalidation_0-auc:0.890452\n",
            "[24]\tvalidation_0-auc:0.890801\n",
            "[25]\tvalidation_0-auc:0.891188\n",
            "[26]\tvalidation_0-auc:0.892506\n",
            "[27]\tvalidation_0-auc:0.892506\n",
            "[28]\tvalidation_0-auc:0.890917\n",
            "[29]\tvalidation_0-auc:0.891692\n",
            "[30]\tvalidation_0-auc:0.886867\n",
            "[31]\tvalidation_0-auc:0.887487\n",
            "[32]\tvalidation_0-auc:0.887022\n",
            "[33]\tvalidation_0-auc:0.886635\n",
            "[34]\tvalidation_0-auc:0.886538\n",
            "[35]\tvalidation_0-auc:0.881016\n",
            "[36]\tvalidation_0-auc:0.881016\n",
            "[37]\tvalidation_0-auc:0.879757\n",
            "[38]\tvalidation_0-auc:0.879485\n",
            "[39]\tvalidation_0-auc:0.880144\n",
            "[40]\tvalidation_0-auc:0.879292\n",
            "[41]\tvalidation_0-auc:0.878827\n",
            "[42]\tvalidation_0-auc:0.878517\n",
            "[43]\tvalidation_0-auc:0.879001\n",
            "[44]\tvalidation_0-auc:0.880745\n",
            "[45]\tvalidation_0-auc:0.881132\n",
            "[46]\tvalidation_0-auc:0.878265\n",
            "[47]\tvalidation_0-auc:0.877528\n",
            "[48]\tvalidation_0-auc:0.877257\n",
            "[49]\tvalidation_0-auc:0.877257\n",
            "[50]\tvalidation_0-auc:0.876928\n",
            "[51]\tvalidation_0-auc:0.876192\n",
            "[52]\tvalidation_0-auc:0.873363\n",
            "[53]\tvalidation_0-auc:0.875029\n",
            "[54]\tvalidation_0-auc:0.874525\n",
            "[55]\tvalidation_0-auc:0.872704\n",
            "Stopping. Best iteration:\n",
            "[5]\tvalidation_0-auc:0.898958\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+----------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall        |       F1 score      |\n",
            "+------------------+--------------------+--------------------+----------------------+---------------------+\n",
            "|     LSTM 0.2     | 0.8714285714285714 |        1.0         | 0.045454545454545456 | 0.08695652173913045 |\n",
            "|     GRU 0.2      | 0.8673469387755102 |        1.0         | 0.015151515151515152 | 0.02985074626865672 |\n",
            "|   XGBoost 0.2    | 0.8734693877551021 | 0.6666666666666666 | 0.12121212121212122  | 0.20512820512820512 |\n",
            "|    Logreg 0.2    | 0.8693877551020408 |        1.0         | 0.030303030303030304 | 0.05882352941176471 |\n",
            "|     SVM 0.2      | 0.8693877551020408 |        1.0         | 0.030303030303030304 | 0.05882352941176471 |\n",
            "|  LSTM beta 0.2   | 0.8555798687089715 |        0.0         |         0.0          |         0.0         |\n",
            "|   GRU beta 0.2   | 0.8708971553610503 | 0.8888888888888888 | 0.12121212121212122  | 0.21333333333333335 |\n",
            "| XGBoost beta 0.2 | 0.9037199124726477 |        1.0         |  0.3333333333333333  |         0.5         |\n",
            "| logreg beta 0.2  | 0.8687089715536105 |        1.0         | 0.09090909090909091  | 0.16666666666666669 |\n",
            "|   svm beta 0.2   | 0.8708971553610503 |        1.0         | 0.10606060606060606  | 0.19178082191780824 |\n",
            "+------------------+--------------------+--------------------+----------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.5984 - accuracy: 0.7416 - val_loss: 0.4432 - val_accuracy: 0.8653\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5413 - accuracy: 0.7591 - val_loss: 0.3632 - val_accuracy: 0.8796\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5212 - accuracy: 0.7725 - val_loss: 0.4036 - val_accuracy: 0.8796\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4897 - accuracy: 0.7893 - val_loss: 0.5849 - val_accuracy: 0.6490\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4975 - accuracy: 0.7597 - val_loss: 0.4383 - val_accuracy: 0.8224\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.5583 - accuracy: 0.7483 - val_loss: 0.3625 - val_accuracy: 0.8653\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4808 - accuracy: 0.7953 - val_loss: 0.5019 - val_accuracy: 0.8245\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4691 - accuracy: 0.8027 - val_loss: 0.4216 - val_accuracy: 0.8694\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4624 - accuracy: 0.8000 - val_loss: 0.3764 - val_accuracy: 0.8776\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4496 - accuracy: 0.8107 - val_loss: 0.3810 - val_accuracy: 0.8755\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.709602\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.720662\n",
            "[2]\tvalidation_0-auc:0.721573\n",
            "[3]\tvalidation_0-auc:0.723896\n",
            "[4]\tvalidation_0-auc:0.725254\n",
            "[5]\tvalidation_0-auc:0.729703\n",
            "[6]\tvalidation_0-auc:0.733097\n",
            "[7]\tvalidation_0-auc:0.741048\n",
            "[8]\tvalidation_0-auc:0.735617\n",
            "[9]\tvalidation_0-auc:0.737207\n",
            "[10]\tvalidation_0-auc:0.732937\n",
            "[11]\tvalidation_0-auc:0.732365\n",
            "[12]\tvalidation_0-auc:0.730793\n",
            "[13]\tvalidation_0-auc:0.734884\n",
            "[14]\tvalidation_0-auc:0.733919\n",
            "[15]\tvalidation_0-auc:0.733223\n",
            "[16]\tvalidation_0-auc:0.735277\n",
            "[17]\tvalidation_0-auc:0.734706\n",
            "[18]\tvalidation_0-auc:0.733187\n",
            "[19]\tvalidation_0-auc:0.733508\n",
            "[20]\tvalidation_0-auc:0.739065\n",
            "[21]\tvalidation_0-auc:0.737171\n",
            "[22]\tvalidation_0-auc:0.738958\n",
            "[23]\tvalidation_0-auc:0.738761\n",
            "[24]\tvalidation_0-auc:0.740548\n",
            "[25]\tvalidation_0-auc:0.740798\n",
            "[26]\tvalidation_0-auc:0.73953\n",
            "[27]\tvalidation_0-auc:0.737171\n",
            "[28]\tvalidation_0-auc:0.738315\n",
            "[29]\tvalidation_0-auc:0.737457\n",
            "[30]\tvalidation_0-auc:0.735706\n",
            "[31]\tvalidation_0-auc:0.737886\n",
            "[32]\tvalidation_0-auc:0.736582\n",
            "[33]\tvalidation_0-auc:0.737278\n",
            "[34]\tvalidation_0-auc:0.736742\n",
            "[35]\tvalidation_0-auc:0.734455\n",
            "[36]\tvalidation_0-auc:0.734527\n",
            "[37]\tvalidation_0-auc:0.735706\n",
            "[38]\tvalidation_0-auc:0.735938\n",
            "[39]\tvalidation_0-auc:0.733919\n",
            "[40]\tvalidation_0-auc:0.735206\n",
            "[41]\tvalidation_0-auc:0.73542\n",
            "[42]\tvalidation_0-auc:0.733401\n",
            "[43]\tvalidation_0-auc:0.730864\n",
            "[44]\tvalidation_0-auc:0.733187\n",
            "[45]\tvalidation_0-auc:0.732937\n",
            "[46]\tvalidation_0-auc:0.730149\n",
            "[47]\tvalidation_0-auc:0.732222\n",
            "[48]\tvalidation_0-auc:0.732847\n",
            "[49]\tvalidation_0-auc:0.733097\n",
            "[50]\tvalidation_0-auc:0.731096\n",
            "[51]\tvalidation_0-auc:0.731525\n",
            "[52]\tvalidation_0-auc:0.728774\n",
            "[53]\tvalidation_0-auc:0.730382\n",
            "[54]\tvalidation_0-auc:0.730596\n",
            "[55]\tvalidation_0-auc:0.727987\n",
            "[56]\tvalidation_0-auc:0.726844\n",
            "[57]\tvalidation_0-auc:0.723878\n",
            "Stopping. Best iteration:\n",
            "[7]\tvalidation_0-auc:0.741048\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.5858 - accuracy: 0.7316 - val_loss: 0.4369 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5807 - accuracy: 0.7364 - val_loss: 0.4431 - val_accuracy: 0.8556\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5500 - accuracy: 0.7399 - val_loss: 0.3993 - val_accuracy: 0.8556\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 13ms/step - loss: 0.5138 - accuracy: 0.7639 - val_loss: 0.4115 - val_accuracy: 0.8556\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4888 - accuracy: 0.7817 - val_loss: 0.3743 - val_accuracy: 0.8687\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5699 - accuracy: 0.7454 - val_loss: 0.4739 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5048 - accuracy: 0.7728 - val_loss: 0.3980 - val_accuracy: 0.8665\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4894 - accuracy: 0.7893 - val_loss: 0.3944 - val_accuracy: 0.9103\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4774 - accuracy: 0.7852 - val_loss: 0.3904 - val_accuracy: 0.9212\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4524 - accuracy: 0.7962 - val_loss: 0.3353 - val_accuracy: 0.8643\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.828489\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.821902\n",
            "[2]\tvalidation_0-auc:0.810722\n",
            "[3]\tvalidation_0-auc:0.830776\n",
            "[4]\tvalidation_0-auc:0.835019\n",
            "[5]\tvalidation_0-auc:0.836143\n",
            "[6]\tvalidation_0-auc:0.831841\n",
            "[7]\tvalidation_0-auc:0.824091\n",
            "[8]\tvalidation_0-auc:0.831551\n",
            "[9]\tvalidation_0-auc:0.832558\n",
            "[10]\tvalidation_0-auc:0.832461\n",
            "[11]\tvalidation_0-auc:0.83159\n",
            "[12]\tvalidation_0-auc:0.843292\n",
            "[13]\tvalidation_0-auc:0.845656\n",
            "[14]\tvalidation_0-auc:0.846896\n",
            "[15]\tvalidation_0-auc:0.848524\n",
            "[16]\tvalidation_0-auc:0.849841\n",
            "[17]\tvalidation_0-auc:0.849376\n",
            "[18]\tvalidation_0-auc:0.85143\n",
            "[19]\tvalidation_0-auc:0.850655\n",
            "[20]\tvalidation_0-auc:0.851042\n",
            "[21]\tvalidation_0-auc:0.851585\n",
            "[22]\tvalidation_0-auc:0.850074\n",
            "[23]\tvalidation_0-auc:0.853232\n",
            "[24]\tvalidation_0-auc:0.853348\n",
            "[25]\tvalidation_0-auc:0.854084\n",
            "[26]\tvalidation_0-auc:0.853387\n",
            "[27]\tvalidation_0-auc:0.859664\n",
            "[28]\tvalidation_0-auc:0.858076\n",
            "[29]\tvalidation_0-auc:0.857456\n",
            "[30]\tvalidation_0-auc:0.858192\n",
            "[31]\tvalidation_0-auc:0.858231\n",
            "[32]\tvalidation_0-auc:0.859781\n",
            "[33]\tvalidation_0-auc:0.862493\n",
            "[34]\tvalidation_0-auc:0.862067\n",
            "[35]\tvalidation_0-auc:0.861641\n",
            "[36]\tvalidation_0-auc:0.862028\n",
            "[37]\tvalidation_0-auc:0.861951\n",
            "[38]\tvalidation_0-auc:0.860749\n",
            "[39]\tvalidation_0-auc:0.861311\n",
            "[40]\tvalidation_0-auc:0.860808\n",
            "[41]\tvalidation_0-auc:0.860575\n",
            "[42]\tvalidation_0-auc:0.860265\n",
            "[43]\tvalidation_0-auc:0.861544\n",
            "[44]\tvalidation_0-auc:0.861699\n",
            "[45]\tvalidation_0-auc:0.861583\n",
            "[46]\tvalidation_0-auc:0.864063\n",
            "[47]\tvalidation_0-auc:0.866543\n",
            "[48]\tvalidation_0-auc:0.866349\n",
            "[49]\tvalidation_0-auc:0.867046\n",
            "[50]\tvalidation_0-auc:0.866775\n",
            "[51]\tvalidation_0-auc:0.868984\n",
            "[52]\tvalidation_0-auc:0.868984\n",
            "[53]\tvalidation_0-auc:0.866891\n",
            "[54]\tvalidation_0-auc:0.868015\n",
            "[55]\tvalidation_0-auc:0.867938\n",
            "[56]\tvalidation_0-auc:0.867938\n",
            "[57]\tvalidation_0-auc:0.86693\n",
            "[58]\tvalidation_0-auc:0.86662\n",
            "[59]\tvalidation_0-auc:0.867046\n",
            "[60]\tvalidation_0-auc:0.866795\n",
            "[61]\tvalidation_0-auc:0.863074\n",
            "[62]\tvalidation_0-auc:0.862609\n",
            "[63]\tvalidation_0-auc:0.862454\n",
            "[64]\tvalidation_0-auc:0.862454\n",
            "[65]\tvalidation_0-auc:0.863772\n",
            "[66]\tvalidation_0-auc:0.863501\n",
            "[67]\tvalidation_0-auc:0.862513\n",
            "[68]\tvalidation_0-auc:0.862513\n",
            "[69]\tvalidation_0-auc:0.862048\n",
            "[70]\tvalidation_0-auc:0.861815\n",
            "[71]\tvalidation_0-auc:0.86228\n",
            "[72]\tvalidation_0-auc:0.86228\n",
            "[73]\tvalidation_0-auc:0.861137\n",
            "[74]\tvalidation_0-auc:0.861873\n",
            "[75]\tvalidation_0-auc:0.86197\n",
            "[76]\tvalidation_0-auc:0.863365\n",
            "[77]\tvalidation_0-auc:0.865496\n",
            "[78]\tvalidation_0-auc:0.86445\n",
            "[79]\tvalidation_0-auc:0.863753\n",
            "[80]\tvalidation_0-auc:0.863753\n",
            "[81]\tvalidation_0-auc:0.862513\n",
            "[82]\tvalidation_0-auc:0.86197\n",
            "[83]\tvalidation_0-auc:0.862086\n",
            "[84]\tvalidation_0-auc:0.862474\n",
            "[85]\tvalidation_0-auc:0.862823\n",
            "[86]\tvalidation_0-auc:0.860033\n",
            "[87]\tvalidation_0-auc:0.859723\n",
            "[88]\tvalidation_0-auc:0.859529\n",
            "[89]\tvalidation_0-auc:0.857882\n",
            "[90]\tvalidation_0-auc:0.857436\n",
            "[91]\tvalidation_0-auc:0.857204\n",
            "[92]\tvalidation_0-auc:0.856971\n",
            "[93]\tvalidation_0-auc:0.856855\n",
            "[94]\tvalidation_0-auc:0.856157\n",
            "[95]\tvalidation_0-auc:0.854917\n",
            "[96]\tvalidation_0-auc:0.854801\n",
            "[97]\tvalidation_0-auc:0.854801\n",
            "[98]\tvalidation_0-auc:0.853871\n",
            "[99]\tvalidation_0-auc:0.854181\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.8224489795918367 | 0.38461538461538464 |  0.5303030303030303 | 0.44585987261146504 |\n",
            "|      GRU 0.15     | 0.8755102040816326 |  0.6190476190476191 | 0.19696969696969696 | 0.29885057471264365 |\n",
            "|    XGBoost 0.15   | 0.8653061224489796 |         0.5         |  0.3181818181818182 |  0.3888888888888889 |\n",
            "|    Logreg 0.15    | 0.8918367346938776 |  0.782608695652174  |  0.2727272727272727 | 0.40449438202247184 |\n",
            "|      SVM 0.15     | 0.8816326530612245 |  0.7222222222222222 | 0.19696969696969696 | 0.30952380952380953 |\n",
            "|   LSTM beta 0.15  | 0.8687089715536105 |        0.875        | 0.10606060606060606 |  0.1891891891891892 |\n",
            "|   GRU beta 0.15   | 0.8643326039387309 |         1.0         | 0.06060606060606061 |  0.1142857142857143 |\n",
            "| XGBoost beta 0.15 | 0.9059080962800875 |  0.7804878048780488 | 0.48484848484848486 |  0.5981308411214953 |\n",
            "|  logreg beta 0.15 | 0.8971553610503282 |  0.9130434782608695 |  0.3181818181818182 | 0.47191011235955055 |\n",
            "|   svm beta 0.15   | 0.8796498905908097 |         1.0         | 0.16666666666666666 |  0.2857142857142857 |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXKfwY7wX4gx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1384e11-857c-479b-8fbc-d8320c6440e8"
      },
      "source": [
        "Result_cross.to_csv('NVDA_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.760000</td>\n",
              "      <td>0.838776</td>\n",
              "      <td>0.324786</td>\n",
              "      <td>0.206522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.742857</td>\n",
              "      <td>0.846939</td>\n",
              "      <td>0.409449</td>\n",
              "      <td>0.282609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.830612</td>\n",
              "      <td>0.464516</td>\n",
              "      <td>0.391304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.844898</td>\n",
              "      <td>0.424242</td>\n",
              "      <td>0.304348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.844898</td>\n",
              "      <td>0.432836</td>\n",
              "      <td>0.315217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.877462</td>\n",
              "      <td>0.282051</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.743590</td>\n",
              "      <td>0.897155</td>\n",
              "      <td>0.552381</td>\n",
              "      <td>0.439394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.838074</td>\n",
              "      <td>0.139535</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.783784</td>\n",
              "      <td>0.901532</td>\n",
              "      <td>0.563107</td>\n",
              "      <td>0.439394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.886214</td>\n",
              "      <td>0.518519</td>\n",
              "      <td>0.424242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.871429</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.045455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.867347</td>\n",
              "      <td>0.029851</td>\n",
              "      <td>0.015152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.205128</td>\n",
              "      <td>0.121212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.869388</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.030303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.869388</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.030303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.855580</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.870897</td>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.121212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.903720</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.868709</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.870897</td>\n",
              "      <td>0.191781</td>\n",
              "      <td>0.106061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.384615</td>\n",
              "      <td>0.822449</td>\n",
              "      <td>0.445860</td>\n",
              "      <td>0.530303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.619048</td>\n",
              "      <td>0.875510</td>\n",
              "      <td>0.298851</td>\n",
              "      <td>0.196970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.865306</td>\n",
              "      <td>0.388889</td>\n",
              "      <td>0.318182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.891837</td>\n",
              "      <td>0.404494</td>\n",
              "      <td>0.272727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>0.881633</td>\n",
              "      <td>0.309524</td>\n",
              "      <td>0.196970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.868709</td>\n",
              "      <td>0.189189</td>\n",
              "      <td>0.106061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.864333</td>\n",
              "      <td>0.114286</td>\n",
              "      <td>0.060606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.780488</td>\n",
              "      <td>0.905908</td>\n",
              "      <td>0.598131</td>\n",
              "      <td>0.484848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.913043</td>\n",
              "      <td>0.897155</td>\n",
              "      <td>0.471910</td>\n",
              "      <td>0.318182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.879650</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  NVDA  0.760000  0.838776  0.324786  0.206522\n",
              "1            GRU 0.1  NVDA  0.742857  0.846939  0.409449  0.282609\n",
              "2        XGBoost 0.1  NVDA  0.571429  0.830612  0.464516  0.391304\n",
              "3         Logreg 0.1  NVDA  0.700000  0.844898  0.424242  0.304348\n",
              "4            SVM 0.1  NVDA  0.690476  0.844898  0.432836  0.315217\n",
              "5      LSTM beta 0.1  NVDA  0.916667  0.877462  0.282051  0.166667\n",
              "6       GRU beta 0.1  NVDA  0.743590  0.897155  0.552381  0.439394\n",
              "7   XGBoost beta 0.1  NVDA  0.300000  0.838074  0.139535  0.090909\n",
              "8    logreg beta 0.1  NVDA  0.783784  0.901532  0.563107  0.439394\n",
              "9       svm beta 0.1  NVDA  0.666667  0.886214  0.518519  0.424242\n",
              "0           LSTM 0.2  NVDA  1.000000  0.871429  0.086957  0.045455\n",
              "1            GRU 0.2  NVDA  1.000000  0.867347  0.029851  0.015152\n",
              "2        XGBoost 0.2  NVDA  0.666667  0.873469  0.205128  0.121212\n",
              "3         Logreg 0.2  NVDA  1.000000  0.869388  0.058824  0.030303\n",
              "4            SVM 0.2  NVDA  1.000000  0.869388  0.058824  0.030303\n",
              "5      LSTM beta 0.2  NVDA  0.000000  0.855580  0.000000  0.000000\n",
              "6       GRU beta 0.2  NVDA  0.888889  0.870897  0.213333  0.121212\n",
              "7   XGBoost beta 0.2  NVDA  1.000000  0.903720  0.500000  0.333333\n",
              "8    logreg beta 0.2  NVDA  1.000000  0.868709  0.166667  0.090909\n",
              "9       svm beta 0.2  NVDA  1.000000  0.870897  0.191781  0.106061\n",
              "0          LSTM 0.15  NVDA  0.384615  0.822449  0.445860  0.530303\n",
              "1           GRU 0.15  NVDA  0.619048  0.875510  0.298851  0.196970\n",
              "2       XGBoost 0.15  NVDA  0.500000  0.865306  0.388889  0.318182\n",
              "3        Logreg 0.15  NVDA  0.782609  0.891837  0.404494  0.272727\n",
              "4           SVM 0.15  NVDA  0.722222  0.881633  0.309524  0.196970\n",
              "5     LSTM beta 0.15  NVDA  0.875000  0.868709  0.189189  0.106061\n",
              "6      GRU beta 0.15  NVDA  1.000000  0.864333  0.114286  0.060606\n",
              "7  XGBoost beta 0.15  NVDA  0.780488  0.905908  0.598131  0.484848\n",
              "8   logreg beta 0.15  NVDA  0.913043  0.897155  0.471910  0.318182\n",
              "9      svm beta 0.15  NVDA  1.000000  0.879650  0.285714  0.166667"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MQu2phiX4gx"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN8EgkdNX4gx"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKxRkDlpX4gx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0954877b-eb21-453c-825e-c51f86b38160"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"NVDA\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6129 - accuracy: 0.7161 - val_loss: 0.5083 - val_accuracy: 0.8122\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6001 - accuracy: 0.7208 - val_loss: 0.5182 - val_accuracy: 0.8122\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5861 - accuracy: 0.7168 - val_loss: 0.5237 - val_accuracy: 0.8122\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5210 - accuracy: 0.7550 - val_loss: 0.5804 - val_accuracy: 0.6510\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5080 - accuracy: 0.7705 - val_loss: 0.4577 - val_accuracy: 0.8204\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.5840 - accuracy: 0.7248 - val_loss: 0.5098 - val_accuracy: 0.8204\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4940 - accuracy: 0.7812 - val_loss: 0.5140 - val_accuracy: 0.8347\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4523 - accuracy: 0.8148 - val_loss: 0.4689 - val_accuracy: 0.8327\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4402 - accuracy: 0.8047 - val_loss: 0.4591 - val_accuracy: 0.8327\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4371 - accuracy: 0.8040 - val_loss: 0.4502 - val_accuracy: 0.8388\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.711165\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.710782\n",
            "[2]\tvalidation_0-auc:0.724806\n",
            "[3]\tvalidation_0-auc:0.726417\n",
            "[4]\tvalidation_0-auc:0.729353\n",
            "[5]\tvalidation_0-auc:0.735703\n",
            "[6]\tvalidation_0-auc:0.731702\n",
            "[7]\tvalidation_0-auc:0.732357\n",
            "[8]\tvalidation_0-auc:0.735498\n",
            "[9]\tvalidation_0-auc:0.740469\n",
            "[10]\tvalidation_0-auc:0.733177\n",
            "[11]\tvalidation_0-auc:0.733832\n",
            "[12]\tvalidation_0-auc:0.738871\n",
            "[13]\tvalidation_0-auc:0.732699\n",
            "[14]\tvalidation_0-auc:0.732494\n",
            "[15]\tvalidation_0-auc:0.732221\n",
            "[16]\tvalidation_0-auc:0.736072\n",
            "[17]\tvalidation_0-auc:0.735198\n",
            "[18]\tvalidation_0-auc:0.731224\n",
            "[19]\tvalidation_0-auc:0.730309\n",
            "[20]\tvalidation_0-auc:0.730528\n",
            "[21]\tvalidation_0-auc:0.730459\n",
            "[22]\tvalidation_0-auc:0.729613\n",
            "[23]\tvalidation_0-auc:0.731552\n",
            "[24]\tvalidation_0-auc:0.730541\n",
            "[25]\tvalidation_0-auc:0.730446\n",
            "[26]\tvalidation_0-auc:0.72583\n",
            "[27]\tvalidation_0-auc:0.725858\n",
            "[28]\tvalidation_0-auc:0.722307\n",
            "[29]\tvalidation_0-auc:0.721911\n",
            "[30]\tvalidation_0-auc:0.72314\n",
            "[31]\tvalidation_0-auc:0.724943\n",
            "[32]\tvalidation_0-auc:0.724751\n",
            "[33]\tvalidation_0-auc:0.724424\n",
            "[34]\tvalidation_0-auc:0.724751\n",
            "[35]\tvalidation_0-auc:0.72467\n",
            "[36]\tvalidation_0-auc:0.725421\n",
            "[37]\tvalidation_0-auc:0.72527\n",
            "[38]\tvalidation_0-auc:0.725107\n",
            "[39]\tvalidation_0-auc:0.721119\n",
            "[40]\tvalidation_0-auc:0.721228\n",
            "[41]\tvalidation_0-auc:0.721228\n",
            "[42]\tvalidation_0-auc:0.721952\n",
            "[43]\tvalidation_0-auc:0.722853\n",
            "[44]\tvalidation_0-auc:0.723058\n",
            "[45]\tvalidation_0-auc:0.722498\n",
            "[46]\tvalidation_0-auc:0.722744\n",
            "[47]\tvalidation_0-auc:0.722758\n",
            "[48]\tvalidation_0-auc:0.722758\n",
            "[49]\tvalidation_0-auc:0.723782\n",
            "[50]\tvalidation_0-auc:0.722881\n",
            "[51]\tvalidation_0-auc:0.722881\n",
            "[52]\tvalidation_0-auc:0.722881\n",
            "[53]\tvalidation_0-auc:0.723768\n",
            "[54]\tvalidation_0-auc:0.722034\n",
            "[55]\tvalidation_0-auc:0.722908\n",
            "[56]\tvalidation_0-auc:0.722143\n",
            "[57]\tvalidation_0-auc:0.722908\n",
            "[58]\tvalidation_0-auc:0.723318\n",
            "[59]\tvalidation_0-auc:0.722785\n",
            "Stopping. Best iteration:\n",
            "[9]\tvalidation_0-auc:0.740469\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6156 - accuracy: 0.7104 - val_loss: 0.4857 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5991 - accuracy: 0.7145 - val_loss: 0.4392 - val_accuracy: 0.8556\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5673 - accuracy: 0.7097 - val_loss: 0.4341 - val_accuracy: 0.8556\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5183 - accuracy: 0.7419 - val_loss: 0.4814 - val_accuracy: 0.9037\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4956 - accuracy: 0.7763 - val_loss: 0.4270 - val_accuracy: 0.8972\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6002 - accuracy: 0.7179 - val_loss: 0.4660 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5101 - accuracy: 0.7563 - val_loss: 0.3908 - val_accuracy: 0.9190\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4794 - accuracy: 0.7797 - val_loss: 0.3869 - val_accuracy: 0.9015\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4805 - accuracy: 0.7968 - val_loss: 0.3791 - val_accuracy: 0.9125\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4743 - accuracy: 0.7920 - val_loss: 0.4626 - val_accuracy: 0.8753\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.789816\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.738065\n",
            "[2]\tvalidation_0-auc:0.720763\n",
            "[3]\tvalidation_0-auc:0.720646\n",
            "[4]\tvalidation_0-auc:0.740739\n",
            "[5]\tvalidation_0-auc:0.741417\n",
            "[6]\tvalidation_0-auc:0.770984\n",
            "[7]\tvalidation_0-auc:0.748392\n",
            "[8]\tvalidation_0-auc:0.746764\n",
            "[9]\tvalidation_0-auc:0.739518\n",
            "[10]\tvalidation_0-auc:0.725548\n",
            "[11]\tvalidation_0-auc:0.740952\n",
            "[12]\tvalidation_0-auc:0.74752\n",
            "[13]\tvalidation_0-auc:0.739382\n",
            "[14]\tvalidation_0-auc:0.735701\n",
            "[15]\tvalidation_0-auc:0.73357\n",
            "[16]\tvalidation_0-auc:0.747888\n",
            "[17]\tvalidation_0-auc:0.732717\n",
            "[18]\tvalidation_0-auc:0.737096\n",
            "[19]\tvalidation_0-auc:0.737406\n",
            "[20]\tvalidation_0-auc:0.746086\n",
            "[21]\tvalidation_0-auc:0.75124\n",
            "[22]\tvalidation_0-auc:0.750368\n",
            "[23]\tvalidation_0-auc:0.756413\n",
            "[24]\tvalidation_0-auc:0.755987\n",
            "[25]\tvalidation_0-auc:0.756336\n",
            "[26]\tvalidation_0-auc:0.759668\n",
            "[27]\tvalidation_0-auc:0.759668\n",
            "[28]\tvalidation_0-auc:0.75777\n",
            "[29]\tvalidation_0-auc:0.75777\n",
            "[30]\tvalidation_0-auc:0.760405\n",
            "[31]\tvalidation_0-auc:0.754069\n",
            "[32]\tvalidation_0-auc:0.75279\n",
            "[33]\tvalidation_0-auc:0.757886\n",
            "[34]\tvalidation_0-auc:0.757886\n",
            "[35]\tvalidation_0-auc:0.757886\n",
            "[36]\tvalidation_0-auc:0.757498\n",
            "[37]\tvalidation_0-auc:0.756336\n",
            "[38]\tvalidation_0-auc:0.757808\n",
            "[39]\tvalidation_0-auc:0.758312\n",
            "[40]\tvalidation_0-auc:0.752887\n",
            "[41]\tvalidation_0-auc:0.749884\n",
            "[42]\tvalidation_0-auc:0.756646\n",
            "[43]\tvalidation_0-auc:0.755754\n",
            "[44]\tvalidation_0-auc:0.755754\n",
            "[45]\tvalidation_0-auc:0.762652\n",
            "[46]\tvalidation_0-auc:0.762652\n",
            "[47]\tvalidation_0-auc:0.768348\n",
            "[48]\tvalidation_0-auc:0.767244\n",
            "[49]\tvalidation_0-auc:0.766895\n",
            "[50]\tvalidation_0-auc:0.766314\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.789816\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.8204081632653061 | 0.8333333333333334 | 0.05434782608695652 |  0.1020408163265306 |\n",
            "|     GRU 0.1      | 0.8387755102040816 | 0.6101694915254238 |  0.391304347826087  |  0.4768211920529801 |\n",
            "|   XGBoost 0.1    | 0.8306122448979592 | 0.5714285714285714 |  0.391304347826087  |  0.4645161290322581 |\n",
            "|    Logreg 0.1    | 0.8448979591836735 |        0.7         | 0.30434782608695654 | 0.42424242424242425 |\n",
            "|     SVM 0.1      | 0.8448979591836735 | 0.6904761904761905 | 0.31521739130434784 | 0.43283582089552236 |\n",
            "|  LSTM beta 0.1   | 0.8971553610503282 | 0.6862745098039216 |  0.5303030303030303 |  0.5982905982905983 |\n",
            "|   GRU beta 0.1   |  0.87527352297593  | 0.5692307692307692 |  0.5606060606060606 |  0.564885496183206  |\n",
            "| XGBoost beta 0.1 | 0.838074398249453  |        0.3         | 0.09090909090909091 | 0.13953488372093023 |\n",
            "| logreg beta 0.1  | 0.9015317286652079 | 0.7837837837837838 |  0.4393939393939394 |  0.5631067961165048 |\n",
            "|   svm beta 0.1   | 0.8862144420131292 | 0.6666666666666666 | 0.42424242424242425 |  0.5185185185185185 |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.5254 - accuracy: 0.7919 - val_loss: 0.4216 - val_accuracy: 0.8653\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5146 - accuracy: 0.7940 - val_loss: 0.3922 - val_accuracy: 0.8653\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4880 - accuracy: 0.7946 - val_loss: 0.4106 - val_accuracy: 0.8653\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4649 - accuracy: 0.7933 - val_loss: 0.3881 - val_accuracy: 0.8653\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4469 - accuracy: 0.8101 - val_loss: 0.3651 - val_accuracy: 0.8653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.5058 - accuracy: 0.7953 - val_loss: 0.4055 - val_accuracy: 0.8653\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4499 - accuracy: 0.8148 - val_loss: 0.3764 - val_accuracy: 0.8776\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4305 - accuracy: 0.8302 - val_loss: 0.3496 - val_accuracy: 0.8694\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4328 - accuracy: 0.8369 - val_loss: 0.3538 - val_accuracy: 0.8714\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4303 - accuracy: 0.8322 - val_loss: 0.3754 - val_accuracy: 0.8735\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.710674\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.730685\n",
            "[2]\tvalidation_0-auc:0.730685\n",
            "[3]\tvalidation_0-auc:0.748785\n",
            "[4]\tvalidation_0-auc:0.74925\n",
            "[5]\tvalidation_0-auc:0.756754\n",
            "[6]\tvalidation_0-auc:0.748571\n",
            "[7]\tvalidation_0-auc:0.75209\n",
            "[8]\tvalidation_0-auc:0.752484\n",
            "[9]\tvalidation_0-auc:0.750607\n",
            "[10]\tvalidation_0-auc:0.749428\n",
            "[11]\tvalidation_0-auc:0.736081\n",
            "[12]\tvalidation_0-auc:0.743729\n",
            "[13]\tvalidation_0-auc:0.744425\n",
            "[14]\tvalidation_0-auc:0.742353\n",
            "[15]\tvalidation_0-auc:0.743175\n",
            "[16]\tvalidation_0-auc:0.745497\n",
            "[17]\tvalidation_0-auc:0.745265\n",
            "[18]\tvalidation_0-auc:0.746498\n",
            "[19]\tvalidation_0-auc:0.746373\n",
            "[20]\tvalidation_0-auc:0.745426\n",
            "[21]\tvalidation_0-auc:0.744836\n",
            "[22]\tvalidation_0-auc:0.747088\n",
            "[23]\tvalidation_0-auc:0.747016\n",
            "[24]\tvalidation_0-auc:0.745748\n",
            "[25]\tvalidation_0-auc:0.745247\n",
            "[26]\tvalidation_0-auc:0.74539\n",
            "[27]\tvalidation_0-auc:0.740066\n",
            "[28]\tvalidation_0-auc:0.73869\n",
            "[29]\tvalidation_0-auc:0.738618\n",
            "[30]\tvalidation_0-auc:0.736224\n",
            "[31]\tvalidation_0-auc:0.735903\n",
            "[32]\tvalidation_0-auc:0.736046\n",
            "[33]\tvalidation_0-auc:0.736189\n",
            "[34]\tvalidation_0-auc:0.731418\n",
            "[35]\tvalidation_0-auc:0.726629\n",
            "[36]\tvalidation_0-auc:0.726004\n",
            "[37]\tvalidation_0-auc:0.729506\n",
            "[38]\tvalidation_0-auc:0.728613\n",
            "[39]\tvalidation_0-auc:0.72704\n",
            "[40]\tvalidation_0-auc:0.726719\n",
            "[41]\tvalidation_0-auc:0.723485\n",
            "[42]\tvalidation_0-auc:0.723378\n",
            "[43]\tvalidation_0-auc:0.724092\n",
            "[44]\tvalidation_0-auc:0.723842\n",
            "[45]\tvalidation_0-auc:0.719054\n",
            "[46]\tvalidation_0-auc:0.717982\n",
            "[47]\tvalidation_0-auc:0.716785\n",
            "[48]\tvalidation_0-auc:0.716356\n",
            "[49]\tvalidation_0-auc:0.715266\n",
            "[50]\tvalidation_0-auc:0.715552\n",
            "[51]\tvalidation_0-auc:0.714623\n",
            "[52]\tvalidation_0-auc:0.713336\n",
            "[53]\tvalidation_0-auc:0.713211\n",
            "[54]\tvalidation_0-auc:0.71364\n",
            "[55]\tvalidation_0-auc:0.713908\n",
            "Stopping. Best iteration:\n",
            "[5]\tvalidation_0-auc:0.756754\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.5432 - accuracy: 0.7859 - val_loss: 0.4136 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5217 - accuracy: 0.7900 - val_loss: 0.4417 - val_accuracy: 0.8556\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4978 - accuracy: 0.7886 - val_loss: 0.3819 - val_accuracy: 0.8556\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4758 - accuracy: 0.7927 - val_loss: 0.3621 - val_accuracy: 0.8556\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4488 - accuracy: 0.7955 - val_loss: 0.3503 - val_accuracy: 0.8556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.4922 - accuracy: 0.8010 - val_loss: 0.3454 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4312 - accuracy: 0.8325 - val_loss: 0.3421 - val_accuracy: 0.8556\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4351 - accuracy: 0.8291 - val_loss: 0.3397 - val_accuracy: 0.8775\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4073 - accuracy: 0.8339 - val_loss: 0.3123 - val_accuracy: 0.8578\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4042 - accuracy: 0.8312 - val_loss: 0.3726 - val_accuracy: 0.9409\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.84831\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.850868\n",
            "[2]\tvalidation_0-auc:0.84862\n",
            "[3]\tvalidation_0-auc:0.844397\n",
            "[4]\tvalidation_0-auc:0.882624\n",
            "[5]\tvalidation_0-auc:0.898958\n",
            "[6]\tvalidation_0-auc:0.890801\n",
            "[7]\tvalidation_0-auc:0.890839\n",
            "[8]\tvalidation_0-auc:0.889793\n",
            "[9]\tvalidation_0-auc:0.895451\n",
            "[10]\tvalidation_0-auc:0.898744\n",
            "[11]\tvalidation_0-auc:0.897563\n",
            "[12]\tvalidation_0-auc:0.897834\n",
            "[13]\tvalidation_0-auc:0.89795\n",
            "[14]\tvalidation_0-auc:0.897291\n",
            "[15]\tvalidation_0-auc:0.896691\n",
            "[16]\tvalidation_0-auc:0.895199\n",
            "[17]\tvalidation_0-auc:0.892041\n",
            "[18]\tvalidation_0-auc:0.890471\n",
            "[19]\tvalidation_0-auc:0.892409\n",
            "[20]\tvalidation_0-auc:0.890626\n",
            "[21]\tvalidation_0-auc:0.890859\n",
            "[22]\tvalidation_0-auc:0.88896\n",
            "[23]\tvalidation_0-auc:0.890452\n",
            "[24]\tvalidation_0-auc:0.890801\n",
            "[25]\tvalidation_0-auc:0.891188\n",
            "[26]\tvalidation_0-auc:0.892506\n",
            "[27]\tvalidation_0-auc:0.892506\n",
            "[28]\tvalidation_0-auc:0.890917\n",
            "[29]\tvalidation_0-auc:0.891692\n",
            "[30]\tvalidation_0-auc:0.886867\n",
            "[31]\tvalidation_0-auc:0.887487\n",
            "[32]\tvalidation_0-auc:0.887022\n",
            "[33]\tvalidation_0-auc:0.886635\n",
            "[34]\tvalidation_0-auc:0.886538\n",
            "[35]\tvalidation_0-auc:0.881016\n",
            "[36]\tvalidation_0-auc:0.881016\n",
            "[37]\tvalidation_0-auc:0.879757\n",
            "[38]\tvalidation_0-auc:0.879485\n",
            "[39]\tvalidation_0-auc:0.880144\n",
            "[40]\tvalidation_0-auc:0.879292\n",
            "[41]\tvalidation_0-auc:0.878827\n",
            "[42]\tvalidation_0-auc:0.878517\n",
            "[43]\tvalidation_0-auc:0.879001\n",
            "[44]\tvalidation_0-auc:0.880745\n",
            "[45]\tvalidation_0-auc:0.881132\n",
            "[46]\tvalidation_0-auc:0.878265\n",
            "[47]\tvalidation_0-auc:0.877528\n",
            "[48]\tvalidation_0-auc:0.877257\n",
            "[49]\tvalidation_0-auc:0.877257\n",
            "[50]\tvalidation_0-auc:0.876928\n",
            "[51]\tvalidation_0-auc:0.876192\n",
            "[52]\tvalidation_0-auc:0.873363\n",
            "[53]\tvalidation_0-auc:0.875029\n",
            "[54]\tvalidation_0-auc:0.874525\n",
            "[55]\tvalidation_0-auc:0.872704\n",
            "Stopping. Best iteration:\n",
            "[5]\tvalidation_0-auc:0.898958\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+----------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall        |       F1 score      |\n",
            "+------------------+--------------------+--------------------+----------------------+---------------------+\n",
            "|     LSTM 0.2     | 0.8653061224489796 |        0.0         |         0.0          |         0.0         |\n",
            "|     GRU 0.2      | 0.8734693877551021 | 0.8333333333333334 | 0.07575757575757576  |  0.1388888888888889 |\n",
            "|   XGBoost 0.2    | 0.8734693877551021 | 0.6666666666666666 | 0.12121212121212122  | 0.20512820512820512 |\n",
            "|    Logreg 0.2    | 0.8693877551020408 |        1.0         | 0.030303030303030304 | 0.05882352941176471 |\n",
            "|     SVM 0.2      | 0.8693877551020408 |        1.0         | 0.030303030303030304 | 0.05882352941176471 |\n",
            "|  LSTM beta 0.2   | 0.8555798687089715 |        0.0         |         0.0          |         0.0         |\n",
            "|   GRU beta 0.2   | 0.9409190371991247 | 0.9148936170212766 |  0.6515151515151515  |  0.7610619469026548 |\n",
            "| XGBoost beta 0.2 | 0.9037199124726477 |        1.0         |  0.3333333333333333  |         0.5         |\n",
            "| logreg beta 0.2  | 0.8687089715536105 |        1.0         | 0.09090909090909091  | 0.16666666666666669 |\n",
            "|   svm beta 0.2   | 0.8708971553610503 |        1.0         | 0.10606060606060606  | 0.19178082191780824 |\n",
            "+------------------+--------------------+--------------------+----------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.5831 - accuracy: 0.7403 - val_loss: 0.4773 - val_accuracy: 0.8653\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5556 - accuracy: 0.7463 - val_loss: 0.3639 - val_accuracy: 0.8714\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5312 - accuracy: 0.7591 - val_loss: 0.4769 - val_accuracy: 0.8694\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5032 - accuracy: 0.7718 - val_loss: 0.3785 - val_accuracy: 0.8714\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4830 - accuracy: 0.7779 - val_loss: 0.4235 - val_accuracy: 0.8612\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.5828 - accuracy: 0.7356 - val_loss: 0.4199 - val_accuracy: 0.8653\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5075 - accuracy: 0.7705 - val_loss: 0.3558 - val_accuracy: 0.8694\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4613 - accuracy: 0.7913 - val_loss: 0.3975 - val_accuracy: 0.8612\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4620 - accuracy: 0.8060 - val_loss: 0.3889 - val_accuracy: 0.8694\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4666 - accuracy: 0.8040 - val_loss: 0.4386 - val_accuracy: 0.8592\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.709602\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.720662\n",
            "[2]\tvalidation_0-auc:0.721573\n",
            "[3]\tvalidation_0-auc:0.723896\n",
            "[4]\tvalidation_0-auc:0.725254\n",
            "[5]\tvalidation_0-auc:0.729703\n",
            "[6]\tvalidation_0-auc:0.733097\n",
            "[7]\tvalidation_0-auc:0.741048\n",
            "[8]\tvalidation_0-auc:0.735617\n",
            "[9]\tvalidation_0-auc:0.737207\n",
            "[10]\tvalidation_0-auc:0.732937\n",
            "[11]\tvalidation_0-auc:0.732365\n",
            "[12]\tvalidation_0-auc:0.730793\n",
            "[13]\tvalidation_0-auc:0.734884\n",
            "[14]\tvalidation_0-auc:0.733919\n",
            "[15]\tvalidation_0-auc:0.733223\n",
            "[16]\tvalidation_0-auc:0.735277\n",
            "[17]\tvalidation_0-auc:0.734706\n",
            "[18]\tvalidation_0-auc:0.733187\n",
            "[19]\tvalidation_0-auc:0.733508\n",
            "[20]\tvalidation_0-auc:0.739065\n",
            "[21]\tvalidation_0-auc:0.737171\n",
            "[22]\tvalidation_0-auc:0.738958\n",
            "[23]\tvalidation_0-auc:0.738761\n",
            "[24]\tvalidation_0-auc:0.740548\n",
            "[25]\tvalidation_0-auc:0.740798\n",
            "[26]\tvalidation_0-auc:0.73953\n",
            "[27]\tvalidation_0-auc:0.737171\n",
            "[28]\tvalidation_0-auc:0.738315\n",
            "[29]\tvalidation_0-auc:0.737457\n",
            "[30]\tvalidation_0-auc:0.735706\n",
            "[31]\tvalidation_0-auc:0.737886\n",
            "[32]\tvalidation_0-auc:0.736582\n",
            "[33]\tvalidation_0-auc:0.737278\n",
            "[34]\tvalidation_0-auc:0.736742\n",
            "[35]\tvalidation_0-auc:0.734455\n",
            "[36]\tvalidation_0-auc:0.734527\n",
            "[37]\tvalidation_0-auc:0.735706\n",
            "[38]\tvalidation_0-auc:0.735938\n",
            "[39]\tvalidation_0-auc:0.733919\n",
            "[40]\tvalidation_0-auc:0.735206\n",
            "[41]\tvalidation_0-auc:0.73542\n",
            "[42]\tvalidation_0-auc:0.733401\n",
            "[43]\tvalidation_0-auc:0.730864\n",
            "[44]\tvalidation_0-auc:0.733187\n",
            "[45]\tvalidation_0-auc:0.732937\n",
            "[46]\tvalidation_0-auc:0.730149\n",
            "[47]\tvalidation_0-auc:0.732222\n",
            "[48]\tvalidation_0-auc:0.732847\n",
            "[49]\tvalidation_0-auc:0.733097\n",
            "[50]\tvalidation_0-auc:0.731096\n",
            "[51]\tvalidation_0-auc:0.731525\n",
            "[52]\tvalidation_0-auc:0.728774\n",
            "[53]\tvalidation_0-auc:0.730382\n",
            "[54]\tvalidation_0-auc:0.730596\n",
            "[55]\tvalidation_0-auc:0.727987\n",
            "[56]\tvalidation_0-auc:0.726844\n",
            "[57]\tvalidation_0-auc:0.723878\n",
            "Stopping. Best iteration:\n",
            "[7]\tvalidation_0-auc:0.741048\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.5896 - accuracy: 0.7303 - val_loss: 0.5245 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5817 - accuracy: 0.7364 - val_loss: 0.4910 - val_accuracy: 0.8556\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5692 - accuracy: 0.7364 - val_loss: 0.3984 - val_accuracy: 0.8556\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5325 - accuracy: 0.7454 - val_loss: 0.4011 - val_accuracy: 0.8556\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5026 - accuracy: 0.7660 - val_loss: 0.3843 - val_accuracy: 0.8621\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5683 - accuracy: 0.7502 - val_loss: 0.4595 - val_accuracy: 0.8578\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4880 - accuracy: 0.7804 - val_loss: 0.5338 - val_accuracy: 0.8534\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4715 - accuracy: 0.7865 - val_loss: 0.4267 - val_accuracy: 0.8972\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4555 - accuracy: 0.8003 - val_loss: 0.3282 - val_accuracy: 0.8687\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4707 - accuracy: 0.7852 - val_loss: 0.3526 - val_accuracy: 0.8709\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.828489\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.821902\n",
            "[2]\tvalidation_0-auc:0.810722\n",
            "[3]\tvalidation_0-auc:0.830776\n",
            "[4]\tvalidation_0-auc:0.835019\n",
            "[5]\tvalidation_0-auc:0.836143\n",
            "[6]\tvalidation_0-auc:0.831841\n",
            "[7]\tvalidation_0-auc:0.824091\n",
            "[8]\tvalidation_0-auc:0.831551\n",
            "[9]\tvalidation_0-auc:0.832558\n",
            "[10]\tvalidation_0-auc:0.832461\n",
            "[11]\tvalidation_0-auc:0.83159\n",
            "[12]\tvalidation_0-auc:0.843292\n",
            "[13]\tvalidation_0-auc:0.845656\n",
            "[14]\tvalidation_0-auc:0.846896\n",
            "[15]\tvalidation_0-auc:0.848524\n",
            "[16]\tvalidation_0-auc:0.849841\n",
            "[17]\tvalidation_0-auc:0.849376\n",
            "[18]\tvalidation_0-auc:0.85143\n",
            "[19]\tvalidation_0-auc:0.850655\n",
            "[20]\tvalidation_0-auc:0.851042\n",
            "[21]\tvalidation_0-auc:0.851585\n",
            "[22]\tvalidation_0-auc:0.850074\n",
            "[23]\tvalidation_0-auc:0.853232\n",
            "[24]\tvalidation_0-auc:0.853348\n",
            "[25]\tvalidation_0-auc:0.854084\n",
            "[26]\tvalidation_0-auc:0.853387\n",
            "[27]\tvalidation_0-auc:0.859664\n",
            "[28]\tvalidation_0-auc:0.858076\n",
            "[29]\tvalidation_0-auc:0.857456\n",
            "[30]\tvalidation_0-auc:0.858192\n",
            "[31]\tvalidation_0-auc:0.858231\n",
            "[32]\tvalidation_0-auc:0.859781\n",
            "[33]\tvalidation_0-auc:0.862493\n",
            "[34]\tvalidation_0-auc:0.862067\n",
            "[35]\tvalidation_0-auc:0.861641\n",
            "[36]\tvalidation_0-auc:0.862028\n",
            "[37]\tvalidation_0-auc:0.861951\n",
            "[38]\tvalidation_0-auc:0.860749\n",
            "[39]\tvalidation_0-auc:0.861311\n",
            "[40]\tvalidation_0-auc:0.860808\n",
            "[41]\tvalidation_0-auc:0.860575\n",
            "[42]\tvalidation_0-auc:0.860265\n",
            "[43]\tvalidation_0-auc:0.861544\n",
            "[44]\tvalidation_0-auc:0.861699\n",
            "[45]\tvalidation_0-auc:0.861583\n",
            "[46]\tvalidation_0-auc:0.864063\n",
            "[47]\tvalidation_0-auc:0.866543\n",
            "[48]\tvalidation_0-auc:0.866349\n",
            "[49]\tvalidation_0-auc:0.867046\n",
            "[50]\tvalidation_0-auc:0.866775\n",
            "[51]\tvalidation_0-auc:0.868984\n",
            "[52]\tvalidation_0-auc:0.868984\n",
            "[53]\tvalidation_0-auc:0.866891\n",
            "[54]\tvalidation_0-auc:0.868015\n",
            "[55]\tvalidation_0-auc:0.867938\n",
            "[56]\tvalidation_0-auc:0.867938\n",
            "[57]\tvalidation_0-auc:0.86693\n",
            "[58]\tvalidation_0-auc:0.86662\n",
            "[59]\tvalidation_0-auc:0.867046\n",
            "[60]\tvalidation_0-auc:0.866795\n",
            "[61]\tvalidation_0-auc:0.863074\n",
            "[62]\tvalidation_0-auc:0.862609\n",
            "[63]\tvalidation_0-auc:0.862454\n",
            "[64]\tvalidation_0-auc:0.862454\n",
            "[65]\tvalidation_0-auc:0.863772\n",
            "[66]\tvalidation_0-auc:0.863501\n",
            "[67]\tvalidation_0-auc:0.862513\n",
            "[68]\tvalidation_0-auc:0.862513\n",
            "[69]\tvalidation_0-auc:0.862048\n",
            "[70]\tvalidation_0-auc:0.861815\n",
            "[71]\tvalidation_0-auc:0.86228\n",
            "[72]\tvalidation_0-auc:0.86228\n",
            "[73]\tvalidation_0-auc:0.861137\n",
            "[74]\tvalidation_0-auc:0.861873\n",
            "[75]\tvalidation_0-auc:0.86197\n",
            "[76]\tvalidation_0-auc:0.863365\n",
            "[77]\tvalidation_0-auc:0.865496\n",
            "[78]\tvalidation_0-auc:0.86445\n",
            "[79]\tvalidation_0-auc:0.863753\n",
            "[80]\tvalidation_0-auc:0.863753\n",
            "[81]\tvalidation_0-auc:0.862513\n",
            "[82]\tvalidation_0-auc:0.86197\n",
            "[83]\tvalidation_0-auc:0.862086\n",
            "[84]\tvalidation_0-auc:0.862474\n",
            "[85]\tvalidation_0-auc:0.862823\n",
            "[86]\tvalidation_0-auc:0.860033\n",
            "[87]\tvalidation_0-auc:0.859723\n",
            "[88]\tvalidation_0-auc:0.859529\n",
            "[89]\tvalidation_0-auc:0.857882\n",
            "[90]\tvalidation_0-auc:0.857436\n",
            "[91]\tvalidation_0-auc:0.857204\n",
            "[92]\tvalidation_0-auc:0.856971\n",
            "[93]\tvalidation_0-auc:0.856855\n",
            "[94]\tvalidation_0-auc:0.856157\n",
            "[95]\tvalidation_0-auc:0.854917\n",
            "[96]\tvalidation_0-auc:0.854801\n",
            "[97]\tvalidation_0-auc:0.854801\n",
            "[98]\tvalidation_0-auc:0.853871\n",
            "[99]\tvalidation_0-auc:0.854181\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.8612244897959184 | 0.47058823529411764 | 0.24242424242424243 |         0.32        |\n",
            "|      GRU 0.15     | 0.8591836734693877 |  0.4716981132075472 |  0.3787878787878788 | 0.42016806722689076 |\n",
            "|    XGBoost 0.15   | 0.8653061224489796 |         0.5         |  0.3181818181818182 |  0.3888888888888889 |\n",
            "|    Logreg 0.15    | 0.8918367346938776 |  0.782608695652174  |  0.2727272727272727 | 0.40449438202247184 |\n",
            "|      SVM 0.15     | 0.8816326530612245 |  0.7222222222222222 | 0.19696969696969696 | 0.30952380952380953 |\n",
            "|   LSTM beta 0.15  | 0.862144420131291  |         0.8         | 0.06060606060606061 | 0.11267605633802819 |\n",
            "|   GRU beta 0.15   | 0.8708971553610503 |  0.8888888888888888 | 0.12121212121212122 | 0.21333333333333335 |\n",
            "| XGBoost beta 0.15 | 0.9059080962800875 |  0.7804878048780488 | 0.48484848484848486 |  0.5981308411214953 |\n",
            "|  logreg beta 0.15 | 0.8971553610503282 |  0.9130434782608695 |  0.3181818181818182 | 0.47191011235955055 |\n",
            "|   svm beta 0.15   | 0.8796498905908097 |         1.0         | 0.16666666666666666 |  0.2857142857142857 |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9JnnvtXX4gx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f93cc316-cfa2-435a-aa84-1eb93dca4825"
      },
      "source": [
        "Result_purging.to_csv('NVDA_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.820408</td>\n",
              "      <td>0.102041</td>\n",
              "      <td>0.054348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.610169</td>\n",
              "      <td>0.838776</td>\n",
              "      <td>0.476821</td>\n",
              "      <td>0.391304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.830612</td>\n",
              "      <td>0.464516</td>\n",
              "      <td>0.391304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.844898</td>\n",
              "      <td>0.424242</td>\n",
              "      <td>0.304348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.844898</td>\n",
              "      <td>0.432836</td>\n",
              "      <td>0.315217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.686275</td>\n",
              "      <td>0.897155</td>\n",
              "      <td>0.598291</td>\n",
              "      <td>0.530303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.569231</td>\n",
              "      <td>0.875274</td>\n",
              "      <td>0.564885</td>\n",
              "      <td>0.560606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.838074</td>\n",
              "      <td>0.139535</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.783784</td>\n",
              "      <td>0.901532</td>\n",
              "      <td>0.563107</td>\n",
              "      <td>0.439394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.886214</td>\n",
              "      <td>0.518519</td>\n",
              "      <td>0.424242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.865306</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.138889</td>\n",
              "      <td>0.075758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.205128</td>\n",
              "      <td>0.121212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.869388</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.030303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.869388</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.030303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.855580</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.914894</td>\n",
              "      <td>0.940919</td>\n",
              "      <td>0.761062</td>\n",
              "      <td>0.651515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.903720</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.868709</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.870897</td>\n",
              "      <td>0.191781</td>\n",
              "      <td>0.106061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.861224</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.242424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.471698</td>\n",
              "      <td>0.859184</td>\n",
              "      <td>0.420168</td>\n",
              "      <td>0.378788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.865306</td>\n",
              "      <td>0.388889</td>\n",
              "      <td>0.318182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.891837</td>\n",
              "      <td>0.404494</td>\n",
              "      <td>0.272727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>0.881633</td>\n",
              "      <td>0.309524</td>\n",
              "      <td>0.196970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.862144</td>\n",
              "      <td>0.112676</td>\n",
              "      <td>0.060606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.870897</td>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.121212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.780488</td>\n",
              "      <td>0.905908</td>\n",
              "      <td>0.598131</td>\n",
              "      <td>0.484848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.913043</td>\n",
              "      <td>0.897155</td>\n",
              "      <td>0.471910</td>\n",
              "      <td>0.318182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.879650</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  NVDA  0.833333  0.820408  0.102041  0.054348\n",
              "1            GRU 0.1  NVDA  0.610169  0.838776  0.476821  0.391304\n",
              "2        XGBoost 0.1  NVDA  0.571429  0.830612  0.464516  0.391304\n",
              "3         Logreg 0.1  NVDA  0.700000  0.844898  0.424242  0.304348\n",
              "4            SVM 0.1  NVDA  0.690476  0.844898  0.432836  0.315217\n",
              "5      LSTM beta 0.1  NVDA  0.686275  0.897155  0.598291  0.530303\n",
              "6       GRU beta 0.1  NVDA  0.569231  0.875274  0.564885  0.560606\n",
              "7   XGBoost beta 0.1  NVDA  0.300000  0.838074  0.139535  0.090909\n",
              "8    logreg beta 0.1  NVDA  0.783784  0.901532  0.563107  0.439394\n",
              "9       svm beta 0.1  NVDA  0.666667  0.886214  0.518519  0.424242\n",
              "0           LSTM 0.2  NVDA  0.000000  0.865306  0.000000  0.000000\n",
              "1            GRU 0.2  NVDA  0.833333  0.873469  0.138889  0.075758\n",
              "2        XGBoost 0.2  NVDA  0.666667  0.873469  0.205128  0.121212\n",
              "3         Logreg 0.2  NVDA  1.000000  0.869388  0.058824  0.030303\n",
              "4            SVM 0.2  NVDA  1.000000  0.869388  0.058824  0.030303\n",
              "5      LSTM beta 0.2  NVDA  0.000000  0.855580  0.000000  0.000000\n",
              "6       GRU beta 0.2  NVDA  0.914894  0.940919  0.761062  0.651515\n",
              "7   XGBoost beta 0.2  NVDA  1.000000  0.903720  0.500000  0.333333\n",
              "8    logreg beta 0.2  NVDA  1.000000  0.868709  0.166667  0.090909\n",
              "9       svm beta 0.2  NVDA  1.000000  0.870897  0.191781  0.106061\n",
              "0          LSTM 0.15  NVDA  0.470588  0.861224  0.320000  0.242424\n",
              "1           GRU 0.15  NVDA  0.471698  0.859184  0.420168  0.378788\n",
              "2       XGBoost 0.15  NVDA  0.500000  0.865306  0.388889  0.318182\n",
              "3        Logreg 0.15  NVDA  0.782609  0.891837  0.404494  0.272727\n",
              "4           SVM 0.15  NVDA  0.722222  0.881633  0.309524  0.196970\n",
              "5     LSTM beta 0.15  NVDA  0.800000  0.862144  0.112676  0.060606\n",
              "6      GRU beta 0.15  NVDA  0.888889  0.870897  0.213333  0.121212\n",
              "7  XGBoost beta 0.15  NVDA  0.780488  0.905908  0.598131  0.484848\n",
              "8   logreg beta 0.15  NVDA  0.913043  0.897155  0.471910  0.318182\n",
              "9      svm beta 0.15  NVDA  1.000000  0.879650  0.285714  0.166667"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGqhawMTX4gx"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXs7iH8WX4gy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqUeeZsxYLzW"
      },
      "source": [
        "## RHT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bnaIwqDYLzd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "outputId": "fbd9f489-1769-44c0-8402-a035f96b756b"
      },
      "source": [
        "dfs = pd.read_csv(\"RHT.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "# Denoise prices\n",
        "dfs = denoise_data(dfs, '<CLOSE>')\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2203</td>\n",
              "      <td>US1.RHT</td>\n",
              "      <td>D</td>\n",
              "      <td>20190708</td>\n",
              "      <td>0</td>\n",
              "      <td>187.70</td>\n",
              "      <td>187.8100</td>\n",
              "      <td>187.6000</td>\n",
              "      <td>187.70</td>\n",
              "      <td>523715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2202</td>\n",
              "      <td>US1.RHT</td>\n",
              "      <td>D</td>\n",
              "      <td>20190705</td>\n",
              "      <td>0</td>\n",
              "      <td>187.60</td>\n",
              "      <td>187.9300</td>\n",
              "      <td>187.5100</td>\n",
              "      <td>187.75</td>\n",
              "      <td>671573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2201</td>\n",
              "      <td>US1.RHT</td>\n",
              "      <td>D</td>\n",
              "      <td>20190703</td>\n",
              "      <td>0</td>\n",
              "      <td>187.65</td>\n",
              "      <td>187.9500</td>\n",
              "      <td>187.4800</td>\n",
              "      <td>187.70</td>\n",
              "      <td>1956137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2200</td>\n",
              "      <td>US1.RHT</td>\n",
              "      <td>D</td>\n",
              "      <td>20190702</td>\n",
              "      <td>0</td>\n",
              "      <td>188.40</td>\n",
              "      <td>188.4000</td>\n",
              "      <td>187.9000</td>\n",
              "      <td>188.17</td>\n",
              "      <td>1213064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2199</td>\n",
              "      <td>US1.RHT</td>\n",
              "      <td>D</td>\n",
              "      <td>20190701</td>\n",
              "      <td>0</td>\n",
              "      <td>187.95</td>\n",
              "      <td>188.4194</td>\n",
              "      <td>187.5536</td>\n",
              "      <td>188.40</td>\n",
              "      <td>1278873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2199</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.RHT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>37.99</td>\n",
              "      <td>38.8500</td>\n",
              "      <td>37.9500</td>\n",
              "      <td>38.70</td>\n",
              "      <td>2255539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2200</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.RHT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>38.60</td>\n",
              "      <td>38.7499</td>\n",
              "      <td>37.5100</td>\n",
              "      <td>38.19</td>\n",
              "      <td>5042146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2201</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.RHT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>41.40</td>\n",
              "      <td>41.5000</td>\n",
              "      <td>37.3300</td>\n",
              "      <td>38.37</td>\n",
              "      <td>9840635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2202</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.RHT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>40.94</td>\n",
              "      <td>41.6050</td>\n",
              "      <td>40.8900</td>\n",
              "      <td>41.48</td>\n",
              "      <td>3305240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2203</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.RHT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>41.04</td>\n",
              "      <td>41.2000</td>\n",
              "      <td>40.2400</td>\n",
              "      <td>40.63</td>\n",
              "      <td>2037558</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2204 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...    <HIGH>     <LOW>  <CLOSE>    <VOL>\n",
              "0      2203  US1.RHT     D  20190708  ...  187.8100  187.6000   187.70   523715\n",
              "1      2202  US1.RHT     D  20190705  ...  187.9300  187.5100   187.75   671573\n",
              "2      2201  US1.RHT     D  20190703  ...  187.9500  187.4800   187.70  1956137\n",
              "3      2200  US1.RHT     D  20190702  ...  188.4000  187.9000   188.17  1213064\n",
              "4      2199  US1.RHT     D  20190701  ...  188.4194  187.5536   188.40  1278873\n",
              "...     ...      ...   ...       ...  ...       ...       ...      ...      ...\n",
              "2199      4  US1.RHT     D  20101008  ...   38.8500   37.9500    38.70  2255539\n",
              "2200      3  US1.RHT     D  20101007  ...   38.7499   37.5100    38.19  5042146\n",
              "2201      2  US1.RHT     D  20101006  ...   41.5000   37.3300    38.37  9840635\n",
              "2202      1  US1.RHT     D  20101005  ...   41.6050   40.8900    41.48  3305240\n",
              "2203      0  US1.RHT     D  20101004  ...   41.2000   40.2400    40.63  2037558\n",
              "\n",
              "[2204 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf5QQ1zRYLzd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2aaddaed-4cfe-4166-fc36-1d2f4ecbeec7"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 1500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"74e4c7f3-5085-43bb-b4b7-b1dc368d2c79\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"74e4c7f3-5085-43bb-b4b7-b1dc368d2c79\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '74e4c7f3-5085-43bb-b4b7-b1dc368d2c79',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [97.42, 97.25, 96.56, 94.93, 95.54, 94.83, 95.76, 95.52, 96.81, 95.99, 98.2, 99.37, 98.89, 98.52, 90.0, 90.33, 87.85, 87.92, 88.93, 89.29, 88.76, 88.82, 91.94, 91.93, 91.66, 91.78, 91.52, 90.69, 89.56, 89.42, 88.55, 88.77, 87.99, 87.63, 87.9, 86.17, 85.51, 85.6, 89.61, 89.67, 88.22, 88.56, 89.15, 88.58, 88.18, 88.62, 87.89, 87.23, 88.12, 88.3, 88.08, 88.17, 87.74, 87.59, 87.45, 87.04, 87.42, 87.26, 86.74, 86.26, 85.93, 85.42, 85.16, 85.04, 84.905, 85.01, 84.67, 85.62, 86.49, 86.51, 85.93, 86.74, 86.495, 82.32, 82.96, 82.76, 82.55, 81.29, 84.51, 83.64, 84.09, 83.28, 82.57, 82.87, 82.23, 81.57, 82.045, 81.95, 82.04, 82.05, 82.41, 83.21, 82.8, 83.77, 84.83, 84.61, 84.7, 83.76, 82.59, 81.53, 81.06, 80.09, 79.78, 79.25, 79.3, 78.66, 78.86, 78.23, 78.11, 77.7, 76.06, 75.89, 75.5, 75.7, 75.23, 76.43, 75.42, 74.08, 74.19, 73.92, 73.6, 72.77, 73.36, 73.04, 72.33, 71.66, 72.13, 73.8, 73.11, 73.11, 70.12, 69.71, 69.65, 70.0, 70.19, 71.0099, 68.7, 79.79, 79.39, 80.49, 79.96, 80.41, 79.08, 80.55, 78.39, 78.65, 78.88, 78.91, 77.95, 77.71, 76.07, 75.19, 79.11, 80.01, 78.38, 78.81, 78.74, 79.13, 79.65, 79.04, 79.8, 78.64, 76.81, 75.505, 77.04, 76.94, 76.78, 76.68, 76.66, 74.44, 74.75, 74.77, 76.67, 77.46, 77.11, 76.91, 77.03, 77.36, 78.21, 76.96, 77.57, 78.26, 77.5, 76.2, 77.21, 76.84, 77.18, 76.76, 77.99, 77.795, 78.58, 79.44, 79.66, 80.19, 80.85, 80.17, 80.555, 79.12, 78.74, 79.16, 79.95, 77.06, 75.8, 75.96, 73.8, 73.67, 73.75, 72.43, 72.96, 72.26, 72.59, 73.16, 73.36, 73.74, 72.93, 72.97, 72.94, 73.66, 74.35, 74.9, 74.73, 74.89, 74.09, 74.22, 73.72, 73.17, 73.585, 73.98, 73.98, 74.42, 74.37, 75.175, 75.24, 75.83, 74.61, 74.57, 72.905, 74.81, 75.29, 74.43, 73.65, 75.06, 73.55, 73.46, 72.31, 73.52, 72.35, 72.8, 72.82, 73.48, 73.67, 73.91, 72.63, 72.7, 71.42, 72.19, 71.26, 71.98, 72.6, 72.95, 71.5, 70.14, 73.63, 78.37, 79.73, 80.38, 78.87, 77.28, 76.97, 77.01, 77.17, 76.91, 76.93, 78.23, 79.07, 78.88, 78.14, 78.19, 78.23, 77.41, 77.46, 76.53, 76.23, 76.7, 76.41, 74.81, 73.17, 72.43, 73.0, 72.44, 72.93, 72.07, 71.91, 72.15, 73.1, 71.74, 71.73, 71.58, 71.45, 72.35, 74.32, 73.35, 73.64, 74.42, 74.47, 74.45, 74.83, 76.14, 76.05, 75.04, 75.44, 74.65, 74.44, 74.05, 72.62, 72.82, 73.8, 73.63, 74.76, 74.05, 75.26, 75.5, 74.5, 74.04, 73.53, 73.48, 73.25, 72.55, 75.71, 74.88, 74.0736, 72.73, 72.86, 71.55, 72.11, 71.8, 70.07, 69.88, 68.27, 66.94, 67.57, 68.66, 68.26, 68.39, 65.36, 67.98, 67.67, 65.35, 65.3, 66.09, 65.89, 65.35, 66.3, 63.84, 64.1, 61.96, 63.54, 61.62, 60.96, 63.14, 68.6, 67.69, 67.82, 70.43, 70.01, 67.38, 67.99, 70.96, 71.75, 72.45, 70.3, 70.48, 71.6, 73.6, 76.53, 76.61, 78.66, 78.27, 79.17, 79.49, 80.61, 81.93, 81.7, 82.8, 83.65, 83.54, 82.35, 81.74, 82.18, 80.93, 80.44, 81.39, 78.85, 78.75, 78.06, 77.49, 78.58, 79.62, 79.06, 81.02, 80.52, 81.9, 80.32, 81.64, 82.24, 81.42, 82.5, 82.52, 82.05, 82.75, 82.21, 81.33, 80.52, 80.28, 79.09, 77.84, 79.82, 81.45, 80.81, 80.28, 81.03, 81.6, 82.58, 80.84, 80.13, 79.09, 78.87, 79.07, 77.88, 77.78, 77.43, 76.9701, 76.83, 78.26, 78.91, 77.62, 77.71, 75.115, 74.73, 75.97, 76.51, 75.16, 74.17, 72.77, 74.12, 73.62, 73.19, 71.86, 70.49, 70.74, 72.91, 73.15, 73.27, 72.7, 72.74, 71.07, 71.38, 70.99, 71.75, 69.79, 70.53, 70.36, 69.67, 71.03, 68.67, 69.65, 70.28, 70.06, 72.22, 73.83, 74.185, 73.035, 68.97, 69.03, 72.34, 75.15, 77.5, 78.67, 78.77, 78.33, 76.98, 77.05, 77.44, 79.16, 77.62, 77.65, 80.14, 79.05, 78.28, 79.1, 79.46, 79.27, 78.72, 77.15, 78.97, 79.35, 79.4, 78.3, 79.21, 79.06, 80.57, 80.11, 80.69, 79.57, 78.54, 77.26, 76.02, 76.86, 76.6, 76.86, 77.55, 75.92, 75.47, 77.46, 79.13, 78.53, 79.61, 79.47, 79.31, 78.5, 78.26, 78.51, 78.66, 78.21, 78.56, 78.41, 77.37, 78.26, 77.92, 77.06, 77.84, 77.51, 76.73, 77.23, 77.78, 78.44, 77.3, 78.82, 78.85, 78.48, 78.14, 78.47, 77.77, 78.01, 76.59, 76.16, 76.39, 75.99, 74.78, 74.76, 75.01, 74.76, 75.34, 75.25, 76.43, 76.59, 76.59, 76.85, 76.88, 75.765, 75.22, 74.45, 73.92, 74.81, 75.9, 74.91, 75.06, 75.77, 75.46, 75.03, 75.36, 75.37, 75.09, 75.39, 75.75, 76.71, 76.52, 75.3674, 68.45, 69.4, 69.46, 69.49, 69.16, 67.07, 67.1, 67.28, 66.16, 66.34, 65.13, 65.14, 66.85, 66.6, 67.97, 67.21, 68.48, 69.65, 69.12, 69.4, 69.76, 68.91, 68.55, 70.45, 69.48, 69.22, 68.62, 68.48, 67.97, 67.36, 65.74, 64.9, 64.98, 65.29, 64.8, 64.97, 64.23, 63.78, 65.74, 64.69, 65.08, 66.26, 66.07, 65.925, 64.78, 65.15, 65.2, 64.37, 66.36, 67.45, 67.62, 68.73, 69.01, 68.76, 67.5, 68.11, 68.99, 69.13, 69.47, 70.28, 71.1, 70.92, 70.43, 68.9, 68.02, 61.49, 59.49, 58.23, 58.98, 57.89, 58.43, 58.71, 59.08, 60.08, 61.68, 61.68, 62.13, 61.42, 61.4801, 62.15, 62.07, 61.99, 62.33, 62.23, 62.79, 62.93, 63.26, 63.0, 62.38, 61.11, 61.39, 61.32, 61.41, 61.28, 60.74, 59.58, 59.15, 59.16, 58.92, 58.26, 56.85, 57.36, 56.17, 56.02, 55.49, 54.8, 57.12, 56.32, 55.66, 54.92, 54.86, 54.32, 54.24, 55.49, 58.07, 58.56, 56.68, 58.14, 58.08, 57.38, 56.48, 56.14, 55.49, 55.43, 55.88, 56.91, 56.33, 56.69, 57.94, 60.68, 61.07, 60.72, 59.695, 60.54, 61.69, 61.3, 60.81, 61.38, 61.07, 61.21, 61.51, 61.29, 60.91, 60.645, 61.06, 62.03, 61.35, 62.12, 61.57, 61.67, 62.54, 60.95, 59.84, 59.84, 59.77, 59.26, 59.17, 58.805, 58.39, 58.29, 58.45, 58.62, 58.4, 58.12, 58.32, 56.15, 56.71, 56.7, 56.39, 55.88, 55.67, 54.29, 54.84, 53.94, 54.84, 54.38, 55.21, 54.74, 54.78, 54.92, 54.47, 55.72, 56.1, 56.11, 56.175, 55.26, 55.165, 55.34, 55.42, 54.755, 55.004, 55.0499, 55.11, 53.09, 52.4, 52.13, 51.93, 50.5, 51.48, 51.35, 51.41, 50.84, 50.31, 49.71, 49.99, 50.05, 50.11, 50.53, 50.0, 50.8, 50.68, 50.46, 50.1, 49.72, 50.1, 50.005, 49.49, 49.39, 49.3, 50.0, 48.72, 48.34, 48.75, 48.87, 49.43, 48.56, 48.77, 48.645, 48.55, 48.18, 48.37, 49.63, 50.04, 50.89, 50.92, 50.56, 50.62, 50.11, 49.82, 49.34, 50.01, 52.08, 50.0, 49.85, 50.45, 51.34, 53.11, 53.71, 52.98, 52.23, 56.11, 55.98, 57.32, 57.09, 57.42, 58.46, 58.25, 58.8, 58.62, 57.62, 57.41, 59.09, 58.67, 59.5, 60.52, 60.87, 60.55, 60.5, 58.27, 58.98, 59.94, 59.75, 58.97, 58.88, 58.68, 59.29, 58.64, 58.09, 58.31, 58.43, 57.68, 57.48, 56.67, 56.99, 55.54, 54.7, 54.85, 54.69, 56.5, 57.13, 55.57, 57.23, 55.82, 56.74, 58.12, 58.92, 59.15, 58.6, 59.07, 59.2, 59.37, 57.55, 56.81, 57.3, 56.99, 57.42, 56.33, 55.76, 55.84, 56.04, 56.08, 56.25, 55.88, 55.98, 56.08, 56.08, 48.99, 48.92, 48.21, 47.53, 46.78, 46.36, 46.16, 46.69, 47.47, 46.77, 46.65, 46.67, 45.9, 46.18, 46.85, 46.89, 46.55, 46.86, 47.11, 47.09, 46.53, 46.43, 46.55, 47.1, 46.32, 46.37, 44.59, 44.26, 43.31, 43.21, 44.08, 42.93, 43.08, 43.19, 43.27, 43.42, 44.08, 43.6, 43.41, 43.76, 42.36, 42.92, 42.58, 43.29, 43.53, 43.6, 42.61, 42.86, 42.82, 42.865, 43.09, 43.3, 45.45, 45.06, 44.91, 45.27, 45.73, 46.15, 45.92, 46.6, 46.88, 46.74, 52.94, 53.23, 53.6, 53.57, 52.57, 53.02, 52.43, 52.85, 53.48, 52.82, 52.2, 51.06, 50.85, 51.14, 50.84, 50.51, 50.83, 50.32, 50.165, 51.7, 51.58, 51.97, 51.11, 51.33, 51.25, 51.51, 51.97, 53.11, 53.53, 52.3, 52.47, 52.055, 51.26, 51.67, 52.23, 52.46, 52.49, 51.78, 51.08, 50.78, 50.83, 51.77, 50.53, 48.78]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('74e4c7f3-5085-43bb-b4b7-b1dc368d2c79');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"d25d98f4-3415-43bb-b04a-2d13425d3c52\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"d25d98f4-3415-43bb-b04a-2d13425d3c52\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'd25d98f4-3415-43bb-b04a-2d13425d3c52',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d25d98f4-3415-43bb-b04a-2d13425d3c52');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv1Kx3N5YLzd"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hkY2zf-YLzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d9e93e5-bc56-4c82-e36b-257300e67045"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.05, .1, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1500, test_end=1900)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"RHT\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 17ms/step - loss: 0.5319 - accuracy: 0.7882 - val_loss: 0.6570 - val_accuracy: 0.6487\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5230 - accuracy: 0.7924 - val_loss: 0.7062 - val_accuracy: 0.6487\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5181 - accuracy: 0.7924 - val_loss: 0.7226 - val_accuracy: 0.6487\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5162 - accuracy: 0.7924 - val_loss: 0.6692 - val_accuracy: 0.6487\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5071 - accuracy: 0.7832 - val_loss: 0.6826 - val_accuracy: 0.6487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 15ms/step - loss: 0.5316 - accuracy: 0.7924 - val_loss: 0.6738 - val_accuracy: 0.6487\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4651 - accuracy: 0.8059 - val_loss: 0.5676 - val_accuracy: 0.6846\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4485 - accuracy: 0.8160 - val_loss: 0.5401 - val_accuracy: 0.6872\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4300 - accuracy: 0.8134 - val_loss: 0.5111 - val_accuracy: 0.7410\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4155 - accuracy: 0.8328 - val_loss: 0.5118 - val_accuracy: 0.7308\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.792634\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.826303\n",
            "[2]\tvalidation_0-auc:0.840469\n",
            "[3]\tvalidation_0-auc:0.839099\n",
            "[4]\tvalidation_0-auc:0.842229\n",
            "[5]\tvalidation_0-auc:0.839791\n",
            "[6]\tvalidation_0-auc:0.841508\n",
            "[7]\tvalidation_0-auc:0.848086\n",
            "[8]\tvalidation_0-auc:0.846672\n",
            "[9]\tvalidation_0-auc:0.847754\n",
            "[10]\tvalidation_0-auc:0.848821\n",
            "[11]\tvalidation_0-auc:0.848778\n",
            "[12]\tvalidation_0-auc:0.845691\n",
            "[13]\tvalidation_0-auc:0.846412\n",
            "[14]\tvalidation_0-auc:0.844335\n",
            "[15]\tvalidation_0-auc:0.845619\n",
            "[16]\tvalidation_0-auc:0.843369\n",
            "[17]\tvalidation_0-auc:0.841724\n",
            "[18]\tvalidation_0-auc:0.838666\n",
            "[19]\tvalidation_0-auc:0.837699\n",
            "[20]\tvalidation_0-auc:0.836733\n",
            "[21]\tvalidation_0-auc:0.837483\n",
            "[22]\tvalidation_0-auc:0.83604\n",
            "[23]\tvalidation_0-auc:0.835839\n",
            "[24]\tvalidation_0-auc:0.834396\n",
            "[25]\tvalidation_0-auc:0.834165\n",
            "[26]\tvalidation_0-auc:0.832405\n",
            "[27]\tvalidation_0-auc:0.831309\n",
            "[28]\tvalidation_0-auc:0.829059\n",
            "[29]\tvalidation_0-auc:0.827039\n",
            "[30]\tvalidation_0-auc:0.827443\n",
            "[31]\tvalidation_0-auc:0.825683\n",
            "[32]\tvalidation_0-auc:0.824067\n",
            "[33]\tvalidation_0-auc:0.823144\n",
            "[34]\tvalidation_0-auc:0.822942\n",
            "[35]\tvalidation_0-auc:0.823\n",
            "[36]\tvalidation_0-auc:0.81974\n",
            "[37]\tvalidation_0-auc:0.819971\n",
            "[38]\tvalidation_0-auc:0.820548\n",
            "[39]\tvalidation_0-auc:0.81948\n",
            "[40]\tvalidation_0-auc:0.818369\n",
            "[41]\tvalidation_0-auc:0.818225\n",
            "[42]\tvalidation_0-auc:0.818874\n",
            "[43]\tvalidation_0-auc:0.818167\n",
            "[44]\tvalidation_0-auc:0.817186\n",
            "[45]\tvalidation_0-auc:0.816956\n",
            "[46]\tvalidation_0-auc:0.817071\n",
            "[47]\tvalidation_0-auc:0.81684\n",
            "[48]\tvalidation_0-auc:0.816725\n",
            "[49]\tvalidation_0-auc:0.815816\n",
            "[50]\tvalidation_0-auc:0.816682\n",
            "[51]\tvalidation_0-auc:0.816364\n",
            "[52]\tvalidation_0-auc:0.816018\n",
            "[53]\tvalidation_0-auc:0.815326\n",
            "[54]\tvalidation_0-auc:0.814691\n",
            "[55]\tvalidation_0-auc:0.814662\n",
            "[56]\tvalidation_0-auc:0.81472\n",
            "[57]\tvalidation_0-auc:0.813739\n",
            "[58]\tvalidation_0-auc:0.813104\n",
            "[59]\tvalidation_0-auc:0.810825\n",
            "[60]\tvalidation_0-auc:0.808776\n",
            "Stopping. Best iteration:\n",
            "[10]\tvalidation_0-auc:0.848821\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 3s 18ms/step - loss: 0.5401 - accuracy: 0.7848 - val_loss: 0.7160 - val_accuracy: 0.6471\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.5261 - accuracy: 0.7865 - val_loss: 0.7418 - val_accuracy: 0.6471\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.5169 - accuracy: 0.7865 - val_loss: 0.7115 - val_accuracy: 0.6471\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4955 - accuracy: 0.7865 - val_loss: 0.6207 - val_accuracy: 0.6471\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4628 - accuracy: 0.7891 - val_loss: 0.5590 - val_accuracy: 0.6471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 2s 14ms/step - loss: 0.5377 - accuracy: 0.7796 - val_loss: 0.6324 - val_accuracy: 0.6471\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4818 - accuracy: 0.7857 - val_loss: 0.5821 - val_accuracy: 0.6387\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.4531 - accuracy: 0.8003 - val_loss: 0.5456 - val_accuracy: 0.7143\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.4357 - accuracy: 0.8116 - val_loss: 0.5374 - val_accuracy: 0.7311\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.3996 - accuracy: 0.8315 - val_loss: 0.5161 - val_accuracy: 0.7535\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.719697\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.743403\n",
            "[2]\tvalidation_0-auc:0.774153\n",
            "[3]\tvalidation_0-auc:0.772195\n",
            "[4]\tvalidation_0-auc:0.759191\n",
            "[5]\tvalidation_0-auc:0.772933\n",
            "[6]\tvalidation_0-auc:0.774222\n",
            "[7]\tvalidation_0-auc:0.787673\n",
            "[8]\tvalidation_0-auc:0.791538\n",
            "[9]\tvalidation_0-auc:0.791933\n",
            "[10]\tvalidation_0-auc:0.794596\n",
            "[11]\tvalidation_0-auc:0.796657\n",
            "[12]\tvalidation_0-auc:0.796537\n",
            "[13]\tvalidation_0-auc:0.797499\n",
            "[14]\tvalidation_0-auc:0.80346\n",
            "[15]\tvalidation_0-auc:0.807428\n",
            "[16]\tvalidation_0-auc:0.806707\n",
            "[17]\tvalidation_0-auc:0.813681\n",
            "[18]\tvalidation_0-auc:0.814231\n",
            "[19]\tvalidation_0-auc:0.814815\n",
            "[20]\tvalidation_0-auc:0.816395\n",
            "[21]\tvalidation_0-auc:0.814317\n",
            "[22]\tvalidation_0-auc:0.815416\n",
            "[23]\tvalidation_0-auc:0.816997\n",
            "[24]\tvalidation_0-auc:0.81545\n",
            "[25]\tvalidation_0-auc:0.815708\n",
            "[26]\tvalidation_0-auc:0.816086\n",
            "[27]\tvalidation_0-auc:0.816258\n",
            "[28]\tvalidation_0-auc:0.815296\n",
            "[29]\tvalidation_0-auc:0.814987\n",
            "[30]\tvalidation_0-auc:0.815502\n",
            "[31]\tvalidation_0-auc:0.814523\n",
            "[32]\tvalidation_0-auc:0.815571\n",
            "[33]\tvalidation_0-auc:0.817478\n",
            "[34]\tvalidation_0-auc:0.816808\n",
            "[35]\tvalidation_0-auc:0.815502\n",
            "[36]\tvalidation_0-auc:0.815485\n",
            "[37]\tvalidation_0-auc:0.813784\n",
            "[38]\tvalidation_0-auc:0.816034\n",
            "[39]\tvalidation_0-auc:0.817323\n",
            "[40]\tvalidation_0-auc:0.815983\n",
            "[41]\tvalidation_0-auc:0.816704\n",
            "[42]\tvalidation_0-auc:0.815931\n",
            "[43]\tvalidation_0-auc:0.81789\n",
            "[44]\tvalidation_0-auc:0.817374\n",
            "[45]\tvalidation_0-auc:0.817873\n",
            "[46]\tvalidation_0-auc:0.818938\n",
            "[47]\tvalidation_0-auc:0.820089\n",
            "[48]\tvalidation_0-auc:0.821291\n",
            "[49]\tvalidation_0-auc:0.82191\n",
            "[50]\tvalidation_0-auc:0.822253\n",
            "[51]\tvalidation_0-auc:0.820638\n",
            "[52]\tvalidation_0-auc:0.821016\n",
            "[53]\tvalidation_0-auc:0.82203\n",
            "[54]\tvalidation_0-auc:0.821205\n",
            "[55]\tvalidation_0-auc:0.821137\n",
            "[56]\tvalidation_0-auc:0.820879\n",
            "[57]\tvalidation_0-auc:0.820432\n",
            "[58]\tvalidation_0-auc:0.820123\n",
            "[59]\tvalidation_0-auc:0.818457\n",
            "[60]\tvalidation_0-auc:0.819281\n",
            "[61]\tvalidation_0-auc:0.819178\n",
            "[62]\tvalidation_0-auc:0.820071\n",
            "[63]\tvalidation_0-auc:0.820484\n",
            "[64]\tvalidation_0-auc:0.820587\n",
            "[65]\tvalidation_0-auc:0.820381\n",
            "[66]\tvalidation_0-auc:0.819058\n",
            "[67]\tvalidation_0-auc:0.818989\n",
            "[68]\tvalidation_0-auc:0.818199\n",
            "[69]\tvalidation_0-auc:0.818216\n",
            "[70]\tvalidation_0-auc:0.818491\n",
            "[71]\tvalidation_0-auc:0.817374\n",
            "[72]\tvalidation_0-auc:0.817958\n",
            "[73]\tvalidation_0-auc:0.816928\n",
            "[74]\tvalidation_0-auc:0.816722\n",
            "[75]\tvalidation_0-auc:0.816653\n",
            "[76]\tvalidation_0-auc:0.816223\n",
            "[77]\tvalidation_0-auc:0.817289\n",
            "[78]\tvalidation_0-auc:0.817014\n",
            "[79]\tvalidation_0-auc:0.816808\n",
            "[80]\tvalidation_0-auc:0.81722\n",
            "[81]\tvalidation_0-auc:0.816636\n",
            "[82]\tvalidation_0-auc:0.816258\n",
            "[83]\tvalidation_0-auc:0.815674\n",
            "[84]\tvalidation_0-auc:0.815502\n",
            "[85]\tvalidation_0-auc:0.815914\n",
            "[86]\tvalidation_0-auc:0.81588\n",
            "[87]\tvalidation_0-auc:0.815399\n",
            "[88]\tvalidation_0-auc:0.815536\n",
            "[89]\tvalidation_0-auc:0.815296\n",
            "[90]\tvalidation_0-auc:0.815468\n",
            "[91]\tvalidation_0-auc:0.815433\n",
            "[92]\tvalidation_0-auc:0.81399\n",
            "[93]\tvalidation_0-auc:0.813956\n",
            "[94]\tvalidation_0-auc:0.81399\n",
            "[95]\tvalidation_0-auc:0.813784\n",
            "[96]\tvalidation_0-auc:0.815227\n",
            "[97]\tvalidation_0-auc:0.814746\n",
            "[98]\tvalidation_0-auc:0.814334\n",
            "[99]\tvalidation_0-auc:0.815365\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.6487179487179487 |        0.0         |         0.0         |         0.0         |\n",
            "|     GRU 0.1      | 0.7307692307692307 | 0.8636363636363636 |  0.2773722627737226 |  0.4198895027624309 |\n",
            "|   XGBoost 0.1    | 0.764102564102564  | 0.8688524590163934 | 0.38686131386861317 |  0.5353535353535354 |\n",
            "|    Logreg 0.1    | 0.7153846153846154 | 0.8611111111111112 | 0.22627737226277372 |  0.3583815028901734 |\n",
            "|     SVM 0.1      | 0.7256410256410256 |       0.8125       |  0.2846715328467153 | 0.42162162162162165 |\n",
            "|  LSTM beta 0.1   | 0.6470588235294118 |        0.0         |         0.0         |         0.0         |\n",
            "|   GRU beta 0.1   | 0.7535014005602241 |       0.7375       | 0.46825396825396826 |  0.5728155339805825 |\n",
            "| XGBoost beta 0.1 | 0.7703081232492998 | 0.7075471698113207 |  0.5952380952380952 |  0.646551724137931  |\n",
            "| logreg beta 0.1  | 0.7254901960784313 |        0.78        | 0.30952380952380953 |  0.4431818181818182 |\n",
            "|   svm beta 0.1   | 0.7450980392156863 | 0.6666666666666666 |  0.5555555555555556 |  0.606060606060606  |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 16ms/step - loss: 0.3884 - accuracy: 0.8866 - val_loss: 1.7800 - val_accuracy: 0.2308\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3502 - accuracy: 0.8933 - val_loss: 1.6112 - val_accuracy: 0.2308\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3513 - accuracy: 0.8933 - val_loss: 1.6039 - val_accuracy: 0.2308\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3534 - accuracy: 0.8933 - val_loss: 1.6610 - val_accuracy: 0.2308\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3457 - accuracy: 0.8933 - val_loss: 1.6875 - val_accuracy: 0.2308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 15ms/step - loss: 0.3789 - accuracy: 0.8908 - val_loss: 1.8925 - val_accuracy: 0.2308\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3494 - accuracy: 0.8933 - val_loss: 2.1167 - val_accuracy: 0.2308\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3197 - accuracy: 0.8966 - val_loss: 1.8612 - val_accuracy: 0.2436\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3097 - accuracy: 0.8916 - val_loss: 2.1948 - val_accuracy: 0.2359\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3098 - accuracy: 0.8882 - val_loss: 1.7529 - val_accuracy: 0.2513\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.614796\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.613796\n",
            "[2]\tvalidation_0-auc:0.613407\n",
            "[3]\tvalidation_0-auc:0.62937\n",
            "[4]\tvalidation_0-auc:0.684204\n",
            "[5]\tvalidation_0-auc:0.686296\n",
            "[6]\tvalidation_0-auc:0.702074\n",
            "[7]\tvalidation_0-auc:0.70063\n",
            "[8]\tvalidation_0-auc:0.700741\n",
            "[9]\tvalidation_0-auc:0.700444\n",
            "[10]\tvalidation_0-auc:0.700907\n",
            "[11]\tvalidation_0-auc:0.699352\n",
            "[12]\tvalidation_0-auc:0.710741\n",
            "[13]\tvalidation_0-auc:0.704648\n",
            "[14]\tvalidation_0-auc:0.705056\n",
            "[15]\tvalidation_0-auc:0.707056\n",
            "[16]\tvalidation_0-auc:0.709537\n",
            "[17]\tvalidation_0-auc:0.710722\n",
            "[18]\tvalidation_0-auc:0.704278\n",
            "[19]\tvalidation_0-auc:0.708407\n",
            "[20]\tvalidation_0-auc:0.710981\n",
            "[21]\tvalidation_0-auc:0.711926\n",
            "[22]\tvalidation_0-auc:0.715556\n",
            "[23]\tvalidation_0-auc:0.711796\n",
            "[24]\tvalidation_0-auc:0.715944\n",
            "[25]\tvalidation_0-auc:0.71863\n",
            "[26]\tvalidation_0-auc:0.715889\n",
            "[27]\tvalidation_0-auc:0.713333\n",
            "[28]\tvalidation_0-auc:0.711981\n",
            "[29]\tvalidation_0-auc:0.709815\n",
            "[30]\tvalidation_0-auc:0.708926\n",
            "[31]\tvalidation_0-auc:0.705778\n",
            "[32]\tvalidation_0-auc:0.705926\n",
            "[33]\tvalidation_0-auc:0.703593\n",
            "[34]\tvalidation_0-auc:0.700926\n",
            "[35]\tvalidation_0-auc:0.699907\n",
            "[36]\tvalidation_0-auc:0.695982\n",
            "[37]\tvalidation_0-auc:0.694574\n",
            "[38]\tvalidation_0-auc:0.690833\n",
            "[39]\tvalidation_0-auc:0.689241\n",
            "[40]\tvalidation_0-auc:0.688352\n",
            "[41]\tvalidation_0-auc:0.688463\n",
            "[42]\tvalidation_0-auc:0.687556\n",
            "[43]\tvalidation_0-auc:0.687074\n",
            "[44]\tvalidation_0-auc:0.688296\n",
            "[45]\tvalidation_0-auc:0.687\n",
            "[46]\tvalidation_0-auc:0.687426\n",
            "[47]\tvalidation_0-auc:0.686389\n",
            "[48]\tvalidation_0-auc:0.685167\n",
            "[49]\tvalidation_0-auc:0.683278\n",
            "[50]\tvalidation_0-auc:0.681574\n",
            "[51]\tvalidation_0-auc:0.681426\n",
            "[52]\tvalidation_0-auc:0.68163\n",
            "[53]\tvalidation_0-auc:0.682259\n",
            "[54]\tvalidation_0-auc:0.681889\n",
            "[55]\tvalidation_0-auc:0.68063\n",
            "[56]\tvalidation_0-auc:0.681593\n",
            "[57]\tvalidation_0-auc:0.681074\n",
            "[58]\tvalidation_0-auc:0.681074\n",
            "[59]\tvalidation_0-auc:0.680333\n",
            "[60]\tvalidation_0-auc:0.682222\n",
            "[61]\tvalidation_0-auc:0.680741\n",
            "[62]\tvalidation_0-auc:0.681593\n",
            "[63]\tvalidation_0-auc:0.681407\n",
            "[64]\tvalidation_0-auc:0.679259\n",
            "[65]\tvalidation_0-auc:0.679741\n",
            "[66]\tvalidation_0-auc:0.677889\n",
            "[67]\tvalidation_0-auc:0.679556\n",
            "[68]\tvalidation_0-auc:0.678926\n",
            "[69]\tvalidation_0-auc:0.679333\n",
            "[70]\tvalidation_0-auc:0.679482\n",
            "[71]\tvalidation_0-auc:0.678296\n",
            "[72]\tvalidation_0-auc:0.68\n",
            "[73]\tvalidation_0-auc:0.679407\n",
            "[74]\tvalidation_0-auc:0.677741\n",
            "[75]\tvalidation_0-auc:0.677481\n",
            "Stopping. Best iteration:\n",
            "[25]\tvalidation_0-auc:0.71863\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 3s 17ms/step - loss: 0.3779 - accuracy: 0.8859 - val_loss: 1.7956 - val_accuracy: 0.2521\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.3666 - accuracy: 0.8902 - val_loss: 1.7311 - val_accuracy: 0.2521\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.3633 - accuracy: 0.8902 - val_loss: 1.7047 - val_accuracy: 0.2521\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.3631 - accuracy: 0.8902 - val_loss: 1.7143 - val_accuracy: 0.2521\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.3484 - accuracy: 0.8902 - val_loss: 1.5341 - val_accuracy: 0.2521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 3s 16ms/step - loss: 0.3895 - accuracy: 0.8859 - val_loss: 1.7398 - val_accuracy: 0.2521\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.3063 - accuracy: 0.8946 - val_loss: 2.1204 - val_accuracy: 0.2521\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.2872 - accuracy: 0.9092 - val_loss: 1.7708 - val_accuracy: 0.2661\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.2909 - accuracy: 0.9032 - val_loss: 1.9339 - val_accuracy: 0.2577\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.2839 - accuracy: 0.9041 - val_loss: 2.0649 - val_accuracy: 0.2717\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.670454\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.664836\n",
            "[2]\tvalidation_0-auc:0.64124\n",
            "[3]\tvalidation_0-auc:0.633999\n",
            "[4]\tvalidation_0-auc:0.639638\n",
            "[5]\tvalidation_0-auc:0.634623\n",
            "[6]\tvalidation_0-auc:0.636871\n",
            "[7]\tvalidation_0-auc:0.636829\n",
            "[8]\tvalidation_0-auc:0.63633\n",
            "[9]\tvalidation_0-auc:0.63094\n",
            "[10]\tvalidation_0-auc:0.635976\n",
            "[11]\tvalidation_0-auc:0.631273\n",
            "[12]\tvalidation_0-auc:0.637828\n",
            "[13]\tvalidation_0-auc:0.636704\n",
            "[14]\tvalidation_0-auc:0.637474\n",
            "[15]\tvalidation_0-auc:0.638951\n",
            "[16]\tvalidation_0-auc:0.639243\n",
            "[17]\tvalidation_0-auc:0.651352\n",
            "[18]\tvalidation_0-auc:0.65901\n",
            "[19]\tvalidation_0-auc:0.666146\n",
            "[20]\tvalidation_0-auc:0.674241\n",
            "[21]\tvalidation_0-auc:0.680275\n",
            "[22]\tvalidation_0-auc:0.686517\n",
            "[23]\tvalidation_0-auc:0.69149\n",
            "[24]\tvalidation_0-auc:0.687765\n",
            "[25]\tvalidation_0-auc:0.696005\n",
            "[26]\tvalidation_0-auc:0.691677\n",
            "[27]\tvalidation_0-auc:0.687682\n",
            "[28]\tvalidation_0-auc:0.694008\n",
            "[29]\tvalidation_0-auc:0.689305\n",
            "[30]\tvalidation_0-auc:0.694965\n",
            "[31]\tvalidation_0-auc:0.691968\n",
            "[32]\tvalidation_0-auc:0.695839\n",
            "[33]\tvalidation_0-auc:0.697628\n",
            "[34]\tvalidation_0-auc:0.693154\n",
            "[35]\tvalidation_0-auc:0.6964\n",
            "[36]\tvalidation_0-auc:0.701456\n",
            "[37]\tvalidation_0-auc:0.702809\n",
            "[38]\tvalidation_0-auc:0.698564\n",
            "[39]\tvalidation_0-auc:0.702226\n",
            "[40]\tvalidation_0-auc:0.708489\n",
            "[41]\tvalidation_0-auc:0.713005\n",
            "[42]\tvalidation_0-auc:0.713587\n",
            "[43]\tvalidation_0-auc:0.710633\n",
            "[44]\tvalidation_0-auc:0.711715\n",
            "[45]\tvalidation_0-auc:0.707241\n",
            "[46]\tvalidation_0-auc:0.710737\n",
            "[47]\tvalidation_0-auc:0.708864\n",
            "[48]\tvalidation_0-auc:0.702518\n",
            "[49]\tvalidation_0-auc:0.701685\n",
            "[50]\tvalidation_0-auc:0.700895\n",
            "[51]\tvalidation_0-auc:0.701977\n",
            "[52]\tvalidation_0-auc:0.705327\n",
            "[53]\tvalidation_0-auc:0.703288\n",
            "[54]\tvalidation_0-auc:0.705868\n",
            "[55]\tvalidation_0-auc:0.706076\n",
            "[56]\tvalidation_0-auc:0.703953\n",
            "[57]\tvalidation_0-auc:0.700749\n",
            "[58]\tvalidation_0-auc:0.70516\n",
            "[59]\tvalidation_0-auc:0.705077\n",
            "[60]\tvalidation_0-auc:0.707241\n",
            "[61]\tvalidation_0-auc:0.708739\n",
            "[62]\tvalidation_0-auc:0.709571\n",
            "[63]\tvalidation_0-auc:0.709238\n",
            "[64]\tvalidation_0-auc:0.707407\n",
            "[65]\tvalidation_0-auc:0.704869\n",
            "[66]\tvalidation_0-auc:0.703912\n",
            "[67]\tvalidation_0-auc:0.704203\n",
            "[68]\tvalidation_0-auc:0.705368\n",
            "[69]\tvalidation_0-auc:0.704661\n",
            "[70]\tvalidation_0-auc:0.699709\n",
            "[71]\tvalidation_0-auc:0.701956\n",
            "[72]\tvalidation_0-auc:0.701956\n",
            "[73]\tvalidation_0-auc:0.700208\n",
            "[74]\tvalidation_0-auc:0.700083\n",
            "[75]\tvalidation_0-auc:0.703163\n",
            "[76]\tvalidation_0-auc:0.708406\n",
            "[77]\tvalidation_0-auc:0.704494\n",
            "[78]\tvalidation_0-auc:0.70258\n",
            "[79]\tvalidation_0-auc:0.706409\n",
            "[80]\tvalidation_0-auc:0.710071\n",
            "[81]\tvalidation_0-auc:0.708281\n",
            "[82]\tvalidation_0-auc:0.707158\n",
            "[83]\tvalidation_0-auc:0.706617\n",
            "[84]\tvalidation_0-auc:0.705452\n",
            "[85]\tvalidation_0-auc:0.705035\n",
            "[86]\tvalidation_0-auc:0.701914\n",
            "[87]\tvalidation_0-auc:0.700208\n",
            "[88]\tvalidation_0-auc:0.699459\n",
            "[89]\tvalidation_0-auc:0.702871\n",
            "[90]\tvalidation_0-auc:0.701706\n",
            "[91]\tvalidation_0-auc:0.701165\n",
            "[92]\tvalidation_0-auc:0.701873\n",
            "Stopping. Best iteration:\n",
            "[42]\tvalidation_0-auc:0.713587\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+---------------------+--------------------+----------------------+----------------------+\n",
            "|      Model       |       Accuracy      |     Precision      |        Recall        |       F1 score       |\n",
            "+------------------+---------------------+--------------------+----------------------+----------------------+\n",
            "|     LSTM 0.2     | 0.23076923076923078 |        0.0         |         0.0          |         0.0          |\n",
            "|     GRU 0.2      |  0.2512820512820513 |        1.0         | 0.02666666666666667  | 0.05194805194805195  |\n",
            "|   XGBoost 0.2    |  0.2923076923076923 | 0.9615384615384616 | 0.08333333333333333  | 0.15337423312883433  |\n",
            "|    Logreg 0.2    | 0.24102564102564103 |        1.0         | 0.013333333333333334 | 0.02631578947368421  |\n",
            "|     SVM 0.2      |  0.258974358974359  |        1.0         | 0.03666666666666667  |  0.0707395498392283  |\n",
            "|  LSTM beta 0.2   | 0.25210084033613445 |        0.0         |         0.0          |         0.0          |\n",
            "|   GRU beta 0.2   | 0.27170868347338933 |        1.0         | 0.026217228464419477 | 0.051094890510948905 |\n",
            "| XGBoost beta 0.2 | 0.29971988795518206 | 0.8695652173913043 |  0.0749063670411985  | 0.13793103448275865  |\n",
            "| logreg beta 0.2  |  0.2689075630252101 |        1.0         | 0.02247191011235955  | 0.04395604395604395  |\n",
            "|   svm beta 0.2   |  0.2689075630252101 |        1.0         | 0.02247191011235955  | 0.04395604395604395  |\n",
            "+------------------+---------------------+--------------------+----------------------+----------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 16ms/step - loss: 0.5065 - accuracy: 0.8092 - val_loss: 0.7686 - val_accuracy: 0.6128\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4911 - accuracy: 0.8151 - val_loss: 0.7539 - val_accuracy: 0.6128\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4768 - accuracy: 0.8151 - val_loss: 0.8258 - val_accuracy: 0.6128\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4504 - accuracy: 0.8109 - val_loss: 0.6194 - val_accuracy: 0.6128\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4259 - accuracy: 0.8286 - val_loss: 0.6290 - val_accuracy: 0.6385\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 15ms/step - loss: 0.5155 - accuracy: 0.8118 - val_loss: 0.8285 - val_accuracy: 0.6128\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4642 - accuracy: 0.8168 - val_loss: 0.7093 - val_accuracy: 0.6154\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4227 - accuracy: 0.8336 - val_loss: 0.6394 - val_accuracy: 0.6487\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4057 - accuracy: 0.8269 - val_loss: 0.5978 - val_accuracy: 0.7077\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3990 - accuracy: 0.8462 - val_loss: 0.6231 - val_accuracy: 0.6846\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.75045\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.749799\n",
            "[2]\tvalidation_0-auc:0.760633\n",
            "[3]\tvalidation_0-auc:0.77144\n",
            "[4]\tvalidation_0-auc:0.767491\n",
            "[5]\tvalidation_0-auc:0.775596\n",
            "[6]\tvalidation_0-auc:0.777356\n",
            "[7]\tvalidation_0-auc:0.772271\n",
            "[8]\tvalidation_0-auc:0.77931\n",
            "[9]\tvalidation_0-auc:0.783452\n",
            "[10]\tvalidation_0-auc:0.783036\n",
            "[11]\tvalidation_0-auc:0.780944\n",
            "[12]\tvalidation_0-auc:0.780016\n",
            "[13]\tvalidation_0-auc:0.778561\n",
            "[14]\tvalidation_0-auc:0.773352\n",
            "[15]\tvalidation_0-auc:0.771094\n",
            "[16]\tvalidation_0-auc:0.772396\n",
            "[17]\tvalidation_0-auc:0.767505\n",
            "[18]\tvalidation_0-auc:0.767201\n",
            "[19]\tvalidation_0-auc:0.764901\n",
            "[20]\tvalidation_0-auc:0.767187\n",
            "[21]\tvalidation_0-auc:0.764277\n",
            "[22]\tvalidation_0-auc:0.762684\n",
            "[23]\tvalidation_0-auc:0.764277\n",
            "[24]\tvalidation_0-auc:0.76267\n",
            "[25]\tvalidation_0-auc:0.758486\n",
            "[26]\tvalidation_0-auc:0.757461\n",
            "[27]\tvalidation_0-auc:0.757211\n",
            "[28]\tvalidation_0-auc:0.755812\n",
            "[29]\tvalidation_0-auc:0.755563\n",
            "[30]\tvalidation_0-auc:0.755105\n",
            "[31]\tvalidation_0-auc:0.753789\n",
            "[32]\tvalidation_0-auc:0.751046\n",
            "[33]\tvalidation_0-auc:0.749938\n",
            "[34]\tvalidation_0-auc:0.748358\n",
            "[35]\tvalidation_0-auc:0.746585\n",
            "[36]\tvalidation_0-auc:0.745546\n",
            "[37]\tvalidation_0-auc:0.744077\n",
            "[38]\tvalidation_0-auc:0.743897\n",
            "[39]\tvalidation_0-auc:0.745172\n",
            "[40]\tvalidation_0-auc:0.743357\n",
            "[41]\tvalidation_0-auc:0.743855\n",
            "[42]\tvalidation_0-auc:0.744784\n",
            "[43]\tvalidation_0-auc:0.745532\n",
            "[44]\tvalidation_0-auc:0.745338\n",
            "[45]\tvalidation_0-auc:0.742955\n",
            "[46]\tvalidation_0-auc:0.742359\n",
            "[47]\tvalidation_0-auc:0.741666\n",
            "[48]\tvalidation_0-auc:0.741057\n",
            "[49]\tvalidation_0-auc:0.739921\n",
            "[50]\tvalidation_0-auc:0.739616\n",
            "[51]\tvalidation_0-auc:0.7383\n",
            "[52]\tvalidation_0-auc:0.738688\n",
            "[53]\tvalidation_0-auc:0.738133\n",
            "[54]\tvalidation_0-auc:0.736443\n",
            "[55]\tvalidation_0-auc:0.737579\n",
            "[56]\tvalidation_0-auc:0.737164\n",
            "[57]\tvalidation_0-auc:0.737219\n",
            "[58]\tvalidation_0-auc:0.738078\n",
            "[59]\tvalidation_0-auc:0.738535\n",
            "Stopping. Best iteration:\n",
            "[9]\tvalidation_0-auc:0.783452\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 3s 16ms/step - loss: 0.5108 - accuracy: 0.8047 - val_loss: 0.7212 - val_accuracy: 0.6078\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4964 - accuracy: 0.8099 - val_loss: 0.7988 - val_accuracy: 0.6078\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4856 - accuracy: 0.8099 - val_loss: 0.8197 - val_accuracy: 0.6078\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4607 - accuracy: 0.8099 - val_loss: 0.7544 - val_accuracy: 0.6078\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4306 - accuracy: 0.8055 - val_loss: 0.6084 - val_accuracy: 0.6078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 2s 14ms/step - loss: 0.5168 - accuracy: 0.8055 - val_loss: 0.7404 - val_accuracy: 0.6078\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.4417 - accuracy: 0.8142 - val_loss: 0.5734 - val_accuracy: 0.6751\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.4053 - accuracy: 0.8228 - val_loss: 0.5454 - val_accuracy: 0.7255\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.3828 - accuracy: 0.8332 - val_loss: 0.6614 - val_accuracy: 0.6667\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.3864 - accuracy: 0.8228 - val_loss: 0.5607 - val_accuracy: 0.7143\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.759414\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.772992\n",
            "[2]\tvalidation_0-auc:0.769108\n",
            "[3]\tvalidation_0-auc:0.773173\n",
            "[4]\tvalidation_0-auc:0.779526\n",
            "[5]\tvalidation_0-auc:0.789121\n",
            "[6]\tvalidation_0-auc:0.790076\n",
            "[7]\tvalidation_0-auc:0.788891\n",
            "[8]\tvalidation_0-auc:0.791656\n",
            "[9]\tvalidation_0-auc:0.792594\n",
            "[10]\tvalidation_0-auc:0.793845\n",
            "[11]\tvalidation_0-auc:0.791754\n",
            "[12]\tvalidation_0-auc:0.794289\n",
            "[13]\tvalidation_0-auc:0.796215\n",
            "[14]\tvalidation_0-auc:0.800592\n",
            "[15]\tvalidation_0-auc:0.80102\n",
            "[16]\tvalidation_0-auc:0.795935\n",
            "[17]\tvalidation_0-auc:0.794124\n",
            "[18]\tvalidation_0-auc:0.794207\n",
            "[19]\tvalidation_0-auc:0.794009\n",
            "[20]\tvalidation_0-auc:0.796725\n",
            "[21]\tvalidation_0-auc:0.798535\n",
            "[22]\tvalidation_0-auc:0.79972\n",
            "[23]\tvalidation_0-auc:0.800872\n",
            "[24]\tvalidation_0-auc:0.800625\n",
            "[25]\tvalidation_0-auc:0.801086\n",
            "[26]\tvalidation_0-auc:0.801662\n",
            "[27]\tvalidation_0-auc:0.800675\n",
            "[28]\tvalidation_0-auc:0.8013\n",
            "[29]\tvalidation_0-auc:0.803242\n",
            "[30]\tvalidation_0-auc:0.800642\n",
            "[31]\tvalidation_0-auc:0.80209\n",
            "[32]\tvalidation_0-auc:0.802123\n",
            "[33]\tvalidation_0-auc:0.801827\n",
            "[34]\tvalidation_0-auc:0.803176\n",
            "[35]\tvalidation_0-auc:0.803111\n",
            "[36]\tvalidation_0-auc:0.804427\n",
            "[37]\tvalidation_0-auc:0.803127\n",
            "[38]\tvalidation_0-auc:0.80293\n",
            "[39]\tvalidation_0-auc:0.802419\n",
            "[40]\tvalidation_0-auc:0.802172\n",
            "[41]\tvalidation_0-auc:0.801745\n",
            "[42]\tvalidation_0-auc:0.801909\n",
            "[43]\tvalidation_0-auc:0.801843\n",
            "[44]\tvalidation_0-auc:0.801777\n",
            "[45]\tvalidation_0-auc:0.802337\n",
            "[46]\tvalidation_0-auc:0.803522\n",
            "[47]\tvalidation_0-auc:0.804378\n",
            "[48]\tvalidation_0-auc:0.805283\n",
            "[49]\tvalidation_0-auc:0.801926\n",
            "[50]\tvalidation_0-auc:0.802814\n",
            "[51]\tvalidation_0-auc:0.802386\n",
            "[52]\tvalidation_0-auc:0.801531\n",
            "[53]\tvalidation_0-auc:0.79944\n",
            "[54]\tvalidation_0-auc:0.799078\n",
            "[55]\tvalidation_0-auc:0.799243\n",
            "[56]\tvalidation_0-auc:0.79865\n",
            "[57]\tvalidation_0-auc:0.798453\n",
            "[58]\tvalidation_0-auc:0.79865\n",
            "[59]\tvalidation_0-auc:0.798288\n",
            "[60]\tvalidation_0-auc:0.798058\n",
            "[61]\tvalidation_0-auc:0.797893\n",
            "[62]\tvalidation_0-auc:0.797696\n",
            "[63]\tvalidation_0-auc:0.797795\n",
            "[64]\tvalidation_0-auc:0.797005\n",
            "[65]\tvalidation_0-auc:0.796643\n",
            "[66]\tvalidation_0-auc:0.796215\n",
            "[67]\tvalidation_0-auc:0.795359\n",
            "[68]\tvalidation_0-auc:0.796017\n",
            "[69]\tvalidation_0-auc:0.796313\n",
            "[70]\tvalidation_0-auc:0.794964\n",
            "[71]\tvalidation_0-auc:0.795145\n",
            "[72]\tvalidation_0-auc:0.795704\n",
            "[73]\tvalidation_0-auc:0.795112\n",
            "[74]\tvalidation_0-auc:0.795013\n",
            "[75]\tvalidation_0-auc:0.794421\n",
            "[76]\tvalidation_0-auc:0.793828\n",
            "[77]\tvalidation_0-auc:0.794618\n",
            "[78]\tvalidation_0-auc:0.795079\n",
            "[79]\tvalidation_0-auc:0.794519\n",
            "[80]\tvalidation_0-auc:0.794059\n",
            "[81]\tvalidation_0-auc:0.793861\n",
            "[82]\tvalidation_0-auc:0.792972\n",
            "[83]\tvalidation_0-auc:0.791936\n",
            "[84]\tvalidation_0-auc:0.792001\n",
            "[85]\tvalidation_0-auc:0.790191\n",
            "[86]\tvalidation_0-auc:0.791277\n",
            "[87]\tvalidation_0-auc:0.791146\n",
            "[88]\tvalidation_0-auc:0.790092\n",
            "[89]\tvalidation_0-auc:0.79131\n",
            "[90]\tvalidation_0-auc:0.79108\n",
            "[91]\tvalidation_0-auc:0.791804\n",
            "[92]\tvalidation_0-auc:0.790783\n",
            "[93]\tvalidation_0-auc:0.790685\n",
            "[94]\tvalidation_0-auc:0.79029\n",
            "[95]\tvalidation_0-auc:0.789533\n",
            "[96]\tvalidation_0-auc:0.78894\n",
            "[97]\tvalidation_0-auc:0.787031\n",
            "[98]\tvalidation_0-auc:0.786669\n",
            "Stopping. Best iteration:\n",
            "[48]\tvalidation_0-auc:0.805283\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.6384615384615384 | 0.7272727272727273 | 0.10596026490066225 | 0.18497109826589594 |\n",
            "|      GRU 0.15     | 0.6846153846153846 | 0.7916666666666666 | 0.25165562913907286 | 0.38190954773869346 |\n",
            "|    XGBoost 0.15   | 0.6974358974358974 | 0.8235294117647058 |  0.2781456953642384 |  0.4158415841584159 |\n",
            "|    Logreg 0.15    | 0.6487179487179487 | 0.8181818181818182 | 0.11920529801324503 | 0.20809248554913296 |\n",
            "|      SVM 0.15     | 0.6743589743589744 | 0.8157894736842105 |  0.2052980132450331 |  0.328042328042328  |\n",
            "|   LSTM beta 0.15  | 0.6078431372549019 |        0.0         |         0.0         |         0.0         |\n",
            "|   GRU beta 0.15   | 0.7142857142857143 | 0.8275862068965517 | 0.34285714285714286 | 0.48484848484848486 |\n",
            "| XGBoost beta 0.15 | 0.7170868347338936 | 0.696969696969697  |  0.4928571428571429 |  0.5774058577405858 |\n",
            "|  logreg beta 0.15 | 0.6862745098039216 | 0.868421052631579  |  0.2357142857142857 |  0.3707865168539326 |\n",
            "|   svm beta 0.15   | 0.7226890756302521 | 0.7204301075268817 |  0.4785714285714286 |  0.5751072961373391 |\n",
            "+-------------------+--------------------+--------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dbz23v6pYLzd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "outputId": "cd542e0b-bae7-48c7-d1f9-40f9ebb544d4"
      },
      "source": [
        "Result_cross.to_csv('RHT_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.648718</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.863636</td>\n",
              "      <td>0.730769</td>\n",
              "      <td>0.419890</td>\n",
              "      <td>0.277372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.868852</td>\n",
              "      <td>0.764103</td>\n",
              "      <td>0.535354</td>\n",
              "      <td>0.386861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.861111</td>\n",
              "      <td>0.715385</td>\n",
              "      <td>0.358382</td>\n",
              "      <td>0.226277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.725641</td>\n",
              "      <td>0.421622</td>\n",
              "      <td>0.284672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.737500</td>\n",
              "      <td>0.753501</td>\n",
              "      <td>0.572816</td>\n",
              "      <td>0.468254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.707547</td>\n",
              "      <td>0.770308</td>\n",
              "      <td>0.646552</td>\n",
              "      <td>0.595238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.780000</td>\n",
              "      <td>0.725490</td>\n",
              "      <td>0.443182</td>\n",
              "      <td>0.309524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.745098</td>\n",
              "      <td>0.606061</td>\n",
              "      <td>0.555556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.251282</td>\n",
              "      <td>0.051948</td>\n",
              "      <td>0.026667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.961538</td>\n",
              "      <td>0.292308</td>\n",
              "      <td>0.153374</td>\n",
              "      <td>0.083333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.241026</td>\n",
              "      <td>0.026316</td>\n",
              "      <td>0.013333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.258974</td>\n",
              "      <td>0.070740</td>\n",
              "      <td>0.036667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.252101</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.271709</td>\n",
              "      <td>0.051095</td>\n",
              "      <td>0.026217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.299720</td>\n",
              "      <td>0.137931</td>\n",
              "      <td>0.074906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.268908</td>\n",
              "      <td>0.043956</td>\n",
              "      <td>0.022472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.268908</td>\n",
              "      <td>0.043956</td>\n",
              "      <td>0.022472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.638462</td>\n",
              "      <td>0.184971</td>\n",
              "      <td>0.105960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.684615</td>\n",
              "      <td>0.381910</td>\n",
              "      <td>0.251656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.697436</td>\n",
              "      <td>0.415842</td>\n",
              "      <td>0.278146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.648718</td>\n",
              "      <td>0.208092</td>\n",
              "      <td>0.119205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.815789</td>\n",
              "      <td>0.674359</td>\n",
              "      <td>0.328042</td>\n",
              "      <td>0.205298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.607843</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.827586</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.484848</td>\n",
              "      <td>0.342857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.696970</td>\n",
              "      <td>0.717087</td>\n",
              "      <td>0.577406</td>\n",
              "      <td>0.492857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.868421</td>\n",
              "      <td>0.686275</td>\n",
              "      <td>0.370787</td>\n",
              "      <td>0.235714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.720430</td>\n",
              "      <td>0.722689</td>\n",
              "      <td>0.575107</td>\n",
              "      <td>0.478571</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  RHT  0.000000  0.648718  0.000000  0.000000\n",
              "1            GRU 0.1  RHT  0.863636  0.730769  0.419890  0.277372\n",
              "2        XGBoost 0.1  RHT  0.868852  0.764103  0.535354  0.386861\n",
              "3         Logreg 0.1  RHT  0.861111  0.715385  0.358382  0.226277\n",
              "4            SVM 0.1  RHT  0.812500  0.725641  0.421622  0.284672\n",
              "5      LSTM beta 0.1  RHT  0.000000  0.647059  0.000000  0.000000\n",
              "6       GRU beta 0.1  RHT  0.737500  0.753501  0.572816  0.468254\n",
              "7   XGBoost beta 0.1  RHT  0.707547  0.770308  0.646552  0.595238\n",
              "8    logreg beta 0.1  RHT  0.780000  0.725490  0.443182  0.309524\n",
              "9       svm beta 0.1  RHT  0.666667  0.745098  0.606061  0.555556\n",
              "0           LSTM 0.2  RHT  0.000000  0.230769  0.000000  0.000000\n",
              "1            GRU 0.2  RHT  1.000000  0.251282  0.051948  0.026667\n",
              "2        XGBoost 0.2  RHT  0.961538  0.292308  0.153374  0.083333\n",
              "3         Logreg 0.2  RHT  1.000000  0.241026  0.026316  0.013333\n",
              "4            SVM 0.2  RHT  1.000000  0.258974  0.070740  0.036667\n",
              "5      LSTM beta 0.2  RHT  0.000000  0.252101  0.000000  0.000000\n",
              "6       GRU beta 0.2  RHT  1.000000  0.271709  0.051095  0.026217\n",
              "7   XGBoost beta 0.2  RHT  0.869565  0.299720  0.137931  0.074906\n",
              "8    logreg beta 0.2  RHT  1.000000  0.268908  0.043956  0.022472\n",
              "9       svm beta 0.2  RHT  1.000000  0.268908  0.043956  0.022472\n",
              "0          LSTM 0.15  RHT  0.727273  0.638462  0.184971  0.105960\n",
              "1           GRU 0.15  RHT  0.791667  0.684615  0.381910  0.251656\n",
              "2       XGBoost 0.15  RHT  0.823529  0.697436  0.415842  0.278146\n",
              "3        Logreg 0.15  RHT  0.818182  0.648718  0.208092  0.119205\n",
              "4           SVM 0.15  RHT  0.815789  0.674359  0.328042  0.205298\n",
              "5     LSTM beta 0.15  RHT  0.000000  0.607843  0.000000  0.000000\n",
              "6      GRU beta 0.15  RHT  0.827586  0.714286  0.484848  0.342857\n",
              "7  XGBoost beta 0.15  RHT  0.696970  0.717087  0.577406  0.492857\n",
              "8   logreg beta 0.15  RHT  0.868421  0.686275  0.370787  0.235714\n",
              "9      svm beta 0.15  RHT  0.720430  0.722689  0.575107  0.478571"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT7obJAWYLzd"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1510:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1510:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1510:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1510:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1510:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1543:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1543:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1543:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1543:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1543:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzxbq-1ZYLzd"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23fCsGalYLzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ce98cf9-49ec-4ac9-ccf2-9beb1b12102b"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1500, test_end=1900)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"RHT\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 17ms/step - loss: 0.5415 - accuracy: 0.7849 - val_loss: 0.7184 - val_accuracy: 0.6487\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5292 - accuracy: 0.7924 - val_loss: 0.6810 - val_accuracy: 0.6487\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5216 - accuracy: 0.7924 - val_loss: 0.6697 - val_accuracy: 0.6487\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5125 - accuracy: 0.7924 - val_loss: 0.6570 - val_accuracy: 0.6487\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5078 - accuracy: 0.7924 - val_loss: 0.6289 - val_accuracy: 0.6487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 15ms/step - loss: 0.5396 - accuracy: 0.7815 - val_loss: 0.6833 - val_accuracy: 0.6487\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5112 - accuracy: 0.7924 - val_loss: 0.6368 - val_accuracy: 0.6487\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4569 - accuracy: 0.8059 - val_loss: 0.5953 - val_accuracy: 0.6692\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4274 - accuracy: 0.8202 - val_loss: 0.5502 - val_accuracy: 0.7179\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4212 - accuracy: 0.8244 - val_loss: 0.5454 - val_accuracy: 0.7231\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.792634\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.826303\n",
            "[2]\tvalidation_0-auc:0.840469\n",
            "[3]\tvalidation_0-auc:0.839099\n",
            "[4]\tvalidation_0-auc:0.842229\n",
            "[5]\tvalidation_0-auc:0.839791\n",
            "[6]\tvalidation_0-auc:0.841508\n",
            "[7]\tvalidation_0-auc:0.848086\n",
            "[8]\tvalidation_0-auc:0.846672\n",
            "[9]\tvalidation_0-auc:0.847754\n",
            "[10]\tvalidation_0-auc:0.848821\n",
            "[11]\tvalidation_0-auc:0.848778\n",
            "[12]\tvalidation_0-auc:0.845691\n",
            "[13]\tvalidation_0-auc:0.846412\n",
            "[14]\tvalidation_0-auc:0.844335\n",
            "[15]\tvalidation_0-auc:0.845619\n",
            "[16]\tvalidation_0-auc:0.843369\n",
            "[17]\tvalidation_0-auc:0.841724\n",
            "[18]\tvalidation_0-auc:0.838666\n",
            "[19]\tvalidation_0-auc:0.837699\n",
            "[20]\tvalidation_0-auc:0.836733\n",
            "[21]\tvalidation_0-auc:0.837483\n",
            "[22]\tvalidation_0-auc:0.83604\n",
            "[23]\tvalidation_0-auc:0.835839\n",
            "[24]\tvalidation_0-auc:0.834396\n",
            "[25]\tvalidation_0-auc:0.834165\n",
            "[26]\tvalidation_0-auc:0.832405\n",
            "[27]\tvalidation_0-auc:0.831309\n",
            "[28]\tvalidation_0-auc:0.829059\n",
            "[29]\tvalidation_0-auc:0.827039\n",
            "[30]\tvalidation_0-auc:0.827443\n",
            "[31]\tvalidation_0-auc:0.825683\n",
            "[32]\tvalidation_0-auc:0.824067\n",
            "[33]\tvalidation_0-auc:0.823144\n",
            "[34]\tvalidation_0-auc:0.822942\n",
            "[35]\tvalidation_0-auc:0.823\n",
            "[36]\tvalidation_0-auc:0.81974\n",
            "[37]\tvalidation_0-auc:0.819971\n",
            "[38]\tvalidation_0-auc:0.820548\n",
            "[39]\tvalidation_0-auc:0.81948\n",
            "[40]\tvalidation_0-auc:0.818369\n",
            "[41]\tvalidation_0-auc:0.818225\n",
            "[42]\tvalidation_0-auc:0.818874\n",
            "[43]\tvalidation_0-auc:0.818167\n",
            "[44]\tvalidation_0-auc:0.817186\n",
            "[45]\tvalidation_0-auc:0.816956\n",
            "[46]\tvalidation_0-auc:0.817071\n",
            "[47]\tvalidation_0-auc:0.81684\n",
            "[48]\tvalidation_0-auc:0.816725\n",
            "[49]\tvalidation_0-auc:0.815816\n",
            "[50]\tvalidation_0-auc:0.816682\n",
            "[51]\tvalidation_0-auc:0.816364\n",
            "[52]\tvalidation_0-auc:0.816018\n",
            "[53]\tvalidation_0-auc:0.815326\n",
            "[54]\tvalidation_0-auc:0.814691\n",
            "[55]\tvalidation_0-auc:0.814662\n",
            "[56]\tvalidation_0-auc:0.81472\n",
            "[57]\tvalidation_0-auc:0.813739\n",
            "[58]\tvalidation_0-auc:0.813104\n",
            "[59]\tvalidation_0-auc:0.810825\n",
            "[60]\tvalidation_0-auc:0.808776\n",
            "Stopping. Best iteration:\n",
            "[10]\tvalidation_0-auc:0.848821\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 3s 17ms/step - loss: 0.5384 - accuracy: 0.7813 - val_loss: 0.6866 - val_accuracy: 0.6471\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.5348 - accuracy: 0.7865 - val_loss: 0.6775 - val_accuracy: 0.6471\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.5288 - accuracy: 0.7865 - val_loss: 0.6952 - val_accuracy: 0.6471\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.5183 - accuracy: 0.7865 - val_loss: 0.7286 - val_accuracy: 0.6471\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.5155 - accuracy: 0.7865 - val_loss: 0.6946 - val_accuracy: 0.6471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 3s 15ms/step - loss: 0.5371 - accuracy: 0.7857 - val_loss: 0.6468 - val_accuracy: 0.6471\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4760 - accuracy: 0.7926 - val_loss: 0.5462 - val_accuracy: 0.6583\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.4265 - accuracy: 0.8245 - val_loss: 0.4807 - val_accuracy: 0.7871\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.4129 - accuracy: 0.8194 - val_loss: 0.5100 - val_accuracy: 0.7479\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.4047 - accuracy: 0.8289 - val_loss: 0.5189 - val_accuracy: 0.7731\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.719697\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.743403\n",
            "[2]\tvalidation_0-auc:0.774153\n",
            "[3]\tvalidation_0-auc:0.772195\n",
            "[4]\tvalidation_0-auc:0.759191\n",
            "[5]\tvalidation_0-auc:0.772933\n",
            "[6]\tvalidation_0-auc:0.774222\n",
            "[7]\tvalidation_0-auc:0.787673\n",
            "[8]\tvalidation_0-auc:0.791538\n",
            "[9]\tvalidation_0-auc:0.791933\n",
            "[10]\tvalidation_0-auc:0.794596\n",
            "[11]\tvalidation_0-auc:0.796657\n",
            "[12]\tvalidation_0-auc:0.796537\n",
            "[13]\tvalidation_0-auc:0.797499\n",
            "[14]\tvalidation_0-auc:0.80346\n",
            "[15]\tvalidation_0-auc:0.807428\n",
            "[16]\tvalidation_0-auc:0.806707\n",
            "[17]\tvalidation_0-auc:0.813681\n",
            "[18]\tvalidation_0-auc:0.814231\n",
            "[19]\tvalidation_0-auc:0.814815\n",
            "[20]\tvalidation_0-auc:0.816395\n",
            "[21]\tvalidation_0-auc:0.814317\n",
            "[22]\tvalidation_0-auc:0.815416\n",
            "[23]\tvalidation_0-auc:0.816997\n",
            "[24]\tvalidation_0-auc:0.81545\n",
            "[25]\tvalidation_0-auc:0.815708\n",
            "[26]\tvalidation_0-auc:0.816086\n",
            "[27]\tvalidation_0-auc:0.816258\n",
            "[28]\tvalidation_0-auc:0.815296\n",
            "[29]\tvalidation_0-auc:0.814987\n",
            "[30]\tvalidation_0-auc:0.815502\n",
            "[31]\tvalidation_0-auc:0.814523\n",
            "[32]\tvalidation_0-auc:0.815571\n",
            "[33]\tvalidation_0-auc:0.817478\n",
            "[34]\tvalidation_0-auc:0.816808\n",
            "[35]\tvalidation_0-auc:0.815502\n",
            "[36]\tvalidation_0-auc:0.815485\n",
            "[37]\tvalidation_0-auc:0.813784\n",
            "[38]\tvalidation_0-auc:0.816034\n",
            "[39]\tvalidation_0-auc:0.817323\n",
            "[40]\tvalidation_0-auc:0.815983\n",
            "[41]\tvalidation_0-auc:0.816704\n",
            "[42]\tvalidation_0-auc:0.815931\n",
            "[43]\tvalidation_0-auc:0.81789\n",
            "[44]\tvalidation_0-auc:0.817374\n",
            "[45]\tvalidation_0-auc:0.817873\n",
            "[46]\tvalidation_0-auc:0.818938\n",
            "[47]\tvalidation_0-auc:0.820089\n",
            "[48]\tvalidation_0-auc:0.821291\n",
            "[49]\tvalidation_0-auc:0.82191\n",
            "[50]\tvalidation_0-auc:0.822253\n",
            "[51]\tvalidation_0-auc:0.820638\n",
            "[52]\tvalidation_0-auc:0.821016\n",
            "[53]\tvalidation_0-auc:0.82203\n",
            "[54]\tvalidation_0-auc:0.821205\n",
            "[55]\tvalidation_0-auc:0.821137\n",
            "[56]\tvalidation_0-auc:0.820879\n",
            "[57]\tvalidation_0-auc:0.820432\n",
            "[58]\tvalidation_0-auc:0.820123\n",
            "[59]\tvalidation_0-auc:0.818457\n",
            "[60]\tvalidation_0-auc:0.819281\n",
            "[61]\tvalidation_0-auc:0.819178\n",
            "[62]\tvalidation_0-auc:0.820071\n",
            "[63]\tvalidation_0-auc:0.820484\n",
            "[64]\tvalidation_0-auc:0.820587\n",
            "[65]\tvalidation_0-auc:0.820381\n",
            "[66]\tvalidation_0-auc:0.819058\n",
            "[67]\tvalidation_0-auc:0.818989\n",
            "[68]\tvalidation_0-auc:0.818199\n",
            "[69]\tvalidation_0-auc:0.818216\n",
            "[70]\tvalidation_0-auc:0.818491\n",
            "[71]\tvalidation_0-auc:0.817374\n",
            "[72]\tvalidation_0-auc:0.817958\n",
            "[73]\tvalidation_0-auc:0.816928\n",
            "[74]\tvalidation_0-auc:0.816722\n",
            "[75]\tvalidation_0-auc:0.816653\n",
            "[76]\tvalidation_0-auc:0.816223\n",
            "[77]\tvalidation_0-auc:0.817289\n",
            "[78]\tvalidation_0-auc:0.817014\n",
            "[79]\tvalidation_0-auc:0.816808\n",
            "[80]\tvalidation_0-auc:0.81722\n",
            "[81]\tvalidation_0-auc:0.816636\n",
            "[82]\tvalidation_0-auc:0.816258\n",
            "[83]\tvalidation_0-auc:0.815674\n",
            "[84]\tvalidation_0-auc:0.815502\n",
            "[85]\tvalidation_0-auc:0.815914\n",
            "[86]\tvalidation_0-auc:0.81588\n",
            "[87]\tvalidation_0-auc:0.815399\n",
            "[88]\tvalidation_0-auc:0.815536\n",
            "[89]\tvalidation_0-auc:0.815296\n",
            "[90]\tvalidation_0-auc:0.815468\n",
            "[91]\tvalidation_0-auc:0.815433\n",
            "[92]\tvalidation_0-auc:0.81399\n",
            "[93]\tvalidation_0-auc:0.813956\n",
            "[94]\tvalidation_0-auc:0.81399\n",
            "[95]\tvalidation_0-auc:0.813784\n",
            "[96]\tvalidation_0-auc:0.815227\n",
            "[97]\tvalidation_0-auc:0.814746\n",
            "[98]\tvalidation_0-auc:0.814334\n",
            "[99]\tvalidation_0-auc:0.815365\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.6487179487179487 |        0.0         |         0.0         |         0.0         |\n",
            "|     GRU 0.1      | 0.7230769230769231 | 0.8918918918918919 | 0.24087591240875914 | 0.37931034482758624 |\n",
            "|   XGBoost 0.1    | 0.764102564102564  | 0.8688524590163934 | 0.38686131386861317 |  0.5353535353535354 |\n",
            "|    Logreg 0.1    | 0.7153846153846154 | 0.8611111111111112 | 0.22627737226277372 |  0.3583815028901734 |\n",
            "|     SVM 0.1      | 0.7256410256410256 |       0.8125       |  0.2846715328467153 | 0.42162162162162165 |\n",
            "|  LSTM beta 0.1   | 0.6470588235294118 |        0.0         |         0.0         |         0.0         |\n",
            "|   GRU beta 0.1   | 0.773109243697479  | 0.8169014084507042 |  0.4603174603174603 |  0.5888324873096447 |\n",
            "| XGBoost beta 0.1 | 0.7703081232492998 | 0.7075471698113207 |  0.5952380952380952 |  0.646551724137931  |\n",
            "| logreg beta 0.1  | 0.7254901960784313 |        0.78        | 0.30952380952380953 |  0.4431818181818182 |\n",
            "|   svm beta 0.1   | 0.7450980392156863 | 0.6666666666666666 |  0.5555555555555556 |  0.606060606060606  |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 17ms/step - loss: 0.3895 - accuracy: 0.8908 - val_loss: 1.8053 - val_accuracy: 0.2308\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3505 - accuracy: 0.8933 - val_loss: 1.5961 - val_accuracy: 0.2308\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3597 - accuracy: 0.8933 - val_loss: 1.6346 - val_accuracy: 0.2308\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3459 - accuracy: 0.8933 - val_loss: 1.8764 - val_accuracy: 0.2308\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3414 - accuracy: 0.8933 - val_loss: 1.7166 - val_accuracy: 0.2308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 15ms/step - loss: 0.4012 - accuracy: 0.8857 - val_loss: 1.8550 - val_accuracy: 0.2308\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3618 - accuracy: 0.8933 - val_loss: 1.9007 - val_accuracy: 0.2308\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3406 - accuracy: 0.8933 - val_loss: 1.6994 - val_accuracy: 0.2308\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3124 - accuracy: 0.8958 - val_loss: 1.4173 - val_accuracy: 0.2590\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3034 - accuracy: 0.8958 - val_loss: 1.7068 - val_accuracy: 0.2641\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.614796\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.613796\n",
            "[2]\tvalidation_0-auc:0.613407\n",
            "[3]\tvalidation_0-auc:0.62937\n",
            "[4]\tvalidation_0-auc:0.684204\n",
            "[5]\tvalidation_0-auc:0.686296\n",
            "[6]\tvalidation_0-auc:0.702074\n",
            "[7]\tvalidation_0-auc:0.70063\n",
            "[8]\tvalidation_0-auc:0.700741\n",
            "[9]\tvalidation_0-auc:0.700444\n",
            "[10]\tvalidation_0-auc:0.700907\n",
            "[11]\tvalidation_0-auc:0.699352\n",
            "[12]\tvalidation_0-auc:0.710741\n",
            "[13]\tvalidation_0-auc:0.704648\n",
            "[14]\tvalidation_0-auc:0.705056\n",
            "[15]\tvalidation_0-auc:0.707056\n",
            "[16]\tvalidation_0-auc:0.709537\n",
            "[17]\tvalidation_0-auc:0.710722\n",
            "[18]\tvalidation_0-auc:0.704278\n",
            "[19]\tvalidation_0-auc:0.708407\n",
            "[20]\tvalidation_0-auc:0.710981\n",
            "[21]\tvalidation_0-auc:0.711926\n",
            "[22]\tvalidation_0-auc:0.715556\n",
            "[23]\tvalidation_0-auc:0.711796\n",
            "[24]\tvalidation_0-auc:0.715944\n",
            "[25]\tvalidation_0-auc:0.71863\n",
            "[26]\tvalidation_0-auc:0.715889\n",
            "[27]\tvalidation_0-auc:0.713333\n",
            "[28]\tvalidation_0-auc:0.711981\n",
            "[29]\tvalidation_0-auc:0.709815\n",
            "[30]\tvalidation_0-auc:0.708926\n",
            "[31]\tvalidation_0-auc:0.705778\n",
            "[32]\tvalidation_0-auc:0.705926\n",
            "[33]\tvalidation_0-auc:0.703593\n",
            "[34]\tvalidation_0-auc:0.700926\n",
            "[35]\tvalidation_0-auc:0.699907\n",
            "[36]\tvalidation_0-auc:0.695982\n",
            "[37]\tvalidation_0-auc:0.694574\n",
            "[38]\tvalidation_0-auc:0.690833\n",
            "[39]\tvalidation_0-auc:0.689241\n",
            "[40]\tvalidation_0-auc:0.688352\n",
            "[41]\tvalidation_0-auc:0.688463\n",
            "[42]\tvalidation_0-auc:0.687556\n",
            "[43]\tvalidation_0-auc:0.687074\n",
            "[44]\tvalidation_0-auc:0.688296\n",
            "[45]\tvalidation_0-auc:0.687\n",
            "[46]\tvalidation_0-auc:0.687426\n",
            "[47]\tvalidation_0-auc:0.686389\n",
            "[48]\tvalidation_0-auc:0.685167\n",
            "[49]\tvalidation_0-auc:0.683278\n",
            "[50]\tvalidation_0-auc:0.681574\n",
            "[51]\tvalidation_0-auc:0.681426\n",
            "[52]\tvalidation_0-auc:0.68163\n",
            "[53]\tvalidation_0-auc:0.682259\n",
            "[54]\tvalidation_0-auc:0.681889\n",
            "[55]\tvalidation_0-auc:0.68063\n",
            "[56]\tvalidation_0-auc:0.681593\n",
            "[57]\tvalidation_0-auc:0.681074\n",
            "[58]\tvalidation_0-auc:0.681074\n",
            "[59]\tvalidation_0-auc:0.680333\n",
            "[60]\tvalidation_0-auc:0.682222\n",
            "[61]\tvalidation_0-auc:0.680741\n",
            "[62]\tvalidation_0-auc:0.681593\n",
            "[63]\tvalidation_0-auc:0.681407\n",
            "[64]\tvalidation_0-auc:0.679259\n",
            "[65]\tvalidation_0-auc:0.679741\n",
            "[66]\tvalidation_0-auc:0.677889\n",
            "[67]\tvalidation_0-auc:0.679556\n",
            "[68]\tvalidation_0-auc:0.678926\n",
            "[69]\tvalidation_0-auc:0.679333\n",
            "[70]\tvalidation_0-auc:0.679482\n",
            "[71]\tvalidation_0-auc:0.678296\n",
            "[72]\tvalidation_0-auc:0.68\n",
            "[73]\tvalidation_0-auc:0.679407\n",
            "[74]\tvalidation_0-auc:0.677741\n",
            "[75]\tvalidation_0-auc:0.677481\n",
            "Stopping. Best iteration:\n",
            "[25]\tvalidation_0-auc:0.71863\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 3s 17ms/step - loss: 0.3945 - accuracy: 0.8859 - val_loss: 1.7395 - val_accuracy: 0.2521\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 12ms/step - loss: 0.3584 - accuracy: 0.8902 - val_loss: 1.5328 - val_accuracy: 0.2521\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 12ms/step - loss: 0.3732 - accuracy: 0.8902 - val_loss: 1.5412 - val_accuracy: 0.2521\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.3561 - accuracy: 0.8902 - val_loss: 1.6267 - val_accuracy: 0.2521\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.3543 - accuracy: 0.8902 - val_loss: 1.6588 - val_accuracy: 0.2521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 3s 16ms/step - loss: 0.3994 - accuracy: 0.8825 - val_loss: 1.8079 - val_accuracy: 0.2521\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.3263 - accuracy: 0.8937 - val_loss: 1.8832 - val_accuracy: 0.2521\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.2860 - accuracy: 0.9032 - val_loss: 1.7415 - val_accuracy: 0.2577\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.2777 - accuracy: 0.9118 - val_loss: 1.6003 - val_accuracy: 0.2829\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.2654 - accuracy: 0.9127 - val_loss: 1.4509 - val_accuracy: 0.3417\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.670454\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.664836\n",
            "[2]\tvalidation_0-auc:0.64124\n",
            "[3]\tvalidation_0-auc:0.633999\n",
            "[4]\tvalidation_0-auc:0.639638\n",
            "[5]\tvalidation_0-auc:0.634623\n",
            "[6]\tvalidation_0-auc:0.636871\n",
            "[7]\tvalidation_0-auc:0.636829\n",
            "[8]\tvalidation_0-auc:0.63633\n",
            "[9]\tvalidation_0-auc:0.63094\n",
            "[10]\tvalidation_0-auc:0.635976\n",
            "[11]\tvalidation_0-auc:0.631273\n",
            "[12]\tvalidation_0-auc:0.637828\n",
            "[13]\tvalidation_0-auc:0.636704\n",
            "[14]\tvalidation_0-auc:0.637474\n",
            "[15]\tvalidation_0-auc:0.638951\n",
            "[16]\tvalidation_0-auc:0.639243\n",
            "[17]\tvalidation_0-auc:0.651352\n",
            "[18]\tvalidation_0-auc:0.65901\n",
            "[19]\tvalidation_0-auc:0.666146\n",
            "[20]\tvalidation_0-auc:0.674241\n",
            "[21]\tvalidation_0-auc:0.680275\n",
            "[22]\tvalidation_0-auc:0.686517\n",
            "[23]\tvalidation_0-auc:0.69149\n",
            "[24]\tvalidation_0-auc:0.687765\n",
            "[25]\tvalidation_0-auc:0.696005\n",
            "[26]\tvalidation_0-auc:0.691677\n",
            "[27]\tvalidation_0-auc:0.687682\n",
            "[28]\tvalidation_0-auc:0.694008\n",
            "[29]\tvalidation_0-auc:0.689305\n",
            "[30]\tvalidation_0-auc:0.694965\n",
            "[31]\tvalidation_0-auc:0.691968\n",
            "[32]\tvalidation_0-auc:0.695839\n",
            "[33]\tvalidation_0-auc:0.697628\n",
            "[34]\tvalidation_0-auc:0.693154\n",
            "[35]\tvalidation_0-auc:0.6964\n",
            "[36]\tvalidation_0-auc:0.701456\n",
            "[37]\tvalidation_0-auc:0.702809\n",
            "[38]\tvalidation_0-auc:0.698564\n",
            "[39]\tvalidation_0-auc:0.702226\n",
            "[40]\tvalidation_0-auc:0.708489\n",
            "[41]\tvalidation_0-auc:0.713005\n",
            "[42]\tvalidation_0-auc:0.713587\n",
            "[43]\tvalidation_0-auc:0.710633\n",
            "[44]\tvalidation_0-auc:0.711715\n",
            "[45]\tvalidation_0-auc:0.707241\n",
            "[46]\tvalidation_0-auc:0.710737\n",
            "[47]\tvalidation_0-auc:0.708864\n",
            "[48]\tvalidation_0-auc:0.702518\n",
            "[49]\tvalidation_0-auc:0.701685\n",
            "[50]\tvalidation_0-auc:0.700895\n",
            "[51]\tvalidation_0-auc:0.701977\n",
            "[52]\tvalidation_0-auc:0.705327\n",
            "[53]\tvalidation_0-auc:0.703288\n",
            "[54]\tvalidation_0-auc:0.705868\n",
            "[55]\tvalidation_0-auc:0.706076\n",
            "[56]\tvalidation_0-auc:0.703953\n",
            "[57]\tvalidation_0-auc:0.700749\n",
            "[58]\tvalidation_0-auc:0.70516\n",
            "[59]\tvalidation_0-auc:0.705077\n",
            "[60]\tvalidation_0-auc:0.707241\n",
            "[61]\tvalidation_0-auc:0.708739\n",
            "[62]\tvalidation_0-auc:0.709571\n",
            "[63]\tvalidation_0-auc:0.709238\n",
            "[64]\tvalidation_0-auc:0.707407\n",
            "[65]\tvalidation_0-auc:0.704869\n",
            "[66]\tvalidation_0-auc:0.703912\n",
            "[67]\tvalidation_0-auc:0.704203\n",
            "[68]\tvalidation_0-auc:0.705368\n",
            "[69]\tvalidation_0-auc:0.704661\n",
            "[70]\tvalidation_0-auc:0.699709\n",
            "[71]\tvalidation_0-auc:0.701956\n",
            "[72]\tvalidation_0-auc:0.701956\n",
            "[73]\tvalidation_0-auc:0.700208\n",
            "[74]\tvalidation_0-auc:0.700083\n",
            "[75]\tvalidation_0-auc:0.703163\n",
            "[76]\tvalidation_0-auc:0.708406\n",
            "[77]\tvalidation_0-auc:0.704494\n",
            "[78]\tvalidation_0-auc:0.70258\n",
            "[79]\tvalidation_0-auc:0.706409\n",
            "[80]\tvalidation_0-auc:0.710071\n",
            "[81]\tvalidation_0-auc:0.708281\n",
            "[82]\tvalidation_0-auc:0.707158\n",
            "[83]\tvalidation_0-auc:0.706617\n",
            "[84]\tvalidation_0-auc:0.705452\n",
            "[85]\tvalidation_0-auc:0.705035\n",
            "[86]\tvalidation_0-auc:0.701914\n",
            "[87]\tvalidation_0-auc:0.700208\n",
            "[88]\tvalidation_0-auc:0.699459\n",
            "[89]\tvalidation_0-auc:0.702871\n",
            "[90]\tvalidation_0-auc:0.701706\n",
            "[91]\tvalidation_0-auc:0.701165\n",
            "[92]\tvalidation_0-auc:0.701873\n",
            "Stopping. Best iteration:\n",
            "[42]\tvalidation_0-auc:0.713587\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+---------------------+--------------------+----------------------+---------------------+\n",
            "|      Model       |       Accuracy      |     Precision      |        Recall        |       F1 score      |\n",
            "+------------------+---------------------+--------------------+----------------------+---------------------+\n",
            "|     LSTM 0.2     | 0.23076923076923078 |        0.0         |         0.0          |         0.0         |\n",
            "|     GRU 0.2      |  0.2641025641025641 |        1.0         | 0.043333333333333335 | 0.08306709265175719 |\n",
            "|   XGBoost 0.2    |  0.2923076923076923 | 0.9615384615384616 | 0.08333333333333333  | 0.15337423312883433 |\n",
            "|    Logreg 0.2    | 0.24102564102564103 |        1.0         | 0.013333333333333334 | 0.02631578947368421 |\n",
            "|     SVM 0.2      |  0.258974358974359  |        1.0         | 0.03666666666666667  |  0.0707395498392283 |\n",
            "|  LSTM beta 0.2   | 0.25210084033613445 |        0.0         |         0.0          |         0.0         |\n",
            "|   GRU beta 0.2   | 0.34173669467787116 |        0.9         |  0.1348314606741573  | 0.23452768729641693 |\n",
            "| XGBoost beta 0.2 | 0.29971988795518206 | 0.8695652173913043 |  0.0749063670411985  | 0.13793103448275865 |\n",
            "| logreg beta 0.2  |  0.2689075630252101 |        1.0         | 0.02247191011235955  | 0.04395604395604395 |\n",
            "|   svm beta 0.2   |  0.2689075630252101 |        1.0         | 0.02247191011235955  | 0.04395604395604395 |\n",
            "+------------------+---------------------+--------------------+----------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 18ms/step - loss: 0.5061 - accuracy: 0.8118 - val_loss: 0.7827 - val_accuracy: 0.6128\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4922 - accuracy: 0.8151 - val_loss: 0.7886 - val_accuracy: 0.6128\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4693 - accuracy: 0.8193 - val_loss: 0.8042 - val_accuracy: 0.6128\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4597 - accuracy: 0.8176 - val_loss: 0.6197 - val_accuracy: 0.6667\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4168 - accuracy: 0.8294 - val_loss: 0.6199 - val_accuracy: 0.7103\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 16ms/step - loss: 0.5132 - accuracy: 0.8151 - val_loss: 0.7858 - val_accuracy: 0.6128\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4795 - accuracy: 0.8151 - val_loss: 0.7423 - val_accuracy: 0.6128\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4349 - accuracy: 0.8235 - val_loss: 0.5956 - val_accuracy: 0.6692\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4205 - accuracy: 0.8395 - val_loss: 0.5755 - val_accuracy: 0.7282\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3951 - accuracy: 0.8538 - val_loss: 0.5766 - val_accuracy: 0.7359\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.75045\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.749799\n",
            "[2]\tvalidation_0-auc:0.760633\n",
            "[3]\tvalidation_0-auc:0.77144\n",
            "[4]\tvalidation_0-auc:0.767491\n",
            "[5]\tvalidation_0-auc:0.775596\n",
            "[6]\tvalidation_0-auc:0.777356\n",
            "[7]\tvalidation_0-auc:0.772271\n",
            "[8]\tvalidation_0-auc:0.77931\n",
            "[9]\tvalidation_0-auc:0.783452\n",
            "[10]\tvalidation_0-auc:0.783036\n",
            "[11]\tvalidation_0-auc:0.780944\n",
            "[12]\tvalidation_0-auc:0.780016\n",
            "[13]\tvalidation_0-auc:0.778561\n",
            "[14]\tvalidation_0-auc:0.773352\n",
            "[15]\tvalidation_0-auc:0.771094\n",
            "[16]\tvalidation_0-auc:0.772396\n",
            "[17]\tvalidation_0-auc:0.767505\n",
            "[18]\tvalidation_0-auc:0.767201\n",
            "[19]\tvalidation_0-auc:0.764901\n",
            "[20]\tvalidation_0-auc:0.767187\n",
            "[21]\tvalidation_0-auc:0.764277\n",
            "[22]\tvalidation_0-auc:0.762684\n",
            "[23]\tvalidation_0-auc:0.764277\n",
            "[24]\tvalidation_0-auc:0.76267\n",
            "[25]\tvalidation_0-auc:0.758486\n",
            "[26]\tvalidation_0-auc:0.757461\n",
            "[27]\tvalidation_0-auc:0.757211\n",
            "[28]\tvalidation_0-auc:0.755812\n",
            "[29]\tvalidation_0-auc:0.755563\n",
            "[30]\tvalidation_0-auc:0.755105\n",
            "[31]\tvalidation_0-auc:0.753789\n",
            "[32]\tvalidation_0-auc:0.751046\n",
            "[33]\tvalidation_0-auc:0.749938\n",
            "[34]\tvalidation_0-auc:0.748358\n",
            "[35]\tvalidation_0-auc:0.746585\n",
            "[36]\tvalidation_0-auc:0.745546\n",
            "[37]\tvalidation_0-auc:0.744077\n",
            "[38]\tvalidation_0-auc:0.743897\n",
            "[39]\tvalidation_0-auc:0.745172\n",
            "[40]\tvalidation_0-auc:0.743357\n",
            "[41]\tvalidation_0-auc:0.743855\n",
            "[42]\tvalidation_0-auc:0.744784\n",
            "[43]\tvalidation_0-auc:0.745532\n",
            "[44]\tvalidation_0-auc:0.745338\n",
            "[45]\tvalidation_0-auc:0.742955\n",
            "[46]\tvalidation_0-auc:0.742359\n",
            "[47]\tvalidation_0-auc:0.741666\n",
            "[48]\tvalidation_0-auc:0.741057\n",
            "[49]\tvalidation_0-auc:0.739921\n",
            "[50]\tvalidation_0-auc:0.739616\n",
            "[51]\tvalidation_0-auc:0.7383\n",
            "[52]\tvalidation_0-auc:0.738688\n",
            "[53]\tvalidation_0-auc:0.738133\n",
            "[54]\tvalidation_0-auc:0.736443\n",
            "[55]\tvalidation_0-auc:0.737579\n",
            "[56]\tvalidation_0-auc:0.737164\n",
            "[57]\tvalidation_0-auc:0.737219\n",
            "[58]\tvalidation_0-auc:0.738078\n",
            "[59]\tvalidation_0-auc:0.738535\n",
            "Stopping. Best iteration:\n",
            "[9]\tvalidation_0-auc:0.783452\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 3s 17ms/step - loss: 0.5201 - accuracy: 0.8064 - val_loss: 0.7033 - val_accuracy: 0.6078\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.5024 - accuracy: 0.8099 - val_loss: 0.7287 - val_accuracy: 0.6078\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4957 - accuracy: 0.8099 - val_loss: 0.7615 - val_accuracy: 0.6078\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4638 - accuracy: 0.8099 - val_loss: 0.7089 - val_accuracy: 0.6078\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4343 - accuracy: 0.8099 - val_loss: 0.6486 - val_accuracy: 0.6078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 3s 16ms/step - loss: 0.5251 - accuracy: 0.8107 - val_loss: 0.7633 - val_accuracy: 0.6078\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4553 - accuracy: 0.8133 - val_loss: 0.6700 - val_accuracy: 0.6218\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.4092 - accuracy: 0.8211 - val_loss: 0.5350 - val_accuracy: 0.7199\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.3807 - accuracy: 0.8418 - val_loss: 0.6008 - val_accuracy: 0.7283\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.3832 - accuracy: 0.8418 - val_loss: 0.5198 - val_accuracy: 0.7283\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.759414\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.772992\n",
            "[2]\tvalidation_0-auc:0.769108\n",
            "[3]\tvalidation_0-auc:0.773173\n",
            "[4]\tvalidation_0-auc:0.779526\n",
            "[5]\tvalidation_0-auc:0.789121\n",
            "[6]\tvalidation_0-auc:0.790076\n",
            "[7]\tvalidation_0-auc:0.788891\n",
            "[8]\tvalidation_0-auc:0.791656\n",
            "[9]\tvalidation_0-auc:0.792594\n",
            "[10]\tvalidation_0-auc:0.793845\n",
            "[11]\tvalidation_0-auc:0.791754\n",
            "[12]\tvalidation_0-auc:0.794289\n",
            "[13]\tvalidation_0-auc:0.796215\n",
            "[14]\tvalidation_0-auc:0.800592\n",
            "[15]\tvalidation_0-auc:0.80102\n",
            "[16]\tvalidation_0-auc:0.795935\n",
            "[17]\tvalidation_0-auc:0.794124\n",
            "[18]\tvalidation_0-auc:0.794207\n",
            "[19]\tvalidation_0-auc:0.794009\n",
            "[20]\tvalidation_0-auc:0.796725\n",
            "[21]\tvalidation_0-auc:0.798535\n",
            "[22]\tvalidation_0-auc:0.79972\n",
            "[23]\tvalidation_0-auc:0.800872\n",
            "[24]\tvalidation_0-auc:0.800625\n",
            "[25]\tvalidation_0-auc:0.801086\n",
            "[26]\tvalidation_0-auc:0.801662\n",
            "[27]\tvalidation_0-auc:0.800675\n",
            "[28]\tvalidation_0-auc:0.8013\n",
            "[29]\tvalidation_0-auc:0.803242\n",
            "[30]\tvalidation_0-auc:0.800642\n",
            "[31]\tvalidation_0-auc:0.80209\n",
            "[32]\tvalidation_0-auc:0.802123\n",
            "[33]\tvalidation_0-auc:0.801827\n",
            "[34]\tvalidation_0-auc:0.803176\n",
            "[35]\tvalidation_0-auc:0.803111\n",
            "[36]\tvalidation_0-auc:0.804427\n",
            "[37]\tvalidation_0-auc:0.803127\n",
            "[38]\tvalidation_0-auc:0.80293\n",
            "[39]\tvalidation_0-auc:0.802419\n",
            "[40]\tvalidation_0-auc:0.802172\n",
            "[41]\tvalidation_0-auc:0.801745\n",
            "[42]\tvalidation_0-auc:0.801909\n",
            "[43]\tvalidation_0-auc:0.801843\n",
            "[44]\tvalidation_0-auc:0.801777\n",
            "[45]\tvalidation_0-auc:0.802337\n",
            "[46]\tvalidation_0-auc:0.803522\n",
            "[47]\tvalidation_0-auc:0.804378\n",
            "[48]\tvalidation_0-auc:0.805283\n",
            "[49]\tvalidation_0-auc:0.801926\n",
            "[50]\tvalidation_0-auc:0.802814\n",
            "[51]\tvalidation_0-auc:0.802386\n",
            "[52]\tvalidation_0-auc:0.801531\n",
            "[53]\tvalidation_0-auc:0.79944\n",
            "[54]\tvalidation_0-auc:0.799078\n",
            "[55]\tvalidation_0-auc:0.799243\n",
            "[56]\tvalidation_0-auc:0.79865\n",
            "[57]\tvalidation_0-auc:0.798453\n",
            "[58]\tvalidation_0-auc:0.79865\n",
            "[59]\tvalidation_0-auc:0.798288\n",
            "[60]\tvalidation_0-auc:0.798058\n",
            "[61]\tvalidation_0-auc:0.797893\n",
            "[62]\tvalidation_0-auc:0.797696\n",
            "[63]\tvalidation_0-auc:0.797795\n",
            "[64]\tvalidation_0-auc:0.797005\n",
            "[65]\tvalidation_0-auc:0.796643\n",
            "[66]\tvalidation_0-auc:0.796215\n",
            "[67]\tvalidation_0-auc:0.795359\n",
            "[68]\tvalidation_0-auc:0.796017\n",
            "[69]\tvalidation_0-auc:0.796313\n",
            "[70]\tvalidation_0-auc:0.794964\n",
            "[71]\tvalidation_0-auc:0.795145\n",
            "[72]\tvalidation_0-auc:0.795704\n",
            "[73]\tvalidation_0-auc:0.795112\n",
            "[74]\tvalidation_0-auc:0.795013\n",
            "[75]\tvalidation_0-auc:0.794421\n",
            "[76]\tvalidation_0-auc:0.793828\n",
            "[77]\tvalidation_0-auc:0.794618\n",
            "[78]\tvalidation_0-auc:0.795079\n",
            "[79]\tvalidation_0-auc:0.794519\n",
            "[80]\tvalidation_0-auc:0.794059\n",
            "[81]\tvalidation_0-auc:0.793861\n",
            "[82]\tvalidation_0-auc:0.792972\n",
            "[83]\tvalidation_0-auc:0.791936\n",
            "[84]\tvalidation_0-auc:0.792001\n",
            "[85]\tvalidation_0-auc:0.790191\n",
            "[86]\tvalidation_0-auc:0.791277\n",
            "[87]\tvalidation_0-auc:0.791146\n",
            "[88]\tvalidation_0-auc:0.790092\n",
            "[89]\tvalidation_0-auc:0.79131\n",
            "[90]\tvalidation_0-auc:0.79108\n",
            "[91]\tvalidation_0-auc:0.791804\n",
            "[92]\tvalidation_0-auc:0.790783\n",
            "[93]\tvalidation_0-auc:0.790685\n",
            "[94]\tvalidation_0-auc:0.79029\n",
            "[95]\tvalidation_0-auc:0.789533\n",
            "[96]\tvalidation_0-auc:0.78894\n",
            "[97]\tvalidation_0-auc:0.787031\n",
            "[98]\tvalidation_0-auc:0.786669\n",
            "Stopping. Best iteration:\n",
            "[48]\tvalidation_0-auc:0.805283\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.7102564102564103 | 0.7794117647058824 |  0.3509933774834437 |  0.4840182648401827 |\n",
            "|      GRU 0.15     | 0.735897435897436  | 0.8157894736842105 |  0.4105960264900662 |  0.5462555066079295 |\n",
            "|    XGBoost 0.15   | 0.6974358974358974 | 0.8235294117647058 |  0.2781456953642384 |  0.4158415841584159 |\n",
            "|    Logreg 0.15    | 0.6487179487179487 | 0.8181818181818182 | 0.11920529801324503 | 0.20809248554913296 |\n",
            "|      SVM 0.15     | 0.6743589743589744 | 0.8157894736842105 |  0.2052980132450331 |  0.328042328042328  |\n",
            "|   LSTM beta 0.15  | 0.6078431372549019 |        0.0         |         0.0         |         0.0         |\n",
            "|   GRU beta 0.15   | 0.7282913165266106 | 0.6776859504132231 |  0.5857142857142857 |  0.6283524904214559 |\n",
            "| XGBoost beta 0.15 | 0.7170868347338936 | 0.696969696969697  |  0.4928571428571429 |  0.5774058577405858 |\n",
            "|  logreg beta 0.15 | 0.6862745098039216 | 0.868421052631579  |  0.2357142857142857 |  0.3707865168539326 |\n",
            "|   svm beta 0.15   | 0.7226890756302521 | 0.7204301075268817 |  0.4785714285714286 |  0.5751072961373391 |\n",
            "+-------------------+--------------------+--------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSy3uKygYLzd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "91f6a0af-d153-4936-808a-9f519af2121a"
      },
      "source": [
        "Result_purging.to_csv('RHT_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.638889</td>\n",
              "      <td>0.685279</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.319444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.755556</td>\n",
              "      <td>0.751269</td>\n",
              "      <td>0.581197</td>\n",
              "      <td>0.472222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.708333</td>\n",
              "      <td>0.736041</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.472222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.779412</td>\n",
              "      <td>0.730964</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.368056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.641975</td>\n",
              "      <td>0.692893</td>\n",
              "      <td>0.462222</td>\n",
              "      <td>0.361111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.620499</td>\n",
              "      <td>0.104575</td>\n",
              "      <td>0.055556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.708738</td>\n",
              "      <td>0.720222</td>\n",
              "      <td>0.591093</td>\n",
              "      <td>0.506944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.768421</td>\n",
              "      <td>0.742382</td>\n",
              "      <td>0.610879</td>\n",
              "      <td>0.506944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.795181</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.581498</td>\n",
              "      <td>0.458333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.835052</td>\n",
              "      <td>0.781163</td>\n",
              "      <td>0.672199</td>\n",
              "      <td>0.562500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.497462</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.956522</td>\n",
              "      <td>0.604061</td>\n",
              "      <td>0.360656</td>\n",
              "      <td>0.222222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.855263</td>\n",
              "      <td>0.634518</td>\n",
              "      <td>0.474453</td>\n",
              "      <td>0.328283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.588832</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.196970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.655556</td>\n",
              "      <td>0.568528</td>\n",
              "      <td>0.409722</td>\n",
              "      <td>0.297980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.484765</td>\n",
              "      <td>0.114286</td>\n",
              "      <td>0.060606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.686981</td>\n",
              "      <td>0.638978</td>\n",
              "      <td>0.505051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.904110</td>\n",
              "      <td>0.614958</td>\n",
              "      <td>0.487085</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.824324</td>\n",
              "      <td>0.584488</td>\n",
              "      <td>0.448529</td>\n",
              "      <td>0.308081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.876923</td>\n",
              "      <td>0.587258</td>\n",
              "      <td>0.433460</td>\n",
              "      <td>0.287879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.796296</td>\n",
              "      <td>0.718274</td>\n",
              "      <td>0.607774</td>\n",
              "      <td>0.491429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.853659</td>\n",
              "      <td>0.703046</td>\n",
              "      <td>0.544747</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.867470</td>\n",
              "      <td>0.710660</td>\n",
              "      <td>0.558140</td>\n",
              "      <td>0.411429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.953846</td>\n",
              "      <td>0.705584</td>\n",
              "      <td>0.516667</td>\n",
              "      <td>0.354286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.861538</td>\n",
              "      <td>0.675127</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.320000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.858268</td>\n",
              "      <td>0.767313</td>\n",
              "      <td>0.721854</td>\n",
              "      <td>0.622857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.935484</td>\n",
              "      <td>0.739612</td>\n",
              "      <td>0.649254</td>\n",
              "      <td>0.497143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.922330</td>\n",
              "      <td>0.756233</td>\n",
              "      <td>0.683453</td>\n",
              "      <td>0.542857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.988372</td>\n",
              "      <td>0.747922</td>\n",
              "      <td>0.651341</td>\n",
              "      <td>0.485714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.942529</td>\n",
              "      <td>0.728532</td>\n",
              "      <td>0.625954</td>\n",
              "      <td>0.468571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.648718</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.723077</td>\n",
              "      <td>0.379310</td>\n",
              "      <td>0.240876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.868852</td>\n",
              "      <td>0.764103</td>\n",
              "      <td>0.535354</td>\n",
              "      <td>0.386861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.861111</td>\n",
              "      <td>0.715385</td>\n",
              "      <td>0.358382</td>\n",
              "      <td>0.226277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.725641</td>\n",
              "      <td>0.421622</td>\n",
              "      <td>0.284672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.816901</td>\n",
              "      <td>0.773109</td>\n",
              "      <td>0.588832</td>\n",
              "      <td>0.460317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.707547</td>\n",
              "      <td>0.770308</td>\n",
              "      <td>0.646552</td>\n",
              "      <td>0.595238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.780000</td>\n",
              "      <td>0.725490</td>\n",
              "      <td>0.443182</td>\n",
              "      <td>0.309524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.745098</td>\n",
              "      <td>0.606061</td>\n",
              "      <td>0.555556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.264103</td>\n",
              "      <td>0.083067</td>\n",
              "      <td>0.043333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.961538</td>\n",
              "      <td>0.292308</td>\n",
              "      <td>0.153374</td>\n",
              "      <td>0.083333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.241026</td>\n",
              "      <td>0.026316</td>\n",
              "      <td>0.013333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.258974</td>\n",
              "      <td>0.070740</td>\n",
              "      <td>0.036667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.252101</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.341737</td>\n",
              "      <td>0.234528</td>\n",
              "      <td>0.134831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.299720</td>\n",
              "      <td>0.137931</td>\n",
              "      <td>0.074906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.268908</td>\n",
              "      <td>0.043956</td>\n",
              "      <td>0.022472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.268908</td>\n",
              "      <td>0.043956</td>\n",
              "      <td>0.022472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.779412</td>\n",
              "      <td>0.710256</td>\n",
              "      <td>0.484018</td>\n",
              "      <td>0.350993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.815789</td>\n",
              "      <td>0.735897</td>\n",
              "      <td>0.546256</td>\n",
              "      <td>0.410596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.697436</td>\n",
              "      <td>0.415842</td>\n",
              "      <td>0.278146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.648718</td>\n",
              "      <td>0.208092</td>\n",
              "      <td>0.119205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.815789</td>\n",
              "      <td>0.674359</td>\n",
              "      <td>0.328042</td>\n",
              "      <td>0.205298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.607843</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.677686</td>\n",
              "      <td>0.728291</td>\n",
              "      <td>0.628352</td>\n",
              "      <td>0.585714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.696970</td>\n",
              "      <td>0.717087</td>\n",
              "      <td>0.577406</td>\n",
              "      <td>0.492857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.868421</td>\n",
              "      <td>0.686275</td>\n",
              "      <td>0.370787</td>\n",
              "      <td>0.235714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.720430</td>\n",
              "      <td>0.722689</td>\n",
              "      <td>0.575107</td>\n",
              "      <td>0.478571</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  RHT  0.638889  0.685279  0.425926  0.319444\n",
              "1            GRU 0.1  RHT  0.755556  0.751269  0.581197  0.472222\n",
              "2        XGBoost 0.1  RHT  0.708333  0.736041  0.566667  0.472222\n",
              "3         Logreg 0.1  RHT  0.779412  0.730964  0.500000  0.368056\n",
              "4            SVM 0.1  RHT  0.641975  0.692893  0.462222  0.361111\n",
              "5      LSTM beta 0.1  RHT  0.888889  0.620499  0.104575  0.055556\n",
              "6       GRU beta 0.1  RHT  0.708738  0.720222  0.591093  0.506944\n",
              "7   XGBoost beta 0.1  RHT  0.768421  0.742382  0.610879  0.506944\n",
              "8    logreg beta 0.1  RHT  0.795181  0.736842  0.581498  0.458333\n",
              "9       svm beta 0.1  RHT  0.835052  0.781163  0.672199  0.562500\n",
              "0           LSTM 0.2  RHT  0.000000  0.497462  0.000000  0.000000\n",
              "1            GRU 0.2  RHT  0.956522  0.604061  0.360656  0.222222\n",
              "2        XGBoost 0.2  RHT  0.855263  0.634518  0.474453  0.328283\n",
              "3         Logreg 0.2  RHT  0.928571  0.588832  0.325000  0.196970\n",
              "4            SVM 0.2  RHT  0.655556  0.568528  0.409722  0.297980\n",
              "5      LSTM beta 0.2  RHT  1.000000  0.484765  0.114286  0.060606\n",
              "6       GRU beta 0.2  RHT  0.869565  0.686981  0.638978  0.505051\n",
              "7   XGBoost beta 0.2  RHT  0.904110  0.614958  0.487085  0.333333\n",
              "8    logreg beta 0.2  RHT  0.824324  0.584488  0.448529  0.308081\n",
              "9       svm beta 0.2  RHT  0.876923  0.587258  0.433460  0.287879\n",
              "0          LSTM 0.15  RHT  0.796296  0.718274  0.607774  0.491429\n",
              "1           GRU 0.15  RHT  0.853659  0.703046  0.544747  0.400000\n",
              "2       XGBoost 0.15  RHT  0.867470  0.710660  0.558140  0.411429\n",
              "3        Logreg 0.15  RHT  0.953846  0.705584  0.516667  0.354286\n",
              "4           SVM 0.15  RHT  0.861538  0.675127  0.466667  0.320000\n",
              "5     LSTM beta 0.15  RHT  0.858268  0.767313  0.721854  0.622857\n",
              "6      GRU beta 0.15  RHT  0.935484  0.739612  0.649254  0.497143\n",
              "7  XGBoost beta 0.15  RHT  0.922330  0.756233  0.683453  0.542857\n",
              "8   logreg beta 0.15  RHT  0.988372  0.747922  0.651341  0.485714\n",
              "9      svm beta 0.15  RHT  0.942529  0.728532  0.625954  0.468571\n",
              "0           LSTM 0.1  RHT  0.000000  0.648718  0.000000  0.000000\n",
              "1            GRU 0.1  RHT  0.891892  0.723077  0.379310  0.240876\n",
              "2        XGBoost 0.1  RHT  0.868852  0.764103  0.535354  0.386861\n",
              "3         Logreg 0.1  RHT  0.861111  0.715385  0.358382  0.226277\n",
              "4            SVM 0.1  RHT  0.812500  0.725641  0.421622  0.284672\n",
              "5      LSTM beta 0.1  RHT  0.000000  0.647059  0.000000  0.000000\n",
              "6       GRU beta 0.1  RHT  0.816901  0.773109  0.588832  0.460317\n",
              "7   XGBoost beta 0.1  RHT  0.707547  0.770308  0.646552  0.595238\n",
              "8    logreg beta 0.1  RHT  0.780000  0.725490  0.443182  0.309524\n",
              "9       svm beta 0.1  RHT  0.666667  0.745098  0.606061  0.555556\n",
              "0           LSTM 0.2  RHT  0.000000  0.230769  0.000000  0.000000\n",
              "1            GRU 0.2  RHT  1.000000  0.264103  0.083067  0.043333\n",
              "2        XGBoost 0.2  RHT  0.961538  0.292308  0.153374  0.083333\n",
              "3         Logreg 0.2  RHT  1.000000  0.241026  0.026316  0.013333\n",
              "4            SVM 0.2  RHT  1.000000  0.258974  0.070740  0.036667\n",
              "5      LSTM beta 0.2  RHT  0.000000  0.252101  0.000000  0.000000\n",
              "6       GRU beta 0.2  RHT  0.900000  0.341737  0.234528  0.134831\n",
              "7   XGBoost beta 0.2  RHT  0.869565  0.299720  0.137931  0.074906\n",
              "8    logreg beta 0.2  RHT  1.000000  0.268908  0.043956  0.022472\n",
              "9       svm beta 0.2  RHT  1.000000  0.268908  0.043956  0.022472\n",
              "0          LSTM 0.15  RHT  0.779412  0.710256  0.484018  0.350993\n",
              "1           GRU 0.15  RHT  0.815789  0.735897  0.546256  0.410596\n",
              "2       XGBoost 0.15  RHT  0.823529  0.697436  0.415842  0.278146\n",
              "3        Logreg 0.15  RHT  0.818182  0.648718  0.208092  0.119205\n",
              "4           SVM 0.15  RHT  0.815789  0.674359  0.328042  0.205298\n",
              "5     LSTM beta 0.15  RHT  0.000000  0.607843  0.000000  0.000000\n",
              "6      GRU beta 0.15  RHT  0.677686  0.728291  0.628352  0.585714\n",
              "7  XGBoost beta 0.15  RHT  0.696970  0.717087  0.577406  0.492857\n",
              "8   logreg beta 0.15  RHT  0.868421  0.686275  0.370787  0.235714\n",
              "9      svm beta 0.15  RHT  0.720430  0.722689  0.575107  0.478571"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlOBh5ljYLze"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1510:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1510:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1510:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1510:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1510:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1543:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1543:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1543:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1543:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1543:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIanpF7WYLze"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY_JW9t6Y6WY"
      },
      "source": [
        "## STX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WEFTQ7GY6We",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c1efc3-bb86-4656-dadc-8df00cddbc6a"
      },
      "source": [
        "dfs = pd.read_csv(\"STX.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "# Denoise prices\n",
        "dfs = denoise_data(dfs, '<CLOSE>')\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2767</td>\n",
              "      <td>US1.STX</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>83.16</td>\n",
              "      <td>85.00</td>\n",
              "      <td>81.38</td>\n",
              "      <td>84.41</td>\n",
              "      <td>43575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.STX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>83.38</td>\n",
              "      <td>83.69</td>\n",
              "      <td>82.50</td>\n",
              "      <td>82.56</td>\n",
              "      <td>40131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.STX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>83.86</td>\n",
              "      <td>84.45</td>\n",
              "      <td>82.50</td>\n",
              "      <td>83.05</td>\n",
              "      <td>36683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.STX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>86.20</td>\n",
              "      <td>87.00</td>\n",
              "      <td>83.96</td>\n",
              "      <td>84.17</td>\n",
              "      <td>65032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2763</td>\n",
              "      <td>US1.STX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>86.01</td>\n",
              "      <td>88.17</td>\n",
              "      <td>86.01</td>\n",
              "      <td>86.89</td>\n",
              "      <td>36916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2763</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.STX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>12.23</td>\n",
              "      <td>12.41</td>\n",
              "      <td>11.92</td>\n",
              "      <td>12.20</td>\n",
              "      <td>19259065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.STX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>11.66</td>\n",
              "      <td>11.93</td>\n",
              "      <td>11.38</td>\n",
              "      <td>11.88</td>\n",
              "      <td>15252399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.STX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>11.78</td>\n",
              "      <td>11.92</td>\n",
              "      <td>11.42</td>\n",
              "      <td>11.49</td>\n",
              "      <td>20353875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.STX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>11.68</td>\n",
              "      <td>11.95</td>\n",
              "      <td>11.29</td>\n",
              "      <td>11.94</td>\n",
              "      <td>18677214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.STX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>11.63</td>\n",
              "      <td>11.67</td>\n",
              "      <td>11.30</td>\n",
              "      <td>11.48</td>\n",
              "      <td>10203081</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2768 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...  <HIGH>  <LOW>  <CLOSE>     <VOL>\n",
              "0      2767  US1.STX     D  20211001  ...   85.00  81.38    84.41     43575\n",
              "1      2766  US1.STX     D  20210930  ...   83.69  82.50    82.56     40131\n",
              "2      2765  US1.STX     D  20210929  ...   84.45  82.50    83.05     36683\n",
              "3      2764  US1.STX     D  20210928  ...   87.00  83.96    84.17     65032\n",
              "4      2763  US1.STX     D  20210927  ...   88.17  86.01    86.89     36916\n",
              "...     ...      ...   ...       ...  ...     ...    ...      ...       ...\n",
              "2763      4  US1.STX     D  20101008  ...   12.41  11.92    12.20  19259065\n",
              "2764      3  US1.STX     D  20101007  ...   11.93  11.38    11.88  15252399\n",
              "2765      2  US1.STX     D  20101006  ...   11.92  11.42    11.49  20353875\n",
              "2766      1  US1.STX     D  20101005  ...   11.95  11.29    11.94  18677214\n",
              "2767      0  US1.STX     D  20101004  ...   11.67  11.30    11.48  10203081\n",
              "\n",
              "[2768 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ef-MU4CY6We",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1677dd2c-7325-4d9a-c79a-c53c8ec5c814"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"b341c840-c80b-4586-bc3d-0cf17dbcf845\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"b341c840-c80b-4586-bc3d-0cf17dbcf845\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'b341c840-c80b-4586-bc3d-0cf17dbcf845',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [53.62, 53.67, 53.11, 52.24, 52.86, 53.78, 53.81, 53.68, 53.08, 51.85, 52.75, 52.39, 52.94, 56.76, 56.04, 55.7, 56.15, 56.31, 56.2, 55.32, 54.77, 54.35, 54.11, 52.18, 49.66, 50.22, 48.93, 47.16, 46.94, 47.09, 46.95, 47.59, 47.24, 46.38, 46.44, 46.02, 44.57, 44.64, 45.83, 44.81, 44.91, 45.15, 44.58, 44.7, 44.45, 44.94, 46.28, 46.33, 47.44, 47.48, 47.8, 48.08, 48.94, 47.72, 47.37, 47.31, 47.4, 47.01, 47.37, 48.35, 47.63, 46.66, 47.24, 46.04, 46.59, 47.26, 47.42, 47.41, 47.51, 47.12, 46.72, 47.25, 46.21, 47.32, 46.46, 46.18, 45.45, 45.54, 44.37, 44.27, 44.55, 43.64, 43.8, 43.93, 43.56, 44.41, 44.05, 44.15, 42.51, 41.85, 43.92, 43.73, 42.61, 43.19, 43.74, 44.72, 46.16, 44.72, 45.2159, 45.69, 45.6, 44.62, 44.04, 47.41, 47.57, 49.01, 48.56, 49.38, 49.61, 48.39, 48.32, 48.33, 44.94, 44.99, 46.08, 46.99, 48.21, 48.95, 50.49, 50.29, 50.645, 49.38, 49.66, 49.48, 49.72, 48.9, 49.37, 49.21, 49.23, 49.59, 48.75, 48.43, 47.9, 45.99, 45.82, 47.2, 46.84, 46.93, 48.96, 46.21, 47.27, 48.15, 47.68, 47.2, 47.68, 48.09, 48.63, 46.22, 45.18, 46.28, 46.29, 46.43, 46.35, 46.56, 46.82, 47.49, 48.06, 47.01, 45.12, 45.9, 45.88, 45.71, 45.78, 45.73, 45.37, 44.66, 45.02, 44.46, 45.24, 45.15, 45.57, 45.25, 44.29, 43.75, 42.78, 43.26, 43.66, 40.98, 38.76, 38.5, 40.23, 38.725, 39.72, 39.64, 40.13, 40.68, 40.6, 40.15, 39.13, 39.16, 38.14, 36.5, 39.08, 38.64, 38.36, 38.45, 37.91, 35.8684, 36.35, 36.58, 36.17, 37.32, 37.44, 38.54, 39.38, 39.99, 39.51, 39.8, 39.9, 41.65, 41.57, 44.77, 43.09, 42.65, 44.09, 44.09, 43.91, 42.9, 42.53, 42.12, 42.07, 43.3, 43.86, 42.72, 42.55, 41.88, 43.02, 45.35, 45.8, 45.42, 44.95, 44.27, 42.98, 40.23, 38.86, 37.83, 38.79, 42.6, 41.68, 43.47, 43.61, 43.82, 43.76, 44.13, 44.1, 42.75, 43.0, 43.34, 43.67, 44.01, 45.86, 46.81, 46.66, 47.79, 46.93, 46.97, 47.35, 46.58, 47.07, 48.5, 49.15, 49.27, 49.01, 48.09, 47.98, 47.94, 48.29, 48.21, 49.47, 48.98, 50.3, 49.8, 49.38, 49.37, 49.41, 53.52, 53.14, 55.99, 56.46, 56.36, 55.76, 55.73, 55.31, 54.8, 54.55, 54.26, 53.06, 52.9, 53.23, 51.01, 50.87, 51.03, 55.04, 54.87, 54.23, 53.88, 53.41, 53.07, 52.62, 54.0, 54.7, 57.54, 56.43, 55.73, 56.35, 56.47, 57.19, 58.1, 58.11, 58.45, 59.01, 59.03, 57.25, 58.89, 58.74, 58.6, 58.6, 56.7, 57.63, 56.46, 56.36, 55.68, 56.51, 55.83, 58.22, 58.45, 58.08, 57.94, 58.97, 58.44, 59.08, 56.94, 57.11, 57.94, 55.97, 55.24, 58.68, 59.18, 57.47, 57.89, 56.35, 58.03, 57.25, 57.47, 58.3, 58.28, 58.01, 57.25, 56.31, 59.01, 59.37, 57.77, 57.08, 56.71, 57.27, 57.23, 56.49, 55.94, 55.68, 54.97, 54.7, 54.22, 57.89, 59.52, 59.34, 58.3, 58.27, 58.22, 59.63, 59.91, 62.111, 61.9, 61.14, 60.61, 59.87, 58.57, 59.34, 56.27, 56.63, 58.24, 58.93, 58.3, 57.76, 58.52, 56.9, 57.37, 58.14, 56.28, 59.03, 59.9, 59.9, 60.41, 60.09, 59.56, 59.93, 59.68, 60.21, 60.18, 58.88, 56.29, 56.23, 55.14, 54.12, 53.98, 53.415, 52.86, 53.54, 53.3, 51.365, 51.43, 52.13, 51.41, 51.52, 51.37, 49.63, 49.16, 47.82, 47.44, 49.53, 48.7, 49.65, 51.77, 54.45, 55.2, 54.98, 55.12, 54.93, 53.18, 52.96, 54.06, 53.16, 52.62, 52.01, 52.14, 51.505, 51.25, 49.0, 47.06, 46.42, 46.01, 42.93, 42.63, 42.83, 42.93, 41.85, 42.02, 42.095, 42.14, 42.18, 41.95, 41.54, 42.18, 43.0, 42.04, 41.95, 42.1, 42.12, 40.46, 40.49, 39.5901, 39.17, 39.13, 39.21, 39.1, 38.57, 38.755, 39.9801, 39.08, 40.29, 40.34, 40.15, 39.86, 39.42, 39.13, 37.81, 37.49, 37.66, 37.98, 37.41, 37.71, 37.12, 37.45, 36.34, 36.77, 36.97, 36.98, 36.67, 36.89, 38.18, 38.16, 38.85, 39.3501, 34.93, 34.35, 34.26, 33.97, 34.5, 34.4, 33.62, 33.5, 33.76, 33.85, 33.79, 34.11, 33.98, 34.06, 34.2, 33.18, 32.98, 33.17, 32.875, 33.16, 33.58, 32.72, 32.385, 32.18, 33.52, 33.53, 32.65, 32.78, 31.9, 30.94, 32.0, 32.34, 32.78, 32.75, 31.79, 31.54, 31.5, 31.45, 31.48, 31.26, 31.01, 31.01, 32.28, 31.46, 31.37, 31.51, 32.47, 32.29, 32.56, 32.3, 31.54, 32.61, 32.99, 33.48, 33.43, 33.58, 33.43, 33.39, 32.96, 32.6, 32.55, 32.34, 33.14, 39.78, 39.59, 39.43, 39.24, 38.82, 39.07, 39.49, 38.32, 39.69, 38.72, 38.39, 38.2, 37.29, 39.0148, 38.59, 38.75, 38.91, 40.17, 39.51, 42.4, 42.67, 42.01, 41.66, 41.04, 41.83, 42.02, 41.51, 41.28, 42.03, 41.54, 42.37, 42.77, 41.94, 41.89, 42.09, 43.22, 42.88, 43.56, 43.24, 42.52, 43.11, 42.73, 42.63, 42.77, 42.51, 42.26, 42.16, 44.0, 43.48, 43.04, 42.475, 43.11, 42.83, 42.87, 43.55, 42.575, 42.52, 42.44, 42.18, 42.13, 42.3, 42.03, 50.53, 49.42, 48.265, 48.55, 48.56, 48.54, 48.49, 48.17, 48.34, 49.1, 47.605, 46.85, 46.245, 46.0, 45.97, 45.3, 45.93, 47.13, 45.67, 45.28, 44.91, 45.24, 45.13, 45.01, 44.51, 46.08, 46.88, 47.23, 47.06, 46.7, 47.57, 47.28, 46.68, 48.1, 48.28, 48.51, 48.97, 48.92, 49.48, 48.19, 47.69, 47.75, 47.39, 47.35, 47.43, 47.14, 47.57, 48.14, 47.87, 47.81, 46.46, 46.36, 45.8, 46.09, 45.44, 46.28, 45.27, 44.79, 45.15, 44.9, 44.5, 43.89, 42.66, 37.42, 36.37, 36.47, 36.33, 37.24, 37.02, 36.9, 36.75, 36.85, 37.64, 38.04, 38.49, 39.055, 39.12, 39.625, 38.18, 38.02, 38.8, 39.375, 39.11, 39.02, 38.91, 38.76, 38.72, 39.1, 40.61, 40.2, 40.85, 39.16, 39.38, 39.22, 40.26, 39.15, 39.16, 38.87, 38.11, 40.1, 39.5, 39.37, 39.07, 38.82, 38.95, 39.82, 39.29, 39.23, 38.26, 38.51, 37.99, 37.25, 36.17, 34.77, 34.49, 34.65, 33.0, 33.07, 32.64, 33.71, 34.32, 34.16, 34.24, 34.84, 34.26, 34.53, 34.5, 34.97, 34.32, 35.34, 35.01, 35.16, 35.17, 35.23, 35.1, 37.97, 37.88, 37.81, 38.54, 38.24, 38.41, 38.55, 37.79, 38.21, 38.43, 37.21, 36.46, 36.4, 36.68, 35.53, 35.95, 36.41, 36.46, 36.07, 36.16, 36.55, 36.1, 36.31, 36.5, 34.47, 33.92, 33.72, 33.73, 33.92, 33.72, 33.33, 32.05, 31.58, 32.12, 31.87, 32.33, 32.07, 31.78, 32.46, 32.0, 31.55, 31.75, 31.38, 32.49, 32.61, 32.13, 31.26, 30.72, 30.65, 32.415, 32.03, 32.59, 32.05, 32.45, 31.44, 31.58, 31.22, 30.5, 29.99, 30.16, 28.89, 29.18, 29.84, 29.35, 24.09, 24.01, 23.73, 23.52, 23.17, 24.03, 24.35, 23.94, 22.525, 20.865, 23.16, 24.48, 23.88, 23.86, 23.43, 23.21, 22.69, 22.71, 22.76, 22.68, 23.14, 24.14, 24.25, 24.65, 23.89, 23.64, 23.755, 23.01, 22.56, 21.66, 21.55, 21.48, 20.88, 20.8, 20.41, 20.52, 20.24, 19.635, 19.15, 19.09, 18.79, 18.72, 19.25, 18.86, 19.07, 20.01, 20.155, 19.44, 20.48, 21.7, 26.9, 27.34, 27.0, 26.3, 25.32, 25.43, 25.67, 25.33, 25.78, 25.59, 27.11, 33.93, 35.04, 34.98, 33.22, 33.11, 33.72, 32.86, 33.55, 33.69, 34.45, 34.16, 33.81, 33.55, 33.23, 34.81, 37.04, 36.54, 36.58, 34.93, 34.49, 34.62, 35.36, 36.17, 34.95, 34.21, 34.24, 35.48, 34.53, 34.76, 33.935, 33.23, 31.36, 31.7, 30.92, 31.08, 30.75, 32.22, 31.34, 33.1, 31.97, 31.33, 29.87, 29.02, 30.19, 29.41, 30.36, 31.6, 32.2, 30.52, 28.6, 30.41, 29.05, 26.76, 26.97, 27.75, 26.4, 27.93, 27.57, 29.21, 29.83, 30.87, 32.11, 31.08, 31.575, 31.35, 32.52, 34.52, 34.75, 36.34, 36.27, 36.65, 37.18, 37.15, 36.74, 36.65, 36.63, 35.45, 34.7, 34.1045, 33.91, 34.78, 34.25, 33.66, 35.9899, 36.225, 35.72, 35.43, 35.1, 34.45, 33.77, 34.545, 36.52, 35.94, 35.19, 34.57, 34.27, 34.71, 34.4, 34.28, 34.38, 33.89, 34.3, 33.07, 34.07, 36.11, 36.54, 38.28, 38.82, 38.86, 38.81, 40.38, 38.88, 38.07, 38.06, 39.34, 39.18, 39.55, 41.15, 39.24, 37.13, 38.52, 38.74, 39.45, 41.3938, 47.8, 48.61, 49.0, 49.03, 48.34, 47.3, 46.2, 44.85, 43.18, 42.86, 44.78, 41.77, 42.06, 42.89, 44.1, 43.56, 43.8, 46.19, 45.6, 47.76, 49.16, 48.62, 48.33, 48.93, 49.21, 50.11, 49.61, 48.19, 49.48, 49.77, 50.61, 51.39, 50.68, 50.05, 47.9, 46.33, 47.63, 48.73, 48.65, 49.42, 52.05, 52.45, 52.0, 51.37, 51.63, 51.01, 51.15, 49.48, 51.32, 51.87, 51.38, 51.46, 50.6, 51.29, 49.42, 48.88, 47.9, 47.32, 48.02, 49.06, 48.11, 47.72, 48.15, 47.91, 47.31, 47.61, 46.28, 45.94, 46.06, 46.4, 47.82, 47.02, 48.15, 47.575, 47.51, 49.03, 50.06, 50.68, 51.98, 52.29, 53.87, 53.34, 54.14, 53.18, 53.2, 53.36, 53.54, 54.3, 53.59, 52.63, 53.05, 54.42, 54.57, 55.66, 55.24, 55.21, 55.65, 56.36, 55.35, 54.58, 55.78, 55.37, 54.6, 55.14, 56.88, 56.9, 57.42, 56.59, 56.16, 56.76, 57.27, 56.43, 56.3, 57.7, 59.22, 59.61, 58.72, 58.16, 59.0, 57.91, 58.33, 58.75, 58.23, 58.41, 59.14, 57.44, 55.97, 56.66, 55.77, 55.39, 55.59, 54.7, 53.31, 53.755, 52.74, 52.44, 51.79, 52.03, 53.08, 52.755, 53.52, 53.61, 55.33, 55.85, 56.5, 55.7, 55.31, 53.66, 54.84, 53.75, 54.54, 54.36, 54.41, 56.97, 56.91, 58.42, 58.09, 58.83, 61.56, 61.11, 60.69, 60.02, 60.7, 61.99, 62.19, 61.46, 61.18, 61.14, 61.82, 61.17, 60.8, 60.2, 60.39, 59.94, 60.41, 59.92, 59.25, 57.8, 56.4399, 58.68, 57.53, 57.07, 59.06, 63.98, 63.94, 63.29, 63.12, 63.41, 62.29, 64.59, 64.74, 65.05, 66.44, 65.43, 64.26, 63.84, 65.64, 66.01, 66.5, 67.26, 68.19, 68.76, 68.82, 68.65, 68.63, 68.41, 67.61, 64.99, 63.28, 63.71, 64.04, 65.51, 65.22, 65.77, 66.24, 66.39, 66.09, 66.84, 67.53, 65.94, 66.2484, 65.85, 65.99, 66.03, 64.93, 65.54, 64.63, 64.65, 64.0, 62.88, 62.12, 62.315, 61.81, 61.76, 61.57, 62.535, 63.75, 63.885, 64.18, 62.82, 60.14, 60.22, 59.47, 58.73, 58.35, 56.51, 54.28, 55.205, 53.85, 52.945, 52.37, 51.96, 52.56, 52.99, 54.94, 56.24, 57.06, 55.24, 56.15, 56.09, 55.51, 55.11, 57.27, 57.25, 56.78, 56.26, 57.445, 56.78, 57.6, 58.31, 59.795, 58.97, 59.245, 58.84, 61.0, 60.87, 61.25, 60.82, 61.29, 61.31, 60.96, 61.69, 61.83, 62.58, 62.1, 61.18, 60.65, 60.96, 60.365, 60.43, 59.84, 59.58, 59.22, 58.75, 58.38, 57.935, 56.8, 56.99, 56.97, 56.84, 56.93, 57.38, 58.7, 58.19, 58.6, 60.09, 59.39, 59.14, 59.5, 59.58, 59.19, 59.24, 58.57, 58.8, 59.5, 61.31, 59.68, 59.67, 59.45, 58.83, 58.48, 58.69, 58.96, 59.11, 59.03, 58.87, 56.81, 56.925, 56.71, 56.77, 56.0, 55.95, 56.27, 56.58, 57.11, 56.81, 55.91, 55.36, 54.22, 54.5, 54.52, 54.66, 55.5, 54.37, 53.63, 53.55, 53.82, 53.74, 53.06, 53.12, 53.36, 51.95, 51.61, 51.16, 50.88, 51.71, 51.19, 50.89, 51.25, 50.77, 49.79, 49.54, 49.28, 49.98, 50.26, 50.51, 51.03, 50.73, 52.58, 53.5, 52.59, 52.73, 53.94, 54.11, 55.65, 55.69, 55.57, 55.65, 54.87, 54.82, 53.39, 53.96, 55.69, 55.54, 54.96, 56.06, 57.38, 57.16, 57.0, 56.17, 54.92, 54.8, 55.56, 55.13, 53.0, 53.5, 53.11, 51.67, 51.92, 50.72, 50.16, 49.75, 50.75, 49.88, 50.13, 49.29, 50.11, 52.56, 53.19, 52.29, 52.19, 51.94, 51.73, 50.69, 51.15, 50.76, 50.02, 49.04, 49.71, 50.14, 49.6, 49.21, 49.37, 49.89, 49.52, 49.19, 49.11, 49.99, 51.09, 52.87, 53.11, 53.66, 51.515, 58.06, 58.56, 60.74, 61.0, 61.24, 61.19, 60.665, 60.84, 60.32, 58.54, 58.39, 58.02, 59.63, 58.8, 56.94, 56.83, 55.43, 56.16, 56.53, 55.65, 56.01, 55.9, 56.01, 55.68, 52.49, 52.67, 52.97, 51.34, 49.99, 49.41, 49.57, 50.94, 50.71, 51.41, 49.85, 49.96, 49.62, 49.85, 49.02, 49.01, 48.79, 48.45, 48.34, 48.22, 47.72, 47.86, 48.51, 48.95, 50.09, 48.77, 47.8, 47.9, 48.17, 47.04, 48.48, 48.49, 49.26, 49.22, 48.67, 49.51, 48.25, 49.81, 49.8, 48.56, 48.09, 49.13]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b341c840-c80b-4586-bc3d-0cf17dbcf845');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"b2de36ed-2928-47f8-9e76-4b2c3733d8da\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"b2de36ed-2928-47f8-9e76-4b2c3733d8da\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'b2de36ed-2928-47f8-9e76-4b2c3733d8da',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b2de36ed-2928-47f8-9e76-4b2c3733d8da');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUWgGHO3Y6We"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tQNHd60Y6We",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e29174b-8e47-4ed0-bc6a-d0f97a2379d4"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.05, .1, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"STX\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6957 - accuracy: 0.5007 - val_loss: 0.6993 - val_accuracy: 0.3531\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6674 - accuracy: 0.5926 - val_loss: 0.5859 - val_accuracy: 0.7388\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6353 - accuracy: 0.6732 - val_loss: 0.6101 - val_accuracy: 0.6898\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6274 - accuracy: 0.6456 - val_loss: 0.5332 - val_accuracy: 0.7592\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6103 - accuracy: 0.6718 - val_loss: 0.5403 - val_accuracy: 0.7469\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6749 - accuracy: 0.5752 - val_loss: 0.5725 - val_accuracy: 0.7531\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5907 - accuracy: 0.6812 - val_loss: 0.5382 - val_accuracy: 0.7367\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5753 - accuracy: 0.7228 - val_loss: 0.4936 - val_accuracy: 0.7755\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5703 - accuracy: 0.7013 - val_loss: 0.4913 - val_accuracy: 0.7755\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5643 - accuracy: 0.7248 - val_loss: 0.4948 - val_accuracy: 0.7694\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.816615\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.821292\n",
            "[2]\tvalidation_0-auc:0.81884\n",
            "[3]\tvalidation_0-auc:0.820764\n",
            "[4]\tvalidation_0-auc:0.81925\n",
            "[5]\tvalidation_0-auc:0.820891\n",
            "[6]\tvalidation_0-auc:0.820372\n",
            "[7]\tvalidation_0-auc:0.821292\n",
            "[8]\tvalidation_0-auc:0.821055\n",
            "[9]\tvalidation_0-auc:0.821694\n",
            "[10]\tvalidation_0-auc:0.823727\n",
            "[11]\tvalidation_0-auc:0.822879\n",
            "[12]\tvalidation_0-auc:0.823936\n",
            "[13]\tvalidation_0-auc:0.82824\n",
            "[14]\tvalidation_0-auc:0.827902\n",
            "[15]\tvalidation_0-auc:0.828112\n",
            "[16]\tvalidation_0-auc:0.828687\n",
            "[17]\tvalidation_0-auc:0.82865\n",
            "[18]\tvalidation_0-auc:0.828677\n",
            "[19]\tvalidation_0-auc:0.829252\n",
            "[20]\tvalidation_0-auc:0.829288\n",
            "[21]\tvalidation_0-auc:0.828924\n",
            "[22]\tvalidation_0-auc:0.828933\n",
            "[23]\tvalidation_0-auc:0.829398\n",
            "[24]\tvalidation_0-auc:0.829398\n",
            "[25]\tvalidation_0-auc:0.830483\n",
            "[26]\tvalidation_0-auc:0.829808\n",
            "[27]\tvalidation_0-auc:0.8302\n",
            "[28]\tvalidation_0-auc:0.830319\n",
            "[29]\tvalidation_0-auc:0.830091\n",
            "[30]\tvalidation_0-auc:0.830765\n",
            "[31]\tvalidation_0-auc:0.830847\n",
            "[32]\tvalidation_0-auc:0.830501\n",
            "[33]\tvalidation_0-auc:0.830656\n",
            "[34]\tvalidation_0-auc:0.830638\n",
            "[35]\tvalidation_0-auc:0.831221\n",
            "[36]\tvalidation_0-auc:0.832096\n",
            "[37]\tvalidation_0-auc:0.832251\n",
            "[38]\tvalidation_0-auc:0.831923\n",
            "[39]\tvalidation_0-auc:0.831704\n",
            "[40]\tvalidation_0-auc:0.831832\n",
            "[41]\tvalidation_0-auc:0.831887\n",
            "[42]\tvalidation_0-auc:0.83278\n",
            "[43]\tvalidation_0-auc:0.832561\n",
            "[44]\tvalidation_0-auc:0.832343\n",
            "[45]\tvalidation_0-auc:0.832124\n",
            "[46]\tvalidation_0-auc:0.831978\n",
            "[47]\tvalidation_0-auc:0.832169\n",
            "[48]\tvalidation_0-auc:0.831914\n",
            "[49]\tvalidation_0-auc:0.830729\n",
            "[50]\tvalidation_0-auc:0.831312\n",
            "[51]\tvalidation_0-auc:0.830236\n",
            "[52]\tvalidation_0-auc:0.830474\n",
            "[53]\tvalidation_0-auc:0.830027\n",
            "[54]\tvalidation_0-auc:0.830355\n",
            "[55]\tvalidation_0-auc:0.830391\n",
            "[56]\tvalidation_0-auc:0.830446\n",
            "[57]\tvalidation_0-auc:0.830337\n",
            "[58]\tvalidation_0-auc:0.830246\n",
            "[59]\tvalidation_0-auc:0.829972\n",
            "[60]\tvalidation_0-auc:0.829753\n",
            "[61]\tvalidation_0-auc:0.830629\n",
            "[62]\tvalidation_0-auc:0.830483\n",
            "[63]\tvalidation_0-auc:0.83061\n",
            "[64]\tvalidation_0-auc:0.831048\n",
            "[65]\tvalidation_0-auc:0.830556\n",
            "[66]\tvalidation_0-auc:0.830227\n",
            "[67]\tvalidation_0-auc:0.8303\n",
            "[68]\tvalidation_0-auc:0.829635\n",
            "[69]\tvalidation_0-auc:0.829544\n",
            "[70]\tvalidation_0-auc:0.829507\n",
            "[71]\tvalidation_0-auc:0.829489\n",
            "[72]\tvalidation_0-auc:0.829416\n",
            "[73]\tvalidation_0-auc:0.829379\n",
            "[74]\tvalidation_0-auc:0.829252\n",
            "[75]\tvalidation_0-auc:0.829033\n",
            "[76]\tvalidation_0-auc:0.829471\n",
            "[77]\tvalidation_0-auc:0.829762\n",
            "[78]\tvalidation_0-auc:0.82958\n",
            "[79]\tvalidation_0-auc:0.829197\n",
            "[80]\tvalidation_0-auc:0.828978\n",
            "[81]\tvalidation_0-auc:0.828869\n",
            "[82]\tvalidation_0-auc:0.829434\n",
            "[83]\tvalidation_0-auc:0.829106\n",
            "[84]\tvalidation_0-auc:0.828431\n",
            "[85]\tvalidation_0-auc:0.828413\n",
            "[86]\tvalidation_0-auc:0.827866\n",
            "[87]\tvalidation_0-auc:0.828067\n",
            "[88]\tvalidation_0-auc:0.827921\n",
            "[89]\tvalidation_0-auc:0.827757\n",
            "[90]\tvalidation_0-auc:0.827793\n",
            "[91]\tvalidation_0-auc:0.827793\n",
            "[92]\tvalidation_0-auc:0.827629\n",
            "Stopping. Best iteration:\n",
            "[42]\tvalidation_0-auc:0.83278\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6950 - accuracy: 0.5045 - val_loss: 0.6808 - val_accuracy: 0.6214\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6910 - accuracy: 0.5333 - val_loss: 0.6637 - val_accuracy: 0.6214\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6623 - accuracy: 0.6019 - val_loss: 0.6077 - val_accuracy: 0.6958\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6117 - accuracy: 0.6767 - val_loss: 0.6396 - val_accuracy: 0.6433\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5937 - accuracy: 0.6911 - val_loss: 0.6289 - val_accuracy: 0.6499\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6819 - accuracy: 0.5635 - val_loss: 0.6372 - val_accuracy: 0.7352\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6265 - accuracy: 0.6719 - val_loss: 0.5634 - val_accuracy: 0.7177\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5809 - accuracy: 0.7179 - val_loss: 0.5333 - val_accuracy: 0.7418\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5827 - accuracy: 0.7152 - val_loss: 0.5350 - val_accuracy: 0.7112\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5690 - accuracy: 0.7200 - val_loss: 0.5275 - val_accuracy: 0.7330\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.740118\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.737951\n",
            "[2]\tvalidation_0-auc:0.727998\n",
            "[3]\tvalidation_0-auc:0.742958\n",
            "[4]\tvalidation_0-auc:0.739142\n",
            "[5]\tvalidation_0-auc:0.751272\n",
            "[6]\tvalidation_0-auc:0.755902\n",
            "[7]\tvalidation_0-auc:0.772867\n",
            "[8]\tvalidation_0-auc:0.756198\n",
            "[9]\tvalidation_0-auc:0.761245\n",
            "[10]\tvalidation_0-auc:0.765937\n",
            "[11]\tvalidation_0-auc:0.767504\n",
            "[12]\tvalidation_0-auc:0.766792\n",
            "[13]\tvalidation_0-auc:0.771463\n",
            "[14]\tvalidation_0-auc:0.769366\n",
            "[15]\tvalidation_0-auc:0.772989\n",
            "[16]\tvalidation_0-auc:0.774414\n",
            "[17]\tvalidation_0-auc:0.777508\n",
            "[18]\tvalidation_0-auc:0.779319\n",
            "[19]\tvalidation_0-auc:0.781293\n",
            "[20]\tvalidation_0-auc:0.786667\n",
            "[21]\tvalidation_0-auc:0.787664\n",
            "[22]\tvalidation_0-auc:0.786392\n",
            "[23]\tvalidation_0-auc:0.788712\n",
            "[24]\tvalidation_0-auc:0.78973\n",
            "[25]\tvalidation_0-auc:0.786717\n",
            "[26]\tvalidation_0-auc:0.79089\n",
            "[27]\tvalidation_0-auc:0.793098\n",
            "[28]\tvalidation_0-auc:0.795144\n",
            "[29]\tvalidation_0-auc:0.792864\n",
            "[30]\tvalidation_0-auc:0.795408\n",
            "[31]\tvalidation_0-auc:0.796528\n",
            "[32]\tvalidation_0-auc:0.795815\n",
            "[33]\tvalidation_0-auc:0.794238\n",
            "[34]\tvalidation_0-auc:0.791582\n",
            "[35]\tvalidation_0-auc:0.788071\n",
            "[36]\tvalidation_0-auc:0.789251\n",
            "[37]\tvalidation_0-auc:0.786962\n",
            "[38]\tvalidation_0-auc:0.785913\n",
            "[39]\tvalidation_0-auc:0.785771\n",
            "[40]\tvalidation_0-auc:0.785567\n",
            "[41]\tvalidation_0-auc:0.787257\n",
            "[42]\tvalidation_0-auc:0.78454\n",
            "[43]\tvalidation_0-auc:0.785608\n",
            "[44]\tvalidation_0-auc:0.785466\n",
            "[45]\tvalidation_0-auc:0.784061\n",
            "[46]\tvalidation_0-auc:0.784713\n",
            "[47]\tvalidation_0-auc:0.786524\n",
            "[48]\tvalidation_0-auc:0.789302\n",
            "[49]\tvalidation_0-auc:0.789282\n",
            "[50]\tvalidation_0-auc:0.787511\n",
            "[51]\tvalidation_0-auc:0.785842\n",
            "[52]\tvalidation_0-auc:0.784845\n",
            "[53]\tvalidation_0-auc:0.784519\n",
            "[54]\tvalidation_0-auc:0.784967\n",
            "[55]\tvalidation_0-auc:0.784764\n",
            "[56]\tvalidation_0-auc:0.78515\n",
            "[57]\tvalidation_0-auc:0.783664\n",
            "[58]\tvalidation_0-auc:0.782229\n",
            "[59]\tvalidation_0-auc:0.782616\n",
            "[60]\tvalidation_0-auc:0.782494\n",
            "[61]\tvalidation_0-auc:0.782698\n",
            "[62]\tvalidation_0-auc:0.782759\n",
            "[63]\tvalidation_0-auc:0.782474\n",
            "[64]\tvalidation_0-auc:0.779604\n",
            "[65]\tvalidation_0-auc:0.77995\n",
            "[66]\tvalidation_0-auc:0.778118\n",
            "[67]\tvalidation_0-auc:0.777701\n",
            "[68]\tvalidation_0-auc:0.777599\n",
            "[69]\tvalidation_0-auc:0.778596\n",
            "[70]\tvalidation_0-auc:0.776174\n",
            "[71]\tvalidation_0-auc:0.774322\n",
            "[72]\tvalidation_0-auc:0.775482\n",
            "[73]\tvalidation_0-auc:0.776195\n",
            "[74]\tvalidation_0-auc:0.776459\n",
            "[75]\tvalidation_0-auc:0.777212\n",
            "[76]\tvalidation_0-auc:0.776622\n",
            "[77]\tvalidation_0-auc:0.775075\n",
            "[78]\tvalidation_0-auc:0.774628\n",
            "[79]\tvalidation_0-auc:0.774628\n",
            "[80]\tvalidation_0-auc:0.773101\n",
            "[81]\tvalidation_0-auc:0.771005\n",
            "Stopping. Best iteration:\n",
            "[31]\tvalidation_0-auc:0.796528\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.746938775510204  | 0.6310160427807486 | 0.6820809248554913 | 0.6555555555555556 |\n",
            "|     GRU 0.1      | 0.7693877551020408 |        0.7         | 0.6069364161849711 | 0.6501547987616099 |\n",
            "|   XGBoost 0.1    | 0.7571428571428571 | 0.6534090909090909 | 0.6647398843930635 | 0.6590257879656161 |\n",
            "|    Logreg 0.1    | 0.7448979591836735 | 0.6348314606741573 | 0.653179190751445  | 0.6438746438746439 |\n",
            "|     SVM 0.1      | 0.7571428571428571 | 0.651685393258427  | 0.6705202312138728 | 0.6609686609686609 |\n",
            "|  LSTM beta 0.1   | 0.649890590809628  | 0.525096525096525  | 0.7861271676300579 | 0.6296296296296297 |\n",
            "|   GRU beta 0.1   | 0.7330415754923414 | 0.7142857142857143 | 0.4913294797687861 | 0.5821917808219178 |\n",
            "| XGBoost beta 0.1 | 0.7199124726477024 | 0.6285714285714286 | 0.6358381502890174 | 0.632183908045977  |\n",
            "| logreg beta 0.1  | 0.7177242888402626 | 0.6309523809523809 | 0.6127167630057804 | 0.6217008797653959 |\n",
            "|   svm beta 0.1   | 0.7045951859956237 | 0.6079545454545454 | 0.6184971098265896 | 0.6131805157593122 |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6928 - accuracy: 0.5309 - val_loss: 0.6809 - val_accuracy: 0.7612\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6685 - accuracy: 0.6174 - val_loss: 0.6673 - val_accuracy: 0.6286\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6379 - accuracy: 0.6544 - val_loss: 0.5525 - val_accuracy: 0.7510\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6367 - accuracy: 0.6477 - val_loss: 0.5871 - val_accuracy: 0.7592\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6341 - accuracy: 0.6564 - val_loss: 0.5865 - val_accuracy: 0.7388\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6719 - accuracy: 0.5705 - val_loss: 0.5948 - val_accuracy: 0.7347\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6252 - accuracy: 0.6779 - val_loss: 0.5616 - val_accuracy: 0.7388\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6126 - accuracy: 0.6691 - val_loss: 0.5162 - val_accuracy: 0.7714\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6146 - accuracy: 0.6584 - val_loss: 0.5160 - val_accuracy: 0.7776\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6054 - accuracy: 0.6819 - val_loss: 0.5295 - val_accuracy: 0.7551\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.773453\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.790804\n",
            "[2]\tvalidation_0-auc:0.796306\n",
            "[3]\tvalidation_0-auc:0.793243\n",
            "[4]\tvalidation_0-auc:0.793333\n",
            "[5]\tvalidation_0-auc:0.797142\n",
            "[6]\tvalidation_0-auc:0.793716\n",
            "[7]\tvalidation_0-auc:0.79421\n",
            "[8]\tvalidation_0-auc:0.794281\n",
            "[9]\tvalidation_0-auc:0.791147\n",
            "[10]\tvalidation_0-auc:0.792104\n",
            "[11]\tvalidation_0-auc:0.793716\n",
            "[12]\tvalidation_0-auc:0.792386\n",
            "[13]\tvalidation_0-auc:0.792034\n",
            "[14]\tvalidation_0-auc:0.793616\n",
            "[15]\tvalidation_0-auc:0.792598\n",
            "[16]\tvalidation_0-auc:0.79156\n",
            "[17]\tvalidation_0-auc:0.792578\n",
            "[18]\tvalidation_0-auc:0.793041\n",
            "[19]\tvalidation_0-auc:0.793948\n",
            "[20]\tvalidation_0-auc:0.793696\n",
            "[21]\tvalidation_0-auc:0.79417\n",
            "[22]\tvalidation_0-auc:0.793908\n",
            "[23]\tvalidation_0-auc:0.792054\n",
            "[24]\tvalidation_0-auc:0.792527\n",
            "[25]\tvalidation_0-auc:0.79283\n",
            "[26]\tvalidation_0-auc:0.792487\n",
            "[27]\tvalidation_0-auc:0.791782\n",
            "[28]\tvalidation_0-auc:0.791409\n",
            "[29]\tvalidation_0-auc:0.791288\n",
            "[30]\tvalidation_0-auc:0.790814\n",
            "[31]\tvalidation_0-auc:0.790552\n",
            "[32]\tvalidation_0-auc:0.788376\n",
            "[33]\tvalidation_0-auc:0.789797\n",
            "[34]\tvalidation_0-auc:0.789192\n",
            "[35]\tvalidation_0-auc:0.78895\n",
            "[36]\tvalidation_0-auc:0.789293\n",
            "[37]\tvalidation_0-auc:0.79026\n",
            "[38]\tvalidation_0-auc:0.792678\n",
            "[39]\tvalidation_0-auc:0.792628\n",
            "[40]\tvalidation_0-auc:0.791107\n",
            "[41]\tvalidation_0-auc:0.792054\n",
            "[42]\tvalidation_0-auc:0.792457\n",
            "[43]\tvalidation_0-auc:0.792477\n",
            "[44]\tvalidation_0-auc:0.791973\n",
            "[45]\tvalidation_0-auc:0.79157\n",
            "[46]\tvalidation_0-auc:0.791379\n",
            "[47]\tvalidation_0-auc:0.791016\n",
            "[48]\tvalidation_0-auc:0.790734\n",
            "[49]\tvalidation_0-auc:0.790925\n",
            "[50]\tvalidation_0-auc:0.791046\n",
            "[51]\tvalidation_0-auc:0.791268\n",
            "[52]\tvalidation_0-auc:0.790885\n",
            "[53]\tvalidation_0-auc:0.79151\n",
            "[54]\tvalidation_0-auc:0.792527\n",
            "[55]\tvalidation_0-auc:0.792427\n",
            "Stopping. Best iteration:\n",
            "[5]\tvalidation_0-auc:0.797142\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 20ms/step - loss: 0.6758 - accuracy: 0.5738 - val_loss: 0.7044 - val_accuracy: 0.5842\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6350 - accuracy: 0.6658 - val_loss: 0.8899 - val_accuracy: 0.4967\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6341 - accuracy: 0.6575 - val_loss: 0.6135 - val_accuracy: 0.6608\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5929 - accuracy: 0.7255 - val_loss: 0.5864 - val_accuracy: 0.6761\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5645 - accuracy: 0.7358 - val_loss: 0.6097 - val_accuracy: 0.7024\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6368 - accuracy: 0.6424 - val_loss: 0.5928 - val_accuracy: 0.7330\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5595 - accuracy: 0.7412 - val_loss: 0.5656 - val_accuracy: 0.7112\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5432 - accuracy: 0.7433 - val_loss: 0.6217 - val_accuracy: 0.7046\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5335 - accuracy: 0.7358 - val_loss: 0.6082 - val_accuracy: 0.7177\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5391 - accuracy: 0.7412 - val_loss: 0.6116 - val_accuracy: 0.7155\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.660717\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.672075\n",
            "[2]\tvalidation_0-auc:0.671741\n",
            "[3]\tvalidation_0-auc:0.677832\n",
            "[4]\tvalidation_0-auc:0.678912\n",
            "[5]\tvalidation_0-auc:0.678845\n",
            "[6]\tvalidation_0-auc:0.681484\n",
            "[7]\tvalidation_0-auc:0.67968\n",
            "[8]\tvalidation_0-auc:0.676651\n",
            "[9]\tvalidation_0-auc:0.703231\n",
            "[10]\tvalidation_0-auc:0.677375\n",
            "[11]\tvalidation_0-auc:0.69125\n",
            "[12]\tvalidation_0-auc:0.694802\n",
            "[13]\tvalidation_0-auc:0.695136\n",
            "[14]\tvalidation_0-auc:0.711483\n",
            "[15]\tvalidation_0-auc:0.706082\n",
            "[16]\tvalidation_0-auc:0.711995\n",
            "[17]\tvalidation_0-auc:0.715959\n",
            "[18]\tvalidation_0-auc:0.710425\n",
            "[19]\tvalidation_0-auc:0.713109\n",
            "[20]\tvalidation_0-auc:0.713198\n",
            "[21]\tvalidation_0-auc:0.715191\n",
            "[22]\tvalidation_0-auc:0.715814\n",
            "[23]\tvalidation_0-auc:0.719578\n",
            "[24]\tvalidation_0-auc:0.71724\n",
            "[25]\tvalidation_0-auc:0.718977\n",
            "[26]\tvalidation_0-auc:0.709467\n",
            "[27]\tvalidation_0-auc:0.706361\n",
            "[28]\tvalidation_0-auc:0.708499\n",
            "[29]\tvalidation_0-auc:0.70832\n",
            "[30]\tvalidation_0-auc:0.709612\n",
            "[31]\tvalidation_0-auc:0.708287\n",
            "[32]\tvalidation_0-auc:0.709167\n",
            "[33]\tvalidation_0-auc:0.709033\n",
            "[34]\tvalidation_0-auc:0.706316\n",
            "[35]\tvalidation_0-auc:0.706795\n",
            "[36]\tvalidation_0-auc:0.706973\n",
            "[37]\tvalidation_0-auc:0.706984\n",
            "[38]\tvalidation_0-auc:0.707541\n",
            "[39]\tvalidation_0-auc:0.707407\n",
            "[40]\tvalidation_0-auc:0.705024\n",
            "[41]\tvalidation_0-auc:0.70645\n",
            "[42]\tvalidation_0-auc:0.708788\n",
            "[43]\tvalidation_0-auc:0.707897\n",
            "[44]\tvalidation_0-auc:0.707519\n",
            "[45]\tvalidation_0-auc:0.708231\n",
            "[46]\tvalidation_0-auc:0.709478\n",
            "[47]\tvalidation_0-auc:0.70224\n",
            "[48]\tvalidation_0-auc:0.701884\n",
            "[49]\tvalidation_0-auc:0.701149\n",
            "[50]\tvalidation_0-auc:0.700637\n",
            "[51]\tvalidation_0-auc:0.699011\n",
            "[52]\tvalidation_0-auc:0.699212\n",
            "[53]\tvalidation_0-auc:0.695448\n",
            "[54]\tvalidation_0-auc:0.697096\n",
            "[55]\tvalidation_0-auc:0.697719\n",
            "[56]\tvalidation_0-auc:0.698254\n",
            "[57]\tvalidation_0-auc:0.698187\n",
            "[58]\tvalidation_0-auc:0.699412\n",
            "[59]\tvalidation_0-auc:0.700102\n",
            "[60]\tvalidation_0-auc:0.700214\n",
            "[61]\tvalidation_0-auc:0.699368\n",
            "[62]\tvalidation_0-auc:0.700214\n",
            "[63]\tvalidation_0-auc:0.695002\n",
            "[64]\tvalidation_0-auc:0.696561\n",
            "[65]\tvalidation_0-auc:0.694691\n",
            "[66]\tvalidation_0-auc:0.693778\n",
            "[67]\tvalidation_0-auc:0.694023\n",
            "[68]\tvalidation_0-auc:0.694067\n",
            "[69]\tvalidation_0-auc:0.695069\n",
            "[70]\tvalidation_0-auc:0.696205\n",
            "[71]\tvalidation_0-auc:0.696895\n",
            "[72]\tvalidation_0-auc:0.695292\n",
            "[73]\tvalidation_0-auc:0.698677\n",
            "Stopping. Best iteration:\n",
            "[23]\tvalidation_0-auc:0.719578\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|     LSTM 0.2     | 0.7387755102040816 | 0.543859649122807  |  0.6503496503496503 | 0.5923566878980893 |\n",
            "|     GRU 0.2      | 0.7551020408163265 | 0.583941605839416  |  0.5594405594405595 | 0.5714285714285715 |\n",
            "|   XGBoost 0.2    | 0.736734693877551  | 0.5426829268292683 |  0.6223776223776224 | 0.5798045602605864 |\n",
            "|    Logreg 0.2    | 0.7448979591836735 |      0.55625       |  0.6223776223776224 | 0.5874587458745875 |\n",
            "|     SVM 0.2      | 0.7510204081632653 | 0.5686274509803921 |  0.6083916083916084 | 0.5878378378378378 |\n",
            "|  LSTM beta 0.2   | 0.7024070021881839 |        0.52        |  0.6363636363636364 | 0.5723270440251573 |\n",
            "|   GRU beta 0.2   | 0.7155361050328227 | 0.5474452554744526 |  0.5244755244755245 | 0.5357142857142857 |\n",
            "| XGBoost beta 0.2 | 0.6980306345733042 | 0.5229357798165137 |  0.3986013986013986 | 0.4523809523809524 |\n",
            "| logreg beta 0.2  | 0.7155361050328227 | 0.5488721804511278 |  0.5104895104895105 | 0.5289855072463768 |\n",
            "|   svm beta 0.2   | 0.6892778993435449 | 0.5041322314049587 | 0.42657342657342656 | 0.4621212121212121 |\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6950 - accuracy: 0.5054 - val_loss: 0.6874 - val_accuracy: 0.7306\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6699 - accuracy: 0.6007 - val_loss: 0.6197 - val_accuracy: 0.7286\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6381 - accuracy: 0.6369 - val_loss: 0.6039 - val_accuracy: 0.7184\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6262 - accuracy: 0.6772 - val_loss: 0.5836 - val_accuracy: 0.7306\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6251 - accuracy: 0.6631 - val_loss: 0.5855 - val_accuracy: 0.7408\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6758 - accuracy: 0.5698 - val_loss: 0.5984 - val_accuracy: 0.7184\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6257 - accuracy: 0.6591 - val_loss: 0.5636 - val_accuracy: 0.7286\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6062 - accuracy: 0.6826 - val_loss: 0.5771 - val_accuracy: 0.7163\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5942 - accuracy: 0.6852 - val_loss: 0.5467 - val_accuracy: 0.7367\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6002 - accuracy: 0.6859 - val_loss: 0.5687 - val_accuracy: 0.7367\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.773817\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.78442\n",
            "[2]\tvalidation_0-auc:0.786919\n",
            "[3]\tvalidation_0-auc:0.786374\n",
            "[4]\tvalidation_0-auc:0.789358\n",
            "[5]\tvalidation_0-auc:0.796587\n",
            "[6]\tvalidation_0-auc:0.796475\n",
            "[7]\tvalidation_0-auc:0.796267\n",
            "[8]\tvalidation_0-auc:0.796994\n",
            "[9]\tvalidation_0-auc:0.797262\n",
            "[10]\tvalidation_0-auc:0.797314\n",
            "[11]\tvalidation_0-auc:0.794503\n",
            "[12]\tvalidation_0-auc:0.795792\n",
            "[13]\tvalidation_0-auc:0.79478\n",
            "[14]\tvalidation_0-auc:0.793967\n",
            "[15]\tvalidation_0-auc:0.794512\n",
            "[16]\tvalidation_0-auc:0.799173\n",
            "[17]\tvalidation_0-auc:0.799848\n",
            "[18]\tvalidation_0-auc:0.799294\n",
            "[19]\tvalidation_0-auc:0.798499\n",
            "[20]\tvalidation_0-auc:0.796994\n",
            "[21]\tvalidation_0-auc:0.795091\n",
            "[22]\tvalidation_0-auc:0.794175\n",
            "[23]\tvalidation_0-auc:0.793846\n",
            "[24]\tvalidation_0-auc:0.793543\n",
            "[25]\tvalidation_0-auc:0.792506\n",
            "[26]\tvalidation_0-auc:0.791113\n",
            "[27]\tvalidation_0-auc:0.79056\n",
            "[28]\tvalidation_0-auc:0.790127\n",
            "[29]\tvalidation_0-auc:0.790084\n",
            "[30]\tvalidation_0-auc:0.788856\n",
            "[31]\tvalidation_0-auc:0.788908\n",
            "[32]\tvalidation_0-auc:0.788493\n",
            "[33]\tvalidation_0-auc:0.788285\n",
            "[34]\tvalidation_0-auc:0.789859\n",
            "[35]\tvalidation_0-auc:0.790274\n",
            "[36]\tvalidation_0-auc:0.789375\n",
            "[37]\tvalidation_0-auc:0.787905\n",
            "[38]\tvalidation_0-auc:0.787749\n",
            "[39]\tvalidation_0-auc:0.787801\n",
            "[40]\tvalidation_0-auc:0.787126\n",
            "[41]\tvalidation_0-auc:0.786936\n",
            "[42]\tvalidation_0-auc:0.787593\n",
            "[43]\tvalidation_0-auc:0.788354\n",
            "[44]\tvalidation_0-auc:0.788545\n",
            "[45]\tvalidation_0-auc:0.788216\n",
            "[46]\tvalidation_0-auc:0.788726\n",
            "[47]\tvalidation_0-auc:0.788986\n",
            "[48]\tvalidation_0-auc:0.788432\n",
            "[49]\tvalidation_0-auc:0.78838\n",
            "[50]\tvalidation_0-auc:0.788225\n",
            "[51]\tvalidation_0-auc:0.787567\n",
            "[52]\tvalidation_0-auc:0.787671\n",
            "[53]\tvalidation_0-auc:0.78883\n",
            "[54]\tvalidation_0-auc:0.789842\n",
            "[55]\tvalidation_0-auc:0.790188\n",
            "[56]\tvalidation_0-auc:0.789392\n",
            "[57]\tvalidation_0-auc:0.787784\n",
            "[58]\tvalidation_0-auc:0.787887\n",
            "[59]\tvalidation_0-auc:0.787853\n",
            "[60]\tvalidation_0-auc:0.785795\n",
            "[61]\tvalidation_0-auc:0.786089\n",
            "[62]\tvalidation_0-auc:0.786192\n",
            "[63]\tvalidation_0-auc:0.785042\n",
            "[64]\tvalidation_0-auc:0.785284\n",
            "[65]\tvalidation_0-auc:0.785648\n",
            "[66]\tvalidation_0-auc:0.784956\n",
            "[67]\tvalidation_0-auc:0.784247\n",
            "Stopping. Best iteration:\n",
            "[17]\tvalidation_0-auc:0.799848\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6889 - accuracy: 0.5415 - val_loss: 0.6728 - val_accuracy: 0.6893\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6564 - accuracy: 0.6294 - val_loss: 0.6626 - val_accuracy: 0.6521\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6105 - accuracy: 0.6843 - val_loss: 0.6602 - val_accuracy: 0.6849\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5826 - accuracy: 0.7001 - val_loss: 0.6216 - val_accuracy: 0.6893\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5955 - accuracy: 0.6740 - val_loss: 0.5912 - val_accuracy: 0.7046\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6567 - accuracy: 0.6239 - val_loss: 0.6151 - val_accuracy: 0.6389\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5952 - accuracy: 0.6898 - val_loss: 0.6176 - val_accuracy: 0.6674\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5759 - accuracy: 0.7200 - val_loss: 0.5953 - val_accuracy: 0.6718\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5657 - accuracy: 0.7268 - val_loss: 0.5950 - val_accuracy: 0.6871\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5638 - accuracy: 0.7248 - val_loss: 0.5928 - val_accuracy: 0.7002\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.692972\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.713038\n",
            "[2]\tvalidation_0-auc:0.737062\n",
            "[3]\tvalidation_0-auc:0.735619\n",
            "[4]\tvalidation_0-auc:0.740806\n",
            "[5]\tvalidation_0-auc:0.745788\n",
            "[6]\tvalidation_0-auc:0.745515\n",
            "[7]\tvalidation_0-auc:0.743506\n",
            "[8]\tvalidation_0-auc:0.741868\n",
            "[9]\tvalidation_0-auc:0.74101\n",
            "[10]\tvalidation_0-auc:0.748879\n",
            "[11]\tvalidation_0-auc:0.751258\n",
            "[12]\tvalidation_0-auc:0.758161\n",
            "[13]\tvalidation_0-auc:0.760082\n",
            "[14]\tvalidation_0-auc:0.764381\n",
            "[15]\tvalidation_0-auc:0.764898\n",
            "[16]\tvalidation_0-auc:0.766282\n",
            "[17]\tvalidation_0-auc:0.765259\n",
            "[18]\tvalidation_0-auc:0.761583\n",
            "[19]\tvalidation_0-auc:0.766361\n",
            "[20]\tvalidation_0-auc:0.765668\n",
            "[21]\tvalidation_0-auc:0.76716\n",
            "[22]\tvalidation_0-auc:0.768272\n",
            "[23]\tvalidation_0-auc:0.7656\n",
            "[24]\tvalidation_0-auc:0.765395\n",
            "[25]\tvalidation_0-auc:0.764381\n",
            "[26]\tvalidation_0-auc:0.765181\n",
            "[27]\tvalidation_0-auc:0.764537\n",
            "[28]\tvalidation_0-auc:0.765454\n",
            "[29]\tvalidation_0-auc:0.765464\n",
            "[30]\tvalidation_0-auc:0.762324\n",
            "[31]\tvalidation_0-auc:0.762695\n",
            "[32]\tvalidation_0-auc:0.762402\n",
            "[33]\tvalidation_0-auc:0.764879\n",
            "[34]\tvalidation_0-auc:0.762617\n",
            "[35]\tvalidation_0-auc:0.761837\n",
            "[36]\tvalidation_0-auc:0.762324\n",
            "[37]\tvalidation_0-auc:0.759068\n",
            "[38]\tvalidation_0-auc:0.759828\n",
            "[39]\tvalidation_0-auc:0.759301\n",
            "[40]\tvalidation_0-auc:0.756572\n",
            "[41]\tvalidation_0-auc:0.75663\n",
            "[42]\tvalidation_0-auc:0.756981\n",
            "[43]\tvalidation_0-auc:0.756532\n",
            "[44]\tvalidation_0-auc:0.75624\n",
            "[45]\tvalidation_0-auc:0.755177\n",
            "[46]\tvalidation_0-auc:0.753705\n",
            "[47]\tvalidation_0-auc:0.753919\n",
            "[48]\tvalidation_0-auc:0.754573\n",
            "[49]\tvalidation_0-auc:0.755236\n",
            "[50]\tvalidation_0-auc:0.754553\n",
            "[51]\tvalidation_0-auc:0.754729\n",
            "[52]\tvalidation_0-auc:0.754553\n",
            "[53]\tvalidation_0-auc:0.755948\n",
            "[54]\tvalidation_0-auc:0.753422\n",
            "[55]\tvalidation_0-auc:0.753305\n",
            "[56]\tvalidation_0-auc:0.75193\n",
            "[57]\tvalidation_0-auc:0.75158\n",
            "[58]\tvalidation_0-auc:0.751326\n",
            "[59]\tvalidation_0-auc:0.751384\n",
            "[60]\tvalidation_0-auc:0.752496\n",
            "[61]\tvalidation_0-auc:0.753452\n",
            "[62]\tvalidation_0-auc:0.753471\n",
            "[63]\tvalidation_0-auc:0.753783\n",
            "[64]\tvalidation_0-auc:0.750253\n",
            "[65]\tvalidation_0-auc:0.75119\n",
            "[66]\tvalidation_0-auc:0.750039\n",
            "[67]\tvalidation_0-auc:0.749591\n",
            "[68]\tvalidation_0-auc:0.750546\n",
            "[69]\tvalidation_0-auc:0.75115\n",
            "[70]\tvalidation_0-auc:0.749668\n",
            "[71]\tvalidation_0-auc:0.748128\n",
            "[72]\tvalidation_0-auc:0.749298\n",
            "Stopping. Best iteration:\n",
            "[22]\tvalidation_0-auc:0.768272\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |        Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.7408163265306122 | 0.8034188034188035 | 0.47474747474747475 | 0.5968253968253968 |\n",
            "|      GRU 0.15     | 0.736734693877551  | 0.6994219653179191 |  0.6111111111111112 | 0.652291105121294  |\n",
            "|    XGBoost 0.15   | 0.7387755102040816 | 0.6966292134831461 |  0.6262626262626263 | 0.6595744680851064 |\n",
            "|    Logreg 0.15    | 0.7306122448979592 | 0.675531914893617  |  0.6414141414141414 | 0.6580310880829016 |\n",
            "|      SVM 0.15     | 0.7387755102040816 | 0.6988636363636364 |  0.6212121212121212 | 0.6577540106951871 |\n",
            "|   LSTM beta 0.15  | 0.7045951859956237 | 0.7032258064516129 |  0.5505050505050505 | 0.6175637393767706 |\n",
            "|   GRU beta 0.15   | 0.700218818380744  | 0.6942675159235668 |  0.5505050505050505 | 0.6140845070422536 |\n",
            "| XGBoost beta 0.15 | 0.6608315098468271 | 0.6869565217391305 |  0.398989898989899  | 0.5047923322683706 |\n",
            "|  logreg beta 0.15 | 0.7177242888402626 | 0.7315436241610739 |  0.5505050505050505 | 0.6282420749279539 |\n",
            "|   svm beta 0.15   | 0.6761487964989059 | 0.6865671641791045 | 0.46464646464646464 | 0.5542168674698794 |\n",
            "+-------------------+--------------------+--------------------+---------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMuMAwCjY6We",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "926c39dc-8775-425b-e050-137ebc358b37"
      },
      "source": [
        "Result_cross.to_csv('RMD_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.631016</td>\n",
              "      <td>0.746939</td>\n",
              "      <td>0.655556</td>\n",
              "      <td>0.682081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.769388</td>\n",
              "      <td>0.650155</td>\n",
              "      <td>0.606936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.653409</td>\n",
              "      <td>0.757143</td>\n",
              "      <td>0.659026</td>\n",
              "      <td>0.664740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.634831</td>\n",
              "      <td>0.744898</td>\n",
              "      <td>0.643875</td>\n",
              "      <td>0.653179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.651685</td>\n",
              "      <td>0.757143</td>\n",
              "      <td>0.660969</td>\n",
              "      <td>0.670520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.525097</td>\n",
              "      <td>0.649891</td>\n",
              "      <td>0.629630</td>\n",
              "      <td>0.786127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.733042</td>\n",
              "      <td>0.582192</td>\n",
              "      <td>0.491329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.628571</td>\n",
              "      <td>0.719912</td>\n",
              "      <td>0.632184</td>\n",
              "      <td>0.635838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.630952</td>\n",
              "      <td>0.717724</td>\n",
              "      <td>0.621701</td>\n",
              "      <td>0.612717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.607955</td>\n",
              "      <td>0.704595</td>\n",
              "      <td>0.613181</td>\n",
              "      <td>0.618497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.543860</td>\n",
              "      <td>0.738776</td>\n",
              "      <td>0.592357</td>\n",
              "      <td>0.650350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.583942</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.559441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.542683</td>\n",
              "      <td>0.736735</td>\n",
              "      <td>0.579805</td>\n",
              "      <td>0.622378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.556250</td>\n",
              "      <td>0.744898</td>\n",
              "      <td>0.587459</td>\n",
              "      <td>0.622378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.568627</td>\n",
              "      <td>0.751020</td>\n",
              "      <td>0.587838</td>\n",
              "      <td>0.608392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.702407</td>\n",
              "      <td>0.572327</td>\n",
              "      <td>0.636364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.547445</td>\n",
              "      <td>0.715536</td>\n",
              "      <td>0.535714</td>\n",
              "      <td>0.524476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.522936</td>\n",
              "      <td>0.698031</td>\n",
              "      <td>0.452381</td>\n",
              "      <td>0.398601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.548872</td>\n",
              "      <td>0.715536</td>\n",
              "      <td>0.528986</td>\n",
              "      <td>0.510490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.504132</td>\n",
              "      <td>0.689278</td>\n",
              "      <td>0.462121</td>\n",
              "      <td>0.426573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.803419</td>\n",
              "      <td>0.740816</td>\n",
              "      <td>0.596825</td>\n",
              "      <td>0.474747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.699422</td>\n",
              "      <td>0.736735</td>\n",
              "      <td>0.652291</td>\n",
              "      <td>0.611111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.696629</td>\n",
              "      <td>0.738776</td>\n",
              "      <td>0.659574</td>\n",
              "      <td>0.626263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.675532</td>\n",
              "      <td>0.730612</td>\n",
              "      <td>0.658031</td>\n",
              "      <td>0.641414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.698864</td>\n",
              "      <td>0.738776</td>\n",
              "      <td>0.657754</td>\n",
              "      <td>0.621212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.703226</td>\n",
              "      <td>0.704595</td>\n",
              "      <td>0.617564</td>\n",
              "      <td>0.550505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.694268</td>\n",
              "      <td>0.700219</td>\n",
              "      <td>0.614085</td>\n",
              "      <td>0.550505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.686957</td>\n",
              "      <td>0.660832</td>\n",
              "      <td>0.504792</td>\n",
              "      <td>0.398990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.731544</td>\n",
              "      <td>0.717724</td>\n",
              "      <td>0.628242</td>\n",
              "      <td>0.550505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.686567</td>\n",
              "      <td>0.676149</td>\n",
              "      <td>0.554217</td>\n",
              "      <td>0.464646</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  STX  0.631016  0.746939  0.655556  0.682081\n",
              "1            GRU 0.1  STX  0.700000  0.769388  0.650155  0.606936\n",
              "2        XGBoost 0.1  STX  0.653409  0.757143  0.659026  0.664740\n",
              "3         Logreg 0.1  STX  0.634831  0.744898  0.643875  0.653179\n",
              "4            SVM 0.1  STX  0.651685  0.757143  0.660969  0.670520\n",
              "5      LSTM beta 0.1  STX  0.525097  0.649891  0.629630  0.786127\n",
              "6       GRU beta 0.1  STX  0.714286  0.733042  0.582192  0.491329\n",
              "7   XGBoost beta 0.1  STX  0.628571  0.719912  0.632184  0.635838\n",
              "8    logreg beta 0.1  STX  0.630952  0.717724  0.621701  0.612717\n",
              "9       svm beta 0.1  STX  0.607955  0.704595  0.613181  0.618497\n",
              "0           LSTM 0.2  STX  0.543860  0.738776  0.592357  0.650350\n",
              "1            GRU 0.2  STX  0.583942  0.755102  0.571429  0.559441\n",
              "2        XGBoost 0.2  STX  0.542683  0.736735  0.579805  0.622378\n",
              "3         Logreg 0.2  STX  0.556250  0.744898  0.587459  0.622378\n",
              "4            SVM 0.2  STX  0.568627  0.751020  0.587838  0.608392\n",
              "5      LSTM beta 0.2  STX  0.520000  0.702407  0.572327  0.636364\n",
              "6       GRU beta 0.2  STX  0.547445  0.715536  0.535714  0.524476\n",
              "7   XGBoost beta 0.2  STX  0.522936  0.698031  0.452381  0.398601\n",
              "8    logreg beta 0.2  STX  0.548872  0.715536  0.528986  0.510490\n",
              "9       svm beta 0.2  STX  0.504132  0.689278  0.462121  0.426573\n",
              "0          LSTM 0.15  STX  0.803419  0.740816  0.596825  0.474747\n",
              "1           GRU 0.15  STX  0.699422  0.736735  0.652291  0.611111\n",
              "2       XGBoost 0.15  STX  0.696629  0.738776  0.659574  0.626263\n",
              "3        Logreg 0.15  STX  0.675532  0.730612  0.658031  0.641414\n",
              "4           SVM 0.15  STX  0.698864  0.738776  0.657754  0.621212\n",
              "5     LSTM beta 0.15  STX  0.703226  0.704595  0.617564  0.550505\n",
              "6      GRU beta 0.15  STX  0.694268  0.700219  0.614085  0.550505\n",
              "7  XGBoost beta 0.15  STX  0.686957  0.660832  0.504792  0.398990\n",
              "8   logreg beta 0.15  STX  0.731544  0.717724  0.628242  0.550505\n",
              "9      svm beta 0.15  STX  0.686567  0.676149  0.554217  0.464646"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs3mNVC3Y6We"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZggcPhqY6We"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWbEskqKY6We",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7771bfba-2641-4fe2-cc40-0199da8156f1"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"STX\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 17ms/step - loss: 0.6957 - accuracy: 0.5141 - val_loss: 0.6664 - val_accuracy: 0.6469\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6691 - accuracy: 0.5893 - val_loss: 0.6234 - val_accuracy: 0.7327\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6227 - accuracy: 0.6772 - val_loss: 0.5741 - val_accuracy: 0.7041\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5941 - accuracy: 0.6886 - val_loss: 0.6018 - val_accuracy: 0.6857\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5832 - accuracy: 0.7034 - val_loss: 0.5182 - val_accuracy: 0.7551\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6735 - accuracy: 0.5658 - val_loss: 0.5501 - val_accuracy: 0.7388\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5900 - accuracy: 0.6899 - val_loss: 0.5157 - val_accuracy: 0.7429\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5769 - accuracy: 0.7047 - val_loss: 0.4862 - val_accuracy: 0.7673\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5610 - accuracy: 0.7188 - val_loss: 0.4803 - val_accuracy: 0.7653\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5624 - accuracy: 0.7201 - val_loss: 0.4879 - val_accuracy: 0.7653\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.816615\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.821292\n",
            "[2]\tvalidation_0-auc:0.81884\n",
            "[3]\tvalidation_0-auc:0.820764\n",
            "[4]\tvalidation_0-auc:0.81925\n",
            "[5]\tvalidation_0-auc:0.820891\n",
            "[6]\tvalidation_0-auc:0.820372\n",
            "[7]\tvalidation_0-auc:0.821292\n",
            "[8]\tvalidation_0-auc:0.821055\n",
            "[9]\tvalidation_0-auc:0.821694\n",
            "[10]\tvalidation_0-auc:0.823727\n",
            "[11]\tvalidation_0-auc:0.822879\n",
            "[12]\tvalidation_0-auc:0.823936\n",
            "[13]\tvalidation_0-auc:0.82824\n",
            "[14]\tvalidation_0-auc:0.827902\n",
            "[15]\tvalidation_0-auc:0.828112\n",
            "[16]\tvalidation_0-auc:0.828687\n",
            "[17]\tvalidation_0-auc:0.82865\n",
            "[18]\tvalidation_0-auc:0.828677\n",
            "[19]\tvalidation_0-auc:0.829252\n",
            "[20]\tvalidation_0-auc:0.829288\n",
            "[21]\tvalidation_0-auc:0.828924\n",
            "[22]\tvalidation_0-auc:0.828933\n",
            "[23]\tvalidation_0-auc:0.829398\n",
            "[24]\tvalidation_0-auc:0.829398\n",
            "[25]\tvalidation_0-auc:0.830483\n",
            "[26]\tvalidation_0-auc:0.829808\n",
            "[27]\tvalidation_0-auc:0.8302\n",
            "[28]\tvalidation_0-auc:0.830319\n",
            "[29]\tvalidation_0-auc:0.830091\n",
            "[30]\tvalidation_0-auc:0.830765\n",
            "[31]\tvalidation_0-auc:0.830847\n",
            "[32]\tvalidation_0-auc:0.830501\n",
            "[33]\tvalidation_0-auc:0.830656\n",
            "[34]\tvalidation_0-auc:0.830638\n",
            "[35]\tvalidation_0-auc:0.831221\n",
            "[36]\tvalidation_0-auc:0.832096\n",
            "[37]\tvalidation_0-auc:0.832251\n",
            "[38]\tvalidation_0-auc:0.831923\n",
            "[39]\tvalidation_0-auc:0.831704\n",
            "[40]\tvalidation_0-auc:0.831832\n",
            "[41]\tvalidation_0-auc:0.831887\n",
            "[42]\tvalidation_0-auc:0.83278\n",
            "[43]\tvalidation_0-auc:0.832561\n",
            "[44]\tvalidation_0-auc:0.832343\n",
            "[45]\tvalidation_0-auc:0.832124\n",
            "[46]\tvalidation_0-auc:0.831978\n",
            "[47]\tvalidation_0-auc:0.832169\n",
            "[48]\tvalidation_0-auc:0.831914\n",
            "[49]\tvalidation_0-auc:0.830729\n",
            "[50]\tvalidation_0-auc:0.831312\n",
            "[51]\tvalidation_0-auc:0.830236\n",
            "[52]\tvalidation_0-auc:0.830474\n",
            "[53]\tvalidation_0-auc:0.830027\n",
            "[54]\tvalidation_0-auc:0.830355\n",
            "[55]\tvalidation_0-auc:0.830391\n",
            "[56]\tvalidation_0-auc:0.830446\n",
            "[57]\tvalidation_0-auc:0.830337\n",
            "[58]\tvalidation_0-auc:0.830246\n",
            "[59]\tvalidation_0-auc:0.829972\n",
            "[60]\tvalidation_0-auc:0.829753\n",
            "[61]\tvalidation_0-auc:0.830629\n",
            "[62]\tvalidation_0-auc:0.830483\n",
            "[63]\tvalidation_0-auc:0.83061\n",
            "[64]\tvalidation_0-auc:0.831048\n",
            "[65]\tvalidation_0-auc:0.830556\n",
            "[66]\tvalidation_0-auc:0.830227\n",
            "[67]\tvalidation_0-auc:0.8303\n",
            "[68]\tvalidation_0-auc:0.829635\n",
            "[69]\tvalidation_0-auc:0.829544\n",
            "[70]\tvalidation_0-auc:0.829507\n",
            "[71]\tvalidation_0-auc:0.829489\n",
            "[72]\tvalidation_0-auc:0.829416\n",
            "[73]\tvalidation_0-auc:0.829379\n",
            "[74]\tvalidation_0-auc:0.829252\n",
            "[75]\tvalidation_0-auc:0.829033\n",
            "[76]\tvalidation_0-auc:0.829471\n",
            "[77]\tvalidation_0-auc:0.829762\n",
            "[78]\tvalidation_0-auc:0.82958\n",
            "[79]\tvalidation_0-auc:0.829197\n",
            "[80]\tvalidation_0-auc:0.828978\n",
            "[81]\tvalidation_0-auc:0.828869\n",
            "[82]\tvalidation_0-auc:0.829434\n",
            "[83]\tvalidation_0-auc:0.829106\n",
            "[84]\tvalidation_0-auc:0.828431\n",
            "[85]\tvalidation_0-auc:0.828413\n",
            "[86]\tvalidation_0-auc:0.827866\n",
            "[87]\tvalidation_0-auc:0.828067\n",
            "[88]\tvalidation_0-auc:0.827921\n",
            "[89]\tvalidation_0-auc:0.827757\n",
            "[90]\tvalidation_0-auc:0.827793\n",
            "[91]\tvalidation_0-auc:0.827793\n",
            "[92]\tvalidation_0-auc:0.827629\n",
            "Stopping. Best iteration:\n",
            "[42]\tvalidation_0-auc:0.83278\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6949 - accuracy: 0.5086 - val_loss: 0.6839 - val_accuracy: 0.6214\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6919 - accuracy: 0.5175 - val_loss: 0.6529 - val_accuracy: 0.6214\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6658 - accuracy: 0.5951 - val_loss: 0.6428 - val_accuracy: 0.6980\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6184 - accuracy: 0.6726 - val_loss: 0.5684 - val_accuracy: 0.7309\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.5904 - accuracy: 0.6960 - val_loss: 0.5548 - val_accuracy: 0.7112\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6937 - accuracy: 0.5285 - val_loss: 0.6804 - val_accuracy: 0.6302\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6647 - accuracy: 0.5957 - val_loss: 0.5841 - val_accuracy: 0.7352\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6123 - accuracy: 0.6925 - val_loss: 0.6173 - val_accuracy: 0.6718\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5892 - accuracy: 0.7213 - val_loss: 0.5575 - val_accuracy: 0.7046\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5827 - accuracy: 0.6911 - val_loss: 0.5342 - val_accuracy: 0.7090\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.740118\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.737951\n",
            "[2]\tvalidation_0-auc:0.727998\n",
            "[3]\tvalidation_0-auc:0.742958\n",
            "[4]\tvalidation_0-auc:0.739142\n",
            "[5]\tvalidation_0-auc:0.751272\n",
            "[6]\tvalidation_0-auc:0.755902\n",
            "[7]\tvalidation_0-auc:0.772867\n",
            "[8]\tvalidation_0-auc:0.756198\n",
            "[9]\tvalidation_0-auc:0.761245\n",
            "[10]\tvalidation_0-auc:0.765937\n",
            "[11]\tvalidation_0-auc:0.767504\n",
            "[12]\tvalidation_0-auc:0.766792\n",
            "[13]\tvalidation_0-auc:0.771463\n",
            "[14]\tvalidation_0-auc:0.769366\n",
            "[15]\tvalidation_0-auc:0.772989\n",
            "[16]\tvalidation_0-auc:0.774414\n",
            "[17]\tvalidation_0-auc:0.777508\n",
            "[18]\tvalidation_0-auc:0.779319\n",
            "[19]\tvalidation_0-auc:0.781293\n",
            "[20]\tvalidation_0-auc:0.786667\n",
            "[21]\tvalidation_0-auc:0.787664\n",
            "[22]\tvalidation_0-auc:0.786392\n",
            "[23]\tvalidation_0-auc:0.788712\n",
            "[24]\tvalidation_0-auc:0.78973\n",
            "[25]\tvalidation_0-auc:0.786717\n",
            "[26]\tvalidation_0-auc:0.79089\n",
            "[27]\tvalidation_0-auc:0.793098\n",
            "[28]\tvalidation_0-auc:0.795144\n",
            "[29]\tvalidation_0-auc:0.792864\n",
            "[30]\tvalidation_0-auc:0.795408\n",
            "[31]\tvalidation_0-auc:0.796528\n",
            "[32]\tvalidation_0-auc:0.795815\n",
            "[33]\tvalidation_0-auc:0.794238\n",
            "[34]\tvalidation_0-auc:0.791582\n",
            "[35]\tvalidation_0-auc:0.788071\n",
            "[36]\tvalidation_0-auc:0.789251\n",
            "[37]\tvalidation_0-auc:0.786962\n",
            "[38]\tvalidation_0-auc:0.785913\n",
            "[39]\tvalidation_0-auc:0.785771\n",
            "[40]\tvalidation_0-auc:0.785567\n",
            "[41]\tvalidation_0-auc:0.787257\n",
            "[42]\tvalidation_0-auc:0.78454\n",
            "[43]\tvalidation_0-auc:0.785608\n",
            "[44]\tvalidation_0-auc:0.785466\n",
            "[45]\tvalidation_0-auc:0.784061\n",
            "[46]\tvalidation_0-auc:0.784713\n",
            "[47]\tvalidation_0-auc:0.786524\n",
            "[48]\tvalidation_0-auc:0.789302\n",
            "[49]\tvalidation_0-auc:0.789282\n",
            "[50]\tvalidation_0-auc:0.787511\n",
            "[51]\tvalidation_0-auc:0.785842\n",
            "[52]\tvalidation_0-auc:0.784845\n",
            "[53]\tvalidation_0-auc:0.784519\n",
            "[54]\tvalidation_0-auc:0.784967\n",
            "[55]\tvalidation_0-auc:0.784764\n",
            "[56]\tvalidation_0-auc:0.78515\n",
            "[57]\tvalidation_0-auc:0.783664\n",
            "[58]\tvalidation_0-auc:0.782229\n",
            "[59]\tvalidation_0-auc:0.782616\n",
            "[60]\tvalidation_0-auc:0.782494\n",
            "[61]\tvalidation_0-auc:0.782698\n",
            "[62]\tvalidation_0-auc:0.782759\n",
            "[63]\tvalidation_0-auc:0.782474\n",
            "[64]\tvalidation_0-auc:0.779604\n",
            "[65]\tvalidation_0-auc:0.77995\n",
            "[66]\tvalidation_0-auc:0.778118\n",
            "[67]\tvalidation_0-auc:0.777701\n",
            "[68]\tvalidation_0-auc:0.777599\n",
            "[69]\tvalidation_0-auc:0.778596\n",
            "[70]\tvalidation_0-auc:0.776174\n",
            "[71]\tvalidation_0-auc:0.774322\n",
            "[72]\tvalidation_0-auc:0.775482\n",
            "[73]\tvalidation_0-auc:0.776195\n",
            "[74]\tvalidation_0-auc:0.776459\n",
            "[75]\tvalidation_0-auc:0.777212\n",
            "[76]\tvalidation_0-auc:0.776622\n",
            "[77]\tvalidation_0-auc:0.775075\n",
            "[78]\tvalidation_0-auc:0.774628\n",
            "[79]\tvalidation_0-auc:0.774628\n",
            "[80]\tvalidation_0-auc:0.773101\n",
            "[81]\tvalidation_0-auc:0.771005\n",
            "Stopping. Best iteration:\n",
            "[31]\tvalidation_0-auc:0.796528\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.7551020408163265 | 0.6432432432432432 | 0.6878612716763006 | 0.664804469273743  |\n",
            "|     GRU 0.1      | 0.7653061224489796 | 0.6858974358974359 | 0.6184971098265896 | 0.6504559270516718 |\n",
            "|   XGBoost 0.1    | 0.7571428571428571 | 0.6534090909090909 | 0.6647398843930635 | 0.6590257879656161 |\n",
            "|    Logreg 0.1    | 0.7448979591836735 | 0.6348314606741573 | 0.653179190751445  | 0.6438746438746439 |\n",
            "|     SVM 0.1      | 0.7571428571428571 | 0.651685393258427  | 0.6705202312138728 | 0.6609686609686609 |\n",
            "|  LSTM beta 0.1   | 0.7111597374179431 | 0.6518518518518519 | 0.5086705202312138 | 0.5714285714285715 |\n",
            "|   GRU beta 0.1   | 0.7089715536105032 | 0.631578947368421  | 0.5549132947976878 | 0.5907692307692307 |\n",
            "| XGBoost beta 0.1 | 0.7199124726477024 | 0.6285714285714286 | 0.6358381502890174 | 0.632183908045977  |\n",
            "| logreg beta 0.1  | 0.7177242888402626 | 0.6309523809523809 | 0.6127167630057804 | 0.6217008797653959 |\n",
            "|   svm beta 0.1   | 0.7045951859956237 | 0.6079545454545454 | 0.6184971098265896 | 0.6131805157593122 |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6928 - accuracy: 0.5282 - val_loss: 0.7995 - val_accuracy: 0.2918\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6683 - accuracy: 0.5906 - val_loss: 0.5728 - val_accuracy: 0.7429\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6392 - accuracy: 0.6470 - val_loss: 0.5614 - val_accuracy: 0.7429\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6395 - accuracy: 0.6450 - val_loss: 0.5953 - val_accuracy: 0.7388\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6392 - accuracy: 0.6517 - val_loss: 0.6217 - val_accuracy: 0.7163\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6897 - accuracy: 0.5503 - val_loss: 0.6095 - val_accuracy: 0.7408\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6394 - accuracy: 0.6255 - val_loss: 0.5749 - val_accuracy: 0.7143\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6125 - accuracy: 0.6725 - val_loss: 0.5639 - val_accuracy: 0.7286\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6170 - accuracy: 0.6738 - val_loss: 0.5252 - val_accuracy: 0.7776\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6141 - accuracy: 0.6705 - val_loss: 0.6021 - val_accuracy: 0.7020\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.773453\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.790804\n",
            "[2]\tvalidation_0-auc:0.796306\n",
            "[3]\tvalidation_0-auc:0.793243\n",
            "[4]\tvalidation_0-auc:0.793333\n",
            "[5]\tvalidation_0-auc:0.797142\n",
            "[6]\tvalidation_0-auc:0.793716\n",
            "[7]\tvalidation_0-auc:0.79421\n",
            "[8]\tvalidation_0-auc:0.794281\n",
            "[9]\tvalidation_0-auc:0.791147\n",
            "[10]\tvalidation_0-auc:0.792104\n",
            "[11]\tvalidation_0-auc:0.793716\n",
            "[12]\tvalidation_0-auc:0.792386\n",
            "[13]\tvalidation_0-auc:0.792034\n",
            "[14]\tvalidation_0-auc:0.793616\n",
            "[15]\tvalidation_0-auc:0.792598\n",
            "[16]\tvalidation_0-auc:0.79156\n",
            "[17]\tvalidation_0-auc:0.792578\n",
            "[18]\tvalidation_0-auc:0.793041\n",
            "[19]\tvalidation_0-auc:0.793948\n",
            "[20]\tvalidation_0-auc:0.793696\n",
            "[21]\tvalidation_0-auc:0.79417\n",
            "[22]\tvalidation_0-auc:0.793908\n",
            "[23]\tvalidation_0-auc:0.792054\n",
            "[24]\tvalidation_0-auc:0.792527\n",
            "[25]\tvalidation_0-auc:0.79283\n",
            "[26]\tvalidation_0-auc:0.792487\n",
            "[27]\tvalidation_0-auc:0.791782\n",
            "[28]\tvalidation_0-auc:0.791409\n",
            "[29]\tvalidation_0-auc:0.791288\n",
            "[30]\tvalidation_0-auc:0.790814\n",
            "[31]\tvalidation_0-auc:0.790552\n",
            "[32]\tvalidation_0-auc:0.788376\n",
            "[33]\tvalidation_0-auc:0.789797\n",
            "[34]\tvalidation_0-auc:0.789192\n",
            "[35]\tvalidation_0-auc:0.78895\n",
            "[36]\tvalidation_0-auc:0.789293\n",
            "[37]\tvalidation_0-auc:0.79026\n",
            "[38]\tvalidation_0-auc:0.792678\n",
            "[39]\tvalidation_0-auc:0.792628\n",
            "[40]\tvalidation_0-auc:0.791107\n",
            "[41]\tvalidation_0-auc:0.792054\n",
            "[42]\tvalidation_0-auc:0.792457\n",
            "[43]\tvalidation_0-auc:0.792477\n",
            "[44]\tvalidation_0-auc:0.791973\n",
            "[45]\tvalidation_0-auc:0.79157\n",
            "[46]\tvalidation_0-auc:0.791379\n",
            "[47]\tvalidation_0-auc:0.791016\n",
            "[48]\tvalidation_0-auc:0.790734\n",
            "[49]\tvalidation_0-auc:0.790925\n",
            "[50]\tvalidation_0-auc:0.791046\n",
            "[51]\tvalidation_0-auc:0.791268\n",
            "[52]\tvalidation_0-auc:0.790885\n",
            "[53]\tvalidation_0-auc:0.79151\n",
            "[54]\tvalidation_0-auc:0.792527\n",
            "[55]\tvalidation_0-auc:0.792427\n",
            "Stopping. Best iteration:\n",
            "[5]\tvalidation_0-auc:0.797142\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6637 - accuracy: 0.6012 - val_loss: 0.6522 - val_accuracy: 0.6346\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5949 - accuracy: 0.6987 - val_loss: 0.6196 - val_accuracy: 0.6718\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5719 - accuracy: 0.7159 - val_loss: 0.6072 - val_accuracy: 0.6915\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5560 - accuracy: 0.7303 - val_loss: 0.6342 - val_accuracy: 0.6871\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5393 - accuracy: 0.7529 - val_loss: 0.6038 - val_accuracy: 0.7002\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6197 - accuracy: 0.6685 - val_loss: 0.5771 - val_accuracy: 0.7199\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5539 - accuracy: 0.7289 - val_loss: 0.6460 - val_accuracy: 0.7068\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5501 - accuracy: 0.7522 - val_loss: 0.5468 - val_accuracy: 0.7068\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5527 - accuracy: 0.7378 - val_loss: 0.5793 - val_accuracy: 0.7024\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5450 - accuracy: 0.7378 - val_loss: 0.5774 - val_accuracy: 0.6980\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.660717\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.672075\n",
            "[2]\tvalidation_0-auc:0.671741\n",
            "[3]\tvalidation_0-auc:0.677832\n",
            "[4]\tvalidation_0-auc:0.678912\n",
            "[5]\tvalidation_0-auc:0.678845\n",
            "[6]\tvalidation_0-auc:0.681484\n",
            "[7]\tvalidation_0-auc:0.67968\n",
            "[8]\tvalidation_0-auc:0.676651\n",
            "[9]\tvalidation_0-auc:0.703231\n",
            "[10]\tvalidation_0-auc:0.677375\n",
            "[11]\tvalidation_0-auc:0.69125\n",
            "[12]\tvalidation_0-auc:0.694802\n",
            "[13]\tvalidation_0-auc:0.695136\n",
            "[14]\tvalidation_0-auc:0.711483\n",
            "[15]\tvalidation_0-auc:0.706082\n",
            "[16]\tvalidation_0-auc:0.711995\n",
            "[17]\tvalidation_0-auc:0.715959\n",
            "[18]\tvalidation_0-auc:0.710425\n",
            "[19]\tvalidation_0-auc:0.713109\n",
            "[20]\tvalidation_0-auc:0.713198\n",
            "[21]\tvalidation_0-auc:0.715191\n",
            "[22]\tvalidation_0-auc:0.715814\n",
            "[23]\tvalidation_0-auc:0.719578\n",
            "[24]\tvalidation_0-auc:0.71724\n",
            "[25]\tvalidation_0-auc:0.718977\n",
            "[26]\tvalidation_0-auc:0.709467\n",
            "[27]\tvalidation_0-auc:0.706361\n",
            "[28]\tvalidation_0-auc:0.708499\n",
            "[29]\tvalidation_0-auc:0.70832\n",
            "[30]\tvalidation_0-auc:0.709612\n",
            "[31]\tvalidation_0-auc:0.708287\n",
            "[32]\tvalidation_0-auc:0.709167\n",
            "[33]\tvalidation_0-auc:0.709033\n",
            "[34]\tvalidation_0-auc:0.706316\n",
            "[35]\tvalidation_0-auc:0.706795\n",
            "[36]\tvalidation_0-auc:0.706973\n",
            "[37]\tvalidation_0-auc:0.706984\n",
            "[38]\tvalidation_0-auc:0.707541\n",
            "[39]\tvalidation_0-auc:0.707407\n",
            "[40]\tvalidation_0-auc:0.705024\n",
            "[41]\tvalidation_0-auc:0.70645\n",
            "[42]\tvalidation_0-auc:0.708788\n",
            "[43]\tvalidation_0-auc:0.707897\n",
            "[44]\tvalidation_0-auc:0.707519\n",
            "[45]\tvalidation_0-auc:0.708231\n",
            "[46]\tvalidation_0-auc:0.709478\n",
            "[47]\tvalidation_0-auc:0.70224\n",
            "[48]\tvalidation_0-auc:0.701884\n",
            "[49]\tvalidation_0-auc:0.701149\n",
            "[50]\tvalidation_0-auc:0.700637\n",
            "[51]\tvalidation_0-auc:0.699011\n",
            "[52]\tvalidation_0-auc:0.699212\n",
            "[53]\tvalidation_0-auc:0.695448\n",
            "[54]\tvalidation_0-auc:0.697096\n",
            "[55]\tvalidation_0-auc:0.697719\n",
            "[56]\tvalidation_0-auc:0.698254\n",
            "[57]\tvalidation_0-auc:0.698187\n",
            "[58]\tvalidation_0-auc:0.699412\n",
            "[59]\tvalidation_0-auc:0.700102\n",
            "[60]\tvalidation_0-auc:0.700214\n",
            "[61]\tvalidation_0-auc:0.699368\n",
            "[62]\tvalidation_0-auc:0.700214\n",
            "[63]\tvalidation_0-auc:0.695002\n",
            "[64]\tvalidation_0-auc:0.696561\n",
            "[65]\tvalidation_0-auc:0.694691\n",
            "[66]\tvalidation_0-auc:0.693778\n",
            "[67]\tvalidation_0-auc:0.694023\n",
            "[68]\tvalidation_0-auc:0.694067\n",
            "[69]\tvalidation_0-auc:0.695069\n",
            "[70]\tvalidation_0-auc:0.696205\n",
            "[71]\tvalidation_0-auc:0.696895\n",
            "[72]\tvalidation_0-auc:0.695292\n",
            "[73]\tvalidation_0-auc:0.698677\n",
            "Stopping. Best iteration:\n",
            "[23]\tvalidation_0-auc:0.719578\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.2     | 0.7163265306122449 | 0.5108695652173914 |  0.6573426573426573 |  0.5749235474006117 |\n",
            "|     GRU 0.2      | 0.7020408163265306 | 0.4928909952606635 |  0.7272727272727273 |  0.5875706214689266 |\n",
            "|   XGBoost 0.2    | 0.736734693877551  | 0.5426829268292683 |  0.6223776223776224 |  0.5798045602605864 |\n",
            "|    Logreg 0.2    | 0.7448979591836735 |      0.55625       |  0.6223776223776224 |  0.5874587458745875 |\n",
            "|     SVM 0.2      | 0.7510204081632653 | 0.5686274509803921 |  0.6083916083916084 |  0.5878378378378378 |\n",
            "|  LSTM beta 0.2   | 0.700218818380744  | 0.5214285714285715 |  0.5104895104895105 |  0.5159010600706714 |\n",
            "|   GRU beta 0.2   | 0.6980306345733042 | 0.5193798449612403 | 0.46853146853146854 | 0.49264705882352944 |\n",
            "| XGBoost beta 0.2 | 0.6980306345733042 | 0.5229357798165137 |  0.3986013986013986 |  0.4523809523809524 |\n",
            "| logreg beta 0.2  | 0.7155361050328227 | 0.5488721804511278 |  0.5104895104895105 |  0.5289855072463768 |\n",
            "|   svm beta 0.2   | 0.6892778993435449 | 0.5041322314049587 | 0.42657342657342656 |  0.4621212121212121 |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 4s 15ms/step - loss: 0.6950 - accuracy: 0.5081 - val_loss: 0.6734 - val_accuracy: 0.5959\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6657 - accuracy: 0.6121 - val_loss: 0.6244 - val_accuracy: 0.7306\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6395 - accuracy: 0.6456 - val_loss: 0.6260 - val_accuracy: 0.6776\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6342 - accuracy: 0.6517 - val_loss: 0.6059 - val_accuracy: 0.7102\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6182 - accuracy: 0.6591 - val_loss: 0.5923 - val_accuracy: 0.7327\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6841 - accuracy: 0.5383 - val_loss: 0.6398 - val_accuracy: 0.7082\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6403 - accuracy: 0.6315 - val_loss: 0.5805 - val_accuracy: 0.7245\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6045 - accuracy: 0.6711 - val_loss: 0.5801 - val_accuracy: 0.7143\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6085 - accuracy: 0.6745 - val_loss: 0.5733 - val_accuracy: 0.7327\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5904 - accuracy: 0.6812 - val_loss: 0.5566 - val_accuracy: 0.7224\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.773817\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.78442\n",
            "[2]\tvalidation_0-auc:0.786919\n",
            "[3]\tvalidation_0-auc:0.786374\n",
            "[4]\tvalidation_0-auc:0.789358\n",
            "[5]\tvalidation_0-auc:0.796587\n",
            "[6]\tvalidation_0-auc:0.796475\n",
            "[7]\tvalidation_0-auc:0.796267\n",
            "[8]\tvalidation_0-auc:0.796994\n",
            "[9]\tvalidation_0-auc:0.797262\n",
            "[10]\tvalidation_0-auc:0.797314\n",
            "[11]\tvalidation_0-auc:0.794503\n",
            "[12]\tvalidation_0-auc:0.795792\n",
            "[13]\tvalidation_0-auc:0.79478\n",
            "[14]\tvalidation_0-auc:0.793967\n",
            "[15]\tvalidation_0-auc:0.794512\n",
            "[16]\tvalidation_0-auc:0.799173\n",
            "[17]\tvalidation_0-auc:0.799848\n",
            "[18]\tvalidation_0-auc:0.799294\n",
            "[19]\tvalidation_0-auc:0.798499\n",
            "[20]\tvalidation_0-auc:0.796994\n",
            "[21]\tvalidation_0-auc:0.795091\n",
            "[22]\tvalidation_0-auc:0.794175\n",
            "[23]\tvalidation_0-auc:0.793846\n",
            "[24]\tvalidation_0-auc:0.793543\n",
            "[25]\tvalidation_0-auc:0.792506\n",
            "[26]\tvalidation_0-auc:0.791113\n",
            "[27]\tvalidation_0-auc:0.79056\n",
            "[28]\tvalidation_0-auc:0.790127\n",
            "[29]\tvalidation_0-auc:0.790084\n",
            "[30]\tvalidation_0-auc:0.788856\n",
            "[31]\tvalidation_0-auc:0.788908\n",
            "[32]\tvalidation_0-auc:0.788493\n",
            "[33]\tvalidation_0-auc:0.788285\n",
            "[34]\tvalidation_0-auc:0.789859\n",
            "[35]\tvalidation_0-auc:0.790274\n",
            "[36]\tvalidation_0-auc:0.789375\n",
            "[37]\tvalidation_0-auc:0.787905\n",
            "[38]\tvalidation_0-auc:0.787749\n",
            "[39]\tvalidation_0-auc:0.787801\n",
            "[40]\tvalidation_0-auc:0.787126\n",
            "[41]\tvalidation_0-auc:0.786936\n",
            "[42]\tvalidation_0-auc:0.787593\n",
            "[43]\tvalidation_0-auc:0.788354\n",
            "[44]\tvalidation_0-auc:0.788545\n",
            "[45]\tvalidation_0-auc:0.788216\n",
            "[46]\tvalidation_0-auc:0.788726\n",
            "[47]\tvalidation_0-auc:0.788986\n",
            "[48]\tvalidation_0-auc:0.788432\n",
            "[49]\tvalidation_0-auc:0.78838\n",
            "[50]\tvalidation_0-auc:0.788225\n",
            "[51]\tvalidation_0-auc:0.787567\n",
            "[52]\tvalidation_0-auc:0.787671\n",
            "[53]\tvalidation_0-auc:0.78883\n",
            "[54]\tvalidation_0-auc:0.789842\n",
            "[55]\tvalidation_0-auc:0.790188\n",
            "[56]\tvalidation_0-auc:0.789392\n",
            "[57]\tvalidation_0-auc:0.787784\n",
            "[58]\tvalidation_0-auc:0.787887\n",
            "[59]\tvalidation_0-auc:0.787853\n",
            "[60]\tvalidation_0-auc:0.785795\n",
            "[61]\tvalidation_0-auc:0.786089\n",
            "[62]\tvalidation_0-auc:0.786192\n",
            "[63]\tvalidation_0-auc:0.785042\n",
            "[64]\tvalidation_0-auc:0.785284\n",
            "[65]\tvalidation_0-auc:0.785648\n",
            "[66]\tvalidation_0-auc:0.784956\n",
            "[67]\tvalidation_0-auc:0.784247\n",
            "Stopping. Best iteration:\n",
            "[17]\tvalidation_0-auc:0.799848\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6905 - accuracy: 0.5498 - val_loss: 0.6675 - val_accuracy: 0.5952\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6608 - accuracy: 0.6349 - val_loss: 0.6786 - val_accuracy: 0.6280\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6285 - accuracy: 0.6630 - val_loss: 0.6411 - val_accuracy: 0.6915\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5909 - accuracy: 0.7021 - val_loss: 0.6431 - val_accuracy: 0.6674\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5787 - accuracy: 0.7014 - val_loss: 0.6130 - val_accuracy: 0.7177\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6439 - accuracy: 0.6376 - val_loss: 0.6121 - val_accuracy: 0.7309\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5919 - accuracy: 0.7035 - val_loss: 0.5994 - val_accuracy: 0.7243\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5789 - accuracy: 0.7159 - val_loss: 0.6032 - val_accuracy: 0.7243\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5676 - accuracy: 0.7111 - val_loss: 0.6019 - val_accuracy: 0.7265\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5729 - accuracy: 0.7268 - val_loss: 0.6334 - val_accuracy: 0.6455\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.692972\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.713038\n",
            "[2]\tvalidation_0-auc:0.737062\n",
            "[3]\tvalidation_0-auc:0.735619\n",
            "[4]\tvalidation_0-auc:0.740806\n",
            "[5]\tvalidation_0-auc:0.745788\n",
            "[6]\tvalidation_0-auc:0.745515\n",
            "[7]\tvalidation_0-auc:0.743506\n",
            "[8]\tvalidation_0-auc:0.741868\n",
            "[9]\tvalidation_0-auc:0.74101\n",
            "[10]\tvalidation_0-auc:0.748879\n",
            "[11]\tvalidation_0-auc:0.751258\n",
            "[12]\tvalidation_0-auc:0.758161\n",
            "[13]\tvalidation_0-auc:0.760082\n",
            "[14]\tvalidation_0-auc:0.764381\n",
            "[15]\tvalidation_0-auc:0.764898\n",
            "[16]\tvalidation_0-auc:0.766282\n",
            "[17]\tvalidation_0-auc:0.765259\n",
            "[18]\tvalidation_0-auc:0.761583\n",
            "[19]\tvalidation_0-auc:0.766361\n",
            "[20]\tvalidation_0-auc:0.765668\n",
            "[21]\tvalidation_0-auc:0.76716\n",
            "[22]\tvalidation_0-auc:0.768272\n",
            "[23]\tvalidation_0-auc:0.7656\n",
            "[24]\tvalidation_0-auc:0.765395\n",
            "[25]\tvalidation_0-auc:0.764381\n",
            "[26]\tvalidation_0-auc:0.765181\n",
            "[27]\tvalidation_0-auc:0.764537\n",
            "[28]\tvalidation_0-auc:0.765454\n",
            "[29]\tvalidation_0-auc:0.765464\n",
            "[30]\tvalidation_0-auc:0.762324\n",
            "[31]\tvalidation_0-auc:0.762695\n",
            "[32]\tvalidation_0-auc:0.762402\n",
            "[33]\tvalidation_0-auc:0.764879\n",
            "[34]\tvalidation_0-auc:0.762617\n",
            "[35]\tvalidation_0-auc:0.761837\n",
            "[36]\tvalidation_0-auc:0.762324\n",
            "[37]\tvalidation_0-auc:0.759068\n",
            "[38]\tvalidation_0-auc:0.759828\n",
            "[39]\tvalidation_0-auc:0.759301\n",
            "[40]\tvalidation_0-auc:0.756572\n",
            "[41]\tvalidation_0-auc:0.75663\n",
            "[42]\tvalidation_0-auc:0.756981\n",
            "[43]\tvalidation_0-auc:0.756532\n",
            "[44]\tvalidation_0-auc:0.75624\n",
            "[45]\tvalidation_0-auc:0.755177\n",
            "[46]\tvalidation_0-auc:0.753705\n",
            "[47]\tvalidation_0-auc:0.753919\n",
            "[48]\tvalidation_0-auc:0.754573\n",
            "[49]\tvalidation_0-auc:0.755236\n",
            "[50]\tvalidation_0-auc:0.754553\n",
            "[51]\tvalidation_0-auc:0.754729\n",
            "[52]\tvalidation_0-auc:0.754553\n",
            "[53]\tvalidation_0-auc:0.755948\n",
            "[54]\tvalidation_0-auc:0.753422\n",
            "[55]\tvalidation_0-auc:0.753305\n",
            "[56]\tvalidation_0-auc:0.75193\n",
            "[57]\tvalidation_0-auc:0.75158\n",
            "[58]\tvalidation_0-auc:0.751326\n",
            "[59]\tvalidation_0-auc:0.751384\n",
            "[60]\tvalidation_0-auc:0.752496\n",
            "[61]\tvalidation_0-auc:0.753452\n",
            "[62]\tvalidation_0-auc:0.753471\n",
            "[63]\tvalidation_0-auc:0.753783\n",
            "[64]\tvalidation_0-auc:0.750253\n",
            "[65]\tvalidation_0-auc:0.75119\n",
            "[66]\tvalidation_0-auc:0.750039\n",
            "[67]\tvalidation_0-auc:0.749591\n",
            "[68]\tvalidation_0-auc:0.750546\n",
            "[69]\tvalidation_0-auc:0.75115\n",
            "[70]\tvalidation_0-auc:0.749668\n",
            "[71]\tvalidation_0-auc:0.748128\n",
            "[72]\tvalidation_0-auc:0.749298\n",
            "Stopping. Best iteration:\n",
            "[22]\tvalidation_0-auc:0.768272\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |        Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.7326530612244898 | 0.7596899224806202 |  0.494949494949495  | 0.599388379204893  |\n",
            "|      GRU 0.15     | 0.7224489795918367 | 0.7246376811594203 |  0.5050505050505051 | 0.5952380952380952 |\n",
            "|    XGBoost 0.15   | 0.7387755102040816 | 0.6966292134831461 |  0.6262626262626263 | 0.6595744680851064 |\n",
            "|    Logreg 0.15    | 0.7306122448979592 | 0.675531914893617  |  0.6414141414141414 | 0.6580310880829016 |\n",
            "|      SVM 0.15     | 0.7387755102040816 | 0.6988636363636364 |  0.6212121212121212 | 0.6577540106951871 |\n",
            "|   LSTM beta 0.15  | 0.7177242888402626 | 0.7142857142857143 |  0.5808080808080808 | 0.6406685236768802 |\n",
            "|   GRU beta 0.15   | 0.6455142231947484 | 0.6836734693877551 |  0.3383838383838384 | 0.4527027027027027 |\n",
            "| XGBoost beta 0.15 | 0.6608315098468271 | 0.6869565217391305 |  0.398989898989899  | 0.5047923322683706 |\n",
            "|  logreg beta 0.15 | 0.7177242888402626 | 0.7315436241610739 |  0.5505050505050505 | 0.6282420749279539 |\n",
            "|   svm beta 0.15   | 0.6761487964989059 | 0.6865671641791045 | 0.46464646464646464 | 0.5542168674698794 |\n",
            "+-------------------+--------------------+--------------------+---------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVrluUZhY6Wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ab6430f-2dd3-403e-ad64-77a6cab761b4"
      },
      "source": [
        "Result_purging.to_csv('STX_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.643243</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.664804</td>\n",
              "      <td>0.687861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.685897</td>\n",
              "      <td>0.765306</td>\n",
              "      <td>0.650456</td>\n",
              "      <td>0.618497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.653409</td>\n",
              "      <td>0.757143</td>\n",
              "      <td>0.659026</td>\n",
              "      <td>0.664740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.634831</td>\n",
              "      <td>0.744898</td>\n",
              "      <td>0.643875</td>\n",
              "      <td>0.653179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.651685</td>\n",
              "      <td>0.757143</td>\n",
              "      <td>0.660969</td>\n",
              "      <td>0.670520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.651852</td>\n",
              "      <td>0.711160</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.508671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.631579</td>\n",
              "      <td>0.708972</td>\n",
              "      <td>0.590769</td>\n",
              "      <td>0.554913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.628571</td>\n",
              "      <td>0.719912</td>\n",
              "      <td>0.632184</td>\n",
              "      <td>0.635838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.630952</td>\n",
              "      <td>0.717724</td>\n",
              "      <td>0.621701</td>\n",
              "      <td>0.612717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.607955</td>\n",
              "      <td>0.704595</td>\n",
              "      <td>0.613181</td>\n",
              "      <td>0.618497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.510870</td>\n",
              "      <td>0.716327</td>\n",
              "      <td>0.574924</td>\n",
              "      <td>0.657343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.492891</td>\n",
              "      <td>0.702041</td>\n",
              "      <td>0.587571</td>\n",
              "      <td>0.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.542683</td>\n",
              "      <td>0.736735</td>\n",
              "      <td>0.579805</td>\n",
              "      <td>0.622378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.556250</td>\n",
              "      <td>0.744898</td>\n",
              "      <td>0.587459</td>\n",
              "      <td>0.622378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.568627</td>\n",
              "      <td>0.751020</td>\n",
              "      <td>0.587838</td>\n",
              "      <td>0.608392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.521429</td>\n",
              "      <td>0.700219</td>\n",
              "      <td>0.515901</td>\n",
              "      <td>0.510490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.519380</td>\n",
              "      <td>0.698031</td>\n",
              "      <td>0.492647</td>\n",
              "      <td>0.468531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.522936</td>\n",
              "      <td>0.698031</td>\n",
              "      <td>0.452381</td>\n",
              "      <td>0.398601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.548872</td>\n",
              "      <td>0.715536</td>\n",
              "      <td>0.528986</td>\n",
              "      <td>0.510490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.504132</td>\n",
              "      <td>0.689278</td>\n",
              "      <td>0.462121</td>\n",
              "      <td>0.426573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.759690</td>\n",
              "      <td>0.732653</td>\n",
              "      <td>0.599388</td>\n",
              "      <td>0.494949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.724638</td>\n",
              "      <td>0.722449</td>\n",
              "      <td>0.595238</td>\n",
              "      <td>0.505051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.696629</td>\n",
              "      <td>0.738776</td>\n",
              "      <td>0.659574</td>\n",
              "      <td>0.626263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.675532</td>\n",
              "      <td>0.730612</td>\n",
              "      <td>0.658031</td>\n",
              "      <td>0.641414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.698864</td>\n",
              "      <td>0.738776</td>\n",
              "      <td>0.657754</td>\n",
              "      <td>0.621212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.717724</td>\n",
              "      <td>0.640669</td>\n",
              "      <td>0.580808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.683673</td>\n",
              "      <td>0.645514</td>\n",
              "      <td>0.452703</td>\n",
              "      <td>0.338384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.686957</td>\n",
              "      <td>0.660832</td>\n",
              "      <td>0.504792</td>\n",
              "      <td>0.398990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.731544</td>\n",
              "      <td>0.717724</td>\n",
              "      <td>0.628242</td>\n",
              "      <td>0.550505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.686567</td>\n",
              "      <td>0.676149</td>\n",
              "      <td>0.554217</td>\n",
              "      <td>0.464646</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  STX  0.643243  0.755102  0.664804  0.687861\n",
              "1            GRU 0.1  STX  0.685897  0.765306  0.650456  0.618497\n",
              "2        XGBoost 0.1  STX  0.653409  0.757143  0.659026  0.664740\n",
              "3         Logreg 0.1  STX  0.634831  0.744898  0.643875  0.653179\n",
              "4            SVM 0.1  STX  0.651685  0.757143  0.660969  0.670520\n",
              "5      LSTM beta 0.1  STX  0.651852  0.711160  0.571429  0.508671\n",
              "6       GRU beta 0.1  STX  0.631579  0.708972  0.590769  0.554913\n",
              "7   XGBoost beta 0.1  STX  0.628571  0.719912  0.632184  0.635838\n",
              "8    logreg beta 0.1  STX  0.630952  0.717724  0.621701  0.612717\n",
              "9       svm beta 0.1  STX  0.607955  0.704595  0.613181  0.618497\n",
              "0           LSTM 0.2  STX  0.510870  0.716327  0.574924  0.657343\n",
              "1            GRU 0.2  STX  0.492891  0.702041  0.587571  0.727273\n",
              "2        XGBoost 0.2  STX  0.542683  0.736735  0.579805  0.622378\n",
              "3         Logreg 0.2  STX  0.556250  0.744898  0.587459  0.622378\n",
              "4            SVM 0.2  STX  0.568627  0.751020  0.587838  0.608392\n",
              "5      LSTM beta 0.2  STX  0.521429  0.700219  0.515901  0.510490\n",
              "6       GRU beta 0.2  STX  0.519380  0.698031  0.492647  0.468531\n",
              "7   XGBoost beta 0.2  STX  0.522936  0.698031  0.452381  0.398601\n",
              "8    logreg beta 0.2  STX  0.548872  0.715536  0.528986  0.510490\n",
              "9       svm beta 0.2  STX  0.504132  0.689278  0.462121  0.426573\n",
              "0          LSTM 0.15  STX  0.759690  0.732653  0.599388  0.494949\n",
              "1           GRU 0.15  STX  0.724638  0.722449  0.595238  0.505051\n",
              "2       XGBoost 0.15  STX  0.696629  0.738776  0.659574  0.626263\n",
              "3        Logreg 0.15  STX  0.675532  0.730612  0.658031  0.641414\n",
              "4           SVM 0.15  STX  0.698864  0.738776  0.657754  0.621212\n",
              "5     LSTM beta 0.15  STX  0.714286  0.717724  0.640669  0.580808\n",
              "6      GRU beta 0.15  STX  0.683673  0.645514  0.452703  0.338384\n",
              "7  XGBoost beta 0.15  STX  0.686957  0.660832  0.504792  0.398990\n",
              "8   logreg beta 0.15  STX  0.731544  0.717724  0.628242  0.550505\n",
              "9      svm beta 0.15  STX  0.686567  0.676149  0.554217  0.464646"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poMQY66RY6Wf"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2dWRfeBY6Wf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzCPsuaHZPyL"
      },
      "source": [
        "## SYY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ISArb8fZPyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46aeabe9-e3a5-4f07-8e4a-77006a16c28d"
      },
      "source": [
        "dfs = pd.read_csv(\"SYY.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "# Denoise prices\n",
        "dfs = denoise_data(dfs, '<CLOSE>')\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2768</td>\n",
              "      <td>US1.SYY</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>78.90</td>\n",
              "      <td>81.415</td>\n",
              "      <td>78.900</td>\n",
              "      <td>80.85</td>\n",
              "      <td>74171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2767</td>\n",
              "      <td>US1.SYY</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>80.37</td>\n",
              "      <td>80.500</td>\n",
              "      <td>78.430</td>\n",
              "      <td>78.51</td>\n",
              "      <td>98224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.SYY</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>80.62</td>\n",
              "      <td>81.760</td>\n",
              "      <td>80.240</td>\n",
              "      <td>80.94</td>\n",
              "      <td>67594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.SYY</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>81.48</td>\n",
              "      <td>82.660</td>\n",
              "      <td>80.540</td>\n",
              "      <td>80.66</td>\n",
              "      <td>79431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.SYY</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>80.87</td>\n",
              "      <td>82.520</td>\n",
              "      <td>80.870</td>\n",
              "      <td>81.68</td>\n",
              "      <td>116348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.SYY</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>28.59</td>\n",
              "      <td>28.630</td>\n",
              "      <td>28.325</td>\n",
              "      <td>28.52</td>\n",
              "      <td>2238435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.SYY</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>28.64</td>\n",
              "      <td>28.680</td>\n",
              "      <td>28.430</td>\n",
              "      <td>28.59</td>\n",
              "      <td>2226012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.SYY</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>28.84</td>\n",
              "      <td>28.840</td>\n",
              "      <td>28.480</td>\n",
              "      <td>28.51</td>\n",
              "      <td>2150778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.SYY</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>28.48</td>\n",
              "      <td>28.830</td>\n",
              "      <td>28.400</td>\n",
              "      <td>28.74</td>\n",
              "      <td>3718972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.SYY</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>28.53</td>\n",
              "      <td>28.710</td>\n",
              "      <td>28.220</td>\n",
              "      <td>28.33</td>\n",
              "      <td>3195008</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2769 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...  <HIGH>   <LOW>  <CLOSE>    <VOL>\n",
              "0      2768  US1.SYY     D  20211001  ...  81.415  78.900    80.85    74171\n",
              "1      2767  US1.SYY     D  20210930  ...  80.500  78.430    78.51    98224\n",
              "2      2766  US1.SYY     D  20210929  ...  81.760  80.240    80.94    67594\n",
              "3      2765  US1.SYY     D  20210928  ...  82.660  80.540    80.66    79431\n",
              "4      2764  US1.SYY     D  20210927  ...  82.520  80.870    81.68   116348\n",
              "...     ...      ...   ...       ...  ...     ...     ...      ...      ...\n",
              "2764      4  US1.SYY     D  20101008  ...  28.630  28.325    28.52  2238435\n",
              "2765      3  US1.SYY     D  20101007  ...  28.680  28.430    28.59  2226012\n",
              "2766      2  US1.SYY     D  20101006  ...  28.840  28.480    28.51  2150778\n",
              "2767      1  US1.SYY     D  20101005  ...  28.830  28.400    28.74  3718972\n",
              "2768      0  US1.SYY     D  20101004  ...  28.710  28.220    28.33  3195008\n",
              "\n",
              "[2769 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ygb5d7xZPyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "180b94e6-2163-4621-d2d3-4f86f6716018"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"bc59e7fa-c1b2-4749-8620-a7729c88f151\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"bc59e7fa-c1b2-4749-8620-a7729c88f151\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'bc59e7fa-c1b2-4749-8620-a7729c88f151',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [77.59, 78.22, 78.07, 77.96, 78.63, 79.35, 79.36, 78.89, 78.8, 78.7, 78.62, 78.66, 78.13, 78.39, 78.86, 78.39, 77.18, 78.27, 77.76, 77.06, 76.77, 76.69, 75.69, 74.38, 75.09, 74.46, 74.32, 74.48, 74.39, 73.84, 73.36, 72.8, 73.31, 72.25, 72.82, 73.57, 72.98, 72.265, 72.26, 73.35, 72.2, 70.02, 70.64, 69.88, 68.6, 67.14, 68.92, 68.95, 68.56, 69.63, 69.94, 70.88, 70.14, 70.1, 70.17, 70.26, 70.87, 71.87, 71.75, 72.64, 72.83, 72.59, 71.97, 72.05, 71.49, 72.0, 71.75, 71.75, 71.42, 70.67, 70.6, 70.42, 70.31, 71.71, 71.12, 70.67, 70.78, 71.07, 71.68, 71.82, 72.26, 72.54, 72.97, 72.78, 71.72, 71.5, 70.97, 70.81, 69.37, 68.93, 68.8, 74.53, 74.39, 74.54, 75.05, 74.66, 75.4, 75.48, 74.94, 74.91, 74.67, 73.9, 73.62, 73.37, 73.99, 73.56, 73.03, 72.91, 72.97, 70.52, 70.81, 69.97, 70.35, 69.75, 69.11, 69.36, 70.03, 70.41, 70.56, 70.98, 70.63, 70.11, 69.66, 69.01, 68.13, 67.43, 66.97, 67.25, 67.14, 66.85, 66.46, 66.33, 66.77, 66.75, 66.26, 65.62, 66.29, 66.19, 65.56, 65.92, 65.4, 66.18, 66.47, 66.21, 66.14, 66.45, 66.2, 66.46, 65.76, 65.87, 66.27, 66.57, 67.12, 67.58, 67.56, 67.1, 67.42, 67.27, 67.33, 67.21, 67.36, 67.01, 67.54, 66.56, 66.53, 66.94, 66.29, 66.22, 65.12, 65.34, 65.36, 66.64, 63.58, 63.84, 62.9, 62.07, 62.27, 62.01, 62.53, 62.9, 62.67, 62.67, 62.24, 62.22, 62.73, 62.92, 63.04, 63.24, 62.46, 62.785, 61.81, 61.84, 61.34, 62.04, 62.67, 62.27, 62.04, 61.18, 59.95, 60.9, 61.8, 63.49, 64.03, 64.19, 65.44, 66.0, 65.56, 65.31, 64.9, 65.02, 65.95, 66.16, 67.03, 67.4, 67.55, 66.86, 65.34, 64.91, 64.9, 64.5, 65.1, 66.03, 66.49, 66.23, 66.47, 66.88, 67.39, 66.79, 66.57, 66.46, 64.44, 64.51, 71.25, 71.79, 71.33, 72.01, 70.35, 69.8, 71.69, 71.98, 71.49, 70.78, 71.13, 70.55, 70.39, 70.15, 68.96, 68.82, 68.47, 70.4, 71.78, 72.68, 72.01, 72.13, 72.57, 73.28, 73.04, 73.25, 72.84, 72.95, 73.07, 72.97, 72.96, 73.3, 72.92, 73.05, 73.5, 73.54, 73.61, 74.48, 74.94, 75.05, 74.7, 74.97, 75.0, 75.34, 74.82, 73.97, 74.19, 74.41, 74.54, 75.78, 75.21, 75.08, 74.36, 75.18, 74.82, 73.89, 74.89, 74.19, 72.73, 68.4, 70.03, 69.87, 69.56, 69.75, 68.57, 68.04, 67.07, 67.21, 66.67, 71.5, 71.73, 70.92, 70.22, 70.86, 71.0, 70.89, 70.315, 71.08, 70.49, 70.73, 69.84, 69.39, 69.66, 69.32, 69.37, 69.38, 68.54, 67.95, 68.3, 68.35, 68.06, 68.21, 68.22, 67.77, 68.08, 67.42, 66.89, 66.78, 66.89, 66.48, 65.91, 66.22, 66.42, 66.15, 65.76, 65.78, 65.84, 65.74, 65.41, 65.02, 66.05, 65.15, 65.01, 64.6, 64.09, 63.69, 63.64, 63.54, 63.84, 63.34, 62.93, 62.9, 62.82, 62.74, 63.49, 62.75, 63.49, 62.29, 62.18, 62.82, 63.09, 62.5, 63.38, 62.95, 62.46, 61.93, 61.35, 61.15, 61.58, 61.86, 62.5, 61.24, 60.44, 60.52, 60.52, 60.62, 59.87, 59.72, 60.325, 61.2, 59.46, 58.37, 59.96, 59.38, 59.36, 59.97, 59.32, 59.77, 60.3, 60.44, 60.18, 60.12, 59.37, 59.88, 60.7, 60.52, 61.27, 60.47, 60.13, 60.87, 60.02, 59.67, 59.47, 59.66, 59.86, 60.26, 59.67, 58.86, 58.56, 58.9, 60.0, 59.36, 58.36, 58.43, 58.12, 58.31, 57.01, 58.41, 59.64, 57.82, 61.15, 61.99, 62.9, 63.31, 63.39, 64.12, 63.99, 63.74, 63.345, 63.58, 63.22, 62.22, 62.21, 61.72, 61.38, 61.2193, 61.35, 61.39, 61.55, 61.44, 60.84, 60.65, 60.53, 60.74, 61.06, 60.87, 60.76, 60.72, 60.58, 60.78, 61.02, 61.34, 60.94, 60.9, 61.63, 61.82, 61.98, 62.62, 60.92, 59.54, 59.29, 59.52, 57.77, 57.73, 57.69, 56.33, 55.15, 54.92, 54.91, 54.665, 54.97, 54.9, 54.54, 54.1, 54.93, 54.03, 54.19, 54.03, 54.0, 52.59, 54.19, 56.65, 55.85, 55.57, 55.61, 54.7, 54.81, 54.86, 54.56, 54.74, 54.95, 54.89, 54.75, 54.87, 54.81, 54.75, 54.48, 54.28, 53.79, 53.93, 53.98, 54.21, 54.32, 54.24, 53.75, 54.0, 53.95, 54.03, 54.13, 54.13, 54.09, 53.34, 53.9, 54.41, 53.93, 53.92, 53.65, 52.68, 53.5, 52.86, 53.28, 52.93, 53.12, 52.89, 53.03, 52.965, 52.67, 52.72, 52.31, 52.13, 52.33, 51.23, 51.67, 51.62, 51.18, 51.18, 51.2, 51.19, 51.61, 51.06, 51.59, 50.88, 51.3, 52.08, 52.48, 52.33, 52.08, 52.66, 52.57, 52.64, 52.25, 52.57, 52.11, 51.97, 51.55, 51.12, 50.83, 51.02, 50.4, 50.72, 50.57, 50.4, 50.21, 49.24, 49.39, 49.45, 49.105, 49.38, 50.54, 50.34, 49.88, 49.87, 49.29, 49.93, 49.91, 52.8, 53.37, 53.87, 54.49, 54.38, 55.54, 55.44, 55.39, 55.34, 55.18, 54.995, 54.75, 55.09, 55.46, 55.715, 55.4, 54.55, 54.21, 54.42, 54.3, 54.22, 54.24, 54.96, 54.59, 54.44, 54.4, 54.17, 54.03, 53.94, 55.01, 54.55, 54.92, 54.9, 55.33, 54.62, 52.92, 52.865, 52.87, 52.87, 53.43, 53.22, 53.12, 52.85, 52.21, 52.46, 52.11, 52.41, 52.11, 51.9, 52.21, 52.09, 52.025, 51.78, 51.65, 51.55, 51.59, 51.68, 51.93, 52.005, 51.81, 52.2, 52.05, 52.82, 52.655, 52.53, 51.81, 52.11, 52.74, 52.31, 52.58, 51.87, 51.88, 52.17, 52.43, 52.33, 52.31, 52.15, 52.18, 52.76, 53.13, 52.71, 52.85, 53.2, 52.87, 52.75, 52.8, 52.35, 52.61, 52.73, 52.71, 52.8, 52.62, 52.26, 51.82, 52.22, 51.21, 52.5592, 52.38, 52.07, 52.47, 52.73, 52.68, 53.03, 53.06, 53.52, 53.52, 53.81, 53.62, 55.26, 54.89, 54.97, 54.89, 55.29, 55.33, 55.67, 55.39, 55.0, 55.16, 54.99, 55.38, 55.93, 55.72, 56.165, 56.22, 56.26, 56.28, 56.49, 56.61, 56.38, 55.95, 55.51, 55.62, 55.02, 55.15, 54.33, 54.53, 53.97, 53.49, 53.61, 53.1, 53.26, 53.9, 53.7, 53.93, 53.55, 54.11, 53.72, 53.16, 53.36, 53.13, 53.52, 53.49, 53.12, 53.23, 54.16, 54.2, 52.77, 48.04, 47.85, 47.765, 48.075, 48.12, 48.0, 47.47, 47.265, 47.45, 47.73, 47.475, 47.585, 47.93, 48.26, 48.25, 48.33, 48.875, 48.62, 48.02, 48.19, 48.07, 48.28, 47.79, 48.13, 48.91, 49.015, 49.03, 49.44, 49.25, 48.89, 49.55, 50.01, 49.7, 49.1008, 49.24, 49.37, 49.59, 49.63, 49.535, 50.34, 49.81, 52.62, 53.09, 53.455, 53.24, 52.05, 51.87, 51.95, 52.415, 52.14, 52.46, 52.44, 52.75, 52.9, 52.47, 52.45, 52.23, 52.1, 52.17, 52.205, 51.76, 51.83, 51.8, 51.22, 51.24, 51.5, 51.31, 51.93, 51.91, 51.8, 51.82, 51.6, 51.94, 52.13, 52.08, 51.74, 51.82, 52.15, 51.94, 52.005, 51.96, 52.02, 51.08, 51.62, 51.615, 51.16, 51.255, 51.2, 50.71, 50.74, 50.21, 49.5, 49.71, 49.87, 50.28, 49.95, 50.06, 49.95, 49.81, 49.76, 48.67, 48.4797, 48.43, 48.79, 49.0, 48.68, 48.67, 48.78, 48.89, 48.75, 48.79, 48.11, 48.65, 48.84, 48.58, 48.74, 48.41, 48.57, 49.06, 48.7, 49.29, 50.21, 50.14, 50.19, 50.23, 50.18, 49.73, 49.35, 48.88, 48.53, 48.5, 48.59, 46.08, 46.27, 46.42, 46.3, 46.31, 45.77, 45.59, 46.74, 46.99, 46.86, 46.81, 46.6, 47.1, 46.95, 46.72, 46.96, 46.77, 46.97, 46.69, 46.985, 47.1, 46.73, 46.63, 46.46, 46.24, 46.245, 46.35, 46.36, 46.34, 46.61, 46.12, 46.03, 45.87, 45.81, 45.835, 45.11, 45.04, 44.88, 45.1, 45.03, 45.005, 45.04, 44.56, 44.11, 43.25, 43.57, 43.09, 42.91, 42.77, 45.005, 44.49, 44.37, 43.74, 43.19, 42.6, 42.7, 42.6, 43.12, 43.06, 43.44, 43.23, 42.78, 43.12, 39.78, 39.48, 39.3201, 39.25, 39.12, 39.6, 39.405, 39.66, 39.97, 39.85, 40.13, 40.05, 40.69, 40.1399, 39.79, 39.81, 40.18, 40.66, 40.63, 40.99, 41.45, 41.53, 41.52, 41.3, 41.54, 41.29, 40.89, 40.56, 41.27, 41.54, 41.1201, 41.22, 40.91, 41.11, 41.11, 41.17, 41.07, 41.1, 40.5, 41.015, 41.04, 41.1, 41.55, 41.5, 41.31, 41.41, 40.91, 41.41, 41.155, 40.47, 40.59, 39.97, 40.21, 40.35, 40.52, 40.475, 40.94, 41.08, 40.78, 40.95, 41.04, 41.25, 41.8, 41.58, 41.63, 41.48, 41.77, 41.71, 41.55, 41.08, 41.42, 41.43, 40.8, 40.43, 41.23, 41.23, 40.93, 40.94, 40.55, 40.06, 40.205, 39.39, 38.6, 38.97, 38.92, 38.73, 39.7301, 39.72, 39.45, 39.35, 39.86, 39.41, 39.52, 39.52, 39.46, 39.88, 39.94, 39.37, 39.59, 40.06, 39.55, 39.91, 39.75, 39.09, 39.86, 39.91, 40.31, 39.96, 39.06, 39.28, 39.71, 40.54, 40.975, 41.15, 41.15, 41.39, 38.52, 38.64, 37.98, 37.58, 36.93, 36.26, 36.77, 36.46, 36.26, 36.33, 36.06, 36.26, 36.015, 35.79, 36.01, 36.15, 36.21, 36.3, 36.27, 36.345, 36.14, 36.02, 36.16, 36.1, 36.05, 36.01, 36.055, 36.05, 35.68, 35.92, 35.99, 36.09, 37.55, 38.41, 38.3, 38.75, 37.61, 37.62, 37.365, 37.4, 37.31, 37.25, 37.01, 36.93, 37.14, 37.22, 37.11, 37.08, 37.105, 37.56, 37.57, 37.29, 37.22, 37.17, 37.24, 37.54, 37.49, 37.68, 38.1, 38.07, 38.29, 37.475, 37.28, 37.42, 36.86, 36.61, 36.5, 36.62, 36.38, 35.96, 36.1, 36.66, 37.16, 37.02, 37.425, 37.74, 37.78, 37.92, 37.97, 37.62, 37.61, 37.29, 37.335, 37.31, 37.66, 37.81, 37.63, 38.18, 38.17, 38.16, 37.96, 37.84, 37.64, 37.55, 37.72, 38.26, 38.165, 37.88, 38.01, 38.25, 38.65, 38.6, 38.21, 38.535, 38.52, 38.7, 38.57, 38.8, 38.4, 38.46, 38.92, 38.64, 39.11, 38.8, 38.9, 38.97, 38.98, 39.14, 39.465, 39.555, 39.61, 39.68, 38.52, 39.93, 39.565, 39.55, 39.65, 38.91, 39.8, 40.1, 40.26, 39.86, 39.87, 39.52, 38.44, 39.18, 39.9, 40.11, 40.71, 40.92, 40.98, 40.7, 40.31, 39.56, 39.99, 40.25, 40.28, 40.76, 40.88, 41.25, 41.12, 40.4, 39.26, 39.44, 39.86, 39.69, 40.27, 40.69, 40.6, 40.59, 40.7, 40.91, 40.49, 40.74, 40.11, 39.73, 39.6, 39.33, 40.13, 39.41, 39.98, 40.01, 39.92, 39.82, 39.95, 39.97, 40.25, 40.26, 39.94, 39.47, 39.555, 39.69, 39.11, 39.14, 39.15, 38.89, 38.58, 38.74, 38.925, 38.53, 38.66, 38.14, 37.98, 37.83, 37.35, 37.47, 38.525, 38.56, 38.21, 38.21, 38.24, 38.04, 37.92, 37.66, 37.615, 36.99, 36.5099, 36.22, 36.6, 36.68, 36.44, 37.03, 37.17, 37.67, 37.05, 37.575, 37.69, 37.19, 36.99, 37.97, 37.95, 37.52, 37.46, 38.0, 36.75, 37.58, 37.975, 38.0, 38.0, 38.14, 37.78, 37.83, 38.25, 38.27, 38.26, 38.76, 38.62, 38.62, 38.51, 38.2, 37.81, 37.8, 37.76, 37.78, 37.79, 37.59, 37.67, 37.56, 37.67, 37.6, 37.44, 37.64, 37.74, 37.35, 37.45, 36.26, 36.28, 35.96, 35.53, 35.98, 35.78, 35.69, 36.25, 36.6, 36.61, 36.735, 36.9, 37.02, 36.95, 36.7, 36.74, 36.45, 37.07, 36.86, 36.85, 36.98, 36.94, 37.11, 36.77, 37.07, 37.24, 37.16, 37.15, 37.46, 37.85, 37.61, 37.46, 37.79, 37.54, 37.19, 37.24, 37.22, 36.9071, 36.89, 36.94, 37.65, 37.69, 37.82, 37.71, 37.67, 37.53, 37.22, 37.26, 37.63, 37.55, 37.17, 36.665, 36.76, 36.58, 36.48, 36.5, 36.48, 36.7, 36.58, 36.42, 36.63, 36.815, 36.76, 36.81, 36.6, 36.75, 36.57, 37.185, 36.2, 36.21, 36.42, 36.24, 36.46, 36.05, 36.39, 36.42, 36.37, 36.34, 36.04, 35.92, 35.77, 35.74, 35.35, 35.93, 36.18, 35.84, 35.87, 35.66, 35.85, 35.74, 36.11, 36.14, 35.97, 35.47, 35.85, 35.85, 35.85, 36.07, 36.25, 36.15, 36.5, 36.16, 35.95, 35.84, 36.21, 36.14, 36.2, 36.22, 36.3, 36.19, 36.39, 36.01, 36.02, 35.95, 36.0, 36.1, 35.98, 36.05, 35.84, 35.83, 35.92, 35.71, 35.62, 35.33, 35.67, 35.39, 35.51, 35.28, 34.755, 34.59, 34.52, 35.07, 35.15, 35.06, 35.74, 35.57, 35.48, 36.12, 36.42, 36.55, 36.52, 36.89, 36.76, 36.65, 36.29, 36.36, 35.94, 35.99, 36.46, 36.0, 36.06, 35.91, 36.09, 36.67, 36.82, 36.53, 36.47, 36.51, 36.47, 36.25, 36.27, 35.96, 36.19, 36.26, 36.24, 36.95, 36.98, 37.63, 34.31, 33.56, 33.64, 33.7, 33.64, 33.8043, 33.81, 34.17, 33.79, 33.95, 33.52, 33.33, 33.61, 33.64, 33.705, 33.58, 33.39, 33.1499, 33.08, 33.13, 33.14, 33.6501, 33.32, 33.96, 32.56, 32.35, 32.48, 32.86, 33.24, 32.98, 32.82, 32.55]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('bc59e7fa-c1b2-4749-8620-a7729c88f151');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"5f786ede-0bd7-4087-ad66-8c06b7d5e7f5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"5f786ede-0bd7-4087-ad66-8c06b7d5e7f5\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '5f786ede-0bd7-4087-ad66-8c06b7d5e7f5',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('5f786ede-0bd7-4087-ad66-8c06b7d5e7f5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgRpVZRpZPyL"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI-cTjlTZPyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f95a2f59-75cd-46e8-c452-0e08bb019b6a"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.05, .1, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"SYY\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6592 - accuracy: 0.6396 - val_loss: 0.5518 - val_accuracy: 0.8857\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6544 - accuracy: 0.6450 - val_loss: 0.5046 - val_accuracy: 0.8857\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6386 - accuracy: 0.6557 - val_loss: 0.4101 - val_accuracy: 0.8898\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6114 - accuracy: 0.6752 - val_loss: 0.5532 - val_accuracy: 0.7735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6017 - accuracy: 0.6866 - val_loss: 0.6022 - val_accuracy: 0.7408\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6565 - accuracy: 0.6423 - val_loss: 0.5127 - val_accuracy: 0.8857\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6374 - accuracy: 0.6611 - val_loss: 0.4194 - val_accuracy: 0.8857\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5989 - accuracy: 0.6819 - val_loss: 0.4868 - val_accuracy: 0.8510\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5890 - accuracy: 0.6805 - val_loss: 0.4766 - val_accuracy: 0.8510\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5704 - accuracy: 0.7121 - val_loss: 0.5043 - val_accuracy: 0.8265\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.678386\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.675177\n",
            "[2]\tvalidation_0-auc:0.670445\n",
            "[3]\tvalidation_0-auc:0.680855\n",
            "[4]\tvalidation_0-auc:0.67707\n",
            "[5]\tvalidation_0-auc:0.672914\n",
            "[6]\tvalidation_0-auc:0.672626\n",
            "[7]\tvalidation_0-auc:0.674375\n",
            "[8]\tvalidation_0-auc:0.674128\n",
            "[9]\tvalidation_0-auc:0.668388\n",
            "[10]\tvalidation_0-auc:0.669828\n",
            "[11]\tvalidation_0-auc:0.670054\n",
            "[12]\tvalidation_0-auc:0.667832\n",
            "[13]\tvalidation_0-auc:0.671721\n",
            "[14]\tvalidation_0-auc:0.672976\n",
            "[15]\tvalidation_0-auc:0.675856\n",
            "[16]\tvalidation_0-auc:0.674004\n",
            "[17]\tvalidation_0-auc:0.674416\n",
            "[18]\tvalidation_0-auc:0.674292\n",
            "[19]\tvalidation_0-auc:0.675774\n",
            "[20]\tvalidation_0-auc:0.676267\n",
            "[21]\tvalidation_0-auc:0.67707\n",
            "[22]\tvalidation_0-auc:0.675465\n",
            "[23]\tvalidation_0-auc:0.674889\n",
            "[24]\tvalidation_0-auc:0.674704\n",
            "[25]\tvalidation_0-auc:0.675341\n",
            "[26]\tvalidation_0-auc:0.675774\n",
            "[27]\tvalidation_0-auc:0.677625\n",
            "[28]\tvalidation_0-auc:0.677666\n",
            "[29]\tvalidation_0-auc:0.680176\n",
            "[30]\tvalidation_0-auc:0.680053\n",
            "[31]\tvalidation_0-auc:0.680382\n",
            "[32]\tvalidation_0-auc:0.679045\n",
            "[33]\tvalidation_0-auc:0.678427\n",
            "[34]\tvalidation_0-auc:0.678345\n",
            "[35]\tvalidation_0-auc:0.678427\n",
            "[36]\tvalidation_0-auc:0.680279\n",
            "[37]\tvalidation_0-auc:0.679291\n",
            "[38]\tvalidation_0-auc:0.680238\n",
            "[39]\tvalidation_0-auc:0.681596\n",
            "[40]\tvalidation_0-auc:0.68283\n",
            "[41]\tvalidation_0-auc:0.682871\n",
            "[42]\tvalidation_0-auc:0.682665\n",
            "[43]\tvalidation_0-auc:0.680958\n",
            "[44]\tvalidation_0-auc:0.68067\n",
            "[45]\tvalidation_0-auc:0.680999\n",
            "[46]\tvalidation_0-auc:0.68141\n",
            "[47]\tvalidation_0-auc:0.681328\n",
            "[48]\tvalidation_0-auc:0.682645\n",
            "[49]\tvalidation_0-auc:0.682316\n",
            "[50]\tvalidation_0-auc:0.681246\n",
            "[51]\tvalidation_0-auc:0.683427\n",
            "[52]\tvalidation_0-auc:0.683056\n",
            "[53]\tvalidation_0-auc:0.683673\n",
            "[54]\tvalidation_0-auc:0.683715\n",
            "[55]\tvalidation_0-auc:0.685772\n",
            "[56]\tvalidation_0-auc:0.686307\n",
            "[57]\tvalidation_0-auc:0.686266\n",
            "[58]\tvalidation_0-auc:0.685402\n",
            "[59]\tvalidation_0-auc:0.685155\n",
            "[60]\tvalidation_0-auc:0.685402\n",
            "[61]\tvalidation_0-auc:0.686986\n",
            "[62]\tvalidation_0-auc:0.686883\n",
            "[63]\tvalidation_0-auc:0.687006\n",
            "[64]\tvalidation_0-auc:0.686842\n",
            "[65]\tvalidation_0-auc:0.687582\n",
            "[66]\tvalidation_0-auc:0.687294\n",
            "[67]\tvalidation_0-auc:0.687047\n",
            "[68]\tvalidation_0-auc:0.687006\n",
            "[69]\tvalidation_0-auc:0.686842\n",
            "[70]\tvalidation_0-auc:0.687089\n",
            "[71]\tvalidation_0-auc:0.687006\n",
            "[72]\tvalidation_0-auc:0.688035\n",
            "[73]\tvalidation_0-auc:0.68857\n",
            "[74]\tvalidation_0-auc:0.688529\n",
            "[75]\tvalidation_0-auc:0.687911\n",
            "[76]\tvalidation_0-auc:0.687665\n",
            "[77]\tvalidation_0-auc:0.687047\n",
            "[78]\tvalidation_0-auc:0.688899\n",
            "[79]\tvalidation_0-auc:0.690051\n",
            "[80]\tvalidation_0-auc:0.691327\n",
            "[81]\tvalidation_0-auc:0.690586\n",
            "[82]\tvalidation_0-auc:0.691738\n",
            "[83]\tvalidation_0-auc:0.691615\n",
            "[84]\tvalidation_0-auc:0.690874\n",
            "[85]\tvalidation_0-auc:0.690504\n",
            "[86]\tvalidation_0-auc:0.690586\n",
            "[87]\tvalidation_0-auc:0.690174\n",
            "[88]\tvalidation_0-auc:0.689475\n",
            "[89]\tvalidation_0-auc:0.691327\n",
            "[90]\tvalidation_0-auc:0.691779\n",
            "[91]\tvalidation_0-auc:0.691738\n",
            "[92]\tvalidation_0-auc:0.691944\n",
            "[93]\tvalidation_0-auc:0.691944\n",
            "[94]\tvalidation_0-auc:0.692067\n",
            "[95]\tvalidation_0-auc:0.691738\n",
            "[96]\tvalidation_0-auc:0.69252\n",
            "[97]\tvalidation_0-auc:0.691697\n",
            "[98]\tvalidation_0-auc:0.691038\n",
            "[99]\tvalidation_0-auc:0.690997\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 4s 15ms/step - loss: 0.6575 - accuracy: 0.6541 - val_loss: 0.5482 - val_accuracy: 0.8775\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6307 - accuracy: 0.6664 - val_loss: 0.6455 - val_accuracy: 0.5886\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5908 - accuracy: 0.6973 - val_loss: 0.5784 - val_accuracy: 0.6937\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5562 - accuracy: 0.7207 - val_loss: 0.4211 - val_accuracy: 0.8862\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5387 - accuracy: 0.7461 - val_loss: 0.4394 - val_accuracy: 0.8425\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6357 - accuracy: 0.6568 - val_loss: 0.4939 - val_accuracy: 0.8775\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5632 - accuracy: 0.7282 - val_loss: 0.4355 - val_accuracy: 0.8884\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5281 - accuracy: 0.7598 - val_loss: 0.5175 - val_accuracy: 0.7615\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5250 - accuracy: 0.7618 - val_loss: 0.4065 - val_accuracy: 0.8862\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5094 - accuracy: 0.7653 - val_loss: 0.4568 - val_accuracy: 0.8118\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.66726\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.672805\n",
            "[2]\tvalidation_0-auc:0.678171\n",
            "[3]\tvalidation_0-auc:0.677124\n",
            "[4]\tvalidation_0-auc:0.672871\n",
            "[5]\tvalidation_0-auc:0.673762\n",
            "[6]\tvalidation_0-auc:0.673495\n",
            "[7]\tvalidation_0-auc:0.675699\n",
            "[8]\tvalidation_0-auc:0.672716\n",
            "[9]\tvalidation_0-auc:0.670756\n",
            "[10]\tvalidation_0-auc:0.67305\n",
            "[11]\tvalidation_0-auc:0.671981\n",
            "[12]\tvalidation_0-auc:0.669264\n",
            "[13]\tvalidation_0-auc:0.66824\n",
            "[14]\tvalidation_0-auc:0.671424\n",
            "[15]\tvalidation_0-auc:0.669598\n",
            "[16]\tvalidation_0-auc:0.66971\n",
            "[17]\tvalidation_0-auc:0.672315\n",
            "[18]\tvalidation_0-auc:0.673807\n",
            "[19]\tvalidation_0-auc:0.673361\n",
            "[20]\tvalidation_0-auc:0.67345\n",
            "[21]\tvalidation_0-auc:0.675321\n",
            "[22]\tvalidation_0-auc:0.677235\n",
            "[23]\tvalidation_0-auc:0.675566\n",
            "[24]\tvalidation_0-auc:0.677814\n",
            "[25]\tvalidation_0-auc:0.674831\n",
            "[26]\tvalidation_0-auc:0.676701\n",
            "[27]\tvalidation_0-auc:0.67826\n",
            "[28]\tvalidation_0-auc:0.679418\n",
            "[29]\tvalidation_0-auc:0.679551\n",
            "[30]\tvalidation_0-auc:0.680598\n",
            "[31]\tvalidation_0-auc:0.678638\n",
            "[32]\tvalidation_0-auc:0.677614\n",
            "[33]\tvalidation_0-auc:0.678772\n",
            "[34]\tvalidation_0-auc:0.680241\n",
            "[35]\tvalidation_0-auc:0.681043\n",
            "[36]\tvalidation_0-auc:0.680019\n",
            "[37]\tvalidation_0-auc:0.680642\n",
            "[38]\tvalidation_0-auc:0.68033\n",
            "[39]\tvalidation_0-auc:0.679596\n",
            "[40]\tvalidation_0-auc:0.676478\n",
            "[41]\tvalidation_0-auc:0.677102\n",
            "[42]\tvalidation_0-auc:0.677993\n",
            "[43]\tvalidation_0-auc:0.680108\n",
            "[44]\tvalidation_0-auc:0.68082\n",
            "[45]\tvalidation_0-auc:0.680932\n",
            "[46]\tvalidation_0-auc:0.679774\n",
            "[47]\tvalidation_0-auc:0.680553\n",
            "[48]\tvalidation_0-auc:0.680998\n",
            "[49]\tvalidation_0-auc:0.682023\n",
            "[50]\tvalidation_0-auc:0.682824\n",
            "[51]\tvalidation_0-auc:0.683537\n",
            "[52]\tvalidation_0-auc:0.682512\n",
            "[53]\tvalidation_0-auc:0.682824\n",
            "[54]\tvalidation_0-auc:0.683804\n",
            "[55]\tvalidation_0-auc:0.683626\n",
            "[56]\tvalidation_0-auc:0.683492\n",
            "[57]\tvalidation_0-auc:0.682446\n",
            "[58]\tvalidation_0-auc:0.683871\n",
            "[59]\tvalidation_0-auc:0.684004\n",
            "[60]\tvalidation_0-auc:0.684316\n",
            "[61]\tvalidation_0-auc:0.684806\n",
            "[62]\tvalidation_0-auc:0.683025\n",
            "[63]\tvalidation_0-auc:0.683114\n",
            "[64]\tvalidation_0-auc:0.681956\n",
            "[65]\tvalidation_0-auc:0.682624\n",
            "[66]\tvalidation_0-auc:0.680976\n",
            "[67]\tvalidation_0-auc:0.680753\n",
            "[68]\tvalidation_0-auc:0.680598\n",
            "[69]\tvalidation_0-auc:0.679974\n",
            "[70]\tvalidation_0-auc:0.680998\n",
            "[71]\tvalidation_0-auc:0.680642\n",
            "[72]\tvalidation_0-auc:0.681266\n",
            "[73]\tvalidation_0-auc:0.682646\n",
            "[74]\tvalidation_0-auc:0.683091\n",
            "[75]\tvalidation_0-auc:0.683626\n",
            "[76]\tvalidation_0-auc:0.682112\n",
            "[77]\tvalidation_0-auc:0.681755\n",
            "[78]\tvalidation_0-auc:0.681355\n",
            "[79]\tvalidation_0-auc:0.681132\n",
            "[80]\tvalidation_0-auc:0.681399\n",
            "[81]\tvalidation_0-auc:0.681488\n",
            "[82]\tvalidation_0-auc:0.684182\n",
            "[83]\tvalidation_0-auc:0.681555\n",
            "[84]\tvalidation_0-auc:0.681689\n",
            "[85]\tvalidation_0-auc:0.680843\n",
            "[86]\tvalidation_0-auc:0.680843\n",
            "[87]\tvalidation_0-auc:0.68111\n",
            "[88]\tvalidation_0-auc:0.683693\n",
            "[89]\tvalidation_0-auc:0.68347\n",
            "[90]\tvalidation_0-auc:0.684138\n",
            "[91]\tvalidation_0-auc:0.684182\n",
            "[92]\tvalidation_0-auc:0.685696\n",
            "[93]\tvalidation_0-auc:0.685607\n",
            "[94]\tvalidation_0-auc:0.685875\n",
            "[95]\tvalidation_0-auc:0.686721\n",
            "[96]\tvalidation_0-auc:0.687166\n",
            "[97]\tvalidation_0-auc:0.687122\n",
            "[98]\tvalidation_0-auc:0.686988\n",
            "[99]\tvalidation_0-auc:0.68583\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.7408163265306122 |  0.2248062015503876 |  0.5178571428571429 | 0.31351351351351353 |\n",
            "|     GRU 0.1      | 0.826530612244898  |  0.3013698630136986 | 0.39285714285714285 |  0.3410852713178294 |\n",
            "|   XGBoost 0.1    | 0.789795918367347  |  0.2761904761904762 |  0.5178571428571429 |  0.360248447204969  |\n",
            "|    Logreg 0.1    | 0.8775510204081632 |        0.4375       |         0.25        |  0.3181818181818182 |\n",
            "|     SVM 0.1      | 0.8489795918367347 |         0.35        |        0.375        |  0.3620689655172413 |\n",
            "|  LSTM beta 0.1   | 0.8424507658643327 |  0.3333333333333333 |  0.2857142857142857 | 0.30769230769230765 |\n",
            "|   GRU beta 0.1   | 0.811816192560175  |  0.2972972972972973 | 0.39285714285714285 |  0.3384615384615385 |\n",
            "| XGBoost beta 0.1 | 0.7571115973741794 | 0.22772277227722773 |  0.4107142857142857 |  0.2929936305732484 |\n",
            "| logreg beta 0.1  |  0.87527352297593  |  0.4864864864864865 | 0.32142857142857145 |  0.3870967741935484 |\n",
            "|   svm beta 0.1   | 0.8030634573304157 | 0.27631578947368424 |        0.375        |  0.3181818181818182 |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6182 - accuracy: 0.7087 - val_loss: 0.4258 - val_accuracy: 0.8857\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6065 - accuracy: 0.7101 - val_loss: 0.4425 - val_accuracy: 0.8857\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6066 - accuracy: 0.7101 - val_loss: 0.4261 - val_accuracy: 0.8857\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5975 - accuracy: 0.7094 - val_loss: 0.4350 - val_accuracy: 0.8857\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6089 - accuracy: 0.7047 - val_loss: 0.4283 - val_accuracy: 0.8857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6108 - accuracy: 0.7107 - val_loss: 0.4114 - val_accuracy: 0.8857\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5911 - accuracy: 0.7141 - val_loss: 0.3917 - val_accuracy: 0.8857\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5765 - accuracy: 0.7181 - val_loss: 0.4443 - val_accuracy: 0.8939\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5605 - accuracy: 0.7275 - val_loss: 0.4613 - val_accuracy: 0.8837\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5520 - accuracy: 0.7268 - val_loss: 0.3987 - val_accuracy: 0.8959\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.670178\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.666413\n",
            "[2]\tvalidation_0-auc:0.667791\n",
            "[3]\tvalidation_0-auc:0.657711\n",
            "[4]\tvalidation_0-auc:0.665076\n",
            "[5]\tvalidation_0-auc:0.677995\n",
            "[6]\tvalidation_0-auc:0.671597\n",
            "[7]\tvalidation_0-auc:0.669375\n",
            "[8]\tvalidation_0-auc:0.670095\n",
            "[9]\tvalidation_0-auc:0.668779\n",
            "[10]\tvalidation_0-auc:0.665569\n",
            "[11]\tvalidation_0-auc:0.67421\n",
            "[12]\tvalidation_0-auc:0.672832\n",
            "[13]\tvalidation_0-auc:0.676905\n",
            "[14]\tvalidation_0-auc:0.675527\n",
            "[15]\tvalidation_0-auc:0.676226\n",
            "[16]\tvalidation_0-auc:0.67635\n",
            "[17]\tvalidation_0-auc:0.675876\n",
            "[18]\tvalidation_0-auc:0.677255\n",
            "[19]\tvalidation_0-auc:0.677152\n",
            "[20]\tvalidation_0-auc:0.677255\n",
            "[21]\tvalidation_0-auc:0.68141\n",
            "[22]\tvalidation_0-auc:0.68104\n",
            "[23]\tvalidation_0-auc:0.68141\n",
            "[24]\tvalidation_0-auc:0.682028\n",
            "[25]\tvalidation_0-auc:0.681061\n",
            "[26]\tvalidation_0-auc:0.680937\n",
            "[27]\tvalidation_0-auc:0.683838\n",
            "[28]\tvalidation_0-auc:0.683344\n",
            "[29]\tvalidation_0-auc:0.685813\n",
            "[30]\tvalidation_0-auc:0.684208\n",
            "[31]\tvalidation_0-auc:0.686019\n",
            "[32]\tvalidation_0-auc:0.682707\n",
            "[33]\tvalidation_0-auc:0.683118\n",
            "[34]\tvalidation_0-auc:0.683241\n",
            "[35]\tvalidation_0-auc:0.683571\n",
            "[36]\tvalidation_0-auc:0.683818\n",
            "[37]\tvalidation_0-auc:0.683838\n",
            "[38]\tvalidation_0-auc:0.684908\n",
            "[39]\tvalidation_0-auc:0.68534\n",
            "[40]\tvalidation_0-auc:0.684414\n",
            "[41]\tvalidation_0-auc:0.685648\n",
            "[42]\tvalidation_0-auc:0.685566\n",
            "[43]\tvalidation_0-auc:0.686615\n",
            "[44]\tvalidation_0-auc:0.686759\n",
            "[45]\tvalidation_0-auc:0.686142\n",
            "[46]\tvalidation_0-auc:0.687089\n",
            "[47]\tvalidation_0-auc:0.685772\n",
            "[48]\tvalidation_0-auc:0.68499\n",
            "[49]\tvalidation_0-auc:0.685196\n",
            "[50]\tvalidation_0-auc:0.685895\n",
            "[51]\tvalidation_0-auc:0.682521\n",
            "[52]\tvalidation_0-auc:0.682686\n",
            "[53]\tvalidation_0-auc:0.680917\n",
            "[54]\tvalidation_0-auc:0.680094\n",
            "[55]\tvalidation_0-auc:0.679641\n",
            "[56]\tvalidation_0-auc:0.680382\n",
            "[57]\tvalidation_0-auc:0.679189\n",
            "[58]\tvalidation_0-auc:0.678448\n",
            "[59]\tvalidation_0-auc:0.679559\n",
            "[60]\tvalidation_0-auc:0.680217\n",
            "[61]\tvalidation_0-auc:0.679271\n",
            "[62]\tvalidation_0-auc:0.680258\n",
            "[63]\tvalidation_0-auc:0.6803\n",
            "[64]\tvalidation_0-auc:0.680217\n",
            "[65]\tvalidation_0-auc:0.679888\n",
            "[66]\tvalidation_0-auc:0.677831\n",
            "[67]\tvalidation_0-auc:0.677049\n",
            "[68]\tvalidation_0-auc:0.67779\n",
            "[69]\tvalidation_0-auc:0.679724\n",
            "[70]\tvalidation_0-auc:0.679271\n",
            "[71]\tvalidation_0-auc:0.678695\n",
            "[72]\tvalidation_0-auc:0.678818\n",
            "[73]\tvalidation_0-auc:0.677872\n",
            "[74]\tvalidation_0-auc:0.678201\n",
            "[75]\tvalidation_0-auc:0.678613\n",
            "[76]\tvalidation_0-auc:0.681246\n",
            "[77]\tvalidation_0-auc:0.681452\n",
            "[78]\tvalidation_0-auc:0.683097\n",
            "[79]\tvalidation_0-auc:0.682439\n",
            "[80]\tvalidation_0-auc:0.682192\n",
            "[81]\tvalidation_0-auc:0.683962\n",
            "[82]\tvalidation_0-auc:0.685319\n",
            "[83]\tvalidation_0-auc:0.683056\n",
            "[84]\tvalidation_0-auc:0.683756\n",
            "[85]\tvalidation_0-auc:0.681575\n",
            "[86]\tvalidation_0-auc:0.68141\n",
            "[87]\tvalidation_0-auc:0.680382\n",
            "[88]\tvalidation_0-auc:0.678942\n",
            "[89]\tvalidation_0-auc:0.679065\n",
            "[90]\tvalidation_0-auc:0.67853\n",
            "[91]\tvalidation_0-auc:0.678736\n",
            "[92]\tvalidation_0-auc:0.677913\n",
            "[93]\tvalidation_0-auc:0.674827\n",
            "[94]\tvalidation_0-auc:0.676391\n",
            "[95]\tvalidation_0-auc:0.676679\n",
            "[96]\tvalidation_0-auc:0.678777\n",
            "Stopping. Best iteration:\n",
            "[46]\tvalidation_0-auc:0.687089\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6057 - accuracy: 0.7186 - val_loss: 0.4166 - val_accuracy: 0.8775\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5927 - accuracy: 0.7145 - val_loss: 0.4068 - val_accuracy: 0.8621\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5846 - accuracy: 0.7303 - val_loss: 0.4349 - val_accuracy: 0.8775\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5657 - accuracy: 0.7481 - val_loss: 0.5090 - val_accuracy: 0.7374\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5696 - accuracy: 0.7385 - val_loss: 0.4139 - val_accuracy: 0.8775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5750 - accuracy: 0.7248 - val_loss: 0.3878 - val_accuracy: 0.8753\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4836 - accuracy: 0.7721 - val_loss: 0.5257 - val_accuracy: 0.7374\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4627 - accuracy: 0.7900 - val_loss: 0.3933 - val_accuracy: 0.8862\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4531 - accuracy: 0.7996 - val_loss: 0.4293 - val_accuracy: 0.8796\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4449 - accuracy: 0.7962 - val_loss: 0.4669 - val_accuracy: 0.8118\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.662585\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.663275\n",
            "[2]\tvalidation_0-auc:0.667884\n",
            "[3]\tvalidation_0-auc:0.670244\n",
            "[4]\tvalidation_0-auc:0.633194\n",
            "[5]\tvalidation_0-auc:0.633639\n",
            "[6]\tvalidation_0-auc:0.642835\n",
            "[7]\tvalidation_0-auc:0.643859\n",
            "[8]\tvalidation_0-auc:0.641031\n",
            "[9]\tvalidation_0-auc:0.659623\n",
            "[10]\tvalidation_0-auc:0.665568\n",
            "[11]\tvalidation_0-auc:0.670155\n",
            "[12]\tvalidation_0-auc:0.676122\n",
            "[13]\tvalidation_0-auc:0.668374\n",
            "[14]\tvalidation_0-auc:0.67423\n",
            "[15]\tvalidation_0-auc:0.677859\n",
            "[16]\tvalidation_0-auc:0.677146\n",
            "[17]\tvalidation_0-auc:0.670556\n",
            "[18]\tvalidation_0-auc:0.67561\n",
            "[19]\tvalidation_0-auc:0.677169\n",
            "[20]\tvalidation_0-auc:0.676545\n",
            "[21]\tvalidation_0-auc:0.675944\n",
            "[22]\tvalidation_0-auc:0.679328\n",
            "[23]\tvalidation_0-auc:0.677926\n",
            "[24]\tvalidation_0-auc:0.679262\n",
            "[25]\tvalidation_0-auc:0.682713\n",
            "[26]\tvalidation_0-auc:0.683581\n",
            "[27]\tvalidation_0-auc:0.684873\n",
            "[28]\tvalidation_0-auc:0.686209\n",
            "[29]\tvalidation_0-auc:0.682045\n",
            "[30]\tvalidation_0-auc:0.68367\n",
            "[31]\tvalidation_0-auc:0.678082\n",
            "[32]\tvalidation_0-auc:0.678883\n",
            "[33]\tvalidation_0-auc:0.682646\n",
            "[34]\tvalidation_0-auc:0.681488\n",
            "[35]\tvalidation_0-auc:0.683002\n",
            "[36]\tvalidation_0-auc:0.685006\n",
            "[37]\tvalidation_0-auc:0.681911\n",
            "[38]\tvalidation_0-auc:0.683247\n",
            "[39]\tvalidation_0-auc:0.682357\n",
            "[40]\tvalidation_0-auc:0.680843\n",
            "[41]\tvalidation_0-auc:0.679818\n",
            "[42]\tvalidation_0-auc:0.680152\n",
            "[43]\tvalidation_0-auc:0.683091\n",
            "[44]\tvalidation_0-auc:0.68445\n",
            "[45]\tvalidation_0-auc:0.687255\n",
            "[46]\tvalidation_0-auc:0.688881\n",
            "[47]\tvalidation_0-auc:0.688836\n",
            "[48]\tvalidation_0-auc:0.690862\n",
            "[49]\tvalidation_0-auc:0.693\n",
            "[50]\tvalidation_0-auc:0.691575\n",
            "[51]\tvalidation_0-auc:0.691085\n",
            "[52]\tvalidation_0-auc:0.690773\n",
            "[53]\tvalidation_0-auc:0.690907\n",
            "[54]\tvalidation_0-auc:0.691174\n",
            "[55]\tvalidation_0-auc:0.690239\n",
            "[56]\tvalidation_0-auc:0.692599\n",
            "[57]\tvalidation_0-auc:0.694024\n",
            "[58]\tvalidation_0-auc:0.696206\n",
            "[59]\tvalidation_0-auc:0.697097\n",
            "[60]\tvalidation_0-auc:0.699412\n",
            "[61]\tvalidation_0-auc:0.700525\n",
            "[62]\tvalidation_0-auc:0.703732\n",
            "[63]\tvalidation_0-auc:0.699947\n",
            "[64]\tvalidation_0-auc:0.700347\n",
            "[65]\tvalidation_0-auc:0.699902\n",
            "[66]\tvalidation_0-auc:0.699412\n",
            "[67]\tvalidation_0-auc:0.699947\n",
            "[68]\tvalidation_0-auc:0.703821\n",
            "[69]\tvalidation_0-auc:0.705914\n",
            "[70]\tvalidation_0-auc:0.70725\n",
            "[71]\tvalidation_0-auc:0.707294\n",
            "[72]\tvalidation_0-auc:0.707918\n",
            "[73]\tvalidation_0-auc:0.70774\n",
            "[74]\tvalidation_0-auc:0.70892\n",
            "[75]\tvalidation_0-auc:0.708875\n",
            "[76]\tvalidation_0-auc:0.708697\n",
            "[77]\tvalidation_0-auc:0.710924\n",
            "[78]\tvalidation_0-auc:0.709209\n",
            "[79]\tvalidation_0-auc:0.709387\n",
            "[80]\tvalidation_0-auc:0.70725\n",
            "[81]\tvalidation_0-auc:0.707695\n",
            "[82]\tvalidation_0-auc:0.707962\n",
            "[83]\tvalidation_0-auc:0.708986\n",
            "[84]\tvalidation_0-auc:0.709788\n",
            "[85]\tvalidation_0-auc:0.709031\n",
            "[86]\tvalidation_0-auc:0.707561\n",
            "[87]\tvalidation_0-auc:0.708318\n",
            "[88]\tvalidation_0-auc:0.708096\n",
            "[89]\tvalidation_0-auc:0.707339\n",
            "[90]\tvalidation_0-auc:0.705246\n",
            "[91]\tvalidation_0-auc:0.7048\n",
            "[92]\tvalidation_0-auc:0.703776\n",
            "[93]\tvalidation_0-auc:0.703197\n",
            "[94]\tvalidation_0-auc:0.702841\n",
            "[95]\tvalidation_0-auc:0.703153\n",
            "[96]\tvalidation_0-auc:0.703865\n",
            "[97]\tvalidation_0-auc:0.702886\n",
            "[98]\tvalidation_0-auc:0.702218\n",
            "[99]\tvalidation_0-auc:0.703554\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.8857142857142857 |        0.0         |         0.0         |         0.0         |\n",
            "|      GRU 0.15     | 0.8959183673469387 | 0.8571428571428571 | 0.10714285714285714 | 0.19047619047619047 |\n",
            "|    XGBoost 0.15   | 0.8755102040816326 | 0.4074074074074074 | 0.19642857142857142 |  0.2650602409638554 |\n",
            "|    Logreg 0.15    | 0.8959183673469387 | 0.8571428571428571 | 0.10714285714285714 | 0.19047619047619047 |\n",
            "|      SVM 0.15     |        0.9         |        1.0         |        0.125        |  0.2222222222222222 |\n",
            "|   LSTM beta 0.15  | 0.8774617067833698 |        0.0         |         0.0         |         0.0         |\n",
            "|   GRU beta 0.15   | 0.811816192560175  | 0.2972972972972973 | 0.39285714285714285 |  0.3384615384615385 |\n",
            "| XGBoost beta 0.15 | 0.8096280087527352 | 0.2876712328767123 |        0.375        |  0.3255813953488372 |\n",
            "|  logreg beta 0.15 | 0.8905908096280087 | 0.6153846153846154 |  0.2857142857142857 |  0.3902439024390244 |\n",
            "|   svm beta 0.15   | 0.8905908096280087 | 0.6153846153846154 |  0.2857142857142857 |  0.3902439024390244 |\n",
            "+-------------------+--------------------+--------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJIfb2q0ZPyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d96c7f-f682-4763-a70b-a3a99ac8dbe4"
      },
      "source": [
        "Result_cross.to_csv('SYY_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.224806</td>\n",
              "      <td>0.740816</td>\n",
              "      <td>0.313514</td>\n",
              "      <td>0.517857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.301370</td>\n",
              "      <td>0.826531</td>\n",
              "      <td>0.341085</td>\n",
              "      <td>0.392857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.276190</td>\n",
              "      <td>0.789796</td>\n",
              "      <td>0.360248</td>\n",
              "      <td>0.517857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.877551</td>\n",
              "      <td>0.318182</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.350000</td>\n",
              "      <td>0.848980</td>\n",
              "      <td>0.362069</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.842451</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.297297</td>\n",
              "      <td>0.811816</td>\n",
              "      <td>0.338462</td>\n",
              "      <td>0.392857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.227723</td>\n",
              "      <td>0.757112</td>\n",
              "      <td>0.292994</td>\n",
              "      <td>0.410714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.486486</td>\n",
              "      <td>0.875274</td>\n",
              "      <td>0.387097</td>\n",
              "      <td>0.321429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.276316</td>\n",
              "      <td>0.803063</td>\n",
              "      <td>0.318182</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.885714</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.895918</td>\n",
              "      <td>0.190476</td>\n",
              "      <td>0.107143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.407407</td>\n",
              "      <td>0.875510</td>\n",
              "      <td>0.265060</td>\n",
              "      <td>0.196429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.895918</td>\n",
              "      <td>0.190476</td>\n",
              "      <td>0.107143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.877462</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.297297</td>\n",
              "      <td>0.811816</td>\n",
              "      <td>0.338462</td>\n",
              "      <td>0.392857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.287671</td>\n",
              "      <td>0.809628</td>\n",
              "      <td>0.325581</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.890591</td>\n",
              "      <td>0.390244</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.890591</td>\n",
              "      <td>0.390244</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  SYY  0.224806  0.740816  0.313514  0.517857\n",
              "1            GRU 0.1  SYY  0.301370  0.826531  0.341085  0.392857\n",
              "2        XGBoost 0.1  SYY  0.276190  0.789796  0.360248  0.517857\n",
              "3         Logreg 0.1  SYY  0.437500  0.877551  0.318182  0.250000\n",
              "4            SVM 0.1  SYY  0.350000  0.848980  0.362069  0.375000\n",
              "5      LSTM beta 0.1  SYY  0.333333  0.842451  0.307692  0.285714\n",
              "6       GRU beta 0.1  SYY  0.297297  0.811816  0.338462  0.392857\n",
              "7   XGBoost beta 0.1  SYY  0.227723  0.757112  0.292994  0.410714\n",
              "8    logreg beta 0.1  SYY  0.486486  0.875274  0.387097  0.321429\n",
              "9       svm beta 0.1  SYY  0.276316  0.803063  0.318182  0.375000\n",
              "0          LSTM 0.15  SYY  0.000000  0.885714  0.000000  0.000000\n",
              "1           GRU 0.15  SYY  0.857143  0.895918  0.190476  0.107143\n",
              "2       XGBoost 0.15  SYY  0.407407  0.875510  0.265060  0.196429\n",
              "3        Logreg 0.15  SYY  0.857143  0.895918  0.190476  0.107143\n",
              "4           SVM 0.15  SYY  1.000000  0.900000  0.222222  0.125000\n",
              "5     LSTM beta 0.15  SYY  0.000000  0.877462  0.000000  0.000000\n",
              "6      GRU beta 0.15  SYY  0.297297  0.811816  0.338462  0.392857\n",
              "7  XGBoost beta 0.15  SYY  0.287671  0.809628  0.325581  0.375000\n",
              "8   logreg beta 0.15  SYY  0.615385  0.890591  0.390244  0.285714\n",
              "9      svm beta 0.15  SYY  0.615385  0.890591  0.390244  0.285714"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzJKjV8FZPyM"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNCRxvhuZPyM"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmbbIM9VZPyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e04056df-2021-4381-a41c-a5a9771d4fd8"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"SYY\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6619 - accuracy: 0.6396 - val_loss: 0.5449 - val_accuracy: 0.8857\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6513 - accuracy: 0.6470 - val_loss: 0.6163 - val_accuracy: 0.8857\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6329 - accuracy: 0.6523 - val_loss: 0.5904 - val_accuracy: 0.6633\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6135 - accuracy: 0.6711 - val_loss: 0.4898 - val_accuracy: 0.8857\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6023 - accuracy: 0.6805 - val_loss: 0.5017 - val_accuracy: 0.8878\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6569 - accuracy: 0.6443 - val_loss: 0.5039 - val_accuracy: 0.8857\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6279 - accuracy: 0.6658 - val_loss: 0.5391 - val_accuracy: 0.8837\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6043 - accuracy: 0.6805 - val_loss: 0.5127 - val_accuracy: 0.8592\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5877 - accuracy: 0.7047 - val_loss: 0.5683 - val_accuracy: 0.7796\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5832 - accuracy: 0.7101 - val_loss: 0.4847 - val_accuracy: 0.8286\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.678386\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.675177\n",
            "[2]\tvalidation_0-auc:0.670445\n",
            "[3]\tvalidation_0-auc:0.680855\n",
            "[4]\tvalidation_0-auc:0.67707\n",
            "[5]\tvalidation_0-auc:0.672914\n",
            "[6]\tvalidation_0-auc:0.672626\n",
            "[7]\tvalidation_0-auc:0.674375\n",
            "[8]\tvalidation_0-auc:0.674128\n",
            "[9]\tvalidation_0-auc:0.668388\n",
            "[10]\tvalidation_0-auc:0.669828\n",
            "[11]\tvalidation_0-auc:0.670054\n",
            "[12]\tvalidation_0-auc:0.667832\n",
            "[13]\tvalidation_0-auc:0.671721\n",
            "[14]\tvalidation_0-auc:0.672976\n",
            "[15]\tvalidation_0-auc:0.675856\n",
            "[16]\tvalidation_0-auc:0.674004\n",
            "[17]\tvalidation_0-auc:0.674416\n",
            "[18]\tvalidation_0-auc:0.674292\n",
            "[19]\tvalidation_0-auc:0.675774\n",
            "[20]\tvalidation_0-auc:0.676267\n",
            "[21]\tvalidation_0-auc:0.67707\n",
            "[22]\tvalidation_0-auc:0.675465\n",
            "[23]\tvalidation_0-auc:0.674889\n",
            "[24]\tvalidation_0-auc:0.674704\n",
            "[25]\tvalidation_0-auc:0.675341\n",
            "[26]\tvalidation_0-auc:0.675774\n",
            "[27]\tvalidation_0-auc:0.677625\n",
            "[28]\tvalidation_0-auc:0.677666\n",
            "[29]\tvalidation_0-auc:0.680176\n",
            "[30]\tvalidation_0-auc:0.680053\n",
            "[31]\tvalidation_0-auc:0.680382\n",
            "[32]\tvalidation_0-auc:0.679045\n",
            "[33]\tvalidation_0-auc:0.678427\n",
            "[34]\tvalidation_0-auc:0.678345\n",
            "[35]\tvalidation_0-auc:0.678427\n",
            "[36]\tvalidation_0-auc:0.680279\n",
            "[37]\tvalidation_0-auc:0.679291\n",
            "[38]\tvalidation_0-auc:0.680238\n",
            "[39]\tvalidation_0-auc:0.681596\n",
            "[40]\tvalidation_0-auc:0.68283\n",
            "[41]\tvalidation_0-auc:0.682871\n",
            "[42]\tvalidation_0-auc:0.682665\n",
            "[43]\tvalidation_0-auc:0.680958\n",
            "[44]\tvalidation_0-auc:0.68067\n",
            "[45]\tvalidation_0-auc:0.680999\n",
            "[46]\tvalidation_0-auc:0.68141\n",
            "[47]\tvalidation_0-auc:0.681328\n",
            "[48]\tvalidation_0-auc:0.682645\n",
            "[49]\tvalidation_0-auc:0.682316\n",
            "[50]\tvalidation_0-auc:0.681246\n",
            "[51]\tvalidation_0-auc:0.683427\n",
            "[52]\tvalidation_0-auc:0.683056\n",
            "[53]\tvalidation_0-auc:0.683673\n",
            "[54]\tvalidation_0-auc:0.683715\n",
            "[55]\tvalidation_0-auc:0.685772\n",
            "[56]\tvalidation_0-auc:0.686307\n",
            "[57]\tvalidation_0-auc:0.686266\n",
            "[58]\tvalidation_0-auc:0.685402\n",
            "[59]\tvalidation_0-auc:0.685155\n",
            "[60]\tvalidation_0-auc:0.685402\n",
            "[61]\tvalidation_0-auc:0.686986\n",
            "[62]\tvalidation_0-auc:0.686883\n",
            "[63]\tvalidation_0-auc:0.687006\n",
            "[64]\tvalidation_0-auc:0.686842\n",
            "[65]\tvalidation_0-auc:0.687582\n",
            "[66]\tvalidation_0-auc:0.687294\n",
            "[67]\tvalidation_0-auc:0.687047\n",
            "[68]\tvalidation_0-auc:0.687006\n",
            "[69]\tvalidation_0-auc:0.686842\n",
            "[70]\tvalidation_0-auc:0.687089\n",
            "[71]\tvalidation_0-auc:0.687006\n",
            "[72]\tvalidation_0-auc:0.688035\n",
            "[73]\tvalidation_0-auc:0.68857\n",
            "[74]\tvalidation_0-auc:0.688529\n",
            "[75]\tvalidation_0-auc:0.687911\n",
            "[76]\tvalidation_0-auc:0.687665\n",
            "[77]\tvalidation_0-auc:0.687047\n",
            "[78]\tvalidation_0-auc:0.688899\n",
            "[79]\tvalidation_0-auc:0.690051\n",
            "[80]\tvalidation_0-auc:0.691327\n",
            "[81]\tvalidation_0-auc:0.690586\n",
            "[82]\tvalidation_0-auc:0.691738\n",
            "[83]\tvalidation_0-auc:0.691615\n",
            "[84]\tvalidation_0-auc:0.690874\n",
            "[85]\tvalidation_0-auc:0.690504\n",
            "[86]\tvalidation_0-auc:0.690586\n",
            "[87]\tvalidation_0-auc:0.690174\n",
            "[88]\tvalidation_0-auc:0.689475\n",
            "[89]\tvalidation_0-auc:0.691327\n",
            "[90]\tvalidation_0-auc:0.691779\n",
            "[91]\tvalidation_0-auc:0.691738\n",
            "[92]\tvalidation_0-auc:0.691944\n",
            "[93]\tvalidation_0-auc:0.691944\n",
            "[94]\tvalidation_0-auc:0.692067\n",
            "[95]\tvalidation_0-auc:0.691738\n",
            "[96]\tvalidation_0-auc:0.69252\n",
            "[97]\tvalidation_0-auc:0.691697\n",
            "[98]\tvalidation_0-auc:0.691038\n",
            "[99]\tvalidation_0-auc:0.690997\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6448 - accuracy: 0.6507 - val_loss: 0.5430 - val_accuracy: 0.8775\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6372 - accuracy: 0.6465 - val_loss: 0.4477 - val_accuracy: 0.8775\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5976 - accuracy: 0.6946 - val_loss: 0.4275 - val_accuracy: 0.8643\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5985 - accuracy: 0.6815 - val_loss: 0.4306 - val_accuracy: 0.8731\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5578 - accuracy: 0.7213 - val_loss: 0.4621 - val_accuracy: 0.8315\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6168 - accuracy: 0.6836 - val_loss: 0.3973 - val_accuracy: 0.8884\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5353 - accuracy: 0.7426 - val_loss: 0.4157 - val_accuracy: 0.8840\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5135 - accuracy: 0.7666 - val_loss: 0.4165 - val_accuracy: 0.8643\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5031 - accuracy: 0.7763 - val_loss: 0.4048 - val_accuracy: 0.8775\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5136 - accuracy: 0.7673 - val_loss: 0.5100 - val_accuracy: 0.7812\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.66726\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.672805\n",
            "[2]\tvalidation_0-auc:0.678171\n",
            "[3]\tvalidation_0-auc:0.677124\n",
            "[4]\tvalidation_0-auc:0.672871\n",
            "[5]\tvalidation_0-auc:0.673762\n",
            "[6]\tvalidation_0-auc:0.673495\n",
            "[7]\tvalidation_0-auc:0.675699\n",
            "[8]\tvalidation_0-auc:0.672716\n",
            "[9]\tvalidation_0-auc:0.670756\n",
            "[10]\tvalidation_0-auc:0.67305\n",
            "[11]\tvalidation_0-auc:0.671981\n",
            "[12]\tvalidation_0-auc:0.669264\n",
            "[13]\tvalidation_0-auc:0.66824\n",
            "[14]\tvalidation_0-auc:0.671424\n",
            "[15]\tvalidation_0-auc:0.669598\n",
            "[16]\tvalidation_0-auc:0.66971\n",
            "[17]\tvalidation_0-auc:0.672315\n",
            "[18]\tvalidation_0-auc:0.673807\n",
            "[19]\tvalidation_0-auc:0.673361\n",
            "[20]\tvalidation_0-auc:0.67345\n",
            "[21]\tvalidation_0-auc:0.675321\n",
            "[22]\tvalidation_0-auc:0.677235\n",
            "[23]\tvalidation_0-auc:0.675566\n",
            "[24]\tvalidation_0-auc:0.677814\n",
            "[25]\tvalidation_0-auc:0.674831\n",
            "[26]\tvalidation_0-auc:0.676701\n",
            "[27]\tvalidation_0-auc:0.67826\n",
            "[28]\tvalidation_0-auc:0.679418\n",
            "[29]\tvalidation_0-auc:0.679551\n",
            "[30]\tvalidation_0-auc:0.680598\n",
            "[31]\tvalidation_0-auc:0.678638\n",
            "[32]\tvalidation_0-auc:0.677614\n",
            "[33]\tvalidation_0-auc:0.678772\n",
            "[34]\tvalidation_0-auc:0.680241\n",
            "[35]\tvalidation_0-auc:0.681043\n",
            "[36]\tvalidation_0-auc:0.680019\n",
            "[37]\tvalidation_0-auc:0.680642\n",
            "[38]\tvalidation_0-auc:0.68033\n",
            "[39]\tvalidation_0-auc:0.679596\n",
            "[40]\tvalidation_0-auc:0.676478\n",
            "[41]\tvalidation_0-auc:0.677102\n",
            "[42]\tvalidation_0-auc:0.677993\n",
            "[43]\tvalidation_0-auc:0.680108\n",
            "[44]\tvalidation_0-auc:0.68082\n",
            "[45]\tvalidation_0-auc:0.680932\n",
            "[46]\tvalidation_0-auc:0.679774\n",
            "[47]\tvalidation_0-auc:0.680553\n",
            "[48]\tvalidation_0-auc:0.680998\n",
            "[49]\tvalidation_0-auc:0.682023\n",
            "[50]\tvalidation_0-auc:0.682824\n",
            "[51]\tvalidation_0-auc:0.683537\n",
            "[52]\tvalidation_0-auc:0.682512\n",
            "[53]\tvalidation_0-auc:0.682824\n",
            "[54]\tvalidation_0-auc:0.683804\n",
            "[55]\tvalidation_0-auc:0.683626\n",
            "[56]\tvalidation_0-auc:0.683492\n",
            "[57]\tvalidation_0-auc:0.682446\n",
            "[58]\tvalidation_0-auc:0.683871\n",
            "[59]\tvalidation_0-auc:0.684004\n",
            "[60]\tvalidation_0-auc:0.684316\n",
            "[61]\tvalidation_0-auc:0.684806\n",
            "[62]\tvalidation_0-auc:0.683025\n",
            "[63]\tvalidation_0-auc:0.683114\n",
            "[64]\tvalidation_0-auc:0.681956\n",
            "[65]\tvalidation_0-auc:0.682624\n",
            "[66]\tvalidation_0-auc:0.680976\n",
            "[67]\tvalidation_0-auc:0.680753\n",
            "[68]\tvalidation_0-auc:0.680598\n",
            "[69]\tvalidation_0-auc:0.679974\n",
            "[70]\tvalidation_0-auc:0.680998\n",
            "[71]\tvalidation_0-auc:0.680642\n",
            "[72]\tvalidation_0-auc:0.681266\n",
            "[73]\tvalidation_0-auc:0.682646\n",
            "[74]\tvalidation_0-auc:0.683091\n",
            "[75]\tvalidation_0-auc:0.683626\n",
            "[76]\tvalidation_0-auc:0.682112\n",
            "[77]\tvalidation_0-auc:0.681755\n",
            "[78]\tvalidation_0-auc:0.681355\n",
            "[79]\tvalidation_0-auc:0.681132\n",
            "[80]\tvalidation_0-auc:0.681399\n",
            "[81]\tvalidation_0-auc:0.681488\n",
            "[82]\tvalidation_0-auc:0.684182\n",
            "[83]\tvalidation_0-auc:0.681555\n",
            "[84]\tvalidation_0-auc:0.681689\n",
            "[85]\tvalidation_0-auc:0.680843\n",
            "[86]\tvalidation_0-auc:0.680843\n",
            "[87]\tvalidation_0-auc:0.68111\n",
            "[88]\tvalidation_0-auc:0.683693\n",
            "[89]\tvalidation_0-auc:0.68347\n",
            "[90]\tvalidation_0-auc:0.684138\n",
            "[91]\tvalidation_0-auc:0.684182\n",
            "[92]\tvalidation_0-auc:0.685696\n",
            "[93]\tvalidation_0-auc:0.685607\n",
            "[94]\tvalidation_0-auc:0.685875\n",
            "[95]\tvalidation_0-auc:0.686721\n",
            "[96]\tvalidation_0-auc:0.687166\n",
            "[97]\tvalidation_0-auc:0.687122\n",
            "[98]\tvalidation_0-auc:0.686988\n",
            "[99]\tvalidation_0-auc:0.68583\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.8877551020408163 |  0.5294117647058824 | 0.16071428571428573 | 0.24657534246575344 |\n",
            "|     GRU 0.1      | 0.8285714285714286 |         0.3         |        0.375        | 0.33333333333333326 |\n",
            "|   XGBoost 0.1    | 0.789795918367347  |  0.2761904761904762 |  0.5178571428571429 |  0.360248447204969  |\n",
            "|    Logreg 0.1    | 0.8775510204081632 |        0.4375       |         0.25        |  0.3181818181818182 |\n",
            "|     SVM 0.1      | 0.8489795918367347 |         0.35        |        0.375        |  0.3620689655172413 |\n",
            "|  LSTM beta 0.1   | 0.8315098468271335 | 0.29411764705882354 | 0.26785714285714285 |  0.2803738317757009 |\n",
            "|   GRU beta 0.1   | 0.7811816192560175 |  0.2755102040816326 | 0.48214285714285715 |  0.3506493506493506 |\n",
            "| XGBoost beta 0.1 | 0.7571115973741794 | 0.22772277227722773 |  0.4107142857142857 |  0.2929936305732484 |\n",
            "| logreg beta 0.1  |  0.87527352297593  |  0.4864864864864865 | 0.32142857142857145 |  0.3870967741935484 |\n",
            "|   svm beta 0.1   | 0.8030634573304157 | 0.27631578947368424 |        0.375        |  0.3181818181818182 |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6104 - accuracy: 0.7067 - val_loss: 0.4765 - val_accuracy: 0.8857\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6108 - accuracy: 0.7101 - val_loss: 0.4596 - val_accuracy: 0.8857\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6062 - accuracy: 0.7101 - val_loss: 0.4713 - val_accuracy: 0.8857\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6069 - accuracy: 0.7101 - val_loss: 0.4415 - val_accuracy: 0.8857\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6043 - accuracy: 0.7101 - val_loss: 0.4688 - val_accuracy: 0.8857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6191 - accuracy: 0.7081 - val_loss: 0.4938 - val_accuracy: 0.8857\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5998 - accuracy: 0.7141 - val_loss: 0.4642 - val_accuracy: 0.8857\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5708 - accuracy: 0.7275 - val_loss: 0.4725 - val_accuracy: 0.8878\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5498 - accuracy: 0.7322 - val_loss: 0.5554 - val_accuracy: 0.7959\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5593 - accuracy: 0.7329 - val_loss: 0.3859 - val_accuracy: 0.8959\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.670178\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.666413\n",
            "[2]\tvalidation_0-auc:0.667791\n",
            "[3]\tvalidation_0-auc:0.657711\n",
            "[4]\tvalidation_0-auc:0.665076\n",
            "[5]\tvalidation_0-auc:0.677995\n",
            "[6]\tvalidation_0-auc:0.671597\n",
            "[7]\tvalidation_0-auc:0.669375\n",
            "[8]\tvalidation_0-auc:0.670095\n",
            "[9]\tvalidation_0-auc:0.668779\n",
            "[10]\tvalidation_0-auc:0.665569\n",
            "[11]\tvalidation_0-auc:0.67421\n",
            "[12]\tvalidation_0-auc:0.672832\n",
            "[13]\tvalidation_0-auc:0.676905\n",
            "[14]\tvalidation_0-auc:0.675527\n",
            "[15]\tvalidation_0-auc:0.676226\n",
            "[16]\tvalidation_0-auc:0.67635\n",
            "[17]\tvalidation_0-auc:0.675876\n",
            "[18]\tvalidation_0-auc:0.677255\n",
            "[19]\tvalidation_0-auc:0.677152\n",
            "[20]\tvalidation_0-auc:0.677255\n",
            "[21]\tvalidation_0-auc:0.68141\n",
            "[22]\tvalidation_0-auc:0.68104\n",
            "[23]\tvalidation_0-auc:0.68141\n",
            "[24]\tvalidation_0-auc:0.682028\n",
            "[25]\tvalidation_0-auc:0.681061\n",
            "[26]\tvalidation_0-auc:0.680937\n",
            "[27]\tvalidation_0-auc:0.683838\n",
            "[28]\tvalidation_0-auc:0.683344\n",
            "[29]\tvalidation_0-auc:0.685813\n",
            "[30]\tvalidation_0-auc:0.684208\n",
            "[31]\tvalidation_0-auc:0.686019\n",
            "[32]\tvalidation_0-auc:0.682707\n",
            "[33]\tvalidation_0-auc:0.683118\n",
            "[34]\tvalidation_0-auc:0.683241\n",
            "[35]\tvalidation_0-auc:0.683571\n",
            "[36]\tvalidation_0-auc:0.683818\n",
            "[37]\tvalidation_0-auc:0.683838\n",
            "[38]\tvalidation_0-auc:0.684908\n",
            "[39]\tvalidation_0-auc:0.68534\n",
            "[40]\tvalidation_0-auc:0.684414\n",
            "[41]\tvalidation_0-auc:0.685648\n",
            "[42]\tvalidation_0-auc:0.685566\n",
            "[43]\tvalidation_0-auc:0.686615\n",
            "[44]\tvalidation_0-auc:0.686759\n",
            "[45]\tvalidation_0-auc:0.686142\n",
            "[46]\tvalidation_0-auc:0.687089\n",
            "[47]\tvalidation_0-auc:0.685772\n",
            "[48]\tvalidation_0-auc:0.68499\n",
            "[49]\tvalidation_0-auc:0.685196\n",
            "[50]\tvalidation_0-auc:0.685895\n",
            "[51]\tvalidation_0-auc:0.682521\n",
            "[52]\tvalidation_0-auc:0.682686\n",
            "[53]\tvalidation_0-auc:0.680917\n",
            "[54]\tvalidation_0-auc:0.680094\n",
            "[55]\tvalidation_0-auc:0.679641\n",
            "[56]\tvalidation_0-auc:0.680382\n",
            "[57]\tvalidation_0-auc:0.679189\n",
            "[58]\tvalidation_0-auc:0.678448\n",
            "[59]\tvalidation_0-auc:0.679559\n",
            "[60]\tvalidation_0-auc:0.680217\n",
            "[61]\tvalidation_0-auc:0.679271\n",
            "[62]\tvalidation_0-auc:0.680258\n",
            "[63]\tvalidation_0-auc:0.6803\n",
            "[64]\tvalidation_0-auc:0.680217\n",
            "[65]\tvalidation_0-auc:0.679888\n",
            "[66]\tvalidation_0-auc:0.677831\n",
            "[67]\tvalidation_0-auc:0.677049\n",
            "[68]\tvalidation_0-auc:0.67779\n",
            "[69]\tvalidation_0-auc:0.679724\n",
            "[70]\tvalidation_0-auc:0.679271\n",
            "[71]\tvalidation_0-auc:0.678695\n",
            "[72]\tvalidation_0-auc:0.678818\n",
            "[73]\tvalidation_0-auc:0.677872\n",
            "[74]\tvalidation_0-auc:0.678201\n",
            "[75]\tvalidation_0-auc:0.678613\n",
            "[76]\tvalidation_0-auc:0.681246\n",
            "[77]\tvalidation_0-auc:0.681452\n",
            "[78]\tvalidation_0-auc:0.683097\n",
            "[79]\tvalidation_0-auc:0.682439\n",
            "[80]\tvalidation_0-auc:0.682192\n",
            "[81]\tvalidation_0-auc:0.683962\n",
            "[82]\tvalidation_0-auc:0.685319\n",
            "[83]\tvalidation_0-auc:0.683056\n",
            "[84]\tvalidation_0-auc:0.683756\n",
            "[85]\tvalidation_0-auc:0.681575\n",
            "[86]\tvalidation_0-auc:0.68141\n",
            "[87]\tvalidation_0-auc:0.680382\n",
            "[88]\tvalidation_0-auc:0.678942\n",
            "[89]\tvalidation_0-auc:0.679065\n",
            "[90]\tvalidation_0-auc:0.67853\n",
            "[91]\tvalidation_0-auc:0.678736\n",
            "[92]\tvalidation_0-auc:0.677913\n",
            "[93]\tvalidation_0-auc:0.674827\n",
            "[94]\tvalidation_0-auc:0.676391\n",
            "[95]\tvalidation_0-auc:0.676679\n",
            "[96]\tvalidation_0-auc:0.678777\n",
            "Stopping. Best iteration:\n",
            "[46]\tvalidation_0-auc:0.687089\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6077 - accuracy: 0.7152 - val_loss: 0.4218 - val_accuracy: 0.8775\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5654 - accuracy: 0.7289 - val_loss: 0.3743 - val_accuracy: 0.8775\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5424 - accuracy: 0.7406 - val_loss: 0.3757 - val_accuracy: 0.8490\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5090 - accuracy: 0.7632 - val_loss: 0.4012 - val_accuracy: 0.8775\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4938 - accuracy: 0.7728 - val_loss: 0.4426 - val_accuracy: 0.8446\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.5897 - accuracy: 0.7152 - val_loss: 0.4029 - val_accuracy: 0.8775\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5118 - accuracy: 0.7694 - val_loss: 0.4759 - val_accuracy: 0.8468\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4857 - accuracy: 0.7769 - val_loss: 0.3759 - val_accuracy: 0.8906\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4536 - accuracy: 0.7859 - val_loss: 0.3696 - val_accuracy: 0.8906\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4293 - accuracy: 0.8202 - val_loss: 0.3914 - val_accuracy: 0.8840\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.662585\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.663275\n",
            "[2]\tvalidation_0-auc:0.667884\n",
            "[3]\tvalidation_0-auc:0.670244\n",
            "[4]\tvalidation_0-auc:0.633194\n",
            "[5]\tvalidation_0-auc:0.633639\n",
            "[6]\tvalidation_0-auc:0.642835\n",
            "[7]\tvalidation_0-auc:0.643859\n",
            "[8]\tvalidation_0-auc:0.641031\n",
            "[9]\tvalidation_0-auc:0.659623\n",
            "[10]\tvalidation_0-auc:0.665568\n",
            "[11]\tvalidation_0-auc:0.670155\n",
            "[12]\tvalidation_0-auc:0.676122\n",
            "[13]\tvalidation_0-auc:0.668374\n",
            "[14]\tvalidation_0-auc:0.67423\n",
            "[15]\tvalidation_0-auc:0.677859\n",
            "[16]\tvalidation_0-auc:0.677146\n",
            "[17]\tvalidation_0-auc:0.670556\n",
            "[18]\tvalidation_0-auc:0.67561\n",
            "[19]\tvalidation_0-auc:0.677169\n",
            "[20]\tvalidation_0-auc:0.676545\n",
            "[21]\tvalidation_0-auc:0.675944\n",
            "[22]\tvalidation_0-auc:0.679328\n",
            "[23]\tvalidation_0-auc:0.677926\n",
            "[24]\tvalidation_0-auc:0.679262\n",
            "[25]\tvalidation_0-auc:0.682713\n",
            "[26]\tvalidation_0-auc:0.683581\n",
            "[27]\tvalidation_0-auc:0.684873\n",
            "[28]\tvalidation_0-auc:0.686209\n",
            "[29]\tvalidation_0-auc:0.682045\n",
            "[30]\tvalidation_0-auc:0.68367\n",
            "[31]\tvalidation_0-auc:0.678082\n",
            "[32]\tvalidation_0-auc:0.678883\n",
            "[33]\tvalidation_0-auc:0.682646\n",
            "[34]\tvalidation_0-auc:0.681488\n",
            "[35]\tvalidation_0-auc:0.683002\n",
            "[36]\tvalidation_0-auc:0.685006\n",
            "[37]\tvalidation_0-auc:0.681911\n",
            "[38]\tvalidation_0-auc:0.683247\n",
            "[39]\tvalidation_0-auc:0.682357\n",
            "[40]\tvalidation_0-auc:0.680843\n",
            "[41]\tvalidation_0-auc:0.679818\n",
            "[42]\tvalidation_0-auc:0.680152\n",
            "[43]\tvalidation_0-auc:0.683091\n",
            "[44]\tvalidation_0-auc:0.68445\n",
            "[45]\tvalidation_0-auc:0.687255\n",
            "[46]\tvalidation_0-auc:0.688881\n",
            "[47]\tvalidation_0-auc:0.688836\n",
            "[48]\tvalidation_0-auc:0.690862\n",
            "[49]\tvalidation_0-auc:0.693\n",
            "[50]\tvalidation_0-auc:0.691575\n",
            "[51]\tvalidation_0-auc:0.691085\n",
            "[52]\tvalidation_0-auc:0.690773\n",
            "[53]\tvalidation_0-auc:0.690907\n",
            "[54]\tvalidation_0-auc:0.691174\n",
            "[55]\tvalidation_0-auc:0.690239\n",
            "[56]\tvalidation_0-auc:0.692599\n",
            "[57]\tvalidation_0-auc:0.694024\n",
            "[58]\tvalidation_0-auc:0.696206\n",
            "[59]\tvalidation_0-auc:0.697097\n",
            "[60]\tvalidation_0-auc:0.699412\n",
            "[61]\tvalidation_0-auc:0.700525\n",
            "[62]\tvalidation_0-auc:0.703732\n",
            "[63]\tvalidation_0-auc:0.699947\n",
            "[64]\tvalidation_0-auc:0.700347\n",
            "[65]\tvalidation_0-auc:0.699902\n",
            "[66]\tvalidation_0-auc:0.699412\n",
            "[67]\tvalidation_0-auc:0.699947\n",
            "[68]\tvalidation_0-auc:0.703821\n",
            "[69]\tvalidation_0-auc:0.705914\n",
            "[70]\tvalidation_0-auc:0.70725\n",
            "[71]\tvalidation_0-auc:0.707294\n",
            "[72]\tvalidation_0-auc:0.707918\n",
            "[73]\tvalidation_0-auc:0.70774\n",
            "[74]\tvalidation_0-auc:0.70892\n",
            "[75]\tvalidation_0-auc:0.708875\n",
            "[76]\tvalidation_0-auc:0.708697\n",
            "[77]\tvalidation_0-auc:0.710924\n",
            "[78]\tvalidation_0-auc:0.709209\n",
            "[79]\tvalidation_0-auc:0.709387\n",
            "[80]\tvalidation_0-auc:0.70725\n",
            "[81]\tvalidation_0-auc:0.707695\n",
            "[82]\tvalidation_0-auc:0.707962\n",
            "[83]\tvalidation_0-auc:0.708986\n",
            "[84]\tvalidation_0-auc:0.709788\n",
            "[85]\tvalidation_0-auc:0.709031\n",
            "[86]\tvalidation_0-auc:0.707561\n",
            "[87]\tvalidation_0-auc:0.708318\n",
            "[88]\tvalidation_0-auc:0.708096\n",
            "[89]\tvalidation_0-auc:0.707339\n",
            "[90]\tvalidation_0-auc:0.705246\n",
            "[91]\tvalidation_0-auc:0.7048\n",
            "[92]\tvalidation_0-auc:0.703776\n",
            "[93]\tvalidation_0-auc:0.703197\n",
            "[94]\tvalidation_0-auc:0.702841\n",
            "[95]\tvalidation_0-auc:0.703153\n",
            "[96]\tvalidation_0-auc:0.703865\n",
            "[97]\tvalidation_0-auc:0.702886\n",
            "[98]\tvalidation_0-auc:0.702218\n",
            "[99]\tvalidation_0-auc:0.703554\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.8857142857142857 |         0.0         |         0.0         |         0.0         |\n",
            "|      GRU 0.15     | 0.8959183673469387 |  0.8571428571428571 | 0.10714285714285714 | 0.19047619047619047 |\n",
            "|    XGBoost 0.15   | 0.8755102040816326 |  0.4074074074074074 | 0.19642857142857142 |  0.2650602409638554 |\n",
            "|    Logreg 0.15    | 0.8959183673469387 |  0.8571428571428571 | 0.10714285714285714 | 0.19047619047619047 |\n",
            "|      SVM 0.15     |        0.9         |         1.0         |        0.125        |  0.2222222222222222 |\n",
            "|   LSTM beta 0.15  | 0.8446389496717724 | 0.32558139534883723 |         0.25        |  0.2828282828282828 |\n",
            "|   GRU beta 0.15   | 0.8840262582056893 |  0.5454545454545454 | 0.32142857142857145 | 0.40449438202247195 |\n",
            "| XGBoost beta 0.15 | 0.8096280087527352 |  0.2876712328767123 |        0.375        |  0.3255813953488372 |\n",
            "|  logreg beta 0.15 | 0.8905908096280087 |  0.6153846153846154 |  0.2857142857142857 |  0.3902439024390244 |\n",
            "|   svm beta 0.15   | 0.8905908096280087 |  0.6153846153846154 |  0.2857142857142857 |  0.3902439024390244 |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uH7Ob-cZPyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "123335f7-fd5a-4521-929b-7d48c4ddd16c"
      },
      "source": [
        "Result_purging.to_csv('SYY_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.529412</td>\n",
              "      <td>0.887755</td>\n",
              "      <td>0.246575</td>\n",
              "      <td>0.160714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.828571</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.276190</td>\n",
              "      <td>0.789796</td>\n",
              "      <td>0.360248</td>\n",
              "      <td>0.517857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.877551</td>\n",
              "      <td>0.318182</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.350000</td>\n",
              "      <td>0.848980</td>\n",
              "      <td>0.362069</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>0.831510</td>\n",
              "      <td>0.280374</td>\n",
              "      <td>0.267857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.275510</td>\n",
              "      <td>0.781182</td>\n",
              "      <td>0.350649</td>\n",
              "      <td>0.482143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.227723</td>\n",
              "      <td>0.757112</td>\n",
              "      <td>0.292994</td>\n",
              "      <td>0.410714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.486486</td>\n",
              "      <td>0.875274</td>\n",
              "      <td>0.387097</td>\n",
              "      <td>0.321429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.276316</td>\n",
              "      <td>0.803063</td>\n",
              "      <td>0.318182</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.885714</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.895918</td>\n",
              "      <td>0.190476</td>\n",
              "      <td>0.107143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.407407</td>\n",
              "      <td>0.875510</td>\n",
              "      <td>0.265060</td>\n",
              "      <td>0.196429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.895918</td>\n",
              "      <td>0.190476</td>\n",
              "      <td>0.107143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.325581</td>\n",
              "      <td>0.844639</td>\n",
              "      <td>0.282828</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.884026</td>\n",
              "      <td>0.404494</td>\n",
              "      <td>0.321429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.287671</td>\n",
              "      <td>0.809628</td>\n",
              "      <td>0.325581</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.890591</td>\n",
              "      <td>0.390244</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.890591</td>\n",
              "      <td>0.390244</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  SYY  0.529412  0.887755  0.246575  0.160714\n",
              "1            GRU 0.1  SYY  0.300000  0.828571  0.333333  0.375000\n",
              "2        XGBoost 0.1  SYY  0.276190  0.789796  0.360248  0.517857\n",
              "3         Logreg 0.1  SYY  0.437500  0.877551  0.318182  0.250000\n",
              "4            SVM 0.1  SYY  0.350000  0.848980  0.362069  0.375000\n",
              "5      LSTM beta 0.1  SYY  0.294118  0.831510  0.280374  0.267857\n",
              "6       GRU beta 0.1  SYY  0.275510  0.781182  0.350649  0.482143\n",
              "7   XGBoost beta 0.1  SYY  0.227723  0.757112  0.292994  0.410714\n",
              "8    logreg beta 0.1  SYY  0.486486  0.875274  0.387097  0.321429\n",
              "9       svm beta 0.1  SYY  0.276316  0.803063  0.318182  0.375000\n",
              "0          LSTM 0.15  SYY  0.000000  0.885714  0.000000  0.000000\n",
              "1           GRU 0.15  SYY  0.857143  0.895918  0.190476  0.107143\n",
              "2       XGBoost 0.15  SYY  0.407407  0.875510  0.265060  0.196429\n",
              "3        Logreg 0.15  SYY  0.857143  0.895918  0.190476  0.107143\n",
              "4           SVM 0.15  SYY  1.000000  0.900000  0.222222  0.125000\n",
              "5     LSTM beta 0.15  SYY  0.325581  0.844639  0.282828  0.250000\n",
              "6      GRU beta 0.15  SYY  0.545455  0.884026  0.404494  0.321429\n",
              "7  XGBoost beta 0.15  SYY  0.287671  0.809628  0.325581  0.375000\n",
              "8   logreg beta 0.15  SYY  0.615385  0.890591  0.390244  0.285714\n",
              "9      svm beta 0.15  SYY  0.615385  0.890591  0.390244  0.285714"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fS9sETFZPyN"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOWgCHkdZPyO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB5tpi2kZu09"
      },
      "source": [
        "## TWTR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_hU15yoZu1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9922b61-6c0b-4c17-f8e1-535428e6988f"
      },
      "source": [
        "dfs = pd.read_csv(\"TWTR.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "# Denoise prices\n",
        "dfs = denoise_data(dfs, '<CLOSE>')\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1988</td>\n",
              "      <td>US1.TWTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>60.84</td>\n",
              "      <td>62.140</td>\n",
              "      <td>60.570</td>\n",
              "      <td>61.970</td>\n",
              "      <td>291952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1987</td>\n",
              "      <td>US1.TWTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>60.37</td>\n",
              "      <td>61.220</td>\n",
              "      <td>59.880</td>\n",
              "      <td>60.380</td>\n",
              "      <td>307226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1986</td>\n",
              "      <td>US1.TWTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>63.17</td>\n",
              "      <td>63.345</td>\n",
              "      <td>60.020</td>\n",
              "      <td>60.060</td>\n",
              "      <td>285767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1985</td>\n",
              "      <td>US1.TWTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>64.00</td>\n",
              "      <td>64.290</td>\n",
              "      <td>62.170</td>\n",
              "      <td>62.440</td>\n",
              "      <td>332449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1984</td>\n",
              "      <td>US1.TWTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>66.62</td>\n",
              "      <td>67.000</td>\n",
              "      <td>65.220</td>\n",
              "      <td>65.365</td>\n",
              "      <td>331665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1984</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.TWTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20131113</td>\n",
              "      <td>0</td>\n",
              "      <td>41.03</td>\n",
              "      <td>42.870</td>\n",
              "      <td>40.760</td>\n",
              "      <td>42.430</td>\n",
              "      <td>7606967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.TWTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20131112</td>\n",
              "      <td>0</td>\n",
              "      <td>43.66</td>\n",
              "      <td>43.780</td>\n",
              "      <td>41.830</td>\n",
              "      <td>41.920</td>\n",
              "      <td>5902473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1986</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.TWTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20131111</td>\n",
              "      <td>0</td>\n",
              "      <td>40.50</td>\n",
              "      <td>43.000</td>\n",
              "      <td>39.400</td>\n",
              "      <td>43.000</td>\n",
              "      <td>15755276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1987</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.TWTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20131108</td>\n",
              "      <td>0</td>\n",
              "      <td>45.93</td>\n",
              "      <td>46.940</td>\n",
              "      <td>40.685</td>\n",
              "      <td>41.650</td>\n",
              "      <td>26066094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1988</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.TWTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20131107</td>\n",
              "      <td>0</td>\n",
              "      <td>45.98</td>\n",
              "      <td>50.090</td>\n",
              "      <td>44.000</td>\n",
              "      <td>44.910</td>\n",
              "      <td>103209765</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1989 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  <TICKER> <PER>    <DATE>  ...  <HIGH>   <LOW>  <CLOSE>      <VOL>\n",
              "0      1988  US1.TWTR     D  20211001  ...  62.140  60.570   61.970     291952\n",
              "1      1987  US1.TWTR     D  20210930  ...  61.220  59.880   60.380     307226\n",
              "2      1986  US1.TWTR     D  20210929  ...  63.345  60.020   60.060     285767\n",
              "3      1985  US1.TWTR     D  20210928  ...  64.290  62.170   62.440     332449\n",
              "4      1984  US1.TWTR     D  20210927  ...  67.000  65.220   65.365     331665\n",
              "...     ...       ...   ...       ...  ...     ...     ...      ...        ...\n",
              "1984      4  US1.TWTR     D  20131113  ...  42.870  40.760   42.430    7606967\n",
              "1985      3  US1.TWTR     D  20131112  ...  43.780  41.830   41.920    5902473\n",
              "1986      2  US1.TWTR     D  20131111  ...  43.000  39.400   43.000   15755276\n",
              "1987      1  US1.TWTR     D  20131108  ...  46.940  40.685   41.650   26066094\n",
              "1988      0  US1.TWTR     D  20131107  ...  50.090  44.000   44.910  103209765\n",
              "\n",
              "[1989 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "755vVQDpZu1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cfbd41d-984b-41c7-c99b-fcb6dd028d20"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"4ca805b6-98af-4fba-a241-9a5d10e4c45f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"4ca805b6-98af-4fba-a241-9a5d10e4c45f\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '4ca805b6-98af-4fba-a241-9a5d10e4c45f',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [39.68, 40.8, 40.35, 40.03, 39.7, 40.24, 41.18, 41.33, 42.43, 42.51, 41.4, 43.4, 43.23, 42.93, 43.23, 43.25, 42.76, 42.61, 43.2, 43.25, 43.25, 44.25, 45.41, 45.31, 43.344, 41.92, 42.63, 42.48, 41.68, 42.13, 41.46, 41.01, 42.17, 42.8, 42.28, 41.7, 40.58, 40.12, 40.65, 41.83, 40.46, 41.54, 42.06, 41.73, 41.34, 40.36, 42.86, 42.1, 42.31, 41.01, 41.5, 41.53, 38.11, 38.74, 37.895, 37.58, 36.77, 37.68, 37.7087, 38.01, 38.69, 37.83, 37.19, 37.46, 37.64, 36.45, 36.26, 36.0311, 36.22, 36.1, 34.9, 34.74, 35.22, 34.72, 35.58, 35.0, 35.44, 36.28, 36.64, 36.44, 36.15, 36.35, 37.5, 37.21, 37.64, 37.94, 36.6, 36.33, 36.105, 34.41, 36.43, 37.14, 36.83, 37.3, 37.4, 37.18, 38.59, 37.47, 37.14, 37.49, 38.3, 37.93, 36.9302, 36.58, 38.47, 38.79, 38.6, 38.61, 40.24, 40.8001, 39.95, 39.3, 39.89, 39.81, 38.67, 38.49, 39.28, 39.76, 34.39, 34.4, 34.46, 34.4542, 34.7, 34.36, 34.57, 34.74, 35.11, 34.86, 34.7, 34.41, 34.38, 33.75, 33.435, 32.88, 32.86, 32.27, 33.05, 32.58, 33.02, 32.62, 32.58, 31.29, 31.09, 31.25, 31.03, 31.3, 31.16, 30.87, 30.04, 30.12, 30.8, 31.04, 30.5, 30.63, 30.78, 30.4, 31.01, 32.0, 31.71, 30.77, 31.39, 31.64, 31.23, 30.96, 31.115, 30.39, 30.23, 29.99, 30.8, 34.16, 34.37, 33.95, 33.18, 33.54, 32.25, 31.64, 33.14, 32.91, 31.6, 30.975, 32.26, 33.27, 32.86, 32.46, 33.0, 32.35, 32.85, 33.0808, 32.25, 31.805, 31.34, 29.95, 28.01, 28.81, 28.7311, 28.43, 28.68, 28.65, 26.45, 27.3, 29.2801, 32.92, 33.74, 33.43, 35.87, 35.88, 36.2434, 34.445, 33.43, 32.8406, 32.96, 32.57, 33.65, 31.45, 31.3, 32.72, 32.63, 32.82, 31.0, 31.61, 31.05, 31.98, 33.69, 33.14, 32.925, 32.49, 32.01, 34.07, 34.17, 34.98, 34.4, 34.015, 34.29, 34.62, 34.75, 33.86, 32.41, 32.38, 31.8, 27.54, 28.78, 29.18, 28.81, 29.28, 29.55, 29.87, 28.61, 27.995, 27.0, 26.8, 29.28, 28.47, 28.4, 28.23, 29.01, 28.19, 28.3, 28.47, 29.42, 29.02, 29.11, 28.59, 28.5, 29.86, 29.53, 29.22, 28.855, 30.12, 30.38, 29.75, 30.895, 30.54, 30.49, 30.82, 32.73, 34.84, 35.19, 35.65, 35.35, 35.49, 35.89, 34.28, 33.88, 33.82, 33.68, 32.61, 32.74, 32.83, 32.39, 33.175, 32.8, 32.01, 31.96, 31.84, 32.67, 32.97, 31.95, 32.8316, 31.92, 31.83, 31.38, 34.12, 42.94, 44.22, 42.17, 43.3, 43.42, 43.44, 43.34, 44.7, 44.26, 44.47, 45.25, 43.87, 43.74, 44.1163, 46.65, 45.08, 43.89, 44.97, 43.69, 44.79, 43.69, 44.83, 44.19, 45.9, 45.23, 46.12, 44.9, 46.02, 45.8, 46.77, 44.08, 43.48, 41.42, 41.2, 39.69, 40.09, 39.79, 37.92, 36.64, 34.69, 34.36, 34.01, 33.64, 33.53, 33.43, 32.86, 33.64, 32.63, 32.555, 32.77, 32.74, 33.39, 32.735, 32.87, 32.46, 31.855, 31.33, 31.06, 30.67, 30.56, 30.27, 30.31, 29.005, 30.26, 29.88, 30.4678, 31.22, 31.92, 31.54, 31.545, 31.83, 28.58, 28.7501, 28.99, 29.39, 29.54, 28.01, 28.1, 28.65, 28.2499, 27.55, 28.05, 29.005, 28.44, 28.065, 31.905, 31.03, 31.2, 32.73, 31.35, 34.98, 35.57, 35.815, 36.6, 34.095, 35.49, 35.34, 34.86, 35.76, 34.4148, 34.59, 32.995, 32.2651, 31.86, 31.32, 32.1675, 32.67, 32.11, 33.37, 32.83, 33.05, 33.6, 33.75, 33.43, 30.95, 31.5, 30.17, 26.92, 25.25, 25.15, 25.925, 27.15, 25.8186, 25.62, 25.18, 24.2711, 22.17, 22.36, 22.8, 23.34, 23.65, 24.0302, 24.55, 24.68, 25.415, 24.34, 24.24, 24.17, 24.605, 24.32, 23.985, 24.45, 24.505, 24.015, 24.31, 24.23, 24.255, 24.4501, 25.05, 25.19, 25.07, 24.695, 22.22, 22.57, 21.67, 21.645, 22.05, 21.1, 21.0, 21.085, 20.78, 20.4, 20.72, 20.5795, 20.785, 21.8201, 21.82, 22.4198, 22.27, 21.885, 21.135, 20.76, 20.35, 19.91, 20.045, 20.16, 20.325, 19.9, 19.6, 19.655, 19.39, 19.88, 19.7, 20.6, 20.6186, 21.25, 21.68, 20.315, 17.16, 17.25, 17.36, 17.86, 17.8901, 18.03, 18.28, 18.34, 18.63, 18.475, 17.735, 17.41, 17.67, 17.855, 18.25, 17.755, 17.61, 17.1, 16.88, 16.85, 16.95, 16.585, 16.985, 17.6, 17.5853, 17.63, 17.77, 17.59, 18.01, 18.21, 18.1962, 18.18, 17.665, 17.45, 17.23, 16.8201, 16.641, 16.86, 16.9, 16.925, 16.93, 16.77, 16.64, 16.89, 16.98, 16.635, 16.105, 16.0, 15.8676, 16.145, 15.95, 16.085, 15.935, 15.755, 16.145, 16.145, 16.39, 16.29, 16.18, 16.07, 16.2062, 16.06, 16.75, 16.84, 19.63, 19.97, 20.0, 20.11, 20.525, 20.11, 19.97, 19.93, 19.635, 19.32, 19.26, 18.64, 18.075, 18.005, 17.925, 17.82, 17.6, 17.86, 17.6446, 17.93, 18.125, 18.295, 18.495, 18.155, 17.77, 16.9, 17.07, 16.66, 16.835, 16.76, 16.98, 17.05, 16.91, 17.59, 17.43, 17.57, 18.24, 18.3, 18.52, 18.3, 18.44, 18.23, 17.95, 17.98, 18.145, 18.445, 18.37, 18.505, 18.285, 19.49, 19.2252, 18.61, 18.38, 18.54, 18.36, 18.31, 18.68, 18.49, 18.56, 18.24, 17.535, 16.475, 16.6, 15.815, 14.67, 14.71, 14.6375, 14.65, 14.535, 14.44, 14.41, 14.305, 14.41, 14.31, 14.35, 14.29, 14.39, 14.52, 14.68, 14.84, 14.9401, 14.91, 15.04, 14.95, 14.98, 15.14, 14.93, 14.98, 14.53, 15.09, 15.075, 15.19, 15.04, 15.31, 15.22, 15.11, 15.21, 15.23, 15.19, 15.57, 15.74, 15.78, 15.8, 15.77, 16.05, 15.96, 16.028, 16.0657, 16.43, 16.62, 16.35, 16.74, 16.5122, 15.81, 15.5749, 16.4001, 18.71, 18.25, 17.945, 17.61, 17.78, 17.23, 17.615, 16.94, 16.57, 16.8128, 16.73, 16.51, 16.6, 16.58, 16.795, 17.1151, 16.955, 17.24, 17.37, 17.29, 17.365, 17.51, 17.175, 17.1, 16.86, 16.43, 16.29, 16.39, 16.38, 16.605, 16.495, 16.4, 17.0761, 17.91, 18.24, 18.6299, 18.79, 18.93, 19.35, 18.93, 19.64, 19.65, 19.48, 18.23, 18.23, 17.9399, 18.02, 18.49, 18.2, 18.3, 18.05, 18.23, 18.631, 18.61, 18.73, 18.55, 18.62, 18.9889, 19.15, 18.545, 18.3799, 19.1364, 18.37, 18.4, 18.03, 17.57, 17.6142, 17.4832, 17.95, 17.67, 17.39, 17.295, 17.251, 18.035, 18.09, 16.9, 17.06, 16.83, 16.72, 16.8799, 17.79, 18.05, 17.9948, 17.57, 19.815, 19.86, 24.8601, 23.505, 23.99, 23.05, 23.005, 22.955, 23.73, 23.39, 22.63, 18.625, 18.49, 18.39, 18.36, 19.1, 18.295, 18.07, 17.755, 18.145, 18.1092, 18.69, 19.86, 19.92, 19.54, 19.495, 19.215, 18.38, 18.479, 18.295, 18.31, 18.24, 18.7, 18.5475, 18.98, 18.985, 20.17, 20.395, 20.86, 19.535, 19.77, 19.045, 18.675, 18.2, 18.255, 18.13, 17.61, 16.41, 16.641, 16.635, 16.3, 15.7601, 18.44, 18.65, 18.37, 18.4, 18.56, 18.34, 18.66, 18.065, 17.97, 17.732, 18.1, 17.72, 18.085, 17.37, 17.19, 17.135, 17.2701, 16.89, 16.83, 16.42, 15.84, 16.43, 17.03, 16.13, 16.31, 16.35, 16.07, 15.8701, 15.97, 15.38, 14.56, 14.015, 14.605, 14.945, 15.005, 15.28, 15.19, 15.19, 15.02, 15.23, 15.0501, 14.28, 14.4, 14.03, 14.4, 14.43, 14.14, 14.14, 14.34, 14.28, 14.105, 14.09, 14.6, 14.62, 14.195, 14.4, 14.11, 14.845, 14.0, 14.39, 14.61, 14.64, 14.86, 17.75, 17.075, 17.235, 17.52, 17.405, 16.9199, 17.31, 17.59, 17.53, 17.36, 16.56, 16.51, 16.65, 16.97, 17.2699, 17.05, 17.1, 15.98, 16.54, 16.35, 15.965, 15.61, 15.905, 16.0183, 16.8597, 16.885, 16.835, 16.84, 16.705, 16.1899, 17.13, 16.812, 16.62, 17.6599, 18.34, 19.155, 19.3524, 19.31, 18.55, 17.85, 18.12, 17.941, 17.59, 18.0, 18.32, 18.31, 18.32, 18.44, 17.45, 16.36, 15.8878, 14.32, 14.971, 14.395, 14.89, 15.72, 16.92, 16.569, 16.09, 17.9, 16.8, 16.516, 16.78, 17.01, 17.01, 17.84, 17.82, 17.3878, 16.69, 17.94, 19.005, 18.68, 19.6258, 19.63, 19.98, 20.26, 21.39, 21.9, 22.54, 23.14, 22.22, 22.4778, 22.5202, 22.95, 22.6601, 22.57, 22.15, 22.98, 23.31, 24.295, 23.96, 24.92, 24.85, 25.94, 24.32, 25.0, 24.47, 25.03, 25.91, 25.415, 25.54, 25.39, 25.68, 26.05, 25.52, 25.19, 26.28, 26.3153, 25.8999, 25.26, 25.41, 25.1775, 26.12, 26.52, 27.0505, 27.08, 28.26, 28.66, 29.36, 29.14, 29.19, 28.46, 29.0505, 30.87, 31.29, 30.89, 30.29, 29.16, 29.29, 30.91, 30.92, 31.144, 29.73, 29.381, 29.051, 28.7401, 30.85, 30.32, 29.83, 27.6314, 28.17, 26.3199, 24.667, 26.9399, 25.58, 25.24, 25.2921, 26.605, 26.805, 26.835, 27.36, 27.92, 27.41, 27.746, 27.17, 26.889, 27.39, 27.7001, 27.17, 27.18, 28.16, 28.28, 27.825, 27.02, 27.78, 26.84, 26.46, 25.03, 24.48, 25.26, 25.86, 26.0, 27.5949, 28.3, 29.06, 29.0799, 28.55, 29.41, 29.63, 29.5, 27.02, 27.55, 28.48, 29.34, 29.25, 31.005, 31.48, 31.235, 36.573, 34.7201, 35.4, 36.19, 36.08, 36.61, 35.805, 35.65, 36.1, 35.6608, 36.7042, 35.8, 34.925, 34.3551, 34.7285, 35.525, 35.4155, 35.7, 35.42, 36.23, 34.22, 35.25, 35.16, 35.18, 35.37, 35.57, 35.87, 34.66, 34.68, 34.82, 34.66, 35.88, 35.83, 35.85, 35.88, 36.455, 36.99, 36.7199, 37.0, 36.4, 36.61, 36.66, 36.83, 36.42, 36.51, 36.59, 36.69, 36.7953, 37.49, 37.28, 37.11, 37.32, 37.71, 37.48, 37.31, 37.5901, 37.71, 37.2528, 37.42, 37.87, 37.8338, 38.96, 38.49, 42.26, 51.66, 50.84, 51.42, 51.716, 51.34, 51.39, 50.65, 52.04, 51.3, 51.195, 51.61, 51.93, 52.17, 52.28, 52.86, 50.8199, 50.4001, 50.47, 50.0849, 49.88, 50.0001, 49.93, 49.4949, 51.48, 48.4701, 48.45, 47.93, 47.22, 46.93, 46.42, 46.635, 47.05, 46.28, 45.84, 47.5738, 46.74, 47.36, 47.5602, 47.6899, 48.15, 48.1, 49.43, 48.5462, 48.67, 48.43, 49.09, 48.71, 47.79, 48.03, 48.53, 47.95, 47.5076, 46.25, 47.3, 48.03, 41.2801, 40.72, 39.8, 37.45, 37.55, 36.67, 37.14, 38.9, 40.0999, 39.43, 39.0799, 37.81, 37.56, 37.28, 36.92, 39.84, 39.65, 39.35, 40.2, 39.11, 37.28, 38.76, 36.38, 36.55, 35.86, 35.85, 36.41, 37.59, 37.63, 37.57, 38.44, 37.06, 36.72, 35.57, 35.12, 36.8401, 37.099, 36.69, 36.3549, 37.0436, 36.28, 38.47, 38.79, 39.06, 38.92, 39.03, 41.54, 41.14, 39.74, 40.2, 40.01, 39.8, 39.7, 40.6, 40.49, 41.81, 40.06, 42.54, 39.6, 39.57, 40.2901, 40.85, 40.37, 40.89, 40.22, 41.45, 41.81, 42.07, 43.78, 48.56, 49.9199, 49.6699, 49.08, 50.64, 50.67, 48.7699, 48.25, 49.97, 48.58, 48.48, 50.38, 55.29, 55.41, 53.49, 53.49, 53.935, 51.85, 50.03, 51.571, 51.73, 51.88, 51.43, 52.94, 52.16, 51.9405, 52.59, 50.89, 50.69, 50.84, 49.38, 52.075, 52.6499, 52.89, 50.61, 51.9875, 50.68, 50.2131, 49.29, 51.01, 49.69, 49.41, 48.06, 48.194, 46.09, 46.0, 45.1, 45.0599, 45.08, 45.12, 44.75, 45.33, 44.15, 43.81, 43.27, 43.12, 42.9957, 43.45, 43.82, 43.45, 44.11, 45.18, 46.29, 38.6, 37.94, 38.17, 38.7, 37.74, 37.64, 38.04, 37.04, 36.88, 37.42, 37.88, 38.2901, 38.31, 37.83, 38.06, 37.41, 40.23, 41.2501, 41.76, 42.06, 40.94, 40.885, 41.45, 39.43, 38.49, 39.5049, 39.17, 38.91, 38.7436, 38.02, 38.025, 36.87, 36.8, 35.54, 35.37, 34.46, 33.33, 33.88, 32.91, 32.5801, 31.755, 32.44, 33.98, 33.7362, 30.5, 30.46, 31.48, 31.7, 31.7, 32.07, 32.26, 32.77, 32.85, 33.36, 33.93, 31.99, 31.955, 30.68, 31.87, 38.72, 39.02, 39.09, 38.725, 43.075, 40.57, 41.587, 44.67, 45.93, 45.97, 46.12, 45.03, 44.44, 45.52, 40.87, 40.06, 41.26, 42.5099, 41.79, 42.43, 43.12, 44.01, 45.69, 46.64, 46.69, 47.25, 46.19, 44.43, 47.9, 48.8, 50.27, 50.14, 51.27, 51.12, 52.08, 51.95, 53.5599, 54.44, 54.0201, 53.87, 53.52, 54.84, 54.36, 54.27, 53.69, 54.9, 55.77, 55.8799, 54.96, 55.77, 55.87, 56.63, 55.43, 58.251, 57.5599, 56.3, 56.7, 53.96, 52.86, 54.31, 50.02, 65.64, 66.34, 65.2301, 64.5, 63.46, 59.45, 60.458, 57.902, 61.72, 62.81, 62.44, 62.53, 62.2, 60.57, 61.5699, 58.1701, 57.7901, 56.99, 57.05, 59.29, 61.42, 66.29, 68.9701, 67.4, 63.65, 60.51, 63.7899, 73.21, 70.24, 64.6, 59.9377, 57.43, 55.44, 56.41, 56.66, 59.1501, 55.33, 52.311, 52.0, 49.19, 44.96, 45.35, 43.6499, 41.44, 40.74, 41.49, 40.89, 40.15, 39.06, 40.99, 42.09, 41.03, 41.7299, 41.01, 43.94, 44.71, 42.43, 41.92, 43.0, 41.65, 44.91]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4ca805b6-98af-4fba-a241-9a5d10e4c45f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"313d600f-6f13-4639-ad8b-7e76b12b6c55\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"313d600f-6f13-4639-ad8b-7e76b12b6c55\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '313d600f-6f13-4639-ad8b-7e76b12b6c55',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('313d600f-6f13-4639-ad8b-7e76b12b6c55');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM_vTGO7Zu1E"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnDjbfCWZu1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "627da571-9e5a-490d-e3f7-38c70b50f505"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.05, .1, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1400, test_end=1600)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"TWTR\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 3s 17ms/step - loss: 0.6579 - accuracy: 0.6596 - val_loss: 0.8832 - val_accuracy: 0.1895\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.6427 - accuracy: 0.6596 - val_loss: 0.8943 - val_accuracy: 0.1895\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6085 - accuracy: 0.6651 - val_loss: 0.6610 - val_accuracy: 0.7579\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.5810 - accuracy: 0.6945 - val_loss: 0.8017 - val_accuracy: 0.5053\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5583 - accuracy: 0.7055 - val_loss: 0.7446 - val_accuracy: 0.6579\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 2s 15ms/step - loss: 0.6376 - accuracy: 0.6596 - val_loss: 0.8685 - val_accuracy: 0.2211\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5728 - accuracy: 0.7055 - val_loss: 0.7198 - val_accuracy: 0.5579\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5323 - accuracy: 0.7229 - val_loss: 0.8954 - val_accuracy: 0.4737\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5131 - accuracy: 0.7376 - val_loss: 0.8236 - val_accuracy: 0.5000\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5130 - accuracy: 0.7431 - val_loss: 0.7306 - val_accuracy: 0.5789\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.747024\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.736923\n",
            "[2]\tvalidation_0-auc:0.770112\n",
            "[3]\tvalidation_0-auc:0.76921\n",
            "[4]\tvalidation_0-auc:0.769751\n",
            "[5]\tvalidation_0-auc:0.768849\n",
            "[6]\tvalidation_0-auc:0.772457\n",
            "[7]\tvalidation_0-auc:0.772637\n",
            "[8]\tvalidation_0-auc:0.771916\n",
            "[9]\tvalidation_0-auc:0.770833\n",
            "[10]\tvalidation_0-auc:0.771014\n",
            "[11]\tvalidation_0-auc:0.775703\n",
            "[12]\tvalidation_0-auc:0.780483\n",
            "[13]\tvalidation_0-auc:0.780483\n",
            "[14]\tvalidation_0-auc:0.781566\n",
            "[15]\tvalidation_0-auc:0.784091\n",
            "[16]\tvalidation_0-auc:0.78382\n",
            "[17]\tvalidation_0-auc:0.784001\n",
            "[18]\tvalidation_0-auc:0.784181\n",
            "[19]\tvalidation_0-auc:0.78373\n",
            "[20]\tvalidation_0-auc:0.783911\n",
            "[21]\tvalidation_0-auc:0.783189\n",
            "[22]\tvalidation_0-auc:0.787698\n",
            "[23]\tvalidation_0-auc:0.786255\n",
            "[24]\tvalidation_0-auc:0.787338\n",
            "[25]\tvalidation_0-auc:0.786797\n",
            "[26]\tvalidation_0-auc:0.790494\n",
            "[27]\tvalidation_0-auc:0.790675\n",
            "[28]\tvalidation_0-auc:0.790133\n",
            "[29]\tvalidation_0-auc:0.789953\n",
            "[30]\tvalidation_0-auc:0.790314\n",
            "[31]\tvalidation_0-auc:0.790675\n",
            "[32]\tvalidation_0-auc:0.791667\n",
            "[33]\tvalidation_0-auc:0.791667\n",
            "[34]\tvalidation_0-auc:0.791486\n",
            "[35]\tvalidation_0-auc:0.788961\n",
            "[36]\tvalidation_0-auc:0.788961\n",
            "[37]\tvalidation_0-auc:0.788961\n",
            "[38]\tvalidation_0-auc:0.788961\n",
            "[39]\tvalidation_0-auc:0.785985\n",
            "[40]\tvalidation_0-auc:0.785624\n",
            "[41]\tvalidation_0-auc:0.785985\n",
            "[42]\tvalidation_0-auc:0.785444\n",
            "[43]\tvalidation_0-auc:0.786165\n",
            "[44]\tvalidation_0-auc:0.784903\n",
            "[45]\tvalidation_0-auc:0.785083\n",
            "[46]\tvalidation_0-auc:0.785083\n",
            "[47]\tvalidation_0-auc:0.785804\n",
            "[48]\tvalidation_0-auc:0.784361\n",
            "[49]\tvalidation_0-auc:0.783911\n",
            "[50]\tvalidation_0-auc:0.783189\n",
            "[51]\tvalidation_0-auc:0.784001\n",
            "[52]\tvalidation_0-auc:0.784181\n",
            "[53]\tvalidation_0-auc:0.781385\n",
            "[54]\tvalidation_0-auc:0.781025\n",
            "[55]\tvalidation_0-auc:0.782648\n",
            "[56]\tvalidation_0-auc:0.783009\n",
            "[57]\tvalidation_0-auc:0.782828\n",
            "[58]\tvalidation_0-auc:0.783189\n",
            "[59]\tvalidation_0-auc:0.784993\n",
            "[60]\tvalidation_0-auc:0.782287\n",
            "[61]\tvalidation_0-auc:0.781566\n",
            "[62]\tvalidation_0-auc:0.781746\n",
            "[63]\tvalidation_0-auc:0.783009\n",
            "[64]\tvalidation_0-auc:0.78373\n",
            "[65]\tvalidation_0-auc:0.784091\n",
            "[66]\tvalidation_0-auc:0.782287\n",
            "[67]\tvalidation_0-auc:0.781205\n",
            "[68]\tvalidation_0-auc:0.781746\n",
            "[69]\tvalidation_0-auc:0.780664\n",
            "[70]\tvalidation_0-auc:0.781205\n",
            "[71]\tvalidation_0-auc:0.779762\n",
            "[72]\tvalidation_0-auc:0.77886\n",
            "[73]\tvalidation_0-auc:0.77904\n",
            "[74]\tvalidation_0-auc:0.779582\n",
            "[75]\tvalidation_0-auc:0.780303\n",
            "[76]\tvalidation_0-auc:0.77904\n",
            "[77]\tvalidation_0-auc:0.779582\n",
            "[78]\tvalidation_0-auc:0.779401\n",
            "[79]\tvalidation_0-auc:0.77868\n",
            "[80]\tvalidation_0-auc:0.777958\n",
            "[81]\tvalidation_0-auc:0.779401\n",
            "[82]\tvalidation_0-auc:0.781385\n",
            "Stopping. Best iteration:\n",
            "[32]\tvalidation_0-auc:0.791667\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 3s 16ms/step - loss: 0.6514 - accuracy: 0.6556 - val_loss: 1.0318 - val_accuracy: 0.1592\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.6440 - accuracy: 0.6632 - val_loss: 0.9005 - val_accuracy: 0.1592\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.6273 - accuracy: 0.6708 - val_loss: 0.8932 - val_accuracy: 0.1592\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.6073 - accuracy: 0.6812 - val_loss: 0.8250 - val_accuracy: 0.3503\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5793 - accuracy: 0.7039 - val_loss: 0.7008 - val_accuracy: 0.5669\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 2s 15ms/step - loss: 0.6408 - accuracy: 0.6632 - val_loss: 0.8861 - val_accuracy: 0.1592\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.6066 - accuracy: 0.6963 - val_loss: 0.6937 - val_accuracy: 0.5414\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5585 - accuracy: 0.7209 - val_loss: 0.7338 - val_accuracy: 0.5032\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5338 - accuracy: 0.7247 - val_loss: 0.5741 - val_accuracy: 0.7134\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5242 - accuracy: 0.7408 - val_loss: 0.7219 - val_accuracy: 0.5669\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.707576\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.764545\n",
            "[2]\tvalidation_0-auc:0.776364\n",
            "[3]\tvalidation_0-auc:0.777273\n",
            "[4]\tvalidation_0-auc:0.782879\n",
            "[5]\tvalidation_0-auc:0.784849\n",
            "[6]\tvalidation_0-auc:0.796212\n",
            "[7]\tvalidation_0-auc:0.790152\n",
            "[8]\tvalidation_0-auc:0.785\n",
            "[9]\tvalidation_0-auc:0.793333\n",
            "[10]\tvalidation_0-auc:0.795152\n",
            "[11]\tvalidation_0-auc:0.790758\n",
            "[12]\tvalidation_0-auc:0.795303\n",
            "[13]\tvalidation_0-auc:0.796364\n",
            "[14]\tvalidation_0-auc:0.793333\n",
            "[15]\tvalidation_0-auc:0.796212\n",
            "[16]\tvalidation_0-auc:0.793788\n",
            "[17]\tvalidation_0-auc:0.795606\n",
            "[18]\tvalidation_0-auc:0.799697\n",
            "[19]\tvalidation_0-auc:0.801061\n",
            "[20]\tvalidation_0-auc:0.803788\n",
            "[21]\tvalidation_0-auc:0.80197\n",
            "[22]\tvalidation_0-auc:0.799545\n",
            "[23]\tvalidation_0-auc:0.803788\n",
            "[24]\tvalidation_0-auc:0.801061\n",
            "[25]\tvalidation_0-auc:0.800909\n",
            "[26]\tvalidation_0-auc:0.805152\n",
            "[27]\tvalidation_0-auc:0.804848\n",
            "[28]\tvalidation_0-auc:0.804242\n",
            "[29]\tvalidation_0-auc:0.804242\n",
            "[30]\tvalidation_0-auc:0.803636\n",
            "[31]\tvalidation_0-auc:0.804697\n",
            "[32]\tvalidation_0-auc:0.805909\n",
            "[33]\tvalidation_0-auc:0.804091\n",
            "[34]\tvalidation_0-auc:0.804394\n",
            "[35]\tvalidation_0-auc:0.805606\n",
            "[36]\tvalidation_0-auc:0.805909\n",
            "[37]\tvalidation_0-auc:0.808636\n",
            "[38]\tvalidation_0-auc:0.805909\n",
            "[39]\tvalidation_0-auc:0.805303\n",
            "[40]\tvalidation_0-auc:0.806212\n",
            "[41]\tvalidation_0-auc:0.805606\n",
            "[42]\tvalidation_0-auc:0.805\n",
            "[43]\tvalidation_0-auc:0.806818\n",
            "[44]\tvalidation_0-auc:0.80197\n",
            "[45]\tvalidation_0-auc:0.802879\n",
            "[46]\tvalidation_0-auc:0.803182\n",
            "[47]\tvalidation_0-auc:0.803788\n",
            "[48]\tvalidation_0-auc:0.803182\n",
            "[49]\tvalidation_0-auc:0.803182\n",
            "[50]\tvalidation_0-auc:0.802576\n",
            "[51]\tvalidation_0-auc:0.806515\n",
            "[52]\tvalidation_0-auc:0.807727\n",
            "[53]\tvalidation_0-auc:0.807424\n",
            "[54]\tvalidation_0-auc:0.809697\n",
            "[55]\tvalidation_0-auc:0.809697\n",
            "[56]\tvalidation_0-auc:0.808485\n",
            "[57]\tvalidation_0-auc:0.80697\n",
            "[58]\tvalidation_0-auc:0.808788\n",
            "[59]\tvalidation_0-auc:0.809697\n",
            "[60]\tvalidation_0-auc:0.809091\n",
            "[61]\tvalidation_0-auc:0.804242\n",
            "[62]\tvalidation_0-auc:0.805758\n",
            "[63]\tvalidation_0-auc:0.806364\n",
            "[64]\tvalidation_0-auc:0.807273\n",
            "[65]\tvalidation_0-auc:0.807879\n",
            "[66]\tvalidation_0-auc:0.80697\n",
            "[67]\tvalidation_0-auc:0.807273\n",
            "[68]\tvalidation_0-auc:0.805758\n",
            "[69]\tvalidation_0-auc:0.803939\n",
            "[70]\tvalidation_0-auc:0.803939\n",
            "[71]\tvalidation_0-auc:0.804848\n",
            "[72]\tvalidation_0-auc:0.804848\n",
            "[73]\tvalidation_0-auc:0.806061\n",
            "[74]\tvalidation_0-auc:0.804848\n",
            "[75]\tvalidation_0-auc:0.808788\n",
            "[76]\tvalidation_0-auc:0.808788\n",
            "[77]\tvalidation_0-auc:0.808485\n",
            "[78]\tvalidation_0-auc:0.809697\n",
            "[79]\tvalidation_0-auc:0.806667\n",
            "[80]\tvalidation_0-auc:0.808788\n",
            "[81]\tvalidation_0-auc:0.803939\n",
            "[82]\tvalidation_0-auc:0.806667\n",
            "[83]\tvalidation_0-auc:0.806667\n",
            "[84]\tvalidation_0-auc:0.807879\n",
            "[85]\tvalidation_0-auc:0.806364\n",
            "[86]\tvalidation_0-auc:0.802121\n",
            "[87]\tvalidation_0-auc:0.803939\n",
            "[88]\tvalidation_0-auc:0.806364\n",
            "[89]\tvalidation_0-auc:0.808788\n",
            "[90]\tvalidation_0-auc:0.807879\n",
            "[91]\tvalidation_0-auc:0.807879\n",
            "[92]\tvalidation_0-auc:0.809394\n",
            "[93]\tvalidation_0-auc:0.808485\n",
            "[94]\tvalidation_0-auc:0.807576\n",
            "[95]\tvalidation_0-auc:0.807273\n",
            "[96]\tvalidation_0-auc:0.807273\n",
            "[97]\tvalidation_0-auc:0.806364\n",
            "[98]\tvalidation_0-auc:0.804848\n",
            "[99]\tvalidation_0-auc:0.805758\n",
            "end training. \n",
            "\n",
            "+------------------+---------------------+--------------------+---------------------+--------------------+\n",
            "|      Model       |       Accuracy      |     Precision      |        Recall       |      F1 score      |\n",
            "+------------------+---------------------+--------------------+---------------------+--------------------+\n",
            "|     LSTM 0.1     |  0.6578947368421053 | 0.9405940594059405 |  0.6168831168831169 | 0.7450980392156862 |\n",
            "|     GRU 0.1      |  0.5789473684210527 | 0.9302325581395349 |  0.5194805194805194 | 0.6666666666666666 |\n",
            "|   XGBoost 0.1    |  0.631578947368421  |        0.92        |  0.5974025974025974 | 0.7244094488188978 |\n",
            "|    Logreg 0.1    | 0.47368421052631576 |        0.95        | 0.37012987012987014 | 0.5327102803738318 |\n",
            "|     SVM 0.1      |         0.6         | 0.9333333333333333 |  0.5454545454545454 | 0.6885245901639344 |\n",
            "|  LSTM beta 0.1   |  0.5668789808917197 | 0.9210526315789473 |  0.5303030303030303 | 0.673076923076923  |\n",
            "|   GRU beta 0.1   |  0.5668789808917197 | 0.9210526315789473 |  0.5303030303030303 | 0.673076923076923  |\n",
            "| XGBoost beta 0.1 |  0.6305732484076433 | 0.9404761904761905 |  0.5984848484848485 | 0.7314814814814815 |\n",
            "| logreg beta 0.1  |  0.535031847133758  | 0.9402985074626866 |  0.4772727272727273 | 0.6331658291457287 |\n",
            "|   svm beta 0.1   |  0.6242038216560509 | 0.9620253164556962 |  0.5757575757575758 | 0.7203791469194313 |\n",
            "+------------------+---------------------+--------------------+---------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 3s 17ms/step - loss: 0.6460 - accuracy: 0.6679 - val_loss: 0.8769 - val_accuracy: 0.2895\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6002 - accuracy: 0.6844 - val_loss: 0.8887 - val_accuracy: 0.4211\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6113 - accuracy: 0.6661 - val_loss: 0.8197 - val_accuracy: 0.4053\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.5831 - accuracy: 0.6927 - val_loss: 0.7710 - val_accuracy: 0.5421\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.5980 - accuracy: 0.6789 - val_loss: 0.8354 - val_accuracy: 0.3421\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 3s 15ms/step - loss: 0.6355 - accuracy: 0.6578 - val_loss: 0.8223 - val_accuracy: 0.2895\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5836 - accuracy: 0.6927 - val_loss: 0.7270 - val_accuracy: 0.5316\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5544 - accuracy: 0.7046 - val_loss: 0.6303 - val_accuracy: 0.6368\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.5626 - accuracy: 0.6991 - val_loss: 0.6528 - val_accuracy: 0.6263\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5517 - accuracy: 0.7110 - val_loss: 0.7371 - val_accuracy: 0.5368\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.737912\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.740067\n",
            "[2]\tvalidation_0-auc:0.742492\n",
            "[3]\tvalidation_0-auc:0.741481\n",
            "[4]\tvalidation_0-auc:0.747475\n",
            "[5]\tvalidation_0-auc:0.747879\n",
            "[6]\tvalidation_0-auc:0.747609\n",
            "[7]\tvalidation_0-auc:0.736364\n",
            "[8]\tvalidation_0-auc:0.752593\n",
            "[9]\tvalidation_0-auc:0.747071\n",
            "[10]\tvalidation_0-auc:0.754478\n",
            "[11]\tvalidation_0-auc:0.755556\n",
            "[12]\tvalidation_0-auc:0.759057\n",
            "[13]\tvalidation_0-auc:0.760404\n",
            "[14]\tvalidation_0-auc:0.76\n",
            "[15]\tvalidation_0-auc:0.76229\n",
            "[16]\tvalidation_0-auc:0.763906\n",
            "[17]\tvalidation_0-auc:0.775892\n",
            "[18]\tvalidation_0-auc:0.775421\n",
            "[19]\tvalidation_0-auc:0.775017\n",
            "[20]\tvalidation_0-auc:0.778114\n",
            "[21]\tvalidation_0-auc:0.779394\n",
            "[22]\tvalidation_0-auc:0.777374\n",
            "[23]\tvalidation_0-auc:0.778316\n",
            "[24]\tvalidation_0-auc:0.777778\n",
            "[25]\tvalidation_0-auc:0.776768\n",
            "[26]\tvalidation_0-auc:0.777037\n",
            "[27]\tvalidation_0-auc:0.778114\n",
            "[28]\tvalidation_0-auc:0.778721\n",
            "[29]\tvalidation_0-auc:0.783906\n",
            "[30]\tvalidation_0-auc:0.782694\n",
            "[31]\tvalidation_0-auc:0.780471\n",
            "[32]\tvalidation_0-auc:0.782155\n",
            "[33]\tvalidation_0-auc:0.784983\n",
            "[34]\tvalidation_0-auc:0.785455\n",
            "[35]\tvalidation_0-auc:0.78963\n",
            "[36]\tvalidation_0-auc:0.790572\n",
            "[37]\tvalidation_0-auc:0.79165\n",
            "[38]\tvalidation_0-auc:0.792054\n",
            "[39]\tvalidation_0-auc:0.791111\n",
            "[40]\tvalidation_0-auc:0.793064\n",
            "[41]\tvalidation_0-auc:0.793064\n",
            "[42]\tvalidation_0-auc:0.793333\n",
            "[43]\tvalidation_0-auc:0.794007\n",
            "[44]\tvalidation_0-auc:0.793872\n",
            "[45]\tvalidation_0-auc:0.793333\n",
            "[46]\tvalidation_0-auc:0.794949\n",
            "[47]\tvalidation_0-auc:0.797104\n",
            "[48]\tvalidation_0-auc:0.795421\n",
            "[49]\tvalidation_0-auc:0.797172\n",
            "[50]\tvalidation_0-auc:0.798114\n",
            "[51]\tvalidation_0-auc:0.800404\n",
            "[52]\tvalidation_0-auc:0.799192\n",
            "[53]\tvalidation_0-auc:0.800135\n",
            "[54]\tvalidation_0-auc:0.803232\n",
            "[55]\tvalidation_0-auc:0.804175\n",
            "[56]\tvalidation_0-auc:0.80404\n",
            "[57]\tvalidation_0-auc:0.802424\n",
            "[58]\tvalidation_0-auc:0.80404\n",
            "[59]\tvalidation_0-auc:0.802963\n",
            "[60]\tvalidation_0-auc:0.802963\n",
            "[61]\tvalidation_0-auc:0.802424\n",
            "[62]\tvalidation_0-auc:0.802963\n",
            "[63]\tvalidation_0-auc:0.804983\n",
            "[64]\tvalidation_0-auc:0.809293\n",
            "[65]\tvalidation_0-auc:0.810101\n",
            "[66]\tvalidation_0-auc:0.808754\n",
            "[67]\tvalidation_0-auc:0.809158\n",
            "[68]\tvalidation_0-auc:0.808889\n",
            "[69]\tvalidation_0-auc:0.809024\n",
            "[70]\tvalidation_0-auc:0.808619\n",
            "[71]\tvalidation_0-auc:0.809158\n",
            "[72]\tvalidation_0-auc:0.811717\n",
            "[73]\tvalidation_0-auc:0.812525\n",
            "[74]\tvalidation_0-auc:0.812795\n",
            "[75]\tvalidation_0-auc:0.815354\n",
            "[76]\tvalidation_0-auc:0.815488\n",
            "[77]\tvalidation_0-auc:0.813737\n",
            "[78]\tvalidation_0-auc:0.810505\n",
            "[79]\tvalidation_0-auc:0.810236\n",
            "[80]\tvalidation_0-auc:0.808619\n",
            "[81]\tvalidation_0-auc:0.809966\n",
            "[82]\tvalidation_0-auc:0.806734\n",
            "[83]\tvalidation_0-auc:0.805926\n",
            "[84]\tvalidation_0-auc:0.805657\n",
            "[85]\tvalidation_0-auc:0.804175\n",
            "[86]\tvalidation_0-auc:0.804579\n",
            "[87]\tvalidation_0-auc:0.803906\n",
            "[88]\tvalidation_0-auc:0.799192\n",
            "[89]\tvalidation_0-auc:0.798384\n",
            "[90]\tvalidation_0-auc:0.801077\n",
            "[91]\tvalidation_0-auc:0.803367\n",
            "[92]\tvalidation_0-auc:0.80431\n",
            "[93]\tvalidation_0-auc:0.805522\n",
            "[94]\tvalidation_0-auc:0.806195\n",
            "[95]\tvalidation_0-auc:0.806195\n",
            "[96]\tvalidation_0-auc:0.807542\n",
            "[97]\tvalidation_0-auc:0.807946\n",
            "[98]\tvalidation_0-auc:0.807407\n",
            "[99]\tvalidation_0-auc:0.806061\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 3s 17ms/step - loss: 0.6306 - accuracy: 0.6746 - val_loss: 0.6512 - val_accuracy: 0.6433\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5934 - accuracy: 0.7001 - val_loss: 0.5639 - val_accuracy: 0.8025\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5789 - accuracy: 0.7228 - val_loss: 0.6863 - val_accuracy: 0.5669\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5431 - accuracy: 0.7304 - val_loss: 0.5361 - val_accuracy: 0.7898\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5461 - accuracy: 0.7360 - val_loss: 0.6307 - val_accuracy: 0.6306\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 2s 16ms/step - loss: 0.6191 - accuracy: 0.6746 - val_loss: 0.5926 - val_accuracy: 0.7006\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5542 - accuracy: 0.7304 - val_loss: 0.5655 - val_accuracy: 0.6688\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5238 - accuracy: 0.7483 - val_loss: 0.5201 - val_accuracy: 0.7070\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5195 - accuracy: 0.7512 - val_loss: 0.4870 - val_accuracy: 0.7452\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5104 - accuracy: 0.7483 - val_loss: 0.4696 - val_accuracy: 0.7962\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.771118\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.801388\n",
            "[2]\tvalidation_0-auc:0.813053\n",
            "[3]\tvalidation_0-auc:0.808427\n",
            "[4]\tvalidation_0-auc:0.799879\n",
            "[5]\tvalidation_0-auc:0.800583\n",
            "[6]\tvalidation_0-auc:0.801488\n",
            "[7]\tvalidation_0-auc:0.798572\n",
            "[8]\tvalidation_0-auc:0.786102\n",
            "[9]\tvalidation_0-auc:0.780068\n",
            "[10]\tvalidation_0-auc:0.763978\n",
            "[11]\tvalidation_0-auc:0.7678\n",
            "[12]\tvalidation_0-auc:0.760559\n",
            "[13]\tvalidation_0-auc:0.754525\n",
            "[14]\tvalidation_0-auc:0.754123\n",
            "[15]\tvalidation_0-auc:0.746078\n",
            "[16]\tvalidation_0-auc:0.739843\n",
            "[17]\tvalidation_0-auc:0.740648\n",
            "[18]\tvalidation_0-auc:0.740547\n",
            "[19]\tvalidation_0-auc:0.73934\n",
            "[20]\tvalidation_0-auc:0.736726\n",
            "[21]\tvalidation_0-auc:0.73572\n",
            "[22]\tvalidation_0-auc:0.734714\n",
            "[23]\tvalidation_0-auc:0.733709\n",
            "[24]\tvalidation_0-auc:0.73029\n",
            "[25]\tvalidation_0-auc:0.729284\n",
            "[26]\tvalidation_0-auc:0.730893\n",
            "[27]\tvalidation_0-auc:0.726368\n",
            "[28]\tvalidation_0-auc:0.717317\n",
            "[29]\tvalidation_0-auc:0.7143\n",
            "[30]\tvalidation_0-auc:0.715708\n",
            "[31]\tvalidation_0-auc:0.715507\n",
            "[32]\tvalidation_0-auc:0.711685\n",
            "[33]\tvalidation_0-auc:0.710076\n",
            "[34]\tvalidation_0-auc:0.711685\n",
            "[35]\tvalidation_0-auc:0.711887\n",
            "[36]\tvalidation_0-auc:0.710881\n",
            "[37]\tvalidation_0-auc:0.706255\n",
            "[38]\tvalidation_0-auc:0.701428\n",
            "[39]\tvalidation_0-auc:0.701227\n",
            "[40]\tvalidation_0-auc:0.697204\n",
            "[41]\tvalidation_0-auc:0.697808\n",
            "[42]\tvalidation_0-auc:0.700221\n",
            "[43]\tvalidation_0-auc:0.694791\n",
            "[44]\tvalidation_0-auc:0.693986\n",
            "[45]\tvalidation_0-auc:0.693383\n",
            "[46]\tvalidation_0-auc:0.69288\n",
            "[47]\tvalidation_0-auc:0.69288\n",
            "[48]\tvalidation_0-auc:0.690064\n",
            "[49]\tvalidation_0-auc:0.68564\n",
            "[50]\tvalidation_0-auc:0.682723\n",
            "[51]\tvalidation_0-auc:0.682723\n",
            "[52]\tvalidation_0-auc:0.686344\n",
            "Stopping. Best iteration:\n",
            "[2]\tvalidation_0-auc:0.813053\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+---------------------+--------------------+---------------------+---------------------+\n",
            "|      Model       |       Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+------------------+---------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.2     | 0.34210526315789475 |        1.0         | 0.07407407407407407 | 0.13793103448275862 |\n",
            "|     GRU 0.2      |  0.5368421052631579 | 0.9607843137254902 |  0.362962962962963  |  0.5268817204301075 |\n",
            "|   XGBoost 0.2    |         0.6         | 0.927536231884058  |  0.4740740740740741 |  0.627450980392157  |\n",
            "|    Logreg 0.2    |  0.5842105263157895 |       0.9375       |  0.4444444444444444 |  0.6030150753768844 |\n",
            "|     SVM 0.2      | 0.49473684210526314 | 0.975609756097561  |  0.2962962962962963 | 0.45454545454545453 |\n",
            "|  LSTM beta 0.2   |  0.6305732484076433 | 0.9230769230769231 |  0.5309734513274337 |  0.6741573033707865 |\n",
            "|   GRU beta 0.2   |  0.7961783439490446 | 0.8857142857142857 |  0.8230088495575221 |  0.8532110091743119 |\n",
            "| XGBoost beta 0.2 |  0.7261146496815286 | 0.8571428571428571 |  0.7433628318584071 |  0.7962085308056872 |\n",
            "| logreg beta 0.2  |  0.7452229299363057 | 0.8924731182795699 |  0.7345132743362832 |  0.8058252427184466 |\n",
            "|   svm beta 0.2   |  0.7579617834394905 | 0.8440366972477065 |  0.8141592920353983 |  0.8288288288288288 |\n",
            "+------------------+---------------------+--------------------+---------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 3s 17ms/step - loss: 0.6743 - accuracy: 0.6046 - val_loss: 0.7600 - val_accuracy: 0.2895\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6242 - accuracy: 0.6413 - val_loss: 0.6967 - val_accuracy: 0.5947\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6137 - accuracy: 0.6486 - val_loss: 0.7008 - val_accuracy: 0.5947\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6049 - accuracy: 0.6578 - val_loss: 0.7376 - val_accuracy: 0.5579\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.5983 - accuracy: 0.6578 - val_loss: 0.6723 - val_accuracy: 0.6842\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 2s 15ms/step - loss: 0.6616 - accuracy: 0.6174 - val_loss: 0.7266 - val_accuracy: 0.3947\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.6021 - accuracy: 0.6844 - val_loss: 0.6936 - val_accuracy: 0.5789\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5627 - accuracy: 0.6872 - val_loss: 0.6254 - val_accuracy: 0.6737\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5726 - accuracy: 0.6972 - val_loss: 0.6815 - val_accuracy: 0.5632\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5659 - accuracy: 0.7083 - val_loss: 0.6082 - val_accuracy: 0.7105\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.773266\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.763502\n",
            "[2]\tvalidation_0-auc:0.76633\n",
            "[3]\tvalidation_0-auc:0.767205\n",
            "[4]\tvalidation_0-auc:0.768552\n",
            "[5]\tvalidation_0-auc:0.764781\n",
            "[6]\tvalidation_0-auc:0.76404\n",
            "[7]\tvalidation_0-auc:0.763569\n",
            "[8]\tvalidation_0-auc:0.766061\n",
            "[9]\tvalidation_0-auc:0.765791\n",
            "[10]\tvalidation_0-auc:0.767273\n",
            "[11]\tvalidation_0-auc:0.765791\n",
            "[12]\tvalidation_0-auc:0.766195\n",
            "[13]\tvalidation_0-auc:0.765455\n",
            "[14]\tvalidation_0-auc:0.764175\n",
            "[15]\tvalidation_0-auc:0.76404\n",
            "[16]\tvalidation_0-auc:0.764983\n",
            "[17]\tvalidation_0-auc:0.767407\n",
            "[18]\tvalidation_0-auc:0.767071\n",
            "[19]\tvalidation_0-auc:0.766128\n",
            "[20]\tvalidation_0-auc:0.765859\n",
            "[21]\tvalidation_0-auc:0.765926\n",
            "[22]\tvalidation_0-auc:0.765522\n",
            "[23]\tvalidation_0-auc:0.76431\n",
            "[24]\tvalidation_0-auc:0.767946\n",
            "[25]\tvalidation_0-auc:0.770236\n",
            "[26]\tvalidation_0-auc:0.77037\n",
            "[27]\tvalidation_0-auc:0.770505\n",
            "[28]\tvalidation_0-auc:0.770438\n",
            "[29]\tvalidation_0-auc:0.770438\n",
            "[30]\tvalidation_0-auc:0.769428\n",
            "[31]\tvalidation_0-auc:0.769562\n",
            "[32]\tvalidation_0-auc:0.768889\n",
            "[33]\tvalidation_0-auc:0.767946\n",
            "[34]\tvalidation_0-auc:0.767542\n",
            "[35]\tvalidation_0-auc:0.766734\n",
            "[36]\tvalidation_0-auc:0.764983\n",
            "[37]\tvalidation_0-auc:0.766667\n",
            "[38]\tvalidation_0-auc:0.768081\n",
            "[39]\tvalidation_0-auc:0.767946\n",
            "[40]\tvalidation_0-auc:0.76835\n",
            "[41]\tvalidation_0-auc:0.769158\n",
            "[42]\tvalidation_0-auc:0.767677\n",
            "[43]\tvalidation_0-auc:0.767542\n",
            "[44]\tvalidation_0-auc:0.767677\n",
            "[45]\tvalidation_0-auc:0.767677\n",
            "[46]\tvalidation_0-auc:0.766599\n",
            "[47]\tvalidation_0-auc:0.766599\n",
            "[48]\tvalidation_0-auc:0.764444\n",
            "[49]\tvalidation_0-auc:0.772795\n",
            "[50]\tvalidation_0-auc:0.771717\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.773266\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 3s 17ms/step - loss: 0.6690 - accuracy: 0.6093 - val_loss: 0.6410 - val_accuracy: 0.7197\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.6355 - accuracy: 0.6358 - val_loss: 0.5534 - val_accuracy: 0.7197\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.6281 - accuracy: 0.6471 - val_loss: 0.5660 - val_accuracy: 0.7580\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.6086 - accuracy: 0.6840 - val_loss: 0.5776 - val_accuracy: 0.7643\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5698 - accuracy: 0.7067 - val_loss: 0.4468 - val_accuracy: 0.8726\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 2s 15ms/step - loss: 0.6389 - accuracy: 0.6500 - val_loss: 0.5271 - val_accuracy: 0.7325\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5653 - accuracy: 0.7096 - val_loss: 0.4277 - val_accuracy: 0.8790\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5403 - accuracy: 0.7275 - val_loss: 0.4539 - val_accuracy: 0.8471\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5360 - accuracy: 0.7275 - val_loss: 0.4162 - val_accuracy: 0.8790\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5187 - accuracy: 0.7313 - val_loss: 0.4454 - val_accuracy: 0.8408\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.761263\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.756235\n",
            "[2]\tvalidation_0-auc:0.757039\n",
            "[3]\tvalidation_0-auc:0.761062\n",
            "[4]\tvalidation_0-auc:0.76066\n",
            "[5]\tvalidation_0-auc:0.760056\n",
            "[6]\tvalidation_0-auc:0.739139\n",
            "[7]\tvalidation_0-auc:0.73924\n",
            "[8]\tvalidation_0-auc:0.739441\n",
            "[9]\tvalidation_0-auc:0.725463\n",
            "[10]\tvalidation_0-auc:0.724658\n",
            "[11]\tvalidation_0-auc:0.723954\n",
            "[12]\tvalidation_0-auc:0.712591\n",
            "[13]\tvalidation_0-auc:0.709875\n",
            "[14]\tvalidation_0-auc:0.709574\n",
            "[15]\tvalidation_0-auc:0.719027\n",
            "[16]\tvalidation_0-auc:0.714099\n",
            "[17]\tvalidation_0-auc:0.713898\n",
            "[18]\tvalidation_0-auc:0.699517\n",
            "[19]\tvalidation_0-auc:0.702534\n",
            "[20]\tvalidation_0-auc:0.699517\n",
            "[21]\tvalidation_0-auc:0.700925\n",
            "[22]\tvalidation_0-auc:0.697104\n",
            "[23]\tvalidation_0-auc:0.67317\n",
            "[24]\tvalidation_0-auc:0.660499\n",
            "[25]\tvalidation_0-auc:0.659091\n",
            "[26]\tvalidation_0-auc:0.659393\n",
            "[27]\tvalidation_0-auc:0.652957\n",
            "[28]\tvalidation_0-auc:0.647124\n",
            "[29]\tvalidation_0-auc:0.643303\n",
            "[30]\tvalidation_0-auc:0.647124\n",
            "[31]\tvalidation_0-auc:0.632643\n",
            "[32]\tvalidation_0-auc:0.63043\n",
            "[33]\tvalidation_0-auc:0.629224\n",
            "[34]\tvalidation_0-auc:0.630632\n",
            "[35]\tvalidation_0-auc:0.627816\n",
            "[36]\tvalidation_0-auc:0.629023\n",
            "[37]\tvalidation_0-auc:0.628218\n",
            "[38]\tvalidation_0-auc:0.634453\n",
            "[39]\tvalidation_0-auc:0.63566\n",
            "[40]\tvalidation_0-auc:0.635257\n",
            "[41]\tvalidation_0-auc:0.634252\n",
            "[42]\tvalidation_0-auc:0.636263\n",
            "[43]\tvalidation_0-auc:0.63566\n",
            "[44]\tvalidation_0-auc:0.634654\n",
            "[45]\tvalidation_0-auc:0.633447\n",
            "[46]\tvalidation_0-auc:0.625201\n",
            "[47]\tvalidation_0-auc:0.624799\n",
            "[48]\tvalidation_0-auc:0.629224\n",
            "[49]\tvalidation_0-auc:0.631637\n",
            "[50]\tvalidation_0-auc:0.630028\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.761263\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.6842105263157895 | 0.8504672897196262 | 0.674074074074074  | 0.7520661157024793 |\n",
            "|      GRU 0.15     | 0.7105263157894737 | 0.8571428571428571 | 0.7111111111111111 | 0.777327935222672  |\n",
            "|    XGBoost 0.15   | 0.7105263157894737 | 0.8703703703703703 | 0.6962962962962963 | 0.7736625514403292 |\n",
            "|    Logreg 0.15    | 0.6421052631578947 | 0.8764044943820225 | 0.5777777777777777 | 0.6964285714285714 |\n",
            "|      SVM 0.15     | 0.7052631578947368 | 0.8691588785046729 | 0.6888888888888889 | 0.768595041322314  |\n",
            "|   LSTM beta 0.15  | 0.8726114649681529 | 0.8780487804878049 | 0.9557522123893806 | 0.9152542372881357 |\n",
            "|   GRU beta 0.15   | 0.8407643312101911 | 0.9150943396226415 | 0.8584070796460177 | 0.8858447488584474 |\n",
            "| XGBoost beta 0.15 | 0.7961783439490446 | 0.8292682926829268 | 0.9026548672566371 | 0.8644067796610169 |\n",
            "|  logreg beta 0.15 | 0.8535031847133758 | 0.8813559322033898 | 0.9203539823008849 | 0.9004329004329005 |\n",
            "|   svm beta 0.15   | 0.802547770700637  | 0.8306451612903226 | 0.911504424778761  | 0.8691983122362869 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnVh3UGsZu1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfc156cc-7037-4917-a2f3-e420a501403d"
      },
      "source": [
        "Result_cross.to_csv('SYY_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.940594</td>\n",
              "      <td>0.657895</td>\n",
              "      <td>0.745098</td>\n",
              "      <td>0.616883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.930233</td>\n",
              "      <td>0.578947</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.519481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.631579</td>\n",
              "      <td>0.724409</td>\n",
              "      <td>0.597403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.473684</td>\n",
              "      <td>0.532710</td>\n",
              "      <td>0.370130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.688525</td>\n",
              "      <td>0.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.921053</td>\n",
              "      <td>0.566879</td>\n",
              "      <td>0.673077</td>\n",
              "      <td>0.530303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.921053</td>\n",
              "      <td>0.566879</td>\n",
              "      <td>0.673077</td>\n",
              "      <td>0.530303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.940476</td>\n",
              "      <td>0.630573</td>\n",
              "      <td>0.731481</td>\n",
              "      <td>0.598485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.940299</td>\n",
              "      <td>0.535032</td>\n",
              "      <td>0.633166</td>\n",
              "      <td>0.477273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.962025</td>\n",
              "      <td>0.624204</td>\n",
              "      <td>0.720379</td>\n",
              "      <td>0.575758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.342105</td>\n",
              "      <td>0.137931</td>\n",
              "      <td>0.074074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.960784</td>\n",
              "      <td>0.536842</td>\n",
              "      <td>0.526882</td>\n",
              "      <td>0.362963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.927536</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.627451</td>\n",
              "      <td>0.474074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.584211</td>\n",
              "      <td>0.603015</td>\n",
              "      <td>0.444444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.975610</td>\n",
              "      <td>0.494737</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.296296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>0.630573</td>\n",
              "      <td>0.674157</td>\n",
              "      <td>0.530973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.885714</td>\n",
              "      <td>0.796178</td>\n",
              "      <td>0.853211</td>\n",
              "      <td>0.823009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.726115</td>\n",
              "      <td>0.796209</td>\n",
              "      <td>0.743363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.892473</td>\n",
              "      <td>0.745223</td>\n",
              "      <td>0.805825</td>\n",
              "      <td>0.734513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.844037</td>\n",
              "      <td>0.757962</td>\n",
              "      <td>0.828829</td>\n",
              "      <td>0.814159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.850467</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.752066</td>\n",
              "      <td>0.674074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.710526</td>\n",
              "      <td>0.777328</td>\n",
              "      <td>0.711111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.870370</td>\n",
              "      <td>0.710526</td>\n",
              "      <td>0.773663</td>\n",
              "      <td>0.696296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.876404</td>\n",
              "      <td>0.642105</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.577778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.869159</td>\n",
              "      <td>0.705263</td>\n",
              "      <td>0.768595</td>\n",
              "      <td>0.688889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.878049</td>\n",
              "      <td>0.872611</td>\n",
              "      <td>0.915254</td>\n",
              "      <td>0.955752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.915094</td>\n",
              "      <td>0.840764</td>\n",
              "      <td>0.885845</td>\n",
              "      <td>0.858407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.829268</td>\n",
              "      <td>0.796178</td>\n",
              "      <td>0.864407</td>\n",
              "      <td>0.902655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.881356</td>\n",
              "      <td>0.853503</td>\n",
              "      <td>0.900433</td>\n",
              "      <td>0.920354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.830645</td>\n",
              "      <td>0.802548</td>\n",
              "      <td>0.869198</td>\n",
              "      <td>0.911504</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  TWTR  0.940594  0.657895  0.745098  0.616883\n",
              "1            GRU 0.1  TWTR  0.930233  0.578947  0.666667  0.519481\n",
              "2        XGBoost 0.1  TWTR  0.920000  0.631579  0.724409  0.597403\n",
              "3         Logreg 0.1  TWTR  0.950000  0.473684  0.532710  0.370130\n",
              "4            SVM 0.1  TWTR  0.933333  0.600000  0.688525  0.545455\n",
              "5      LSTM beta 0.1  TWTR  0.921053  0.566879  0.673077  0.530303\n",
              "6       GRU beta 0.1  TWTR  0.921053  0.566879  0.673077  0.530303\n",
              "7   XGBoost beta 0.1  TWTR  0.940476  0.630573  0.731481  0.598485\n",
              "8    logreg beta 0.1  TWTR  0.940299  0.535032  0.633166  0.477273\n",
              "9       svm beta 0.1  TWTR  0.962025  0.624204  0.720379  0.575758\n",
              "0           LSTM 0.2  TWTR  1.000000  0.342105  0.137931  0.074074\n",
              "1            GRU 0.2  TWTR  0.960784  0.536842  0.526882  0.362963\n",
              "2        XGBoost 0.2  TWTR  0.927536  0.600000  0.627451  0.474074\n",
              "3         Logreg 0.2  TWTR  0.937500  0.584211  0.603015  0.444444\n",
              "4            SVM 0.2  TWTR  0.975610  0.494737  0.454545  0.296296\n",
              "5      LSTM beta 0.2  TWTR  0.923077  0.630573  0.674157  0.530973\n",
              "6       GRU beta 0.2  TWTR  0.885714  0.796178  0.853211  0.823009\n",
              "7   XGBoost beta 0.2  TWTR  0.857143  0.726115  0.796209  0.743363\n",
              "8    logreg beta 0.2  TWTR  0.892473  0.745223  0.805825  0.734513\n",
              "9       svm beta 0.2  TWTR  0.844037  0.757962  0.828829  0.814159\n",
              "0          LSTM 0.15  TWTR  0.850467  0.684211  0.752066  0.674074\n",
              "1           GRU 0.15  TWTR  0.857143  0.710526  0.777328  0.711111\n",
              "2       XGBoost 0.15  TWTR  0.870370  0.710526  0.773663  0.696296\n",
              "3        Logreg 0.15  TWTR  0.876404  0.642105  0.696429  0.577778\n",
              "4           SVM 0.15  TWTR  0.869159  0.705263  0.768595  0.688889\n",
              "5     LSTM beta 0.15  TWTR  0.878049  0.872611  0.915254  0.955752\n",
              "6      GRU beta 0.15  TWTR  0.915094  0.840764  0.885845  0.858407\n",
              "7  XGBoost beta 0.15  TWTR  0.829268  0.796178  0.864407  0.902655\n",
              "8   logreg beta 0.15  TWTR  0.881356  0.853503  0.900433  0.920354\n",
              "9      svm beta 0.15  TWTR  0.830645  0.802548  0.869198  0.911504"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNVLyy9fZu1E"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1410:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1410:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1410:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1410:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1410:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1443:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1443:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1443:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1443:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1443:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imedSl2KZu1E"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyRtwuO2Zu1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fde3252e-1538-4318-ae2c-5df8f8591778"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1400, test_end=1600)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"TWTR\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 3s 17ms/step - loss: 0.6502 - accuracy: 0.6569 - val_loss: 0.7937 - val_accuracy: 0.1895\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6389 - accuracy: 0.6596 - val_loss: 0.9699 - val_accuracy: 0.1895\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6087 - accuracy: 0.6679 - val_loss: 0.7394 - val_accuracy: 0.5895\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5751 - accuracy: 0.6927 - val_loss: 0.8066 - val_accuracy: 0.4684\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.5642 - accuracy: 0.7174 - val_loss: 0.9575 - val_accuracy: 0.4158\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 2s 15ms/step - loss: 0.6334 - accuracy: 0.6633 - val_loss: 0.7936 - val_accuracy: 0.3105\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5589 - accuracy: 0.7101 - val_loss: 0.8960 - val_accuracy: 0.3632\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5152 - accuracy: 0.7450 - val_loss: 0.7147 - val_accuracy: 0.6211\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5069 - accuracy: 0.7468 - val_loss: 0.8108 - val_accuracy: 0.5789\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5059 - accuracy: 0.7339 - val_loss: 1.0659 - val_accuracy: 0.4579\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.747024\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.736923\n",
            "[2]\tvalidation_0-auc:0.770112\n",
            "[3]\tvalidation_0-auc:0.76921\n",
            "[4]\tvalidation_0-auc:0.769751\n",
            "[5]\tvalidation_0-auc:0.768849\n",
            "[6]\tvalidation_0-auc:0.772457\n",
            "[7]\tvalidation_0-auc:0.772637\n",
            "[8]\tvalidation_0-auc:0.771916\n",
            "[9]\tvalidation_0-auc:0.770833\n",
            "[10]\tvalidation_0-auc:0.771014\n",
            "[11]\tvalidation_0-auc:0.775703\n",
            "[12]\tvalidation_0-auc:0.780483\n",
            "[13]\tvalidation_0-auc:0.780483\n",
            "[14]\tvalidation_0-auc:0.781566\n",
            "[15]\tvalidation_0-auc:0.784091\n",
            "[16]\tvalidation_0-auc:0.78382\n",
            "[17]\tvalidation_0-auc:0.784001\n",
            "[18]\tvalidation_0-auc:0.784181\n",
            "[19]\tvalidation_0-auc:0.78373\n",
            "[20]\tvalidation_0-auc:0.783911\n",
            "[21]\tvalidation_0-auc:0.783189\n",
            "[22]\tvalidation_0-auc:0.787698\n",
            "[23]\tvalidation_0-auc:0.786255\n",
            "[24]\tvalidation_0-auc:0.787338\n",
            "[25]\tvalidation_0-auc:0.786797\n",
            "[26]\tvalidation_0-auc:0.790494\n",
            "[27]\tvalidation_0-auc:0.790675\n",
            "[28]\tvalidation_0-auc:0.790133\n",
            "[29]\tvalidation_0-auc:0.789953\n",
            "[30]\tvalidation_0-auc:0.790314\n",
            "[31]\tvalidation_0-auc:0.790675\n",
            "[32]\tvalidation_0-auc:0.791667\n",
            "[33]\tvalidation_0-auc:0.791667\n",
            "[34]\tvalidation_0-auc:0.791486\n",
            "[35]\tvalidation_0-auc:0.788961\n",
            "[36]\tvalidation_0-auc:0.788961\n",
            "[37]\tvalidation_0-auc:0.788961\n",
            "[38]\tvalidation_0-auc:0.788961\n",
            "[39]\tvalidation_0-auc:0.785985\n",
            "[40]\tvalidation_0-auc:0.785624\n",
            "[41]\tvalidation_0-auc:0.785985\n",
            "[42]\tvalidation_0-auc:0.785444\n",
            "[43]\tvalidation_0-auc:0.786165\n",
            "[44]\tvalidation_0-auc:0.784903\n",
            "[45]\tvalidation_0-auc:0.785083\n",
            "[46]\tvalidation_0-auc:0.785083\n",
            "[47]\tvalidation_0-auc:0.785804\n",
            "[48]\tvalidation_0-auc:0.784361\n",
            "[49]\tvalidation_0-auc:0.783911\n",
            "[50]\tvalidation_0-auc:0.783189\n",
            "[51]\tvalidation_0-auc:0.784001\n",
            "[52]\tvalidation_0-auc:0.784181\n",
            "[53]\tvalidation_0-auc:0.781385\n",
            "[54]\tvalidation_0-auc:0.781025\n",
            "[55]\tvalidation_0-auc:0.782648\n",
            "[56]\tvalidation_0-auc:0.783009\n",
            "[57]\tvalidation_0-auc:0.782828\n",
            "[58]\tvalidation_0-auc:0.783189\n",
            "[59]\tvalidation_0-auc:0.784993\n",
            "[60]\tvalidation_0-auc:0.782287\n",
            "[61]\tvalidation_0-auc:0.781566\n",
            "[62]\tvalidation_0-auc:0.781746\n",
            "[63]\tvalidation_0-auc:0.783009\n",
            "[64]\tvalidation_0-auc:0.78373\n",
            "[65]\tvalidation_0-auc:0.784091\n",
            "[66]\tvalidation_0-auc:0.782287\n",
            "[67]\tvalidation_0-auc:0.781205\n",
            "[68]\tvalidation_0-auc:0.781746\n",
            "[69]\tvalidation_0-auc:0.780664\n",
            "[70]\tvalidation_0-auc:0.781205\n",
            "[71]\tvalidation_0-auc:0.779762\n",
            "[72]\tvalidation_0-auc:0.77886\n",
            "[73]\tvalidation_0-auc:0.77904\n",
            "[74]\tvalidation_0-auc:0.779582\n",
            "[75]\tvalidation_0-auc:0.780303\n",
            "[76]\tvalidation_0-auc:0.77904\n",
            "[77]\tvalidation_0-auc:0.779582\n",
            "[78]\tvalidation_0-auc:0.779401\n",
            "[79]\tvalidation_0-auc:0.77868\n",
            "[80]\tvalidation_0-auc:0.777958\n",
            "[81]\tvalidation_0-auc:0.779401\n",
            "[82]\tvalidation_0-auc:0.781385\n",
            "Stopping. Best iteration:\n",
            "[32]\tvalidation_0-auc:0.791667\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 3s 17ms/step - loss: 0.6521 - accuracy: 0.6594 - val_loss: 0.9859 - val_accuracy: 0.1592\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.6451 - accuracy: 0.6632 - val_loss: 0.9293 - val_accuracy: 0.1592\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.6340 - accuracy: 0.6613 - val_loss: 0.8992 - val_accuracy: 0.1592\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.6209 - accuracy: 0.6689 - val_loss: 0.9445 - val_accuracy: 0.1911\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5669 - accuracy: 0.7190 - val_loss: 0.7739 - val_accuracy: 0.4841\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 2s 15ms/step - loss: 0.6469 - accuracy: 0.6613 - val_loss: 0.9362 - val_accuracy: 0.1592\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.6041 - accuracy: 0.6802 - val_loss: 0.7555 - val_accuracy: 0.4777\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5592 - accuracy: 0.7162 - val_loss: 0.7417 - val_accuracy: 0.5414\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.5528 - accuracy: 0.7105 - val_loss: 0.6568 - val_accuracy: 0.5924\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5479 - accuracy: 0.7351 - val_loss: 0.6204 - val_accuracy: 0.6497\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.707576\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.764545\n",
            "[2]\tvalidation_0-auc:0.776364\n",
            "[3]\tvalidation_0-auc:0.777273\n",
            "[4]\tvalidation_0-auc:0.782879\n",
            "[5]\tvalidation_0-auc:0.784849\n",
            "[6]\tvalidation_0-auc:0.796212\n",
            "[7]\tvalidation_0-auc:0.790152\n",
            "[8]\tvalidation_0-auc:0.785\n",
            "[9]\tvalidation_0-auc:0.793333\n",
            "[10]\tvalidation_0-auc:0.795152\n",
            "[11]\tvalidation_0-auc:0.790758\n",
            "[12]\tvalidation_0-auc:0.795303\n",
            "[13]\tvalidation_0-auc:0.796364\n",
            "[14]\tvalidation_0-auc:0.793333\n",
            "[15]\tvalidation_0-auc:0.796212\n",
            "[16]\tvalidation_0-auc:0.793788\n",
            "[17]\tvalidation_0-auc:0.795606\n",
            "[18]\tvalidation_0-auc:0.799697\n",
            "[19]\tvalidation_0-auc:0.801061\n",
            "[20]\tvalidation_0-auc:0.803788\n",
            "[21]\tvalidation_0-auc:0.80197\n",
            "[22]\tvalidation_0-auc:0.799545\n",
            "[23]\tvalidation_0-auc:0.803788\n",
            "[24]\tvalidation_0-auc:0.801061\n",
            "[25]\tvalidation_0-auc:0.800909\n",
            "[26]\tvalidation_0-auc:0.805152\n",
            "[27]\tvalidation_0-auc:0.804848\n",
            "[28]\tvalidation_0-auc:0.804242\n",
            "[29]\tvalidation_0-auc:0.804242\n",
            "[30]\tvalidation_0-auc:0.803636\n",
            "[31]\tvalidation_0-auc:0.804697\n",
            "[32]\tvalidation_0-auc:0.805909\n",
            "[33]\tvalidation_0-auc:0.804091\n",
            "[34]\tvalidation_0-auc:0.804394\n",
            "[35]\tvalidation_0-auc:0.805606\n",
            "[36]\tvalidation_0-auc:0.805909\n",
            "[37]\tvalidation_0-auc:0.808636\n",
            "[38]\tvalidation_0-auc:0.805909\n",
            "[39]\tvalidation_0-auc:0.805303\n",
            "[40]\tvalidation_0-auc:0.806212\n",
            "[41]\tvalidation_0-auc:0.805606\n",
            "[42]\tvalidation_0-auc:0.805\n",
            "[43]\tvalidation_0-auc:0.806818\n",
            "[44]\tvalidation_0-auc:0.80197\n",
            "[45]\tvalidation_0-auc:0.802879\n",
            "[46]\tvalidation_0-auc:0.803182\n",
            "[47]\tvalidation_0-auc:0.803788\n",
            "[48]\tvalidation_0-auc:0.803182\n",
            "[49]\tvalidation_0-auc:0.803182\n",
            "[50]\tvalidation_0-auc:0.802576\n",
            "[51]\tvalidation_0-auc:0.806515\n",
            "[52]\tvalidation_0-auc:0.807727\n",
            "[53]\tvalidation_0-auc:0.807424\n",
            "[54]\tvalidation_0-auc:0.809697\n",
            "[55]\tvalidation_0-auc:0.809697\n",
            "[56]\tvalidation_0-auc:0.808485\n",
            "[57]\tvalidation_0-auc:0.80697\n",
            "[58]\tvalidation_0-auc:0.808788\n",
            "[59]\tvalidation_0-auc:0.809697\n",
            "[60]\tvalidation_0-auc:0.809091\n",
            "[61]\tvalidation_0-auc:0.804242\n",
            "[62]\tvalidation_0-auc:0.805758\n",
            "[63]\tvalidation_0-auc:0.806364\n",
            "[64]\tvalidation_0-auc:0.807273\n",
            "[65]\tvalidation_0-auc:0.807879\n",
            "[66]\tvalidation_0-auc:0.80697\n",
            "[67]\tvalidation_0-auc:0.807273\n",
            "[68]\tvalidation_0-auc:0.805758\n",
            "[69]\tvalidation_0-auc:0.803939\n",
            "[70]\tvalidation_0-auc:0.803939\n",
            "[71]\tvalidation_0-auc:0.804848\n",
            "[72]\tvalidation_0-auc:0.804848\n",
            "[73]\tvalidation_0-auc:0.806061\n",
            "[74]\tvalidation_0-auc:0.804848\n",
            "[75]\tvalidation_0-auc:0.808788\n",
            "[76]\tvalidation_0-auc:0.808788\n",
            "[77]\tvalidation_0-auc:0.808485\n",
            "[78]\tvalidation_0-auc:0.809697\n",
            "[79]\tvalidation_0-auc:0.806667\n",
            "[80]\tvalidation_0-auc:0.808788\n",
            "[81]\tvalidation_0-auc:0.803939\n",
            "[82]\tvalidation_0-auc:0.806667\n",
            "[83]\tvalidation_0-auc:0.806667\n",
            "[84]\tvalidation_0-auc:0.807879\n",
            "[85]\tvalidation_0-auc:0.806364\n",
            "[86]\tvalidation_0-auc:0.802121\n",
            "[87]\tvalidation_0-auc:0.803939\n",
            "[88]\tvalidation_0-auc:0.806364\n",
            "[89]\tvalidation_0-auc:0.808788\n",
            "[90]\tvalidation_0-auc:0.807879\n",
            "[91]\tvalidation_0-auc:0.807879\n",
            "[92]\tvalidation_0-auc:0.809394\n",
            "[93]\tvalidation_0-auc:0.808485\n",
            "[94]\tvalidation_0-auc:0.807576\n",
            "[95]\tvalidation_0-auc:0.807273\n",
            "[96]\tvalidation_0-auc:0.807273\n",
            "[97]\tvalidation_0-auc:0.806364\n",
            "[98]\tvalidation_0-auc:0.804848\n",
            "[99]\tvalidation_0-auc:0.805758\n",
            "end training. \n",
            "\n",
            "+------------------+---------------------+--------------------+---------------------+--------------------+\n",
            "|      Model       |       Accuracy      |     Precision      |        Recall       |      F1 score      |\n",
            "+------------------+---------------------+--------------------+---------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.41578947368421054 | 0.9777777777777777 |  0.2857142857142857 | 0.4422110552763818 |\n",
            "|     GRU 0.1      | 0.45789473684210524 | 0.9811320754716981 | 0.33766233766233766 | 0.5024154589371981 |\n",
            "|   XGBoost 0.1    |  0.631578947368421  |        0.92        |  0.5974025974025974 | 0.7244094488188978 |\n",
            "|    Logreg 0.1    | 0.47368421052631576 |        0.95        | 0.37012987012987014 | 0.5327102803738318 |\n",
            "|     SVM 0.1      |         0.6         | 0.9333333333333333 |  0.5454545454545454 | 0.6885245901639344 |\n",
            "|  LSTM beta 0.1   |  0.4840764331210191 | 0.9047619047619048 |  0.4318181818181818 | 0.5846153846153846 |\n",
            "|   GRU beta 0.1   |  0.6496815286624203 | 0.9325842696629213 |  0.6287878787878788 | 0.7511312217194569 |\n",
            "| XGBoost beta 0.1 |  0.6305732484076433 | 0.9404761904761905 |  0.5984848484848485 | 0.7314814814814815 |\n",
            "| logreg beta 0.1  |  0.535031847133758  | 0.9402985074626866 |  0.4772727272727273 | 0.6331658291457287 |\n",
            "|   svm beta 0.1   |  0.6242038216560509 | 0.9620253164556962 |  0.5757575757575758 | 0.7203791469194313 |\n",
            "+------------------+---------------------+--------------------+---------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 3s 17ms/step - loss: 0.6501 - accuracy: 0.6651 - val_loss: 0.8603 - val_accuracy: 0.2895\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6323 - accuracy: 0.6725 - val_loss: 0.7920 - val_accuracy: 0.2895\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6030 - accuracy: 0.6917 - val_loss: 0.7186 - val_accuracy: 0.4579\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.5997 - accuracy: 0.6752 - val_loss: 0.7937 - val_accuracy: 0.4947\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.5778 - accuracy: 0.6881 - val_loss: 0.8070 - val_accuracy: 0.4789\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 2s 15ms/step - loss: 0.6447 - accuracy: 0.6633 - val_loss: 0.8308 - val_accuracy: 0.2895\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.5929 - accuracy: 0.6936 - val_loss: 0.7956 - val_accuracy: 0.4737\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5818 - accuracy: 0.7028 - val_loss: 0.5981 - val_accuracy: 0.7105\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5671 - accuracy: 0.7009 - val_loss: 0.6607 - val_accuracy: 0.5947\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5506 - accuracy: 0.7174 - val_loss: 0.7357 - val_accuracy: 0.5368\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.737912\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.740067\n",
            "[2]\tvalidation_0-auc:0.742492\n",
            "[3]\tvalidation_0-auc:0.741481\n",
            "[4]\tvalidation_0-auc:0.747475\n",
            "[5]\tvalidation_0-auc:0.747879\n",
            "[6]\tvalidation_0-auc:0.747609\n",
            "[7]\tvalidation_0-auc:0.736364\n",
            "[8]\tvalidation_0-auc:0.752593\n",
            "[9]\tvalidation_0-auc:0.747071\n",
            "[10]\tvalidation_0-auc:0.754478\n",
            "[11]\tvalidation_0-auc:0.755556\n",
            "[12]\tvalidation_0-auc:0.759057\n",
            "[13]\tvalidation_0-auc:0.760404\n",
            "[14]\tvalidation_0-auc:0.76\n",
            "[15]\tvalidation_0-auc:0.76229\n",
            "[16]\tvalidation_0-auc:0.763906\n",
            "[17]\tvalidation_0-auc:0.775892\n",
            "[18]\tvalidation_0-auc:0.775421\n",
            "[19]\tvalidation_0-auc:0.775017\n",
            "[20]\tvalidation_0-auc:0.778114\n",
            "[21]\tvalidation_0-auc:0.779394\n",
            "[22]\tvalidation_0-auc:0.777374\n",
            "[23]\tvalidation_0-auc:0.778316\n",
            "[24]\tvalidation_0-auc:0.777778\n",
            "[25]\tvalidation_0-auc:0.776768\n",
            "[26]\tvalidation_0-auc:0.777037\n",
            "[27]\tvalidation_0-auc:0.778114\n",
            "[28]\tvalidation_0-auc:0.778721\n",
            "[29]\tvalidation_0-auc:0.783906\n",
            "[30]\tvalidation_0-auc:0.782694\n",
            "[31]\tvalidation_0-auc:0.780471\n",
            "[32]\tvalidation_0-auc:0.782155\n",
            "[33]\tvalidation_0-auc:0.784983\n",
            "[34]\tvalidation_0-auc:0.785455\n",
            "[35]\tvalidation_0-auc:0.78963\n",
            "[36]\tvalidation_0-auc:0.790572\n",
            "[37]\tvalidation_0-auc:0.79165\n",
            "[38]\tvalidation_0-auc:0.792054\n",
            "[39]\tvalidation_0-auc:0.791111\n",
            "[40]\tvalidation_0-auc:0.793064\n",
            "[41]\tvalidation_0-auc:0.793064\n",
            "[42]\tvalidation_0-auc:0.793333\n",
            "[43]\tvalidation_0-auc:0.794007\n",
            "[44]\tvalidation_0-auc:0.793872\n",
            "[45]\tvalidation_0-auc:0.793333\n",
            "[46]\tvalidation_0-auc:0.794949\n",
            "[47]\tvalidation_0-auc:0.797104\n",
            "[48]\tvalidation_0-auc:0.795421\n",
            "[49]\tvalidation_0-auc:0.797172\n",
            "[50]\tvalidation_0-auc:0.798114\n",
            "[51]\tvalidation_0-auc:0.800404\n",
            "[52]\tvalidation_0-auc:0.799192\n",
            "[53]\tvalidation_0-auc:0.800135\n",
            "[54]\tvalidation_0-auc:0.803232\n",
            "[55]\tvalidation_0-auc:0.804175\n",
            "[56]\tvalidation_0-auc:0.80404\n",
            "[57]\tvalidation_0-auc:0.802424\n",
            "[58]\tvalidation_0-auc:0.80404\n",
            "[59]\tvalidation_0-auc:0.802963\n",
            "[60]\tvalidation_0-auc:0.802963\n",
            "[61]\tvalidation_0-auc:0.802424\n",
            "[62]\tvalidation_0-auc:0.802963\n",
            "[63]\tvalidation_0-auc:0.804983\n",
            "[64]\tvalidation_0-auc:0.809293\n",
            "[65]\tvalidation_0-auc:0.810101\n",
            "[66]\tvalidation_0-auc:0.808754\n",
            "[67]\tvalidation_0-auc:0.809158\n",
            "[68]\tvalidation_0-auc:0.808889\n",
            "[69]\tvalidation_0-auc:0.809024\n",
            "[70]\tvalidation_0-auc:0.808619\n",
            "[71]\tvalidation_0-auc:0.809158\n",
            "[72]\tvalidation_0-auc:0.811717\n",
            "[73]\tvalidation_0-auc:0.812525\n",
            "[74]\tvalidation_0-auc:0.812795\n",
            "[75]\tvalidation_0-auc:0.815354\n",
            "[76]\tvalidation_0-auc:0.815488\n",
            "[77]\tvalidation_0-auc:0.813737\n",
            "[78]\tvalidation_0-auc:0.810505\n",
            "[79]\tvalidation_0-auc:0.810236\n",
            "[80]\tvalidation_0-auc:0.808619\n",
            "[81]\tvalidation_0-auc:0.809966\n",
            "[82]\tvalidation_0-auc:0.806734\n",
            "[83]\tvalidation_0-auc:0.805926\n",
            "[84]\tvalidation_0-auc:0.805657\n",
            "[85]\tvalidation_0-auc:0.804175\n",
            "[86]\tvalidation_0-auc:0.804579\n",
            "[87]\tvalidation_0-auc:0.803906\n",
            "[88]\tvalidation_0-auc:0.799192\n",
            "[89]\tvalidation_0-auc:0.798384\n",
            "[90]\tvalidation_0-auc:0.801077\n",
            "[91]\tvalidation_0-auc:0.803367\n",
            "[92]\tvalidation_0-auc:0.80431\n",
            "[93]\tvalidation_0-auc:0.805522\n",
            "[94]\tvalidation_0-auc:0.806195\n",
            "[95]\tvalidation_0-auc:0.806195\n",
            "[96]\tvalidation_0-auc:0.807542\n",
            "[97]\tvalidation_0-auc:0.807946\n",
            "[98]\tvalidation_0-auc:0.807407\n",
            "[99]\tvalidation_0-auc:0.806061\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 3s 17ms/step - loss: 0.6486 - accuracy: 0.6660 - val_loss: 0.8175 - val_accuracy: 0.2803\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.6116 - accuracy: 0.6831 - val_loss: 0.6231 - val_accuracy: 0.6879\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5768 - accuracy: 0.7029 - val_loss: 0.5793 - val_accuracy: 0.7197\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5493 - accuracy: 0.7427 - val_loss: 0.5937 - val_accuracy: 0.6688\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5388 - accuracy: 0.7550 - val_loss: 0.5258 - val_accuracy: 0.7197\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 2s 15ms/step - loss: 0.6383 - accuracy: 0.6764 - val_loss: 0.7698 - val_accuracy: 0.2866\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5495 - accuracy: 0.7342 - val_loss: 0.5100 - val_accuracy: 0.7134\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5304 - accuracy: 0.7502 - val_loss: 0.5020 - val_accuracy: 0.8025\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5107 - accuracy: 0.7465 - val_loss: 0.5517 - val_accuracy: 0.6943\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5196 - accuracy: 0.7502 - val_loss: 0.4712 - val_accuracy: 0.7962\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.771118\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.801388\n",
            "[2]\tvalidation_0-auc:0.813053\n",
            "[3]\tvalidation_0-auc:0.808427\n",
            "[4]\tvalidation_0-auc:0.799879\n",
            "[5]\tvalidation_0-auc:0.800583\n",
            "[6]\tvalidation_0-auc:0.801488\n",
            "[7]\tvalidation_0-auc:0.798572\n",
            "[8]\tvalidation_0-auc:0.786102\n",
            "[9]\tvalidation_0-auc:0.780068\n",
            "[10]\tvalidation_0-auc:0.763978\n",
            "[11]\tvalidation_0-auc:0.7678\n",
            "[12]\tvalidation_0-auc:0.760559\n",
            "[13]\tvalidation_0-auc:0.754525\n",
            "[14]\tvalidation_0-auc:0.754123\n",
            "[15]\tvalidation_0-auc:0.746078\n",
            "[16]\tvalidation_0-auc:0.739843\n",
            "[17]\tvalidation_0-auc:0.740648\n",
            "[18]\tvalidation_0-auc:0.740547\n",
            "[19]\tvalidation_0-auc:0.73934\n",
            "[20]\tvalidation_0-auc:0.736726\n",
            "[21]\tvalidation_0-auc:0.73572\n",
            "[22]\tvalidation_0-auc:0.734714\n",
            "[23]\tvalidation_0-auc:0.733709\n",
            "[24]\tvalidation_0-auc:0.73029\n",
            "[25]\tvalidation_0-auc:0.729284\n",
            "[26]\tvalidation_0-auc:0.730893\n",
            "[27]\tvalidation_0-auc:0.726368\n",
            "[28]\tvalidation_0-auc:0.717317\n",
            "[29]\tvalidation_0-auc:0.7143\n",
            "[30]\tvalidation_0-auc:0.715708\n",
            "[31]\tvalidation_0-auc:0.715507\n",
            "[32]\tvalidation_0-auc:0.711685\n",
            "[33]\tvalidation_0-auc:0.710076\n",
            "[34]\tvalidation_0-auc:0.711685\n",
            "[35]\tvalidation_0-auc:0.711887\n",
            "[36]\tvalidation_0-auc:0.710881\n",
            "[37]\tvalidation_0-auc:0.706255\n",
            "[38]\tvalidation_0-auc:0.701428\n",
            "[39]\tvalidation_0-auc:0.701227\n",
            "[40]\tvalidation_0-auc:0.697204\n",
            "[41]\tvalidation_0-auc:0.697808\n",
            "[42]\tvalidation_0-auc:0.700221\n",
            "[43]\tvalidation_0-auc:0.694791\n",
            "[44]\tvalidation_0-auc:0.693986\n",
            "[45]\tvalidation_0-auc:0.693383\n",
            "[46]\tvalidation_0-auc:0.69288\n",
            "[47]\tvalidation_0-auc:0.69288\n",
            "[48]\tvalidation_0-auc:0.690064\n",
            "[49]\tvalidation_0-auc:0.68564\n",
            "[50]\tvalidation_0-auc:0.682723\n",
            "[51]\tvalidation_0-auc:0.682723\n",
            "[52]\tvalidation_0-auc:0.686344\n",
            "Stopping. Best iteration:\n",
            "[2]\tvalidation_0-auc:0.813053\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+---------------------+--------------------+--------------------+---------------------+\n",
            "|      Model       |       Accuracy      |     Precision      |       Recall       |       F1 score      |\n",
            "+------------------+---------------------+--------------------+--------------------+---------------------+\n",
            "|     LSTM 0.2     |  0.4789473684210526 |        0.95        | 0.2814814814814815 | 0.43428571428571433 |\n",
            "|     GRU 0.2      |  0.5368421052631579 | 0.9607843137254902 | 0.362962962962963  |  0.5268817204301075 |\n",
            "|   XGBoost 0.2    |         0.6         | 0.927536231884058  | 0.4740740740740741 |  0.627450980392157  |\n",
            "|    Logreg 0.2    |  0.5842105263157895 |       0.9375       | 0.4444444444444444 |  0.6030150753768844 |\n",
            "|     SVM 0.2      | 0.49473684210526314 | 0.975609756097561  | 0.2962962962962963 | 0.45454545454545453 |\n",
            "|  LSTM beta 0.2   |  0.7197452229299363 | 0.8556701030927835 | 0.7345132743362832 |  0.7904761904761903 |\n",
            "|   GRU beta 0.2   |  0.7961783439490446 | 0.8932038834951457 | 0.8141592920353983 |  0.851851851851852  |\n",
            "| XGBoost beta 0.2 |  0.7261146496815286 | 0.8571428571428571 | 0.7433628318584071 |  0.7962085308056872 |\n",
            "| logreg beta 0.2  |  0.7452229299363057 | 0.8924731182795699 | 0.7345132743362832 |  0.8058252427184466 |\n",
            "|   svm beta 0.2   |  0.7579617834394905 | 0.8440366972477065 | 0.8141592920353983 |  0.8288288288288288 |\n",
            "+------------------+---------------------+--------------------+--------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 3s 17ms/step - loss: 0.6688 - accuracy: 0.6046 - val_loss: 0.8539 - val_accuracy: 0.2895\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6126 - accuracy: 0.6716 - val_loss: 0.6189 - val_accuracy: 0.7526\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6086 - accuracy: 0.6835 - val_loss: 0.6708 - val_accuracy: 0.6632\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6024 - accuracy: 0.6780 - val_loss: 0.6730 - val_accuracy: 0.6842\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.5952 - accuracy: 0.6697 - val_loss: 0.7991 - val_accuracy: 0.6053\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 3s 15ms/step - loss: 0.6626 - accuracy: 0.6092 - val_loss: 0.6534 - val_accuracy: 0.6158\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5948 - accuracy: 0.6807 - val_loss: 0.6003 - val_accuracy: 0.6947\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5753 - accuracy: 0.6936 - val_loss: 0.6765 - val_accuracy: 0.5737\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5589 - accuracy: 0.7083 - val_loss: 0.6204 - val_accuracy: 0.6737\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5647 - accuracy: 0.6963 - val_loss: 0.5639 - val_accuracy: 0.7368\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.773266\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.763502\n",
            "[2]\tvalidation_0-auc:0.76633\n",
            "[3]\tvalidation_0-auc:0.767205\n",
            "[4]\tvalidation_0-auc:0.768552\n",
            "[5]\tvalidation_0-auc:0.764781\n",
            "[6]\tvalidation_0-auc:0.76404\n",
            "[7]\tvalidation_0-auc:0.763569\n",
            "[8]\tvalidation_0-auc:0.766061\n",
            "[9]\tvalidation_0-auc:0.765791\n",
            "[10]\tvalidation_0-auc:0.767273\n",
            "[11]\tvalidation_0-auc:0.765791\n",
            "[12]\tvalidation_0-auc:0.766195\n",
            "[13]\tvalidation_0-auc:0.765455\n",
            "[14]\tvalidation_0-auc:0.764175\n",
            "[15]\tvalidation_0-auc:0.76404\n",
            "[16]\tvalidation_0-auc:0.764983\n",
            "[17]\tvalidation_0-auc:0.767407\n",
            "[18]\tvalidation_0-auc:0.767071\n",
            "[19]\tvalidation_0-auc:0.766128\n",
            "[20]\tvalidation_0-auc:0.765859\n",
            "[21]\tvalidation_0-auc:0.765926\n",
            "[22]\tvalidation_0-auc:0.765522\n",
            "[23]\tvalidation_0-auc:0.76431\n",
            "[24]\tvalidation_0-auc:0.767946\n",
            "[25]\tvalidation_0-auc:0.770236\n",
            "[26]\tvalidation_0-auc:0.77037\n",
            "[27]\tvalidation_0-auc:0.770505\n",
            "[28]\tvalidation_0-auc:0.770438\n",
            "[29]\tvalidation_0-auc:0.770438\n",
            "[30]\tvalidation_0-auc:0.769428\n",
            "[31]\tvalidation_0-auc:0.769562\n",
            "[32]\tvalidation_0-auc:0.768889\n",
            "[33]\tvalidation_0-auc:0.767946\n",
            "[34]\tvalidation_0-auc:0.767542\n",
            "[35]\tvalidation_0-auc:0.766734\n",
            "[36]\tvalidation_0-auc:0.764983\n",
            "[37]\tvalidation_0-auc:0.766667\n",
            "[38]\tvalidation_0-auc:0.768081\n",
            "[39]\tvalidation_0-auc:0.767946\n",
            "[40]\tvalidation_0-auc:0.76835\n",
            "[41]\tvalidation_0-auc:0.769158\n",
            "[42]\tvalidation_0-auc:0.767677\n",
            "[43]\tvalidation_0-auc:0.767542\n",
            "[44]\tvalidation_0-auc:0.767677\n",
            "[45]\tvalidation_0-auc:0.767677\n",
            "[46]\tvalidation_0-auc:0.766599\n",
            "[47]\tvalidation_0-auc:0.766599\n",
            "[48]\tvalidation_0-auc:0.764444\n",
            "[49]\tvalidation_0-auc:0.772795\n",
            "[50]\tvalidation_0-auc:0.771717\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.773266\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 3s 17ms/step - loss: 0.6621 - accuracy: 0.6112 - val_loss: 0.6617 - val_accuracy: 0.7325\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.6361 - accuracy: 0.6490 - val_loss: 0.5589 - val_accuracy: 0.7197\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 12ms/step - loss: 0.6287 - accuracy: 0.6424 - val_loss: 0.6078 - val_accuracy: 0.7134\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 12ms/step - loss: 0.6051 - accuracy: 0.6575 - val_loss: 0.5682 - val_accuracy: 0.7962\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5962 - accuracy: 0.6708 - val_loss: 0.6745 - val_accuracy: 0.6178\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 2s 16ms/step - loss: 0.6581 - accuracy: 0.6206 - val_loss: 0.6400 - val_accuracy: 0.6306\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5831 - accuracy: 0.6897 - val_loss: 0.4488 - val_accuracy: 0.8662\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5465 - accuracy: 0.7181 - val_loss: 0.4820 - val_accuracy: 0.8471\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5366 - accuracy: 0.7275 - val_loss: 0.4380 - val_accuracy: 0.8217\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5256 - accuracy: 0.7294 - val_loss: 0.4415 - val_accuracy: 0.8471\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.761263\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.756235\n",
            "[2]\tvalidation_0-auc:0.757039\n",
            "[3]\tvalidation_0-auc:0.761062\n",
            "[4]\tvalidation_0-auc:0.76066\n",
            "[5]\tvalidation_0-auc:0.760056\n",
            "[6]\tvalidation_0-auc:0.739139\n",
            "[7]\tvalidation_0-auc:0.73924\n",
            "[8]\tvalidation_0-auc:0.739441\n",
            "[9]\tvalidation_0-auc:0.725463\n",
            "[10]\tvalidation_0-auc:0.724658\n",
            "[11]\tvalidation_0-auc:0.723954\n",
            "[12]\tvalidation_0-auc:0.712591\n",
            "[13]\tvalidation_0-auc:0.709875\n",
            "[14]\tvalidation_0-auc:0.709574\n",
            "[15]\tvalidation_0-auc:0.719027\n",
            "[16]\tvalidation_0-auc:0.714099\n",
            "[17]\tvalidation_0-auc:0.713898\n",
            "[18]\tvalidation_0-auc:0.699517\n",
            "[19]\tvalidation_0-auc:0.702534\n",
            "[20]\tvalidation_0-auc:0.699517\n",
            "[21]\tvalidation_0-auc:0.700925\n",
            "[22]\tvalidation_0-auc:0.697104\n",
            "[23]\tvalidation_0-auc:0.67317\n",
            "[24]\tvalidation_0-auc:0.660499\n",
            "[25]\tvalidation_0-auc:0.659091\n",
            "[26]\tvalidation_0-auc:0.659393\n",
            "[27]\tvalidation_0-auc:0.652957\n",
            "[28]\tvalidation_0-auc:0.647124\n",
            "[29]\tvalidation_0-auc:0.643303\n",
            "[30]\tvalidation_0-auc:0.647124\n",
            "[31]\tvalidation_0-auc:0.632643\n",
            "[32]\tvalidation_0-auc:0.63043\n",
            "[33]\tvalidation_0-auc:0.629224\n",
            "[34]\tvalidation_0-auc:0.630632\n",
            "[35]\tvalidation_0-auc:0.627816\n",
            "[36]\tvalidation_0-auc:0.629023\n",
            "[37]\tvalidation_0-auc:0.628218\n",
            "[38]\tvalidation_0-auc:0.634453\n",
            "[39]\tvalidation_0-auc:0.63566\n",
            "[40]\tvalidation_0-auc:0.635257\n",
            "[41]\tvalidation_0-auc:0.634252\n",
            "[42]\tvalidation_0-auc:0.636263\n",
            "[43]\tvalidation_0-auc:0.63566\n",
            "[44]\tvalidation_0-auc:0.634654\n",
            "[45]\tvalidation_0-auc:0.633447\n",
            "[46]\tvalidation_0-auc:0.625201\n",
            "[47]\tvalidation_0-auc:0.624799\n",
            "[48]\tvalidation_0-auc:0.629224\n",
            "[49]\tvalidation_0-auc:0.631637\n",
            "[50]\tvalidation_0-auc:0.630028\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.761263\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.6052631578947368 | 0.8947368421052632 | 0.5037037037037037 | 0.6445497630331753 |\n",
            "|      GRU 0.15     | 0.7368421052631579 |        0.84        | 0.7777777777777778 | 0.8076923076923077 |\n",
            "|    XGBoost 0.15   | 0.7105263157894737 | 0.8703703703703703 | 0.6962962962962963 | 0.7736625514403292 |\n",
            "|    Logreg 0.15    | 0.6421052631578947 | 0.8764044943820225 | 0.5777777777777777 | 0.6964285714285714 |\n",
            "|      SVM 0.15     | 0.7052631578947368 | 0.8691588785046729 | 0.6888888888888889 | 0.768595041322314  |\n",
            "|   LSTM beta 0.15  | 0.6178343949044586 | 0.8955223880597015 | 0.5309734513274337 | 0.6666666666666667 |\n",
            "|   GRU beta 0.15   | 0.8471337579617835 | 0.8617886178861789 | 0.9380530973451328 | 0.8983050847457628 |\n",
            "| XGBoost beta 0.15 | 0.7961783439490446 | 0.8292682926829268 | 0.9026548672566371 | 0.8644067796610169 |\n",
            "|  logreg beta 0.15 | 0.8535031847133758 | 0.8813559322033898 | 0.9203539823008849 | 0.9004329004329005 |\n",
            "|   svm beta 0.15   | 0.802547770700637  | 0.8306451612903226 | 0.911504424778761  | 0.8691983122362869 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh3w1JW7Zu1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f150f6a5-1a94-4698-8397-b9d8e75a2bee"
      },
      "source": [
        "Result_purging.to_csv('TWTR_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.977778</td>\n",
              "      <td>0.415789</td>\n",
              "      <td>0.442211</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.981132</td>\n",
              "      <td>0.457895</td>\n",
              "      <td>0.502415</td>\n",
              "      <td>0.337662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.631579</td>\n",
              "      <td>0.724409</td>\n",
              "      <td>0.597403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.473684</td>\n",
              "      <td>0.532710</td>\n",
              "      <td>0.370130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.688525</td>\n",
              "      <td>0.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.904762</td>\n",
              "      <td>0.484076</td>\n",
              "      <td>0.584615</td>\n",
              "      <td>0.431818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.932584</td>\n",
              "      <td>0.649682</td>\n",
              "      <td>0.751131</td>\n",
              "      <td>0.628788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.940476</td>\n",
              "      <td>0.630573</td>\n",
              "      <td>0.731481</td>\n",
              "      <td>0.598485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.940299</td>\n",
              "      <td>0.535032</td>\n",
              "      <td>0.633166</td>\n",
              "      <td>0.477273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.962025</td>\n",
              "      <td>0.624204</td>\n",
              "      <td>0.720379</td>\n",
              "      <td>0.575758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.478947</td>\n",
              "      <td>0.434286</td>\n",
              "      <td>0.281481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.960784</td>\n",
              "      <td>0.536842</td>\n",
              "      <td>0.526882</td>\n",
              "      <td>0.362963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.927536</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.627451</td>\n",
              "      <td>0.474074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.584211</td>\n",
              "      <td>0.603015</td>\n",
              "      <td>0.444444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.975610</td>\n",
              "      <td>0.494737</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.296296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.855670</td>\n",
              "      <td>0.719745</td>\n",
              "      <td>0.790476</td>\n",
              "      <td>0.734513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.893204</td>\n",
              "      <td>0.796178</td>\n",
              "      <td>0.851852</td>\n",
              "      <td>0.814159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.726115</td>\n",
              "      <td>0.796209</td>\n",
              "      <td>0.743363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.892473</td>\n",
              "      <td>0.745223</td>\n",
              "      <td>0.805825</td>\n",
              "      <td>0.734513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.844037</td>\n",
              "      <td>0.757962</td>\n",
              "      <td>0.828829</td>\n",
              "      <td>0.814159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.894737</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>0.644550</td>\n",
              "      <td>0.503704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.840000</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.807692</td>\n",
              "      <td>0.777778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.870370</td>\n",
              "      <td>0.710526</td>\n",
              "      <td>0.773663</td>\n",
              "      <td>0.696296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.876404</td>\n",
              "      <td>0.642105</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.577778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.869159</td>\n",
              "      <td>0.705263</td>\n",
              "      <td>0.768595</td>\n",
              "      <td>0.688889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.895522</td>\n",
              "      <td>0.617834</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.530973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.861789</td>\n",
              "      <td>0.847134</td>\n",
              "      <td>0.898305</td>\n",
              "      <td>0.938053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.829268</td>\n",
              "      <td>0.796178</td>\n",
              "      <td>0.864407</td>\n",
              "      <td>0.902655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.881356</td>\n",
              "      <td>0.853503</td>\n",
              "      <td>0.900433</td>\n",
              "      <td>0.920354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.830645</td>\n",
              "      <td>0.802548</td>\n",
              "      <td>0.869198</td>\n",
              "      <td>0.911504</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  TWTR  0.977778  0.415789  0.442211  0.285714\n",
              "1            GRU 0.1  TWTR  0.981132  0.457895  0.502415  0.337662\n",
              "2        XGBoost 0.1  TWTR  0.920000  0.631579  0.724409  0.597403\n",
              "3         Logreg 0.1  TWTR  0.950000  0.473684  0.532710  0.370130\n",
              "4            SVM 0.1  TWTR  0.933333  0.600000  0.688525  0.545455\n",
              "5      LSTM beta 0.1  TWTR  0.904762  0.484076  0.584615  0.431818\n",
              "6       GRU beta 0.1  TWTR  0.932584  0.649682  0.751131  0.628788\n",
              "7   XGBoost beta 0.1  TWTR  0.940476  0.630573  0.731481  0.598485\n",
              "8    logreg beta 0.1  TWTR  0.940299  0.535032  0.633166  0.477273\n",
              "9       svm beta 0.1  TWTR  0.962025  0.624204  0.720379  0.575758\n",
              "0           LSTM 0.2  TWTR  0.950000  0.478947  0.434286  0.281481\n",
              "1            GRU 0.2  TWTR  0.960784  0.536842  0.526882  0.362963\n",
              "2        XGBoost 0.2  TWTR  0.927536  0.600000  0.627451  0.474074\n",
              "3         Logreg 0.2  TWTR  0.937500  0.584211  0.603015  0.444444\n",
              "4            SVM 0.2  TWTR  0.975610  0.494737  0.454545  0.296296\n",
              "5      LSTM beta 0.2  TWTR  0.855670  0.719745  0.790476  0.734513\n",
              "6       GRU beta 0.2  TWTR  0.893204  0.796178  0.851852  0.814159\n",
              "7   XGBoost beta 0.2  TWTR  0.857143  0.726115  0.796209  0.743363\n",
              "8    logreg beta 0.2  TWTR  0.892473  0.745223  0.805825  0.734513\n",
              "9       svm beta 0.2  TWTR  0.844037  0.757962  0.828829  0.814159\n",
              "0          LSTM 0.15  TWTR  0.894737  0.605263  0.644550  0.503704\n",
              "1           GRU 0.15  TWTR  0.840000  0.736842  0.807692  0.777778\n",
              "2       XGBoost 0.15  TWTR  0.870370  0.710526  0.773663  0.696296\n",
              "3        Logreg 0.15  TWTR  0.876404  0.642105  0.696429  0.577778\n",
              "4           SVM 0.15  TWTR  0.869159  0.705263  0.768595  0.688889\n",
              "5     LSTM beta 0.15  TWTR  0.895522  0.617834  0.666667  0.530973\n",
              "6      GRU beta 0.15  TWTR  0.861789  0.847134  0.898305  0.938053\n",
              "7  XGBoost beta 0.15  TWTR  0.829268  0.796178  0.864407  0.902655\n",
              "8   logreg beta 0.15  TWTR  0.881356  0.853503  0.900433  0.920354\n",
              "9      svm beta 0.15  TWTR  0.830645  0.802548  0.869198  0.911504"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qYmyD2kZu1G"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1410:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1410:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1410:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1410:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1410:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1443:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1443:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1443:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1443:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1443:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjDPxMfoZu1G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f65PzbZEaAC6"
      },
      "source": [
        "## WMT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx_ja3kTaAC7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "outputId": "a485a7ec-d0c3-48dc-db53-5cb6c2abc7d3"
      },
      "source": [
        "dfs = pd.read_csv(\"WMT.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "# Denoise prices\n",
        "dfs = denoise_data(dfs, '<CLOSE>')\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.WMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>139.34</td>\n",
              "      <td>139.58</td>\n",
              "      <td>135.940</td>\n",
              "      <td>136.95</td>\n",
              "      <td>382464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.WMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>140.80</td>\n",
              "      <td>141.70</td>\n",
              "      <td>139.335</td>\n",
              "      <td>139.35</td>\n",
              "      <td>295652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.WMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>140.91</td>\n",
              "      <td>141.82</td>\n",
              "      <td>140.330</td>\n",
              "      <td>140.43</td>\n",
              "      <td>208623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2763</td>\n",
              "      <td>US1.WMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>142.02</td>\n",
              "      <td>142.02</td>\n",
              "      <td>139.990</td>\n",
              "      <td>140.51</td>\n",
              "      <td>313123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2762</td>\n",
              "      <td>US1.WMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>142.89</td>\n",
              "      <td>143.51</td>\n",
              "      <td>141.850</td>\n",
              "      <td>142.26</td>\n",
              "      <td>306424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2762</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.WMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>54.35</td>\n",
              "      <td>54.59</td>\n",
              "      <td>54.020</td>\n",
              "      <td>54.41</td>\n",
              "      <td>8421572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2763</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.WMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>54.60</td>\n",
              "      <td>54.82</td>\n",
              "      <td>54.040</td>\n",
              "      <td>54.37</td>\n",
              "      <td>7316985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.WMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>53.92</td>\n",
              "      <td>54.63</td>\n",
              "      <td>53.900</td>\n",
              "      <td>54.53</td>\n",
              "      <td>9474898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.WMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>53.90</td>\n",
              "      <td>54.25</td>\n",
              "      <td>53.830</td>\n",
              "      <td>53.99</td>\n",
              "      <td>9078140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.WMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>53.34</td>\n",
              "      <td>53.70</td>\n",
              "      <td>53.140</td>\n",
              "      <td>53.57</td>\n",
              "      <td>6732639</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2767 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...  <HIGH>    <LOW>  <CLOSE>    <VOL>\n",
              "0      2766  US1.WMT     D  20211001  ...  139.58  135.940   136.95   382464\n",
              "1      2765  US1.WMT     D  20210930  ...  141.70  139.335   139.35   295652\n",
              "2      2764  US1.WMT     D  20210929  ...  141.82  140.330   140.43   208623\n",
              "3      2763  US1.WMT     D  20210928  ...  142.02  139.990   140.51   313123\n",
              "4      2762  US1.WMT     D  20210927  ...  143.51  141.850   142.26   306424\n",
              "...     ...      ...   ...       ...  ...     ...      ...      ...      ...\n",
              "2762      4  US1.WMT     D  20101008  ...   54.59   54.020    54.41  8421572\n",
              "2763      3  US1.WMT     D  20101007  ...   54.82   54.040    54.37  7316985\n",
              "2764      2  US1.WMT     D  20101006  ...   54.63   53.900    54.53  9474898\n",
              "2765      1  US1.WMT     D  20101005  ...   54.25   53.830    53.99  9078140\n",
              "2766      0  US1.WMT     D  20101004  ...   53.70   53.140    53.57  6732639\n",
              "\n",
              "[2767 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUZOa1N2aAC7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "06a1e38f-7312-406b-e9df-5a13a80f1397"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"c270df30-89f8-4e78-84e7-e849668a1ba7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"c270df30-89f8-4e78-84e7-e849668a1ba7\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'c270df30-89f8-4e78-84e7-e849668a1ba7',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [118.15, 116.32, 116.14, 117.85, 118.71, 118.44, 118.32, 118.48, 118.47, 117.62, 116.93, 117.07, 117.16, 116.48, 115.51, 117.43, 116.98, 115.97, 116.03, 116.36, 114.72, 115.45, 115.89, 114.65, 114.19, 114.05, 112.65, 112.35, 111.99, 110.82, 111.91, 112.01, 112.11, 113.79, 112.97, 112.7, 106.23, 107.43, 105.19, 107.28, 108.52, 108.23, 107.27, 105.82, 109.43, 109.41, 110.38, 112.06, 112.27, 113.02, 112.25, 111.96, 112.06, 112.81, 113.92, 114.72, 114.6, 114.74, 114.98, 114.59, 113.91, 112.98, 112.92, 112.7499, 112.0, 112.4, 111.61, 110.63, 110.44, 110.11, 110.17, 110.72, 111.26, 111.14, 110.3, 109.62, 109.67, 109.15, 109.07, 108.65, 108.82, 107.91, 107.51, 106.0258, 105.11, 104.42, 102.56, 101.95, 101.44, 102.19, 102.13, 102.42, 102.68, 101.85, 102.23, 101.12, 101.5, 100.84, 101.27, 99.9, 100.32, 99.87, 101.95, 99.54, 100.31, 101.31, 102.46, 102.07, 101.15, 101.34, 102.82, 101.54, 101.51, 103.52, 103.55, 103.06, 102.38, 103.15, 103.13, 102.93, 102.44, 101.57, 100.8, 99.61, 98.69, 99.22, 98.84, 98.12, 97.16, 96.95, 97.81, 97.52, 97.16, 97.17, 98.33, 98.17, 98.29, 99.05, 98.64, 99.88, 99.67, 98.26, 98.23, 99.03, 98.4, 98.45, 97.575, 97.44, 98.26, 98.34, 97.84, 97.95, 98.99, 98.12, 98.69, 99.13, 99.54, 99.38, 99.88, 102.24, 99.96, 98.52, 97.93, 96.96, 96.17, 95.6, 96.72, 95.65, 95.61, 94.76, 93.81, 96.08, 94.8, 96.69, 97.05, 96.92, 98.36, 98.71, 97.5, 97.71, 96.75, 96.34, 96.25, 94.93, 94.82, 94.94, 94.9, 95.21, 94.53, 93.42, 92.88, 93.32, 93.15, 92.15, 91.61, 90.41, 85.6, 87.08, 87.26, 90.58, 91.07, 90.76, 91.86, 92.95, 93.09, 93.86, 93.92, 93.2, 94.8285, 95.83, 98.75, 97.59, 97.29, 97.46, 95.05, 95.14, 95.15, 94.16, 94.14, 96.78, 97.7, 99.55, 101.58, 102.95, 103.88, 105.55, 104.89, 104.34, 103.32, 102.89, 101.22, 100.58, 100.265, 102.44, 99.8, 98.95, 99.15, 97.54, 97.84, 97.14, 97.15, 96.19, 96.56, 95.83, 93.85, 94.78, 93.92, 95.78, 97.09, 94.7, 93.31, 94.21, 94.07, 95.16, 94.42, 93.93, 94.16, 94.6, 95.1, 94.93, 95.84, 95.76, 95.2401, 95.44, 94.82, 94.58, 95.13, 95.97, 96.63, 96.91, 95.84, 96.45, 96.6, 95.37, 95.84, 96.11, 95.65, 96.12, 94.51, 94.95, 95.19, 95.67, 96.08, 96.0, 97.85, 98.64, 90.25, 90.847, 89.65, 90.19, 89.02, 90.05, 89.78, 89.68, 89.61, 88.7785, 88.23, 89.21, 88.87, 88.14, 88.25, 87.92, 87.95, 87.64, 88.07, 87.74, 88.08, 88.2, 87.63, 87.69, 86.5, 86.54, 87.21, 85.94, 84.5, 84.57, 84.41, 83.98, 85.66, 85.86, 86.88, 85.94, 86.46, 84.64, 84.22, 83.63, 83.6, 83.09, 83.65, 83.82, 84.08, 84.11, 84.3, 84.32, 84.95, 84.55, 84.64, 85.42, 82.99, 82.55, 84.13, 82.41, 82.46, 82.85, 83.0, 83.4001, 84.5, 83.6, 84.34, 86.09, 84.51, 84.42, 83.37, 82.67, 83.05, 85.75, 85.46, 87.55, 86.225, 86.34, 87.38, 88.45, 87.29, 87.945, 87.19, 86.51, 86.11, 86.98, 87.88, 87.57, 87.88, 86.84, 86.02, 85.44, 85.9, 86.45, 86.3, 86.7, 87.82, 87.24, 86.8, 85.59, 88.96, 87.68, 86.05, 87.51, 85.41, 87.15, 88.19, 87.97, 87.46, 89.55, 87.52, 87.69, 88.29, 88.075, 88.68, 87.89, 87.74, 89.07, 89.98, 88.76, 89.05, 90.005, 91.53, 93.09, 92.89, 92.8, 91.5499, 94.11, 104.72, 103.26, 101.71, 100.98, 99.55, 99.4, 100.02, 102.91, 100.9, 100.08, 104.41, 105.54, 106.61, 107.76, 109.57, 108.39, 106.6, 105.81, 105.92, 105.43, 104.59, 104.34, 102.69, 100.685, 100.88, 100.01, 99.69, 100.4, 101.57, 100.12, 99.535, 99.44, 98.56, 98.88, 99.39, 99.25, 99.16, 98.22, 98.06, 98.8, 98.79, 97.87, 97.11, 97.14, 97.8, 96.7, 96.95, 96.55, 96.8, 97.3, 97.83, 97.01, 97.34, 97.26, 97.58, 96.79, 96.62, 96.62, 96.41, 96.5, 97.5, 97.46, 99.63, 89.83, 91.1, 90.99, 90.9201, 90.32, 90.25, 88.93, 88.71, 89.66, 88.81, 87.96, 87.3, 86.94, 88.17, 88.62, 88.5, 87.99, 88.7, 87.435, 86.43, 86.23, 85.985, 85.74, 86.64, 86.09, 85.685, 84.15, 80.51, 78.99, 79.45, 79.13, 79.24, 78.46, 78.14, 78.91, 79.29, 79.41, 79.15, 79.52, 80.0, 80.51, 80.07, 80.01, 80.33, 79.7, 79.85, 79.65, 79.07, 78.91, 80.13, 80.1, 79.83, 78.35, 78.07, 78.55, 78.81, 77.99, 78.67, 78.32, 79.96, 80.01, 79.74, 79.36, 79.7, 80.95, 80.79, 80.67, 80.41, 80.67, 81.59, 81.61, 81.29, 80.47, 80.87, 80.5328, 80.5, 79.99, 79.81, 79.79, 78.9, 78.51, 76.9, 76.15, 76.04, 75.86, 76.21, 76.4, 76.36, 75.04, 73.98, 73.47, 73.23, 75.32, 75.48, 75.32, 75.59, 75.675, 75.9, 76.51, 76.04, 75.5, 74.82, 75.53, 76.25, 75.56, 75.48, 75.26, 78.91, 79.91, 79.52, 79.23, 79.4, 78.95, 79.15, 78.93, 80.26, 79.53, 79.8, 78.6, 78.16, 78.12, 78.3, 78.16, 78.48, 78.53, 78.81, 77.54, 75.1391, 75.12, 76.29, 75.71, 76.12, 76.71, 76.72, 76.1, 76.48, 76.33, 75.74, 75.53, 75.25, 75.15, 75.43, 75.44, 75.04, 74.8, 74.96, 74.77, 74.06, 73.89, 73.5, 73.19, 73.43, 73.44, 73.05, 72.9043, 71.42, 71.63, 71.98, 71.84, 72.07, 71.581, 70.74, 70.32, 69.645, 69.63, 69.89, 70.26, 69.9003, 69.98, 70.2, 70.46, 70.56, 70.72, 69.96, 70.13, 69.88, 69.81, 69.88, 69.91, 70.0, 70.74, 70.43, 70.84, 71.74, 72.39, 71.2746, 71.68, 71.45, 69.35, 68.85, 68.6801, 68.66, 67.76, 68.02, 69.07, 67.81, 66.9, 66.41, 66.48, 66.67, 66.23, 66.75, 66.41, 65.64, 66.72, 66.9, 67.39, 66.67, 67.16, 67.6, 68.11, 68.43, 67.1, 67.97, 68.53, 68.24, 68.69, 68.26, 69.21, 69.07, 68.64, 69.14, 69.27, 69.31, 69.71, 69.53, 69.58, 71.25, 71.8, 71.59, 70.94, 71.08, 71.35, 71.81, 71.66, 70.1, 70.35, 70.59, 70.36, 69.94, 70.88, 70.71, 70.46, 71.39, 71.2, 71.0, 70.81, 70.1, 69.37, 68.54, 69.19, 71.37, 71.41, 70.48, 71.22, 71.4, 71.13, 69.79, 69.78, 69.17, 69.63, 69.45, 69.29, 70.03, 70.01, 69.85, 69.59, 69.38, 69.2, 68.36, 68.74, 68.91, 68.86, 68.23, 68.46, 68.25, 67.47, 67.4, 67.96, 68.7, 69.36, 71.65, 71.77, 72.02, 72.11, 70.72, 71.76, 72.3, 71.63, 72.36, 72.24, 72.19, 71.98, 72.09, 72.86, 72.4, 71.52, 71.47, 71.94, 70.34, 71.795, 72.05, 73.03, 72.49, 72.84, 71.41, 71.32, 71.41, 71.16, 71.22, 72.25, 71.97, 72.71, 72.75, 74.26, 72.91, 72.89, 73.3, 73.88, 73.81, 73.96, 73.545, 73.34, 73.74, 73.31, 72.91, 73.12, 73.79, 72.96, 73.23, 73.33, 73.74, 73.76, 73.54, 73.53, 73.79, 73.68, 73.83, 73.64, 73.7, 73.61, 73.26, 74.04, 73.79, 73.4, 73.84, 73.14, 72.81, 73.03, 72.44, 71.51, 71.52, 71.92, 72.08, 71.75, 71.47, 71.09, 70.95, 71.29, 71.13, 70.95, 70.53, 71.145, 71.09, 71.28, 71.02, 71.07, 70.87, 70.93, 70.5, 70.77, 70.76, 70.81, 70.4503, 70.22, 69.53, 69.86, 69.2, 63.13, 65.06, 66.0, 64.94, 66.87, 66.41, 68.79, 68.96, 68.25, 67.21, 67.17, 67.07, 67.58, 66.88, 68.95, 69.44, 69.31, 69.495, 68.72, 68.48, 69.22, 69.75, 69.81, 69.08, 68.76, 69.17, 68.82, 67.41, 68.06, 68.23, 69.03, 68.62, 69.09, 69.08, 68.5, 68.79, 68.04, 68.11, 68.01, 67.47, 67.88, 67.98, 66.95, 67.44, 68.0, 68.08, 67.37, 67.14, 67.41, 67.53, 68.04, 67.85, 66.77, 66.13, 66.18, 66.43, 66.3, 66.49, 68.04, 67.1, 66.495, 65.62, 64.66, 64.0901, 66.14, 65.89, 66.17, 65.3, 65.78, 65.82, 66.93, 66.99, 66.41, 66.27, 66.85, 67.48, 66.36, 64.19, 63.95, 63.94, 63.45, 62.68, 61.85, 60.85, 62.57, 61.92, 63.05, 61.92, 63.61, 64.23, 63.52, 65.01, 63.6, 62.94, 61.47, 61.3, 61.67, 61.58, 60.74, 60.83, 61.08, 60.55, 59.57, 58.83, 58.98, 60.29, 59.67, 60.39, 59.34, 59.56, 59.13, 59.63, 60.5, 59.67, 59.05, 58.379, 58.97, 58.87, 59.89, 60.25, 59.9122, 60.26, 60.09, 60.7, 60.92, 59.94, 57.88, 56.44, 56.9566, 57.57, 58.7, 58.51, 58.79, 58.61, 58.38, 58.13, 57.6, 57.25, 57.95, 57.6199, 57.48, 58.03, 58.3011, 58.89, 58.64, 58.76, 58.8501, 58.87, 59.34, 60.03, 66.71, 66.92, 66.69, 66.87, 66.35, 65.69, 65.86, 64.98, 64.26, 64.83, 63.77, 63.68, 63.77, 63.81, 63.73, 63.6, 63.72, 63.38, 64.48, 64.68, 64.32, 64.29, 64.63, 64.14, 65.13, 66.38, 63.89, 64.86, 64.45, 63.85, 64.73, 64.94, 66.11, 64.83, 63.1, 63.95, 66.5699, 68.44, 68.59, 69.48, 71.9, 72.38, 72.12, 72.6, 71.94, 71.49, 71.25, 72.78, 73.53, 72.25, 72.19, 72.01, 72.17, 72.24, 72.095, 71.39, 71.59, 72.51, 73.16, 72.75, 73.09, 73.39, 73.83, 73.65, 73.78, 73.88, 73.135, 72.76, 73.04, 73.8, 72.54, 71.84, 71.89, 70.92, 71.43, 72.12, 71.86, 72.405, 72.56, 72.78, 72.71, 72.97, 72.73, 72.34, 71.94, 72.43, 72.95, 72.92, 72.45, 72.62, 73.03, 74.17, 74.88, 74.51, 74.71, 74.28, 74.845, 75.18, 74.9285, 75.88, 76.12, 75.88, 76.41, 79.9, 79.23, 78.71, 78.14, 78.96, 78.12, 78.51, 78.04, 77.64, 78.13, 79.19, 78.58, 78.03, 77.86, 79.1, 79.38, 79.84, 79.2, 78.42, 78.04, 78.11, 77.88, 79.25, 79.735, 80.17, 80.31, 80.66, 80.83, 81.0, 80.47, 80.98, 80.73, 80.71, 82.26, 82.55, 81.33, 81.88, 81.32, 83.07, 83.33, 83.265, 81.55, 82.48, 82.62, 83.29, 81.925, 81.89, 80.69, 82.14, 82.91, 82.59, 83.58, 82.57, 83.36, 83.94, 83.91, 83.79, 83.55, 84.58, 84.625, 84.32, 83.54, 86.28, 85.95, 85.8, 85.86, 86.34, 87.28, 85.89, 87.32, 87.274, 86.64, 86.24, 85.71, 84.98, 87.71, 86.85, 87.54, 88.62, 88.51, 88.29, 86.65, 86.68, 86.76, 87.38, 86.58, 89.31, 90.06, 89.36, 90.46, 88.61, 86.29, 85.68, 85.9, 85.89, 86.79, 86.655, 86.93, 86.27, 86.69, 86.4, 85.17, 85.94, 84.22, 82.98, 83.92, 83.82, 83.81, 83.0, 83.55, 84.25, 84.11, 84.74, 84.92, 86.4, 86.23, 87.68, 84.98, 84.99, 85.4, 84.62, 84.58, 85.0, 83.82, 83.57, 82.96, 82.94, 79.2, 79.03, 79.45, 78.76, 77.8, 77.7, 77.25, 76.25, 76.24, 76.44, 76.38, 76.36, 76.57, 76.38, 76.22, 76.03, 76.03, 75.12, 74.14, 73.81, 75.19, 77.95, 77.56, 78.28, 77.87, 78.24, 77.31, 77.37, 77.32, 76.22, 76.12, 76.49, 76.08, 76.48, 76.14, 77.1, 75.63, 76.32, 76.86, 76.22, 76.23, 76.31, 75.8, 75.8, 76.09, 76.51, 76.72, 76.53, 77.46, 76.55, 76.0, 75.75, 75.53, 75.91, 75.84, 75.51, 75.72, 75.73, 75.54, 74.95, 74.88, 74.47, 73.92, 74.37, 74.02, 74.22, 74.36, 74.66, 73.94, 74.19, 73.34, 73.55, 73.55, 73.6, 74.78, 75.45, 75.7, 75.98, 76.34, 76.99, 76.63, 76.77, 77.06, 76.61, 76.85, 76.86, 76.55, 76.81, 77.06, 77.21, 76.67, 76.06, 75.8, 75.61, 75.26, 75.03, 75.33, 74.92, 75.62, 75.97, 75.79, 75.69, 75.87, 75.72, 75.0, 75.35, 75.28, 75.71, 76.17, 76.63, 77.01, 77.21, 77.33, 77.13, 76.72, 76.76, 76.76, 75.98, 75.54, 75.57, 75.62, 75.39, 75.65, 75.7, 76.61, 77.02, 76.84, 78.73, 79.15, 79.16, 79.18, 78.7, 78.0, 78.03, 78.65, 79.18, 79.68, 79.7, 79.66, 79.75, 78.65, 78.31, 78.05, 77.58, 77.59, 77.66, 77.21, 76.88, 77.37, 76.51, 76.88, 77.96, 78.19, 77.32, 77.31, 77.45, 77.18, 76.77, 76.43, 76.01, 76.15, 76.23, 76.88, 76.76, 76.47, 75.38, 74.38, 74.76, 74.68, 74.29, 74.95, 75.52, 74.92, 74.42, 74.56, 74.898, 74.8, 75.1, 74.11, 74.73, 74.56, 74.79, 73.35, 73.35, 73.12, 73.5299, 74.86, 75.35, 75.8, 75.36, 74.97, 74.79, 73.74, 73.76, 72.82, 72.88, 72.73, 72.66, 74.69, 74.74, 74.11, 74.67, 74.13, 74.45, 74.95, 75.36, 75.84, 76.15, 76.76, 77.66, 77.94, 77.47, 78.04, 78.1, 77.805, 78.44, 78.18, 78.64, 78.91, 78.69, 78.63, 78.49, 78.38, 78.01, 77.87, 77.5, 77.24, 77.95, 77.24, 77.73, 78.07, 78.5, 79.08, 79.08, 79.96, 79.94, 79.44, 80.22, 81.21, 81.1097, 81.0099, 80.935, 80.68, 80.44, 79.81, 78.86, 78.91, 79.25, 79.21, 79.25, 79.09, 78.88, 78.72, 79.0, 77.95, 77.51, 78.15, 77.43, 77.34, 77.07, 76.78, 76.93, 77.07, 77.17, 76.07, 76.42, 75.89, 76.33, 75.16]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c270df30-89f8-4e78-84e7-e849668a1ba7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"6083e24f-7365-442c-ac7a-5d3953a1f5f3\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"6083e24f-7365-442c-ac7a-5d3953a1f5f3\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '6083e24f-7365-442c-ac7a-5d3953a1f5f3',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6083e24f-7365-442c-ac7a-5d3953a1f5f3');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqbGAFB4aAC7"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUFREgSWaAC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f09eccf-761d-4f6a-9885-171e98909795"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.05, .1, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"WMT\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.05\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6861 - accuracy: 0.5799 - val_loss: 0.6709 - val_accuracy: 0.6122\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6804 - accuracy: 0.5799 - val_loss: 0.6695 - val_accuracy: 0.6122\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6725 - accuracy: 0.5899 - val_loss: 0.6320 - val_accuracy: 0.7347\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6304 - accuracy: 0.6624 - val_loss: 0.5589 - val_accuracy: 0.7367\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6359 - accuracy: 0.6591 - val_loss: 0.6473 - val_accuracy: 0.6122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6839 - accuracy: 0.5624 - val_loss: 0.6674 - val_accuracy: 0.6122\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6490 - accuracy: 0.6383 - val_loss: 0.6232 - val_accuracy: 0.6755\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5970 - accuracy: 0.7047 - val_loss: 0.5493 - val_accuracy: 0.7347\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5816 - accuracy: 0.7007 - val_loss: 0.5040 - val_accuracy: 0.7694\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5782 - accuracy: 0.7047 - val_loss: 0.5118 - val_accuracy: 0.7816\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.780088\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.799561\n",
            "[2]\tvalidation_0-auc:0.799588\n",
            "[3]\tvalidation_0-auc:0.810228\n",
            "[4]\tvalidation_0-auc:0.810009\n",
            "[5]\tvalidation_0-auc:0.809395\n",
            "[6]\tvalidation_0-auc:0.811246\n",
            "[7]\tvalidation_0-auc:0.811675\n",
            "[8]\tvalidation_0-auc:0.810272\n",
            "[9]\tvalidation_0-auc:0.812763\n",
            "[10]\tvalidation_0-auc:0.812614\n",
            "[11]\tvalidation_0-auc:0.812632\n",
            "[12]\tvalidation_0-auc:0.813561\n",
            "[13]\tvalidation_0-auc:0.814298\n",
            "[14]\tvalidation_0-auc:0.813018\n",
            "[15]\tvalidation_0-auc:0.812675\n",
            "[16]\tvalidation_0-auc:0.813535\n",
            "[17]\tvalidation_0-auc:0.812342\n",
            "[18]\tvalidation_0-auc:0.812553\n",
            "[19]\tvalidation_0-auc:0.813079\n",
            "[20]\tvalidation_0-auc:0.812465\n",
            "[21]\tvalidation_0-auc:0.813272\n",
            "[22]\tvalidation_0-auc:0.812746\n",
            "[23]\tvalidation_0-auc:0.81243\n",
            "[24]\tvalidation_0-auc:0.811044\n",
            "[25]\tvalidation_0-auc:0.810947\n",
            "[26]\tvalidation_0-auc:0.811193\n",
            "[27]\tvalidation_0-auc:0.812035\n",
            "[28]\tvalidation_0-auc:0.811386\n",
            "[29]\tvalidation_0-auc:0.811246\n",
            "[30]\tvalidation_0-auc:0.810763\n",
            "[31]\tvalidation_0-auc:0.811009\n",
            "[32]\tvalidation_0-auc:0.810281\n",
            "[33]\tvalidation_0-auc:0.809956\n",
            "[34]\tvalidation_0-auc:0.810281\n",
            "[35]\tvalidation_0-auc:0.810298\n",
            "[36]\tvalidation_0-auc:0.810298\n",
            "[37]\tvalidation_0-auc:0.809912\n",
            "[38]\tvalidation_0-auc:0.809982\n",
            "[39]\tvalidation_0-auc:0.810307\n",
            "[40]\tvalidation_0-auc:0.8105\n",
            "[41]\tvalidation_0-auc:0.810167\n",
            "[42]\tvalidation_0-auc:0.810061\n",
            "[43]\tvalidation_0-auc:0.810061\n",
            "[44]\tvalidation_0-auc:0.809149\n",
            "[45]\tvalidation_0-auc:0.808833\n",
            "[46]\tvalidation_0-auc:0.809193\n",
            "[47]\tvalidation_0-auc:0.808719\n",
            "[48]\tvalidation_0-auc:0.808842\n",
            "[49]\tvalidation_0-auc:0.809193\n",
            "[50]\tvalidation_0-auc:0.809184\n",
            "[51]\tvalidation_0-auc:0.808886\n",
            "[52]\tvalidation_0-auc:0.80857\n",
            "[53]\tvalidation_0-auc:0.80857\n",
            "[54]\tvalidation_0-auc:0.808518\n",
            "[55]\tvalidation_0-auc:0.807956\n",
            "[56]\tvalidation_0-auc:0.8075\n",
            "[57]\tvalidation_0-auc:0.806877\n",
            "[58]\tvalidation_0-auc:0.807193\n",
            "[59]\tvalidation_0-auc:0.807158\n",
            "[60]\tvalidation_0-auc:0.806807\n",
            "[61]\tvalidation_0-auc:0.806877\n",
            "[62]\tvalidation_0-auc:0.806965\n",
            "[63]\tvalidation_0-auc:0.806719\n",
            "Stopping. Best iteration:\n",
            "[13]\tvalidation_0-auc:0.814298\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6851 - accuracy: 0.5724 - val_loss: 0.6595 - val_accuracy: 0.6565\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6822 - accuracy: 0.5800 - val_loss: 0.6647 - val_accuracy: 0.6565\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6791 - accuracy: 0.5786 - val_loss: 0.6719 - val_accuracy: 0.6565\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6652 - accuracy: 0.5916 - val_loss: 0.6058 - val_accuracy: 0.6586\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6403 - accuracy: 0.6294 - val_loss: 0.5837 - val_accuracy: 0.7505\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6820 - accuracy: 0.5717 - val_loss: 0.6739 - val_accuracy: 0.6958\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6544 - accuracy: 0.6102 - val_loss: 0.6170 - val_accuracy: 0.6915\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6077 - accuracy: 0.6685 - val_loss: 0.5232 - val_accuracy: 0.7659\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5927 - accuracy: 0.7062 - val_loss: 0.5444 - val_accuracy: 0.7177\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5970 - accuracy: 0.6870 - val_loss: 0.5271 - val_accuracy: 0.7527\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.721826\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.750318\n",
            "[2]\tvalidation_0-auc:0.736369\n",
            "[3]\tvalidation_0-auc:0.73448\n",
            "[4]\tvalidation_0-auc:0.742144\n",
            "[5]\tvalidation_0-auc:0.749618\n",
            "[6]\tvalidation_0-auc:0.746635\n",
            "[7]\tvalidation_0-auc:0.759841\n",
            "[8]\tvalidation_0-auc:0.753471\n",
            "[9]\tvalidation_0-auc:0.760457\n",
            "[10]\tvalidation_0-auc:0.763503\n",
            "[11]\tvalidation_0-auc:0.767176\n",
            "[12]\tvalidation_0-auc:0.769671\n",
            "[13]\tvalidation_0-auc:0.765021\n",
            "[14]\tvalidation_0-auc:0.768079\n",
            "[15]\tvalidation_0-auc:0.768853\n",
            "[16]\tvalidation_0-auc:0.767803\n",
            "[17]\tvalidation_0-auc:0.769183\n",
            "[18]\tvalidation_0-auc:0.769926\n",
            "[19]\tvalidation_0-auc:0.769936\n",
            "[20]\tvalidation_0-auc:0.769735\n",
            "[21]\tvalidation_0-auc:0.772887\n",
            "[22]\tvalidation_0-auc:0.773301\n",
            "[23]\tvalidation_0-auc:0.773493\n",
            "[24]\tvalidation_0-auc:0.772134\n",
            "[25]\tvalidation_0-auc:0.77241\n",
            "[26]\tvalidation_0-auc:0.772155\n",
            "[27]\tvalidation_0-auc:0.771433\n",
            "[28]\tvalidation_0-auc:0.772495\n",
            "[29]\tvalidation_0-auc:0.774915\n",
            "[30]\tvalidation_0-auc:0.774745\n",
            "[31]\tvalidation_0-auc:0.776221\n",
            "[32]\tvalidation_0-auc:0.77603\n",
            "[33]\tvalidation_0-auc:0.776019\n",
            "[34]\tvalidation_0-auc:0.775786\n",
            "[35]\tvalidation_0-auc:0.775764\n",
            "[36]\tvalidation_0-auc:0.776306\n",
            "[37]\tvalidation_0-auc:0.775584\n",
            "[38]\tvalidation_0-auc:0.774586\n",
            "[39]\tvalidation_0-auc:0.77448\n",
            "[40]\tvalidation_0-auc:0.773928\n",
            "[41]\tvalidation_0-auc:0.774289\n",
            "[42]\tvalidation_0-auc:0.773949\n",
            "[43]\tvalidation_0-auc:0.775828\n",
            "[44]\tvalidation_0-auc:0.775446\n",
            "[45]\tvalidation_0-auc:0.776369\n",
            "[46]\tvalidation_0-auc:0.776285\n",
            "[47]\tvalidation_0-auc:0.77603\n",
            "[48]\tvalidation_0-auc:0.775679\n",
            "[49]\tvalidation_0-auc:0.776019\n",
            "[50]\tvalidation_0-auc:0.774342\n",
            "[51]\tvalidation_0-auc:0.775287\n",
            "[52]\tvalidation_0-auc:0.775096\n",
            "[53]\tvalidation_0-auc:0.775308\n",
            "[54]\tvalidation_0-auc:0.777229\n",
            "[55]\tvalidation_0-auc:0.777675\n",
            "[56]\tvalidation_0-auc:0.777208\n",
            "[57]\tvalidation_0-auc:0.77638\n",
            "[58]\tvalidation_0-auc:0.776868\n",
            "[59]\tvalidation_0-auc:0.776529\n",
            "[60]\tvalidation_0-auc:0.775637\n",
            "[61]\tvalidation_0-auc:0.777059\n",
            "[62]\tvalidation_0-auc:0.77672\n",
            "[63]\tvalidation_0-auc:0.775913\n",
            "[64]\tvalidation_0-auc:0.779352\n",
            "[65]\tvalidation_0-auc:0.778822\n",
            "[66]\tvalidation_0-auc:0.778758\n",
            "[67]\tvalidation_0-auc:0.778482\n",
            "[68]\tvalidation_0-auc:0.778758\n",
            "[69]\tvalidation_0-auc:0.778163\n",
            "[70]\tvalidation_0-auc:0.777972\n",
            "[71]\tvalidation_0-auc:0.7781\n",
            "[72]\tvalidation_0-auc:0.779544\n",
            "[73]\tvalidation_0-auc:0.779363\n",
            "[74]\tvalidation_0-auc:0.779448\n",
            "[75]\tvalidation_0-auc:0.780913\n",
            "[76]\tvalidation_0-auc:0.781783\n",
            "[77]\tvalidation_0-auc:0.780594\n",
            "[78]\tvalidation_0-auc:0.780807\n",
            "[79]\tvalidation_0-auc:0.780807\n",
            "[80]\tvalidation_0-auc:0.780552\n",
            "[81]\tvalidation_0-auc:0.780478\n",
            "[82]\tvalidation_0-auc:0.779904\n",
            "[83]\tvalidation_0-auc:0.778609\n",
            "[84]\tvalidation_0-auc:0.778206\n",
            "[85]\tvalidation_0-auc:0.778015\n",
            "[86]\tvalidation_0-auc:0.778079\n",
            "[87]\tvalidation_0-auc:0.777739\n",
            "[88]\tvalidation_0-auc:0.776996\n",
            "[89]\tvalidation_0-auc:0.776614\n",
            "[90]\tvalidation_0-auc:0.776614\n",
            "[91]\tvalidation_0-auc:0.77604\n",
            "[92]\tvalidation_0-auc:0.775701\n",
            "[93]\tvalidation_0-auc:0.775637\n",
            "[94]\tvalidation_0-auc:0.773832\n",
            "[95]\tvalidation_0-auc:0.773535\n",
            "[96]\tvalidation_0-auc:0.773132\n",
            "[97]\tvalidation_0-auc:0.772728\n",
            "[98]\tvalidation_0-auc:0.773811\n",
            "[99]\tvalidation_0-auc:0.773365\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.05     | 0.6122448979591837 |        0.0         |        0.0         |        0.0         |\n",
            "|      GRU 0.05     | 0.7816326530612245 | 0.7243243243243244 | 0.7052631578947368 | 0.7146666666666667 |\n",
            "|    XGBoost 0.05   | 0.7489795918367347 | 0.6830601092896175 | 0.6578947368421053 | 0.6702412868632708 |\n",
            "|    Logreg 0.05    | 0.7489795918367347 | 0.7375886524822695 | 0.5473684210526316 | 0.6283987915407855 |\n",
            "|      SVM 0.05     | 0.763265306122449  | 0.7102272727272727 | 0.6578947368421053 | 0.6830601092896175 |\n",
            "|   LSTM beta 0.05  |  0.75054704595186  | 0.6524822695035462 | 0.5859872611464968 | 0.6174496644295302 |\n",
            "|   GRU beta 0.05   | 0.7527352297592997 | 0.6264367816091954 | 0.6942675159235668 | 0.6586102719033233 |\n",
            "| XGBoost beta 0.05 | 0.7286652078774617 | 0.5854922279792746 | 0.7197452229299363 | 0.6457142857142857 |\n",
            "|  logreg beta 0.05 | 0.7614879649890591 | 0.6739130434782609 | 0.5923566878980892 | 0.6305084745762711 |\n",
            "|   svm beta 0.05   | 0.7417943107221007 | 0.6031746031746031 | 0.7261146496815286 | 0.6589595375722542 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6689 - accuracy: 0.6168 - val_loss: 0.6683 - val_accuracy: 0.6102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6622 - accuracy: 0.6235 - val_loss: 0.6557 - val_accuracy: 0.6918\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6665 - accuracy: 0.6188 - val_loss: 0.6709 - val_accuracy: 0.6102\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6656 - accuracy: 0.6235 - val_loss: 0.6682 - val_accuracy: 0.6102\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6637 - accuracy: 0.6235 - val_loss: 0.6680 - val_accuracy: 0.6102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6676 - accuracy: 0.6215 - val_loss: 0.6627 - val_accuracy: 0.6102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6429 - accuracy: 0.6376 - val_loss: 0.6202 - val_accuracy: 0.6755\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5953 - accuracy: 0.6758 - val_loss: 0.6145 - val_accuracy: 0.6755\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5926 - accuracy: 0.6960 - val_loss: 0.6028 - val_accuracy: 0.6878\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5739 - accuracy: 0.6966 - val_loss: 0.6035 - val_accuracy: 0.7143\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.692693\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.69208\n",
            "[2]\tvalidation_0-auc:0.703742\n",
            "[3]\tvalidation_0-auc:0.702884\n",
            "[4]\tvalidation_0-auc:0.704723\n",
            "[5]\tvalidation_0-auc:0.700397\n",
            "[6]\tvalidation_0-auc:0.70038\n",
            "[7]\tvalidation_0-auc:0.701895\n",
            "[8]\tvalidation_0-auc:0.70045\n",
            "[9]\tvalidation_0-auc:0.700257\n",
            "[10]\tvalidation_0-auc:0.698778\n",
            "[11]\tvalidation_0-auc:0.697272\n",
            "[12]\tvalidation_0-auc:0.69806\n",
            "[13]\tvalidation_0-auc:0.698603\n",
            "[14]\tvalidation_0-auc:0.699163\n",
            "[15]\tvalidation_0-auc:0.699329\n",
            "[16]\tvalidation_0-auc:0.699312\n",
            "[17]\tvalidation_0-auc:0.699618\n",
            "[18]\tvalidation_0-auc:0.698498\n",
            "[19]\tvalidation_0-auc:0.69841\n",
            "[20]\tvalidation_0-auc:0.697929\n",
            "[21]\tvalidation_0-auc:0.69806\n",
            "[22]\tvalidation_0-auc:0.698165\n",
            "[23]\tvalidation_0-auc:0.70073\n",
            "[24]\tvalidation_0-auc:0.700739\n",
            "[25]\tvalidation_0-auc:0.699898\n",
            "[26]\tvalidation_0-auc:0.700546\n",
            "[27]\tvalidation_0-auc:0.700695\n",
            "[28]\tvalidation_0-auc:0.701185\n",
            "[29]\tvalidation_0-auc:0.701002\n",
            "[30]\tvalidation_0-auc:0.700573\n",
            "[31]\tvalidation_0-auc:0.700818\n",
            "[32]\tvalidation_0-auc:0.699968\n",
            "[33]\tvalidation_0-auc:0.700196\n",
            "[34]\tvalidation_0-auc:0.699399\n",
            "[35]\tvalidation_0-auc:0.699487\n",
            "[36]\tvalidation_0-auc:0.699373\n",
            "[37]\tvalidation_0-auc:0.699084\n",
            "[38]\tvalidation_0-auc:0.69827\n",
            "[39]\tvalidation_0-auc:0.697228\n",
            "[40]\tvalidation_0-auc:0.697228\n",
            "[41]\tvalidation_0-auc:0.697823\n",
            "[42]\tvalidation_0-auc:0.697648\n",
            "[43]\tvalidation_0-auc:0.699434\n",
            "[44]\tvalidation_0-auc:0.699469\n",
            "[45]\tvalidation_0-auc:0.698979\n",
            "[46]\tvalidation_0-auc:0.700074\n",
            "[47]\tvalidation_0-auc:0.699776\n",
            "[48]\tvalidation_0-auc:0.699758\n",
            "[49]\tvalidation_0-auc:0.699671\n",
            "[50]\tvalidation_0-auc:0.699513\n",
            "[51]\tvalidation_0-auc:0.699828\n",
            "[52]\tvalidation_0-auc:0.69869\n",
            "[53]\tvalidation_0-auc:0.699356\n",
            "[54]\tvalidation_0-auc:0.699023\n",
            "Stopping. Best iteration:\n",
            "[4]\tvalidation_0-auc:0.704723\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6671 - accuracy: 0.6225 - val_loss: 0.6793 - val_accuracy: 0.5821\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6337 - accuracy: 0.6534 - val_loss: 0.6648 - val_accuracy: 0.5799\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5882 - accuracy: 0.7111 - val_loss: 0.6534 - val_accuracy: 0.6280\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5655 - accuracy: 0.7275 - val_loss: 0.6944 - val_accuracy: 0.6214\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5872 - accuracy: 0.7021 - val_loss: 0.6721 - val_accuracy: 0.6149\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6333 - accuracy: 0.6555 - val_loss: 0.6476 - val_accuracy: 0.5952\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5408 - accuracy: 0.7536 - val_loss: 0.5971 - val_accuracy: 0.6761\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5190 - accuracy: 0.7660 - val_loss: 0.5970 - val_accuracy: 0.6565\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5127 - accuracy: 0.7653 - val_loss: 0.6079 - val_accuracy: 0.6849\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4873 - accuracy: 0.7852 - val_loss: 0.6543 - val_accuracy: 0.6827\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.701571\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.709404\n",
            "[2]\tvalidation_0-auc:0.714453\n",
            "[3]\tvalidation_0-auc:0.714965\n",
            "[4]\tvalidation_0-auc:0.715634\n",
            "[5]\tvalidation_0-auc:0.711786\n",
            "[6]\tvalidation_0-auc:0.714551\n",
            "[7]\tvalidation_0-auc:0.713705\n",
            "[8]\tvalidation_0-auc:0.714965\n",
            "[9]\tvalidation_0-auc:0.714866\n",
            "[10]\tvalidation_0-auc:0.714335\n",
            "[11]\tvalidation_0-auc:0.71216\n",
            "[12]\tvalidation_0-auc:0.711648\n",
            "[13]\tvalidation_0-auc:0.71281\n",
            "[14]\tvalidation_0-auc:0.715969\n",
            "[15]\tvalidation_0-auc:0.716254\n",
            "[16]\tvalidation_0-auc:0.718144\n",
            "[17]\tvalidation_0-auc:0.717799\n",
            "[18]\tvalidation_0-auc:0.71653\n",
            "[19]\tvalidation_0-auc:0.718891\n",
            "[20]\tvalidation_0-auc:0.717425\n",
            "[21]\tvalidation_0-auc:0.718626\n",
            "[22]\tvalidation_0-auc:0.716657\n",
            "[23]\tvalidation_0-auc:0.715329\n",
            "[24]\tvalidation_0-auc:0.71715\n",
            "[25]\tvalidation_0-auc:0.715516\n",
            "[26]\tvalidation_0-auc:0.714788\n",
            "[27]\tvalidation_0-auc:0.71401\n",
            "[28]\tvalidation_0-auc:0.712819\n",
            "[29]\tvalidation_0-auc:0.713922\n",
            "[30]\tvalidation_0-auc:0.709365\n",
            "[31]\tvalidation_0-auc:0.709493\n",
            "[32]\tvalidation_0-auc:0.71031\n",
            "[33]\tvalidation_0-auc:0.710418\n",
            "[34]\tvalidation_0-auc:0.710674\n",
            "[35]\tvalidation_0-auc:0.70965\n",
            "[36]\tvalidation_0-auc:0.709296\n",
            "[37]\tvalidation_0-auc:0.71092\n",
            "[38]\tvalidation_0-auc:0.71031\n",
            "[39]\tvalidation_0-auc:0.710211\n",
            "[40]\tvalidation_0-auc:0.71213\n",
            "[41]\tvalidation_0-auc:0.710566\n",
            "[42]\tvalidation_0-auc:0.710448\n",
            "[43]\tvalidation_0-auc:0.710369\n",
            "[44]\tvalidation_0-auc:0.711756\n",
            "[45]\tvalidation_0-auc:0.711619\n",
            "[46]\tvalidation_0-auc:0.71152\n",
            "[47]\tvalidation_0-auc:0.711107\n",
            "[48]\tvalidation_0-auc:0.712111\n",
            "[49]\tvalidation_0-auc:0.712288\n",
            "[50]\tvalidation_0-auc:0.711855\n",
            "[51]\tvalidation_0-auc:0.712406\n",
            "[52]\tvalidation_0-auc:0.711934\n",
            "[53]\tvalidation_0-auc:0.713056\n",
            "[54]\tvalidation_0-auc:0.713065\n",
            "[55]\tvalidation_0-auc:0.712947\n",
            "[56]\tvalidation_0-auc:0.713046\n",
            "[57]\tvalidation_0-auc:0.713095\n",
            "[58]\tvalidation_0-auc:0.713597\n",
            "[59]\tvalidation_0-auc:0.713479\n",
            "[60]\tvalidation_0-auc:0.713892\n",
            "[61]\tvalidation_0-auc:0.714522\n",
            "[62]\tvalidation_0-auc:0.714719\n",
            "[63]\tvalidation_0-auc:0.714187\n",
            "[64]\tvalidation_0-auc:0.714404\n",
            "[65]\tvalidation_0-auc:0.713459\n",
            "[66]\tvalidation_0-auc:0.714246\n",
            "[67]\tvalidation_0-auc:0.713006\n",
            "[68]\tvalidation_0-auc:0.713557\n",
            "[69]\tvalidation_0-auc:0.71277\n",
            "Stopping. Best iteration:\n",
            "[19]\tvalidation_0-auc:0.718891\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.610204081632653  |        0.0         |         0.0         |        0.0         |\n",
            "|     GRU 0.1      | 0.7142857142857143 | 0.6783216783216783 |  0.5078534031413613 | 0.5808383233532934 |\n",
            "|   XGBoost 0.1    | 0.6918367346938775 | 0.6219512195121951 |  0.5340314136125655 | 0.5746478873239438 |\n",
            "|    Logreg 0.1    | 0.7020408163265306 | 0.7227722772277227 | 0.38219895287958117 | 0.5000000000000001 |\n",
            "|     SVM 0.1      | 0.6877551020408164 | 0.6376811594202898 |  0.4607329842931937 | 0.5349544072948328 |\n",
            "|  LSTM beta 0.1   | 0.6148796498905909 | 0.5449101796407185 | 0.47643979057591623 | 0.5083798882681564 |\n",
            "|   GRU beta 0.1   | 0.6827133479212254 | 0.6885245901639344 |  0.4397905759162304 | 0.536741214057508  |\n",
            "| XGBoost beta 0.1 | 0.6673960612691466 | 0.6181818181818182 |  0.5340314136125655 | 0.5730337078651685 |\n",
            "| logreg beta 0.1  | 0.6717724288840262 | 0.6357615894039735 |  0.5026178010471204 | 0.5614035087719298 |\n",
            "|   svm beta 0.1   | 0.649890590809628  | 0.5987261146496815 | 0.49214659685863876 | 0.5402298850574713 |\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE1IawbDaAC7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "a4fa95b8-aff2-4121-b5e3-ef45d81b8295"
      },
      "source": [
        "Result_cross.to_csv('WMT_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.612245</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.724324</td>\n",
              "      <td>0.781633</td>\n",
              "      <td>0.714667</td>\n",
              "      <td>0.705263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.683060</td>\n",
              "      <td>0.748980</td>\n",
              "      <td>0.670241</td>\n",
              "      <td>0.657895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.737589</td>\n",
              "      <td>0.748980</td>\n",
              "      <td>0.628399</td>\n",
              "      <td>0.547368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.710227</td>\n",
              "      <td>0.763265</td>\n",
              "      <td>0.683060</td>\n",
              "      <td>0.657895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.652482</td>\n",
              "      <td>0.750547</td>\n",
              "      <td>0.617450</td>\n",
              "      <td>0.585987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.626437</td>\n",
              "      <td>0.752735</td>\n",
              "      <td>0.658610</td>\n",
              "      <td>0.694268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.585492</td>\n",
              "      <td>0.728665</td>\n",
              "      <td>0.645714</td>\n",
              "      <td>0.719745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.673913</td>\n",
              "      <td>0.761488</td>\n",
              "      <td>0.630508</td>\n",
              "      <td>0.592357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.603175</td>\n",
              "      <td>0.741794</td>\n",
              "      <td>0.658960</td>\n",
              "      <td>0.726115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.610204</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.678322</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.580838</td>\n",
              "      <td>0.507853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.621951</td>\n",
              "      <td>0.691837</td>\n",
              "      <td>0.574648</td>\n",
              "      <td>0.534031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.722772</td>\n",
              "      <td>0.702041</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.382199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.637681</td>\n",
              "      <td>0.687755</td>\n",
              "      <td>0.534954</td>\n",
              "      <td>0.460733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.544910</td>\n",
              "      <td>0.614880</td>\n",
              "      <td>0.508380</td>\n",
              "      <td>0.476440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.688525</td>\n",
              "      <td>0.682713</td>\n",
              "      <td>0.536741</td>\n",
              "      <td>0.439791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.618182</td>\n",
              "      <td>0.667396</td>\n",
              "      <td>0.573034</td>\n",
              "      <td>0.534031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.635762</td>\n",
              "      <td>0.671772</td>\n",
              "      <td>0.561404</td>\n",
              "      <td>0.502618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.598726</td>\n",
              "      <td>0.649891</td>\n",
              "      <td>0.540230</td>\n",
              "      <td>0.492147</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0          LSTM 0.05  WMT  0.000000  0.612245  0.000000  0.000000\n",
              "1           GRU 0.05  WMT  0.724324  0.781633  0.714667  0.705263\n",
              "2       XGBoost 0.05  WMT  0.683060  0.748980  0.670241  0.657895\n",
              "3        Logreg 0.05  WMT  0.737589  0.748980  0.628399  0.547368\n",
              "4           SVM 0.05  WMT  0.710227  0.763265  0.683060  0.657895\n",
              "5     LSTM beta 0.05  WMT  0.652482  0.750547  0.617450  0.585987\n",
              "6      GRU beta 0.05  WMT  0.626437  0.752735  0.658610  0.694268\n",
              "7  XGBoost beta 0.05  WMT  0.585492  0.728665  0.645714  0.719745\n",
              "8   logreg beta 0.05  WMT  0.673913  0.761488  0.630508  0.592357\n",
              "9      svm beta 0.05  WMT  0.603175  0.741794  0.658960  0.726115\n",
              "0           LSTM 0.1  WMT  0.000000  0.610204  0.000000  0.000000\n",
              "1            GRU 0.1  WMT  0.678322  0.714286  0.580838  0.507853\n",
              "2        XGBoost 0.1  WMT  0.621951  0.691837  0.574648  0.534031\n",
              "3         Logreg 0.1  WMT  0.722772  0.702041  0.500000  0.382199\n",
              "4            SVM 0.1  WMT  0.637681  0.687755  0.534954  0.460733\n",
              "5      LSTM beta 0.1  WMT  0.544910  0.614880  0.508380  0.476440\n",
              "6       GRU beta 0.1  WMT  0.688525  0.682713  0.536741  0.439791\n",
              "7   XGBoost beta 0.1  WMT  0.618182  0.667396  0.573034  0.534031\n",
              "8    logreg beta 0.1  WMT  0.635762  0.671772  0.561404  0.502618\n",
              "9       svm beta 0.1  WMT  0.598726  0.649891  0.540230  0.492147"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQXUsFNmaAC7"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9Ztn64XaAC7"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjUwAKiUaAC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce0306b8-7156-4b10-aede-ff03a723c55d"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"WMT\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.05\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6844 - accuracy: 0.5758 - val_loss: 0.6729 - val_accuracy: 0.6122\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6824 - accuracy: 0.5805 - val_loss: 0.6668 - val_accuracy: 0.6122\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6778 - accuracy: 0.5785 - val_loss: 0.6389 - val_accuracy: 0.6388\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6411 - accuracy: 0.6356 - val_loss: 0.5801 - val_accuracy: 0.7143\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6557 - accuracy: 0.6383 - val_loss: 0.5826 - val_accuracy: 0.7122\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6825 - accuracy: 0.5725 - val_loss: 0.6552 - val_accuracy: 0.6122\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6351 - accuracy: 0.6463 - val_loss: 0.5589 - val_accuracy: 0.7592\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5898 - accuracy: 0.6980 - val_loss: 0.5163 - val_accuracy: 0.7531\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5675 - accuracy: 0.7121 - val_loss: 0.5506 - val_accuracy: 0.7204\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5772 - accuracy: 0.7154 - val_loss: 0.5109 - val_accuracy: 0.7755\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.780088\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.799561\n",
            "[2]\tvalidation_0-auc:0.799588\n",
            "[3]\tvalidation_0-auc:0.810228\n",
            "[4]\tvalidation_0-auc:0.810009\n",
            "[5]\tvalidation_0-auc:0.809395\n",
            "[6]\tvalidation_0-auc:0.811246\n",
            "[7]\tvalidation_0-auc:0.811675\n",
            "[8]\tvalidation_0-auc:0.810272\n",
            "[9]\tvalidation_0-auc:0.812763\n",
            "[10]\tvalidation_0-auc:0.812614\n",
            "[11]\tvalidation_0-auc:0.812632\n",
            "[12]\tvalidation_0-auc:0.813561\n",
            "[13]\tvalidation_0-auc:0.814298\n",
            "[14]\tvalidation_0-auc:0.813018\n",
            "[15]\tvalidation_0-auc:0.812675\n",
            "[16]\tvalidation_0-auc:0.813535\n",
            "[17]\tvalidation_0-auc:0.812342\n",
            "[18]\tvalidation_0-auc:0.812553\n",
            "[19]\tvalidation_0-auc:0.813079\n",
            "[20]\tvalidation_0-auc:0.812465\n",
            "[21]\tvalidation_0-auc:0.813272\n",
            "[22]\tvalidation_0-auc:0.812746\n",
            "[23]\tvalidation_0-auc:0.81243\n",
            "[24]\tvalidation_0-auc:0.811044\n",
            "[25]\tvalidation_0-auc:0.810947\n",
            "[26]\tvalidation_0-auc:0.811193\n",
            "[27]\tvalidation_0-auc:0.812035\n",
            "[28]\tvalidation_0-auc:0.811386\n",
            "[29]\tvalidation_0-auc:0.811246\n",
            "[30]\tvalidation_0-auc:0.810763\n",
            "[31]\tvalidation_0-auc:0.811009\n",
            "[32]\tvalidation_0-auc:0.810281\n",
            "[33]\tvalidation_0-auc:0.809956\n",
            "[34]\tvalidation_0-auc:0.810281\n",
            "[35]\tvalidation_0-auc:0.810298\n",
            "[36]\tvalidation_0-auc:0.810298\n",
            "[37]\tvalidation_0-auc:0.809912\n",
            "[38]\tvalidation_0-auc:0.809982\n",
            "[39]\tvalidation_0-auc:0.810307\n",
            "[40]\tvalidation_0-auc:0.8105\n",
            "[41]\tvalidation_0-auc:0.810167\n",
            "[42]\tvalidation_0-auc:0.810061\n",
            "[43]\tvalidation_0-auc:0.810061\n",
            "[44]\tvalidation_0-auc:0.809149\n",
            "[45]\tvalidation_0-auc:0.808833\n",
            "[46]\tvalidation_0-auc:0.809193\n",
            "[47]\tvalidation_0-auc:0.808719\n",
            "[48]\tvalidation_0-auc:0.808842\n",
            "[49]\tvalidation_0-auc:0.809193\n",
            "[50]\tvalidation_0-auc:0.809184\n",
            "[51]\tvalidation_0-auc:0.808886\n",
            "[52]\tvalidation_0-auc:0.80857\n",
            "[53]\tvalidation_0-auc:0.80857\n",
            "[54]\tvalidation_0-auc:0.808518\n",
            "[55]\tvalidation_0-auc:0.807956\n",
            "[56]\tvalidation_0-auc:0.8075\n",
            "[57]\tvalidation_0-auc:0.806877\n",
            "[58]\tvalidation_0-auc:0.807193\n",
            "[59]\tvalidation_0-auc:0.807158\n",
            "[60]\tvalidation_0-auc:0.806807\n",
            "[61]\tvalidation_0-auc:0.806877\n",
            "[62]\tvalidation_0-auc:0.806965\n",
            "[63]\tvalidation_0-auc:0.806719\n",
            "Stopping. Best iteration:\n",
            "[13]\tvalidation_0-auc:0.814298\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6845 - accuracy: 0.5683 - val_loss: 0.6581 - val_accuracy: 0.6565\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6862 - accuracy: 0.5793 - val_loss: 0.6622 - val_accuracy: 0.6565\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6825 - accuracy: 0.5800 - val_loss: 0.6553 - val_accuracy: 0.6565\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6780 - accuracy: 0.5800 - val_loss: 0.6508 - val_accuracy: 0.6565\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6674 - accuracy: 0.5875 - val_loss: 0.6534 - val_accuracy: 0.6455\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6735 - accuracy: 0.5806 - val_loss: 0.6139 - val_accuracy: 0.6565\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6237 - accuracy: 0.6596 - val_loss: 0.5942 - val_accuracy: 0.7177\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6059 - accuracy: 0.6925 - val_loss: 0.5264 - val_accuracy: 0.7309\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5757 - accuracy: 0.6994 - val_loss: 0.5607 - val_accuracy: 0.7090\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5752 - accuracy: 0.7193 - val_loss: 0.6041 - val_accuracy: 0.6937\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.721826\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.750318\n",
            "[2]\tvalidation_0-auc:0.736369\n",
            "[3]\tvalidation_0-auc:0.73448\n",
            "[4]\tvalidation_0-auc:0.742144\n",
            "[5]\tvalidation_0-auc:0.749618\n",
            "[6]\tvalidation_0-auc:0.746635\n",
            "[7]\tvalidation_0-auc:0.759841\n",
            "[8]\tvalidation_0-auc:0.753471\n",
            "[9]\tvalidation_0-auc:0.760457\n",
            "[10]\tvalidation_0-auc:0.763503\n",
            "[11]\tvalidation_0-auc:0.767176\n",
            "[12]\tvalidation_0-auc:0.769671\n",
            "[13]\tvalidation_0-auc:0.765021\n",
            "[14]\tvalidation_0-auc:0.768079\n",
            "[15]\tvalidation_0-auc:0.768853\n",
            "[16]\tvalidation_0-auc:0.767803\n",
            "[17]\tvalidation_0-auc:0.769183\n",
            "[18]\tvalidation_0-auc:0.769926\n",
            "[19]\tvalidation_0-auc:0.769936\n",
            "[20]\tvalidation_0-auc:0.769735\n",
            "[21]\tvalidation_0-auc:0.772887\n",
            "[22]\tvalidation_0-auc:0.773301\n",
            "[23]\tvalidation_0-auc:0.773493\n",
            "[24]\tvalidation_0-auc:0.772134\n",
            "[25]\tvalidation_0-auc:0.77241\n",
            "[26]\tvalidation_0-auc:0.772155\n",
            "[27]\tvalidation_0-auc:0.771433\n",
            "[28]\tvalidation_0-auc:0.772495\n",
            "[29]\tvalidation_0-auc:0.774915\n",
            "[30]\tvalidation_0-auc:0.774745\n",
            "[31]\tvalidation_0-auc:0.776221\n",
            "[32]\tvalidation_0-auc:0.77603\n",
            "[33]\tvalidation_0-auc:0.776019\n",
            "[34]\tvalidation_0-auc:0.775786\n",
            "[35]\tvalidation_0-auc:0.775764\n",
            "[36]\tvalidation_0-auc:0.776306\n",
            "[37]\tvalidation_0-auc:0.775584\n",
            "[38]\tvalidation_0-auc:0.774586\n",
            "[39]\tvalidation_0-auc:0.77448\n",
            "[40]\tvalidation_0-auc:0.773928\n",
            "[41]\tvalidation_0-auc:0.774289\n",
            "[42]\tvalidation_0-auc:0.773949\n",
            "[43]\tvalidation_0-auc:0.775828\n",
            "[44]\tvalidation_0-auc:0.775446\n",
            "[45]\tvalidation_0-auc:0.776369\n",
            "[46]\tvalidation_0-auc:0.776285\n",
            "[47]\tvalidation_0-auc:0.77603\n",
            "[48]\tvalidation_0-auc:0.775679\n",
            "[49]\tvalidation_0-auc:0.776019\n",
            "[50]\tvalidation_0-auc:0.774342\n",
            "[51]\tvalidation_0-auc:0.775287\n",
            "[52]\tvalidation_0-auc:0.775096\n",
            "[53]\tvalidation_0-auc:0.775308\n",
            "[54]\tvalidation_0-auc:0.777229\n",
            "[55]\tvalidation_0-auc:0.777675\n",
            "[56]\tvalidation_0-auc:0.777208\n",
            "[57]\tvalidation_0-auc:0.77638\n",
            "[58]\tvalidation_0-auc:0.776868\n",
            "[59]\tvalidation_0-auc:0.776529\n",
            "[60]\tvalidation_0-auc:0.775637\n",
            "[61]\tvalidation_0-auc:0.777059\n",
            "[62]\tvalidation_0-auc:0.77672\n",
            "[63]\tvalidation_0-auc:0.775913\n",
            "[64]\tvalidation_0-auc:0.779352\n",
            "[65]\tvalidation_0-auc:0.778822\n",
            "[66]\tvalidation_0-auc:0.778758\n",
            "[67]\tvalidation_0-auc:0.778482\n",
            "[68]\tvalidation_0-auc:0.778758\n",
            "[69]\tvalidation_0-auc:0.778163\n",
            "[70]\tvalidation_0-auc:0.777972\n",
            "[71]\tvalidation_0-auc:0.7781\n",
            "[72]\tvalidation_0-auc:0.779544\n",
            "[73]\tvalidation_0-auc:0.779363\n",
            "[74]\tvalidation_0-auc:0.779448\n",
            "[75]\tvalidation_0-auc:0.780913\n",
            "[76]\tvalidation_0-auc:0.781783\n",
            "[77]\tvalidation_0-auc:0.780594\n",
            "[78]\tvalidation_0-auc:0.780807\n",
            "[79]\tvalidation_0-auc:0.780807\n",
            "[80]\tvalidation_0-auc:0.780552\n",
            "[81]\tvalidation_0-auc:0.780478\n",
            "[82]\tvalidation_0-auc:0.779904\n",
            "[83]\tvalidation_0-auc:0.778609\n",
            "[84]\tvalidation_0-auc:0.778206\n",
            "[85]\tvalidation_0-auc:0.778015\n",
            "[86]\tvalidation_0-auc:0.778079\n",
            "[87]\tvalidation_0-auc:0.777739\n",
            "[88]\tvalidation_0-auc:0.776996\n",
            "[89]\tvalidation_0-auc:0.776614\n",
            "[90]\tvalidation_0-auc:0.776614\n",
            "[91]\tvalidation_0-auc:0.77604\n",
            "[92]\tvalidation_0-auc:0.775701\n",
            "[93]\tvalidation_0-auc:0.775637\n",
            "[94]\tvalidation_0-auc:0.773832\n",
            "[95]\tvalidation_0-auc:0.773535\n",
            "[96]\tvalidation_0-auc:0.773132\n",
            "[97]\tvalidation_0-auc:0.772728\n",
            "[98]\tvalidation_0-auc:0.773811\n",
            "[99]\tvalidation_0-auc:0.773365\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+---------------------+--------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |        Recall       |      F1 score      |\n",
            "+-------------------+--------------------+---------------------+---------------------+--------------------+\n",
            "|     LSTM 0.05     | 0.7122448979591837 |  0.6033755274261603 |  0.7526315789473684 | 0.6697892271662763 |\n",
            "|      GRU 0.05     | 0.7755102040816326 |  0.7061855670103093 |  0.7210526315789474 | 0.7135416666666666 |\n",
            "|    XGBoost 0.05   | 0.7489795918367347 |  0.6830601092896175 |  0.6578947368421053 | 0.6702412868632708 |\n",
            "|    Logreg 0.05    | 0.7489795918367347 |  0.7375886524822695 |  0.5473684210526316 | 0.6283987915407855 |\n",
            "|      SVM 0.05     | 0.763265306122449  |  0.7102272727272727 |  0.6578947368421053 | 0.6830601092896175 |\n",
            "|   LSTM beta 0.05  | 0.6455142231947484 | 0.48427672955974843 | 0.49044585987261147 | 0.4873417721518987 |\n",
            "|   GRU beta 0.05   | 0.6936542669584245 |  0.5320754716981132 |  0.8980891719745223 | 0.6682464454976302 |\n",
            "| XGBoost beta 0.05 | 0.7286652078774617 |  0.5854922279792746 |  0.7197452229299363 | 0.6457142857142857 |\n",
            "|  logreg beta 0.05 | 0.7614879649890591 |  0.6739130434782609 |  0.5923566878980892 | 0.6305084745762711 |\n",
            "|   svm beta 0.05   | 0.7417943107221007 |  0.6031746031746031 |  0.7261146496815286 | 0.6589595375722542 |\n",
            "+-------------------+--------------------+---------------------+---------------------+--------------------+\n",
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6686 - accuracy: 0.6242 - val_loss: 0.6706 - val_accuracy: 0.6102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6643 - accuracy: 0.6235 - val_loss: 0.6664 - val_accuracy: 0.6102\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6548 - accuracy: 0.6315 - val_loss: 0.6318 - val_accuracy: 0.6306\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6246 - accuracy: 0.6517 - val_loss: 0.6217 - val_accuracy: 0.6571\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6065 - accuracy: 0.6772 - val_loss: 0.6206 - val_accuracy: 0.7061\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6639 - accuracy: 0.6188 - val_loss: 0.6602 - val_accuracy: 0.6102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6338 - accuracy: 0.6537 - val_loss: 0.6135 - val_accuracy: 0.6735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6044 - accuracy: 0.6906 - val_loss: 0.6405 - val_accuracy: 0.6429\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5948 - accuracy: 0.6906 - val_loss: 0.6051 - val_accuracy: 0.7000\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5668 - accuracy: 0.7221 - val_loss: 0.6099 - val_accuracy: 0.6837\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.692693\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.69208\n",
            "[2]\tvalidation_0-auc:0.703742\n",
            "[3]\tvalidation_0-auc:0.702884\n",
            "[4]\tvalidation_0-auc:0.704723\n",
            "[5]\tvalidation_0-auc:0.700397\n",
            "[6]\tvalidation_0-auc:0.70038\n",
            "[7]\tvalidation_0-auc:0.701895\n",
            "[8]\tvalidation_0-auc:0.70045\n",
            "[9]\tvalidation_0-auc:0.700257\n",
            "[10]\tvalidation_0-auc:0.698778\n",
            "[11]\tvalidation_0-auc:0.697272\n",
            "[12]\tvalidation_0-auc:0.69806\n",
            "[13]\tvalidation_0-auc:0.698603\n",
            "[14]\tvalidation_0-auc:0.699163\n",
            "[15]\tvalidation_0-auc:0.699329\n",
            "[16]\tvalidation_0-auc:0.699312\n",
            "[17]\tvalidation_0-auc:0.699618\n",
            "[18]\tvalidation_0-auc:0.698498\n",
            "[19]\tvalidation_0-auc:0.69841\n",
            "[20]\tvalidation_0-auc:0.697929\n",
            "[21]\tvalidation_0-auc:0.69806\n",
            "[22]\tvalidation_0-auc:0.698165\n",
            "[23]\tvalidation_0-auc:0.70073\n",
            "[24]\tvalidation_0-auc:0.700739\n",
            "[25]\tvalidation_0-auc:0.699898\n",
            "[26]\tvalidation_0-auc:0.700546\n",
            "[27]\tvalidation_0-auc:0.700695\n",
            "[28]\tvalidation_0-auc:0.701185\n",
            "[29]\tvalidation_0-auc:0.701002\n",
            "[30]\tvalidation_0-auc:0.700573\n",
            "[31]\tvalidation_0-auc:0.700818\n",
            "[32]\tvalidation_0-auc:0.699968\n",
            "[33]\tvalidation_0-auc:0.700196\n",
            "[34]\tvalidation_0-auc:0.699399\n",
            "[35]\tvalidation_0-auc:0.699487\n",
            "[36]\tvalidation_0-auc:0.699373\n",
            "[37]\tvalidation_0-auc:0.699084\n",
            "[38]\tvalidation_0-auc:0.69827\n",
            "[39]\tvalidation_0-auc:0.697228\n",
            "[40]\tvalidation_0-auc:0.697228\n",
            "[41]\tvalidation_0-auc:0.697823\n",
            "[42]\tvalidation_0-auc:0.697648\n",
            "[43]\tvalidation_0-auc:0.699434\n",
            "[44]\tvalidation_0-auc:0.699469\n",
            "[45]\tvalidation_0-auc:0.698979\n",
            "[46]\tvalidation_0-auc:0.700074\n",
            "[47]\tvalidation_0-auc:0.699776\n",
            "[48]\tvalidation_0-auc:0.699758\n",
            "[49]\tvalidation_0-auc:0.699671\n",
            "[50]\tvalidation_0-auc:0.699513\n",
            "[51]\tvalidation_0-auc:0.699828\n",
            "[52]\tvalidation_0-auc:0.69869\n",
            "[53]\tvalidation_0-auc:0.699356\n",
            "[54]\tvalidation_0-auc:0.699023\n",
            "Stopping. Best iteration:\n",
            "[4]\tvalidation_0-auc:0.704723\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6637 - accuracy: 0.6205 - val_loss: 0.6849 - val_accuracy: 0.6039\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6407 - accuracy: 0.6342 - val_loss: 0.6680 - val_accuracy: 0.5821\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5969 - accuracy: 0.7001 - val_loss: 0.6694 - val_accuracy: 0.5864\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5831 - accuracy: 0.7261 - val_loss: 0.6495 - val_accuracy: 0.6193\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5523 - accuracy: 0.7289 - val_loss: 0.7050 - val_accuracy: 0.6171\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6407 - accuracy: 0.6294 - val_loss: 0.6456 - val_accuracy: 0.6302\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5476 - accuracy: 0.7570 - val_loss: 0.6291 - val_accuracy: 0.6718\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5263 - accuracy: 0.7714 - val_loss: 0.5969 - val_accuracy: 0.6718\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5248 - accuracy: 0.7653 - val_loss: 0.6314 - val_accuracy: 0.6586\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4959 - accuracy: 0.7797 - val_loss: 0.6115 - val_accuracy: 0.6608\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.701571\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.709404\n",
            "[2]\tvalidation_0-auc:0.714453\n",
            "[3]\tvalidation_0-auc:0.714965\n",
            "[4]\tvalidation_0-auc:0.715634\n",
            "[5]\tvalidation_0-auc:0.711786\n",
            "[6]\tvalidation_0-auc:0.714551\n",
            "[7]\tvalidation_0-auc:0.713705\n",
            "[8]\tvalidation_0-auc:0.714965\n",
            "[9]\tvalidation_0-auc:0.714866\n",
            "[10]\tvalidation_0-auc:0.714335\n",
            "[11]\tvalidation_0-auc:0.71216\n",
            "[12]\tvalidation_0-auc:0.711648\n",
            "[13]\tvalidation_0-auc:0.71281\n",
            "[14]\tvalidation_0-auc:0.715969\n",
            "[15]\tvalidation_0-auc:0.716254\n",
            "[16]\tvalidation_0-auc:0.718144\n",
            "[17]\tvalidation_0-auc:0.717799\n",
            "[18]\tvalidation_0-auc:0.71653\n",
            "[19]\tvalidation_0-auc:0.718891\n",
            "[20]\tvalidation_0-auc:0.717425\n",
            "[21]\tvalidation_0-auc:0.718626\n",
            "[22]\tvalidation_0-auc:0.716657\n",
            "[23]\tvalidation_0-auc:0.715329\n",
            "[24]\tvalidation_0-auc:0.71715\n",
            "[25]\tvalidation_0-auc:0.715516\n",
            "[26]\tvalidation_0-auc:0.714788\n",
            "[27]\tvalidation_0-auc:0.71401\n",
            "[28]\tvalidation_0-auc:0.712819\n",
            "[29]\tvalidation_0-auc:0.713922\n",
            "[30]\tvalidation_0-auc:0.709365\n",
            "[31]\tvalidation_0-auc:0.709493\n",
            "[32]\tvalidation_0-auc:0.71031\n",
            "[33]\tvalidation_0-auc:0.710418\n",
            "[34]\tvalidation_0-auc:0.710674\n",
            "[35]\tvalidation_0-auc:0.70965\n",
            "[36]\tvalidation_0-auc:0.709296\n",
            "[37]\tvalidation_0-auc:0.71092\n",
            "[38]\tvalidation_0-auc:0.71031\n",
            "[39]\tvalidation_0-auc:0.710211\n",
            "[40]\tvalidation_0-auc:0.71213\n",
            "[41]\tvalidation_0-auc:0.710566\n",
            "[42]\tvalidation_0-auc:0.710448\n",
            "[43]\tvalidation_0-auc:0.710369\n",
            "[44]\tvalidation_0-auc:0.711756\n",
            "[45]\tvalidation_0-auc:0.711619\n",
            "[46]\tvalidation_0-auc:0.71152\n",
            "[47]\tvalidation_0-auc:0.711107\n",
            "[48]\tvalidation_0-auc:0.712111\n",
            "[49]\tvalidation_0-auc:0.712288\n",
            "[50]\tvalidation_0-auc:0.711855\n",
            "[51]\tvalidation_0-auc:0.712406\n",
            "[52]\tvalidation_0-auc:0.711934\n",
            "[53]\tvalidation_0-auc:0.713056\n",
            "[54]\tvalidation_0-auc:0.713065\n",
            "[55]\tvalidation_0-auc:0.712947\n",
            "[56]\tvalidation_0-auc:0.713046\n",
            "[57]\tvalidation_0-auc:0.713095\n",
            "[58]\tvalidation_0-auc:0.713597\n",
            "[59]\tvalidation_0-auc:0.713479\n",
            "[60]\tvalidation_0-auc:0.713892\n",
            "[61]\tvalidation_0-auc:0.714522\n",
            "[62]\tvalidation_0-auc:0.714719\n",
            "[63]\tvalidation_0-auc:0.714187\n",
            "[64]\tvalidation_0-auc:0.714404\n",
            "[65]\tvalidation_0-auc:0.713459\n",
            "[66]\tvalidation_0-auc:0.714246\n",
            "[67]\tvalidation_0-auc:0.713006\n",
            "[68]\tvalidation_0-auc:0.713557\n",
            "[69]\tvalidation_0-auc:0.71277\n",
            "Stopping. Best iteration:\n",
            "[19]\tvalidation_0-auc:0.718891\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.7061224489795919 | 0.6459627329192547 |  0.5445026178010471 |  0.5909090909090909 |\n",
            "|     GRU 0.1      | 0.6836734693877551 | 0.6071428571428571 |  0.5340314136125655 |  0.5682451253481894 |\n",
            "|   XGBoost 0.1    | 0.6918367346938775 | 0.6219512195121951 |  0.5340314136125655 |  0.5746478873239438 |\n",
            "|    Logreg 0.1    | 0.7020408163265306 | 0.7227722772277227 | 0.38219895287958117 |  0.5000000000000001 |\n",
            "|     SVM 0.1      | 0.6877551020408164 | 0.6376811594202898 |  0.4607329842931937 |  0.5349544072948328 |\n",
            "|  LSTM beta 0.1   | 0.6170678336980306 | 0.5579710144927537 |  0.4031413612565445 | 0.46808510638297873 |\n",
            "|   GRU beta 0.1   | 0.6608315098468271 | 0.608433734939759  |  0.5287958115183246 |  0.5658263305322129 |\n",
            "| XGBoost beta 0.1 | 0.6673960612691466 | 0.6181818181818182 |  0.5340314136125655 |  0.5730337078651685 |\n",
            "| logreg beta 0.1  | 0.6717724288840262 | 0.6357615894039735 |  0.5026178010471204 |  0.5614035087719298 |\n",
            "|   svm beta 0.1   | 0.649890590809628  | 0.5987261146496815 | 0.49214659685863876 |  0.5402298850574713 |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-St_8xjOaAC8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "c764a66b-30ba-4c8a-a425-de910c88d2f2"
      },
      "source": [
        "Result_purging.to_csv('WMT_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.603376</td>\n",
              "      <td>0.712245</td>\n",
              "      <td>0.669789</td>\n",
              "      <td>0.752632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.706186</td>\n",
              "      <td>0.775510</td>\n",
              "      <td>0.713542</td>\n",
              "      <td>0.721053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.683060</td>\n",
              "      <td>0.748980</td>\n",
              "      <td>0.670241</td>\n",
              "      <td>0.657895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.737589</td>\n",
              "      <td>0.748980</td>\n",
              "      <td>0.628399</td>\n",
              "      <td>0.547368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.710227</td>\n",
              "      <td>0.763265</td>\n",
              "      <td>0.683060</td>\n",
              "      <td>0.657895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.484277</td>\n",
              "      <td>0.645514</td>\n",
              "      <td>0.487342</td>\n",
              "      <td>0.490446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.532075</td>\n",
              "      <td>0.693654</td>\n",
              "      <td>0.668246</td>\n",
              "      <td>0.898089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.585492</td>\n",
              "      <td>0.728665</td>\n",
              "      <td>0.645714</td>\n",
              "      <td>0.719745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.673913</td>\n",
              "      <td>0.761488</td>\n",
              "      <td>0.630508</td>\n",
              "      <td>0.592357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.603175</td>\n",
              "      <td>0.741794</td>\n",
              "      <td>0.658960</td>\n",
              "      <td>0.726115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.645963</td>\n",
              "      <td>0.706122</td>\n",
              "      <td>0.590909</td>\n",
              "      <td>0.544503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.607143</td>\n",
              "      <td>0.683673</td>\n",
              "      <td>0.568245</td>\n",
              "      <td>0.534031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.621951</td>\n",
              "      <td>0.691837</td>\n",
              "      <td>0.574648</td>\n",
              "      <td>0.534031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.722772</td>\n",
              "      <td>0.702041</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.382199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.637681</td>\n",
              "      <td>0.687755</td>\n",
              "      <td>0.534954</td>\n",
              "      <td>0.460733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.557971</td>\n",
              "      <td>0.617068</td>\n",
              "      <td>0.468085</td>\n",
              "      <td>0.403141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.608434</td>\n",
              "      <td>0.660832</td>\n",
              "      <td>0.565826</td>\n",
              "      <td>0.528796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.618182</td>\n",
              "      <td>0.667396</td>\n",
              "      <td>0.573034</td>\n",
              "      <td>0.534031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.635762</td>\n",
              "      <td>0.671772</td>\n",
              "      <td>0.561404</td>\n",
              "      <td>0.502618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.598726</td>\n",
              "      <td>0.649891</td>\n",
              "      <td>0.540230</td>\n",
              "      <td>0.492147</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0          LSTM 0.05  WMT  0.603376  0.712245  0.669789  0.752632\n",
              "1           GRU 0.05  WMT  0.706186  0.775510  0.713542  0.721053\n",
              "2       XGBoost 0.05  WMT  0.683060  0.748980  0.670241  0.657895\n",
              "3        Logreg 0.05  WMT  0.737589  0.748980  0.628399  0.547368\n",
              "4           SVM 0.05  WMT  0.710227  0.763265  0.683060  0.657895\n",
              "5     LSTM beta 0.05  WMT  0.484277  0.645514  0.487342  0.490446\n",
              "6      GRU beta 0.05  WMT  0.532075  0.693654  0.668246  0.898089\n",
              "7  XGBoost beta 0.05  WMT  0.585492  0.728665  0.645714  0.719745\n",
              "8   logreg beta 0.05  WMT  0.673913  0.761488  0.630508  0.592357\n",
              "9      svm beta 0.05  WMT  0.603175  0.741794  0.658960  0.726115\n",
              "0           LSTM 0.1  WMT  0.645963  0.706122  0.590909  0.544503\n",
              "1            GRU 0.1  WMT  0.607143  0.683673  0.568245  0.534031\n",
              "2        XGBoost 0.1  WMT  0.621951  0.691837  0.574648  0.534031\n",
              "3         Logreg 0.1  WMT  0.722772  0.702041  0.500000  0.382199\n",
              "4            SVM 0.1  WMT  0.637681  0.687755  0.534954  0.460733\n",
              "5      LSTM beta 0.1  WMT  0.557971  0.617068  0.468085  0.403141\n",
              "6       GRU beta 0.1  WMT  0.608434  0.660832  0.565826  0.528796\n",
              "7   XGBoost beta 0.1  WMT  0.618182  0.667396  0.573034  0.534031\n",
              "8    logreg beta 0.1  WMT  0.635762  0.671772  0.561404  0.502618\n",
              "9       svm beta 0.1  WMT  0.598726  0.649891  0.540230  0.492147"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t5nYxWsaAC8"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}