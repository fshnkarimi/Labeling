{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Time_Series_Labelling_Predictions.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jR3SjOL7m0gJ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fshnkarimi/Labeling/blob/main/Time_Series_Labelling_Predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMURxUHNk5sf",
        "outputId": "93657bb6-545b-46fe-d81c-e5a41cd142ea"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR3SjOL7m0gJ"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wURP0Jxui8-d"
      },
      "source": [
        "# Import Libraries \n",
        "import itertools as itt\n",
        "import numbers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from abc import abstractmethod\n",
        "from typing import Iterable, Tuple, List\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import plotly.graph_objects as go\n",
        "import xgboost as xgb\n",
        "from prettytable import PrettyTable\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.signal import butter, lfilter, freqz\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__9u9qrOj01l"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqwGOVvDRqvH"
      },
      "source": [
        "# Labelling algorithm X is price and th is our threshould\n",
        "\n",
        "def labeling(X, th):\n",
        "    n = len(X)\n",
        "    y = np.array([0 for i in range(n)])\n",
        "    FP = X[0]\n",
        "    xh = X[0]\n",
        "    xl = X[0]\n",
        "    HT = 0\n",
        "    LT = 0\n",
        "    cid = 0\n",
        "    FP_N = 0\n",
        "    for i in range(n):\n",
        "        if(X[i] > FP + X[0]*th):\n",
        "            xh, HT, FP_N, cid  = X[i], i, i, 1\n",
        "            break\n",
        "        if(X[i] < FP - X[0]*th):\n",
        "            xh,HT,FP_N,cid  = X[i],i,i,-1\n",
        "            break\n",
        "    for i in range(FP_N+1,n):\n",
        "        if(cid > 0):\n",
        "            if(X[i]>xh):\n",
        "                xh, HT = X[i], i\n",
        "            if(X[i] < xh - xh * th and LT<= HT):\n",
        "                for j in range(n):\n",
        "                    if(j > LT and j <= HT):\n",
        "                        y[j] = 1\n",
        "                xl, LT, cid = X[i], i, -1\n",
        "        if(cid < 0):\n",
        "            if(X[i] < xl):\n",
        "                xl, LT = X[i], i\n",
        "            if(X[i] > xl + xl * th and HT <= LT):\n",
        "                for j in range(n):\n",
        "                    if(j > HT and j <= LT):\n",
        "                        y[j] = 0\n",
        "                xh, HT, cid = X[i], i, 1\n",
        "    return y\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDOBqAAqdWXr"
      },
      "source": [
        "# Implement Cross-validation methods\n",
        "\n",
        "class BaseTimeSeriesCrossValidator:\n",
        "    \"\"\"\n",
        "    Abstract class for time series cross-validation.\n",
        "    Time series cross-validation requires each sample has a prediction time pred_time, at which the features are used to\n",
        "    predict the response, and an evaluation time eval_time, at which the response is known and the error can be\n",
        "    computed. Importantly, it means that unlike in standard sklearn cross-validation, the samples X, response y,\n",
        "    pred_times and eval_times must all be pandas dataframe/series having the same index. It is also assumed that the\n",
        "    samples are time-ordered with respect to the prediction time (i.e. pred_times is non-decreasing).\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_splits : int, default=10\n",
        "        Number of folds. Must be at least 2.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_splits=10):\n",
        "        if not isinstance(n_splits, numbers.Integral):\n",
        "            raise ValueError(f\"The number of folds must be of Integral type. {n_splits} of type {type(n_splits)}\"\n",
        "                             f\" was passed.\")\n",
        "        n_splits = int(n_splits)\n",
        "        if n_splits <= 1:\n",
        "            raise ValueError(f\"K-fold cross-validation requires at least one train/test split by setting n_splits = 2 \"\n",
        "                             f\"or more, got n_splits = {n_splits}.\")\n",
        "        self.n_splits = n_splits\n",
        "        self.pred_times = None\n",
        "        self.eval_times = None\n",
        "        self.indices = None\n",
        "\n",
        "    @abstractmethod\n",
        "    def split(self, X: pd.DataFrame, y: pd.Series = None,\n",
        "              pred_times: pd.Series = None, eval_times: pd.Series = None):\n",
        "        if not isinstance(X, pd.DataFrame) and not isinstance(X, pd.Series):\n",
        "            raise ValueError('X should be a pandas DataFrame/Series.')\n",
        "        if not isinstance(y, pd.Series) and y is not None:\n",
        "            raise ValueError('y should be a pandas Series.')\n",
        "        if not isinstance(pred_times, pd.Series):\n",
        "            raise ValueError('pred_times should be a pandas Series.')\n",
        "        if not isinstance(eval_times, pd.Series):\n",
        "            raise ValueError('eval_times should be a pandas Series.')\n",
        "        if y is not None and (X.index == y.index).sum() != len(y):\n",
        "            raise ValueError('X and y must have the same index')\n",
        "        if (X.index == pred_times.index).sum() != len(pred_times):\n",
        "            raise ValueError('X and pred_times must have the same index')\n",
        "        if (X.index == eval_times.index).sum() != len(eval_times):\n",
        "            raise ValueError('X and eval_times must have the same index')\n",
        "\n",
        "        self.pred_times = pred_times\n",
        "        self.eval_times = eval_times\n",
        "        self.indices = np.arange(X.shape[0])\n",
        "        \n",
        "class CombPurgedKFoldCV(BaseTimeSeriesCrossValidator):\n",
        "    \"\"\"\n",
        "    Purged and embargoed combinatorial cross-validation\n",
        "    As described in Advances in financial machine learning, Marcos Lopez de Prado, 2018.\n",
        "    The samples are decomposed into n_splits folds containing equal numbers of samples, without shuffling. In each cross\n",
        "    validation round, n_test_splits folds are used as the test set, while the other folds are used as the train set.\n",
        "    There are as many rounds as n_test_splits folds among the n_splits folds.\n",
        "    Each sample should be tagged with a prediction time pred_time and an evaluation time eval_time. The split is such\n",
        "    that the intervals [pred_times, eval_times] associated to samples in the train and test set do not overlap. (The\n",
        "    overlapping samples are dropped.) In addition, an \"embargo\" period is defined, giving the minimal time between an\n",
        "    evaluation time in the test set and a prediction time in the training set. This is to avoid, in the presence of\n",
        "    temporal correlation, a contamination of the test set by the train set.\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_splits : int, default=10\n",
        "        Number of folds. Must be at least 2.\n",
        "    n_test_splits : int, default=2\n",
        "        Number of folds used in the test set. Must be at least 1.\n",
        "    embargo_td : pd.Timedelta, default=0\n",
        "        Embargo period (see explanations above).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_splits=10, n_test_splits=2, embargo_td=0):\n",
        "        super().__init__(n_splits)\n",
        "        if not isinstance(n_test_splits, numbers.Integral):\n",
        "            raise ValueError(f\"The number of test folds must be of Integral type. {n_test_splits} of type \"\n",
        "                             f\"{type(n_test_splits)} was passed.\")\n",
        "        n_test_splits = int(n_test_splits)\n",
        "        if n_test_splits <= 0 or n_test_splits > self.n_splits - 1:\n",
        "            raise ValueError(f\"K-fold cross-validation requires at least one train/test split by setting \"\n",
        "                             f\"n_test_splits between 1 and n_splits - 1, got n_test_splits = {n_test_splits}.\")\n",
        "        self.n_test_splits = n_test_splits\n",
        "\n",
        "        if embargo_td < 0:\n",
        "            raise ValueError(f\"The embargo time should be positive, got embargo = {embargo_td}.\")\n",
        "        self.embargo_td = embargo_td\n",
        "\n",
        "    def split(self, X: pd.DataFrame, y: pd.Series = None,\n",
        "              pred_times: pd.Series = None, eval_times: pd.Series = None) -> Iterable[Tuple[np.ndarray, np.ndarray]]:\n",
        "        \"\"\"\n",
        "        Yield the indices of the train and test sets.\n",
        "        Although the samples are passed in the form of a pandas dataframe, the indices returned are position indices,\n",
        "        not labels.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pd.DataFrame, shape (n_samples, n_features), required\n",
        "            Samples. Only used to extract n_samples.\n",
        "        y : pd.Series, not used, inherited from _BaseKFold\n",
        "        pred_times : pd.Series, shape (n_samples,), required\n",
        "            Times at which predictions are made. pred_times.index has to coincide with X.index.\n",
        "        eval_times : pd.Series, shape (n_samples,), required\n",
        "            Times at which the response becomes available and the error can be computed. eval_times.index has to\n",
        "            coincide with X.index.\n",
        "        Returns\n",
        "        -------\n",
        "        train_indices: np.ndarray\n",
        "            A numpy array containing all the indices in the train set.\n",
        "        test_indices : np.ndarray\n",
        "            A numpy array containing all the indices in the test set.\n",
        "        \"\"\"\n",
        "        super().split(X, y, pred_times, eval_times)\n",
        "\n",
        "        # Fold boundaries\n",
        "        fold_bounds = [(fold[0], fold[-1] + 1) for fold in np.array_split(self.indices, self.n_splits)]\n",
        "        # List of all combinations of n_test_splits folds selected to become test sets\n",
        "        selected_fold_bounds = list(itt.combinations(fold_bounds, self.n_test_splits))\n",
        "        \n",
        "        # In order for the first round to have its whole test set at the end of the dataset\n",
        "        selected_fold_bounds.reverse()\n",
        "\n",
        "        for fold_bound_list in selected_fold_bounds:\n",
        "            # Computes the bounds of the test set, and the corresponding indices\n",
        "            test_fold_bounds, test_indices = self.compute_test_set(fold_bound_list)\n",
        "            # Computes the train set indices\n",
        "            train_indices = self.compute_train_set(test_fold_bounds, test_indices)\n",
        "\n",
        "            yield train_indices, test_indices\n",
        "\n",
        "    def compute_train_set(self, test_fold_bounds: List[Tuple[int, int]], test_indices: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the position indices of samples in the train set.\n",
        "        Parameters\n",
        "        ----------\n",
        "        test_fold_bounds : List of tuples of position indices\n",
        "            Each tuple records the bounds of a block of indices in the test set.\n",
        "        test_indices : np.ndarray\n",
        "            A numpy array containing all the indices in the test set.\n",
        "        Returns\n",
        "        -------\n",
        "        train_indices: np.ndarray\n",
        "            A numpy array containing all the indices in the train set.\n",
        "        \"\"\"\n",
        "        # As a first approximation, the train set is the complement of the test set\n",
        "        train_indices = np.setdiff1d(self.indices, test_indices)\n",
        "        # But we now have to purge and embargo\n",
        "        for test_fold_start, test_fold_end in test_fold_bounds:\n",
        "            # Purge\n",
        "            train_indices = purge(self, train_indices, test_fold_start, test_fold_end)\n",
        "            # Embargo\n",
        "            train_indices = embargo(self, train_indices, test_indices, test_fold_end)\n",
        "        return train_indices\n",
        "\n",
        "    def compute_test_set(self, fold_bound_list: List[Tuple[int, int]]) -> Tuple[List[Tuple[int, int]], np.ndarray]:\n",
        "        \"\"\"\n",
        "        Compute the indices of the samples in the test set.\n",
        "        Parameters\n",
        "        ----------\n",
        "        fold_bound_list: List of tuples of position indices\n",
        "            Each tuple records the bounds of the folds belonging to the test set.\n",
        "        Returns\n",
        "        -------\n",
        "        test_fold_bounds: List of tuples of position indices\n",
        "            Like fold_bound_list, but with the neighboring folds in the test set merged.\n",
        "        test_indices: np.ndarray\n",
        "            A numpy array containing the test indices.\n",
        "        \"\"\"\n",
        "        test_indices = np.empty(0)\n",
        "        test_fold_bounds = []\n",
        "        for fold_start, fold_end in fold_bound_list:\n",
        "            # Records the boundaries of the current test split\n",
        "            if not test_fold_bounds or fold_start != test_fold_bounds[-1][-1]:\n",
        "                test_fold_bounds.append((fold_start, fold_end))\n",
        "            # If the current test split is contiguous to the previous one, simply updates the endpoint\n",
        "            elif fold_start == test_fold_bounds[-1][-1]:\n",
        "                test_fold_bounds[-1] = (test_fold_bounds[-1][0], fold_end)\n",
        "            test_indices = np.union1d(test_indices, self.indices[fold_start:fold_end]).astype(int)\n",
        "        return test_fold_bounds, test_indices\n",
        "\n",
        "\n",
        "def embargo(cv: BaseTimeSeriesCrossValidator, train_indices: np.ndarray,\n",
        "            test_indices: np.ndarray, test_fold_end: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Apply the embargo procedure to part of the train set.\n",
        "    This amounts to dropping the train set samples whose prediction time occurs within self.embargo_dt of the test\n",
        "    set sample evaluation times. This method applies the embargo only to the part of the training set immediately\n",
        "    following the end of the test set determined by test_fold_end.\n",
        "    Parameters\n",
        "    ----------\n",
        "    cv: Cross-validation class\n",
        "        Needs to have the attributes cv.pred_times, cv.eval_times, cv.embargo_dt and cv.indices.\n",
        "    train_indices: np.ndarray\n",
        "        A numpy array containing all the indices of the samples currently included in the train set.\n",
        "    test_indices : np.ndarray\n",
        "        A numpy array containing all the indices of the samples in the test set.\n",
        "    test_fold_end : int\n",
        "        Index corresponding to the end of a test set block.\n",
        "    Returns\n",
        "    -------\n",
        "    train_indices: np.ndarray\n",
        "        The same array, with the indices subject to embargo removed.\n",
        "    \"\"\"\n",
        "    if not hasattr(cv, 'embargo_td'):\n",
        "        raise ValueError(\"The passed cross-validation object should have a member cv.embargo_td defining the embargo\"\n",
        "                         \"time.\")\n",
        "    last_test_eval_time = cv.eval_times.iloc[cv.indices[:test_fold_end]].max()\n",
        "    min_train_index = len(cv.pred_times[cv.pred_times <= last_test_eval_time + cv.embargo_td])\n",
        "    if min_train_index < cv.indices.shape[0]:\n",
        "        allowed_indices = np.concatenate((cv.indices[:test_fold_end], cv.indices[min_train_index:]))\n",
        "        train_indices = np.intersect1d(train_indices, allowed_indices)\n",
        "    return train_indices\n",
        "\n",
        "\n",
        "def purge(cv: BaseTimeSeriesCrossValidator, train_indices: np.ndarray,\n",
        "          test_fold_start: int, test_fold_end: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Purge part of the train set.\n",
        "    Given a left boundary index test_fold_start of the test set, this method removes from the train set all the\n",
        "    samples whose evaluation time is posterior to the prediction time of the first test sample after the boundary.\n",
        "    Parameters\n",
        "    ----------\n",
        "    cv: Cross-validation class\n",
        "        Needs to have the attributes cv.pred_times, cv.eval_times and cv.indices.\n",
        "    train_indices: np.ndarray\n",
        "        A numpy array containing all the indices of the samples currently included in the train set.\n",
        "    test_fold_start : int\n",
        "        Index corresponding to the start of a test set block.\n",
        "    test_fold_end : int\n",
        "        Index corresponding to the end of the same test set block.\n",
        "    Returns\n",
        "    -------\n",
        "    train_indices: np.ndarray\n",
        "        A numpy array containing the train indices purged at test_fold_start.\n",
        "    \"\"\"\n",
        "    time_test_fold_start = cv.pred_times.iloc[test_fold_start]\n",
        "    # The train indices before the start of the test fold, purged.\n",
        "    train_indices_1 = np.intersect1d(train_indices, cv.indices[cv.eval_times < time_test_fold_start])\n",
        "    # The train indices after the end of the test fold.\n",
        "    train_indices_2 = np.intersect1d(train_indices, cv.indices[test_fold_end:])\n",
        "\n",
        "    return np.concatenate((train_indices_1, train_indices_2))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf7Vd77V19PT"
      },
      "source": [
        "def get_metrics(label, y_pred, show_results=False):\n",
        "  \n",
        "  results = []\n",
        "  results.append(metrics.accuracy_score(label, y_pred))\n",
        "  results.append(metrics.precision_score(label, y_pred))\n",
        "  results.append(metrics.recall_score(label, y_pred))\n",
        "  results.append(metrics.f1_score(label, y_pred))\n",
        "\n",
        "  if show_results:\n",
        "    print(\"Accuracy:{:.6f}\".format(metrics.accuracy_score(label, y_pred)))\n",
        "    print(\"Precision:{:.6f}\".format(metrics.precision_score(label, y_pred)))\n",
        "    print(\"Recall:{:.6f}\".format(metrics.recall_score(label, y_pred)))\n",
        "    print(\"F1 score:{:.6f}\".format(metrics.f1_score(label, y_pred)))\n",
        "    \n",
        "  return results"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzChyAFBka7-"
      },
      "source": [
        "def data_pre1d(df, index, label, step_size):\n",
        "  x = np.array([df[i + step_size - 1:i+step_size*11:step_size] for i in index if (i-1 + 11*step_size) in index])\n",
        "  y = np.array([label[i-1 + 11*step_size] for i in index if (i-1 + 11*step_size) in index])\n",
        "  x = x / np.mean(x,axis = 1).reshape((len(x), 1))\n",
        "  x = x - np.ones((len(x), 1))\n",
        "  x = x.reshape(x.shape[0], x.shape[1], 1)\n",
        "  y = (y + 1) // 2\n",
        "  return x, y"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rQBNo1O7f2c"
      },
      "source": [
        "def visualize(dfs, col_name, date, th, start, end):\n",
        "  label = labeling(dfs[col_name], th)\n",
        "  date_col = dfs[date].astype(str).str[:4]\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(go.Scatter(x=date_col[start:end], y=dfs[col_name][start:end]))\n",
        "  fig.show()\n",
        "  fig = go.Figure([go.Scatter(x=date_col[start:end], y=label[start:end])])\n",
        "  fig.show() "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEk0ml6W2quS"
      },
      "source": [
        "# Define a class for training models\n",
        "\n",
        "class Train_data():\n",
        "  def __init__(self, data,  train_start, train_end, test_end):\n",
        "    self.train_start = train_start\n",
        "    self.train_end = train_end\n",
        "    self.test_end = test_end \n",
        "    self.data = data\n",
        "    self.train_data = pd.Series(np.array(data[train_start:train_end]))\n",
        "    self.train_label = None\n",
        "    self.test_data = pd.Series(np.array(data[train_end:test_end]))\n",
        "    self.test_label = None\n",
        "    self.splits = []\n",
        "    self.X_train  = []\n",
        "    self.y_train = []\n",
        "    self.X_test  = []\n",
        "    self.y_test = []\n",
        "    self.GRU_beta_result = None\n",
        "    self.LSTM_beta_result = None\n",
        "    self.XGboost_beta_result = None\n",
        "    self.SVM_beta_result = None\n",
        "    self.Logreg_beta_result = None\n",
        "    self.GRU_result = None\n",
        "    self.LSTM_result = None\n",
        "    self.XGboost_result = None\n",
        "    self.SVM_result = None\n",
        "    self.Logreg_result = None\n",
        "    self.GRU_beta_predict = None\n",
        "    self.LSTM_beta_predict = None\n",
        "    self.XGboost_beta_predict = None\n",
        "    self.SVM_beta_predict = None\n",
        "    self.Logreg_beta_predict = None\n",
        "    self.GRU_predict = None\n",
        "    self.LSTM_predict = None\n",
        "    self.XGboost_predict = None\n",
        "    self.SVM_predict = None\n",
        "    self.Logreg_predict = None\n",
        "\n",
        "  def set_threshold(self, th):\n",
        "    label  = labeling(self.data, th)\n",
        "    self.train_label = pd.Series(np.array(label[self.train_start:self.train_end]))\n",
        "    self.test_label = pd.Series(np.array(label[self.train_end:self.test_end]))\n",
        "\n",
        "  def K_fold_purged(self, num_split, num_test, time_gaps, emb=0, purging=True):\n",
        "    n_splits = num_split\n",
        "    n_test_splits = num_test\n",
        "    time_gap = time_gaps\n",
        "    embargo_td = emb\n",
        "    t1_ = self.train_data.index\n",
        "    t1 = pd.Series(t1_).shift(time_gap).fillna(0).astype(int)\n",
        "    t2 = pd.Series(t1_).shift(-time_gap).fillna(1e12).astype(int)\n",
        "    \n",
        "    if purging:\n",
        "      cpkf = CombPurgedKFoldCV(n_splits=n_splits, n_test_splits=n_test_splits, embargo_td=embargo_td)\n",
        "      comb_purged_splits = list(cpkf.split(self.train_data, pred_times=t1, eval_times=t2))\n",
        "      self.splits = comb_purged_splits\n",
        "    \n",
        "    else:\n",
        "      cvts = TimeSeriesSplit(n_splits=n_splits)\n",
        "      cvts_splits = list(cvts.split(self.train_data))\n",
        "      self.splits = cvts_splits\n",
        "\n",
        "\n",
        "  def SVM(self, beta):\n",
        "\n",
        "    Xtrain = self.X_train.reshape(self.X_train.shape[0], self.X_train.shape[1])\n",
        "    Xtest = self.X_test.reshape(self.X_test.shape[0], self.X_test.shape[1])\n",
        "    clf = SVC(C=1)\n",
        "    clf.fit( Xtrain,self.y_train)\n",
        "    y_pred = clf.predict(Xtest)\n",
        "    if beta : \n",
        "      self.SVM_beta_result  =  get_metrics(self.y_test, y_pred,0)\n",
        "      self.SVM_beta_predict = y_pred    \n",
        "    else:\n",
        "      self.SVM_result  =  get_metrics(self.y_test,y_pred,0)\n",
        "      self.SVM_predict = y_pred\n",
        "    \n",
        "\n",
        "  def log_reg(self, beta):\n",
        "    \n",
        "    Xtrain = self.X_train.reshape(self.X_train.shape[0], self.X_train.shape[1])\n",
        "    Xtest = self.X_test.reshape(self.X_test.shape[0], self.X_test.shape[1])\n",
        "    clf = LogisticRegression(C=10, penalty=\"l2\")\n",
        "    clf.fit( Xtrain,self.y_train)\n",
        "    y_pred = clf.predict(Xtest)\n",
        "    if beta: \n",
        "      self.Logreg_beta_result  =  get_metrics(self.y_test,y_pred,0)\n",
        "      self.Logreg_beta_predict = y_pred\n",
        "    else:\n",
        "      self.Logreg_result  =  get_metrics(self.y_test,y_pred,0)\n",
        "      self.Logreg_predict = y_pred\n",
        "    \n",
        "    \n",
        "\n",
        "  def data_preprocess(self, step_size):  \n",
        "    self.X_train, self.y_train = data_pre1d(self.train_data, self.train_data.index, self.train_label,step_size)\n",
        "    self.X_test, self.y_test = data_pre1d(self.test_data, self.test_data.index, self.test_label,step_size)\n",
        "    \n",
        "\n",
        "  def LSTM(self, epoch, layer_size, window_size, lr, beta):\n",
        "    with tf.device('/device:GPU:0'):\n",
        "      model=Sequential()\n",
        "      model.add(tf.compat.v1.keras.layers.CuDNNLSTM(layer_size,return_sequences=True,input_shape=(window_size,1)))\n",
        "      model.add(Dropout(0.4))\n",
        "      model.add(tf.compat.v1.keras.layers.CuDNNLSTM(layer_size))\n",
        "      model.add(Dropout(0.4))\n",
        "      model.add(Dense(1, activation='sigmoid'))\n",
        "      opt = tf.keras.optimizers.Adam(beta_1=0.9,beta_2=0.999,learning_rate=lr)\n",
        "      model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy']) \n",
        "      \n",
        "      if beta:\n",
        "        print(f'start lstm beta training...\\n')\n",
        "        model.fit(self.X_train,self.y_train,validation_data=(self.X_test, self.y_test),epochs=epoch,batch_size=16,verbose=1)\n",
        "        y_pred = model.predict(self.X_test)\n",
        "        y_pred = np.array([1 if y >=0.5 else 0 for y in y_pred])\n",
        "        self.LSTM_beta_result = get_metrics(self.y_test, y_pred, False)\n",
        "        self.LSTM_beta_predict = y_pred\n",
        "      else:\n",
        "        print(f'start lstm training...\\n')\n",
        "        model.fit(self.X_train,self.y_train,validation_data=(self.X_test, self.y_test),epochs=epoch,batch_size=16,verbose=1)\n",
        "        y_pred = model.predict(self.X_test)\n",
        "        y_pred = np.array([1 if y >=0.5 else 0 for y in y_pred])\n",
        "        self.LSTM_result = get_metrics(self.y_test, y_pred, False)\n",
        "        self.LSTM_predict = y_pred\n",
        "      \n",
        "      print('end training. \\n')\n",
        "\n",
        "  def GRU(self, epoch, layer_size, window_size, lr, beta):\n",
        "    with tf.device('/device:GPU:0'):\n",
        "      model=Sequential()\n",
        "      model.add(tf.compat.v1.keras.layers.CuDNNGRU(layer_size,return_sequences=True,input_shape=(window_size,1)))\n",
        "      model.add(Dropout(0.4))\n",
        "      model.add(tf.compat.v1.keras.layers.CuDNNGRU(layer_size))\n",
        "      model.add(Dropout(0.4))\n",
        "      model.add(Dense(1, activation='sigmoid'))\n",
        "      opt = tf.keras.optimizers.Adam(beta_1=0.9,beta_2=0.999,learning_rate=lr)\n",
        "      model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "      if beta:\n",
        "        print('start gru beta training...\\n')\n",
        "        model.fit(self.X_train,self.y_train,validation_data=(self.X_test,self.y_test),epochs=epoch,batch_size=16,verbose=1)\n",
        "        y_pred = model.predict(self.X_test)\n",
        "        y_pred = np.array([1 if y >=0.5 else 0 for y in y_pred])\n",
        "        self.GRU_beta_result = get_metrics(self.y_test,y_pred,False)\n",
        "        self.GRU_beta_predict = y_pred\n",
        "      else:\n",
        "        print('start gru training...\\n')\n",
        "        model.fit(self.X_train,self.y_train,validation_data=(self.X_test,self.y_test),epochs=epoch,batch_size=16,verbose=1)\n",
        "        y_pred = model.predict(self.X_test)\n",
        "        y_pred = np.array([1 if y >=0.5 else 0 for y in y_pred])\n",
        "        self.GRU_result = get_metrics(self.y_test,y_pred,False)\n",
        "        self.GRU_predict = y_pred\n",
        "\n",
        "      print('end training. \\n')\n",
        "\n",
        "  def XGboost(self,es,beta):\n",
        "    Xtrain = self.X_train.reshape(self.X_train.shape[0],self.X_train.shape[1])\n",
        "    Xtest = self.X_test.reshape(self.X_test.shape[0],self.X_test.shape[1])\n",
        "    # XGBoost classifier with Early-stopping\n",
        "    clf = xgb.XGBClassifier(n_jobs=1)\n",
        "    if beta:\n",
        "      print('start xgboost beta training...\\n')\n",
        "      clf.fit(Xtrain, self.y_train, early_stopping_rounds=es, eval_metric=\"auc\",eval_set=[(Xtest, self.y_test)])\n",
        "      y_pred = clf.predict(Xtest)\n",
        "      self.XGboost_beta_result = get_metrics(self.y_test,y_pred,False)\n",
        "      self.XGboost_beta_predict = y_pred\n",
        "    else:\n",
        "      print('start xgboost training...\\n')\n",
        "      clf.fit(Xtrain, self.y_train, early_stopping_rounds=es, eval_metric=\"auc\",eval_set=[(Xtest, self.y_test)])\n",
        "      y_pred = clf.predict(Xtest)\n",
        "      self.XGboost_result = get_metrics(self.y_test,y_pred,False)\n",
        "      self.XGboost_predict = y_pred\n",
        "\n",
        "    print('end training. \\n')\n",
        "\n",
        "\n",
        "  def table(self, th = 0.1):\n",
        "    x = PrettyTable()\n",
        "    x.field_names = [\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 score\"]\n",
        "    self.LSTM_result.insert(0,\"LSTM \"+str(th))\n",
        "    self.GRU_result.insert(0,\"GRU \"+str(th))\n",
        "    self.XGboost_result.insert(0,\"XGBoost \"+str(th))\n",
        "    self.Logreg_result.insert(0,\"Logreg \"+str(th))\n",
        "    self.SVM_result.insert(0,\"SVM \"+str(th))\n",
        "    self.LSTM_beta_result.insert(0,\"LSTM beta \"+str(th))\n",
        "    self.GRU_beta_result.insert(0,\"GRU beta \"+str(th))\n",
        "    self.XGboost_beta_result.insert(0,\"XGBoost beta \"+str(th))\n",
        "    self.Logreg_beta_result.insert(0,\"logreg beta \"+str(th))\n",
        "    self.SVM_beta_result.insert(0,\"svm beta \"+str(th))\n",
        "    x.add_row(self.LSTM_result)\n",
        "    x.add_row(self.GRU_result)\n",
        "    x.add_row(self.XGboost_result)\n",
        "    x.add_row(self.Logreg_result)\n",
        "    x.add_row(self.SVM_result)\n",
        "    x.add_row(self.LSTM_beta_result)\n",
        "    x.add_row(self.GRU_beta_result)\n",
        "    x.add_row(self.XGboost_beta_result)\n",
        "    x.add_row(self.Logreg_beta_result)\n",
        "    x.add_row(self.SVM_beta_result)\n",
        "    return x\n",
        "\n",
        "\n",
        "  \n",
        "  def train_models(self, best_parameters, step_sizes=4, th = 0.1):\n",
        "\n",
        "    lstm_epoch = best_parameters[0]\n",
        "    lstm_layer = best_parameters[1]\n",
        "    lstm_lr = best_parameters[2]\n",
        "    gru_epoch = best_parameters[3]\n",
        "    gru_layer = best_parameters[4]\n",
        "    gru_lr = best_parameters[5]\n",
        "\n",
        "    self.data_preprocess(step_size=1)\n",
        "    self.LSTM(lstm_epoch, lstm_layer, 11, lstm_lr, False)\n",
        "    self.GRU(gru_epoch, gru_layer, 11, gru_lr, False)\n",
        "    self.XGboost(50, False)\n",
        "    self.SVM(False)\n",
        "    self.log_reg(False)\n",
        "\n",
        "    self.data_preprocess(step_size=step_sizes)\n",
        "    self.LSTM(lstm_epoch,lstm_layer,11,lstm_lr,True)\n",
        "    self.GRU(gru_epoch,gru_layer,11,gru_lr,True)\n",
        "    self.XGboost(50,True)\n",
        "    self.SVM(True)\n",
        "    self.log_reg(True)\n",
        "\n",
        "    print(self.table(th = th))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur-dIbFYaJXc"
      },
      "source": [
        "# Default best parameters\n",
        "\n",
        "default_best_param = [5, 10, 0.01, 5, 10, 0.01]\n",
        "\n",
        "def final_result(historical, name, best_param=default_best_param, step_sizes=4, th=0.1):\n",
        "  historical.train_models(best_param, step_sizes, th=th)\n",
        "  results = []\n",
        "  results.append(historical.LSTM_result)\n",
        "  results.append(historical.GRU_result)\n",
        "  results.append(historical.XGboost_result)\n",
        "  results.append(historical.Logreg_result)\n",
        "  results.append(historical.SVM_result)\n",
        "  results.append(historical.LSTM_beta_result)\n",
        "  results.append(historical.GRU_beta_result)\n",
        "  results.append(historical.XGboost_beta_result)\n",
        "  results.append(historical.Logreg_beta_result)\n",
        "  results.append(historical.SVM_beta_result)\n",
        "  res = pd.DataFrame()\n",
        "  \n",
        "  for x in results:\n",
        "    dic = {\n",
        "      \"Name\": name,\n",
        "      \"Model\": x[0],\n",
        "      \"acc\": x[1],\n",
        "      \"Perc\": x[2], \n",
        "      \"recal\": x[3], \n",
        "      \"f1\" :x[4]\n",
        "\n",
        "    }\n",
        "    res = res.append(dic, ignore_index=True)\n",
        "  return res"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EE2k11tpBIo"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ02uVQzLwrK"
      },
      "source": [
        "## AMD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "HLMLm9U-LwrM",
        "outputId": "267ebc1c-e88e-4f25-c20f-d0ed7ce02ac5"
      },
      "source": [
        "dfs = pd.read_csv(\"AMD.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2768</td>\n",
              "      <td>US1.AMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>102.58</td>\n",
              "      <td>102.97</td>\n",
              "      <td>100.670</td>\n",
              "      <td>102.43</td>\n",
              "      <td>1095599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2767</td>\n",
              "      <td>US1.AMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>102.08</td>\n",
              "      <td>104.43</td>\n",
              "      <td>102.070</td>\n",
              "      <td>102.88</td>\n",
              "      <td>1313306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.AMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>102.16</td>\n",
              "      <td>102.64</td>\n",
              "      <td>99.820</td>\n",
              "      <td>100.34</td>\n",
              "      <td>1241696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.AMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>106.90</td>\n",
              "      <td>107.63</td>\n",
              "      <td>101.425</td>\n",
              "      <td>101.52</td>\n",
              "      <td>1661005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.AMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>104.99</td>\n",
              "      <td>108.43</td>\n",
              "      <td>103.440</td>\n",
              "      <td>108.17</td>\n",
              "      <td>1337149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.AMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>6.87</td>\n",
              "      <td>7.10</td>\n",
              "      <td>6.790</td>\n",
              "      <td>7.06</td>\n",
              "      <td>24895450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.AMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>6.89</td>\n",
              "      <td>6.93</td>\n",
              "      <td>6.770</td>\n",
              "      <td>6.87</td>\n",
              "      <td>20010148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.AMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>7.01</td>\n",
              "      <td>7.05</td>\n",
              "      <td>6.780</td>\n",
              "      <td>6.87</td>\n",
              "      <td>23076122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.AMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>7.04</td>\n",
              "      <td>7.12</td>\n",
              "      <td>6.890</td>\n",
              "      <td>6.97</td>\n",
              "      <td>34185258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.AMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>7.03</td>\n",
              "      <td>7.08</td>\n",
              "      <td>6.840</td>\n",
              "      <td>6.95</td>\n",
              "      <td>22229263</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2769 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...  <HIGH>    <LOW>  <CLOSE>     <VOL>\n",
              "0      2768  US1.AMD     D  20211001  ...  102.97  100.670   102.43   1095599\n",
              "1      2767  US1.AMD     D  20210930  ...  104.43  102.070   102.88   1313306\n",
              "2      2766  US1.AMD     D  20210929  ...  102.64   99.820   100.34   1241696\n",
              "3      2765  US1.AMD     D  20210928  ...  107.63  101.425   101.52   1661005\n",
              "4      2764  US1.AMD     D  20210927  ...  108.43  103.440   108.17   1337149\n",
              "...     ...      ...   ...       ...  ...     ...      ...      ...       ...\n",
              "2764      4  US1.AMD     D  20101008  ...    7.10    6.790     7.06  24895450\n",
              "2765      3  US1.AMD     D  20101007  ...    6.93    6.770     6.87  20010148\n",
              "2766      2  US1.AMD     D  20101006  ...    7.05    6.780     6.87  23076122\n",
              "2767      1  US1.AMD     D  20101005  ...    7.12    6.890     6.97  34185258\n",
              "2768      0  US1.AMD     D  20101004  ...    7.08    6.840     6.95  22229263\n",
              "\n",
              "[2769 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10kEvSjaOKw9"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n0OAbKd7r39"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3r8_YciLwrN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c07b6a0-593c-41ad-fa8d-cd01dcc3634d"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .2, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"AMD\", step_sizes=4, th= th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6920 - accuracy: 0.5268 - val_loss: 0.6628 - val_accuracy: 0.6265\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6662 - accuracy: 0.5973 - val_loss: 0.6634 - val_accuracy: 0.6939\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6445 - accuracy: 0.6477 - val_loss: 0.6109 - val_accuracy: 0.7531\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5865 - accuracy: 0.7268 - val_loss: 0.5926 - val_accuracy: 0.6837\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5499 - accuracy: 0.7262 - val_loss: 0.5575 - val_accuracy: 0.7163\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 2s 13ms/step - loss: 0.6790 - accuracy: 0.5691 - val_loss: 0.6249 - val_accuracy: 0.7184\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5642 - accuracy: 0.7154 - val_loss: 0.5617 - val_accuracy: 0.7388\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5348 - accuracy: 0.7369 - val_loss: 0.5962 - val_accuracy: 0.6837\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5457 - accuracy: 0.7289 - val_loss: 0.5617 - val_accuracy: 0.7286\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5255 - accuracy: 0.7597 - val_loss: 0.5710 - val_accuracy: 0.6796\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.739351\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.741701\n",
            "[2]\tvalidation_0-auc:0.734661\n",
            "[3]\tvalidation_0-auc:0.730932\n",
            "[4]\tvalidation_0-auc:0.726456\n",
            "[5]\tvalidation_0-auc:0.733905\n",
            "[6]\tvalidation_0-auc:0.734154\n",
            "[7]\tvalidation_0-auc:0.7375\n",
            "[8]\tvalidation_0-auc:0.742217\n",
            "[9]\tvalidation_0-auc:0.745296\n",
            "[10]\tvalidation_0-auc:0.747521\n",
            "[11]\tvalidation_0-auc:0.746525\n",
            "[12]\tvalidation_0-auc:0.744923\n",
            "[13]\tvalidation_0-auc:0.748598\n",
            "[14]\tvalidation_0-auc:0.749408\n",
            "[15]\tvalidation_0-auc:0.74996\n",
            "[16]\tvalidation_0-auc:0.75182\n",
            "[17]\tvalidation_0-auc:0.750672\n",
            "[18]\tvalidation_0-auc:0.751606\n",
            "[19]\tvalidation_0-auc:0.752283\n",
            "[20]\tvalidation_0-auc:0.751615\n",
            "[21]\tvalidation_0-auc:0.755309\n",
            "[22]\tvalidation_0-auc:0.755166\n",
            "[23]\tvalidation_0-auc:0.75578\n",
            "[24]\tvalidation_0-auc:0.7574\n",
            "[25]\tvalidation_0-auc:0.757026\n",
            "[26]\tvalidation_0-auc:0.756973\n",
            "[27]\tvalidation_0-auc:0.758059\n",
            "[28]\tvalidation_0-auc:0.757845\n",
            "[29]\tvalidation_0-auc:0.757774\n",
            "[30]\tvalidation_0-auc:0.757934\n",
            "[31]\tvalidation_0-auc:0.756492\n",
            "[32]\tvalidation_0-auc:0.754072\n",
            "[33]\tvalidation_0-auc:0.754534\n",
            "[34]\tvalidation_0-auc:0.754143\n",
            "[35]\tvalidation_0-auc:0.75384\n",
            "[36]\tvalidation_0-auc:0.755273\n",
            "[37]\tvalidation_0-auc:0.754205\n",
            "[38]\tvalidation_0-auc:0.753146\n",
            "[39]\tvalidation_0-auc:0.752755\n",
            "[40]\tvalidation_0-auc:0.752772\n",
            "[41]\tvalidation_0-auc:0.753858\n",
            "[42]\tvalidation_0-auc:0.753769\n",
            "[43]\tvalidation_0-auc:0.754196\n",
            "[44]\tvalidation_0-auc:0.753555\n",
            "[45]\tvalidation_0-auc:0.753413\n",
            "[46]\tvalidation_0-auc:0.753289\n",
            "[47]\tvalidation_0-auc:0.753315\n",
            "[48]\tvalidation_0-auc:0.752674\n",
            "[49]\tvalidation_0-auc:0.751277\n",
            "[50]\tvalidation_0-auc:0.751927\n",
            "[51]\tvalidation_0-auc:0.751553\n",
            "[52]\tvalidation_0-auc:0.751989\n",
            "[53]\tvalidation_0-auc:0.751526\n",
            "[54]\tvalidation_0-auc:0.751651\n",
            "[55]\tvalidation_0-auc:0.7519\n",
            "[56]\tvalidation_0-auc:0.750458\n",
            "[57]\tvalidation_0-auc:0.750494\n",
            "[58]\tvalidation_0-auc:0.75012\n",
            "[59]\tvalidation_0-auc:0.750058\n",
            "[60]\tvalidation_0-auc:0.751535\n",
            "[61]\tvalidation_0-auc:0.751055\n",
            "[62]\tvalidation_0-auc:0.751126\n",
            "[63]\tvalidation_0-auc:0.751197\n",
            "[64]\tvalidation_0-auc:0.750378\n",
            "[65]\tvalidation_0-auc:0.749292\n",
            "[66]\tvalidation_0-auc:0.748741\n",
            "[67]\tvalidation_0-auc:0.748491\n",
            "[68]\tvalidation_0-auc:0.747868\n",
            "[69]\tvalidation_0-auc:0.748313\n",
            "[70]\tvalidation_0-auc:0.747424\n",
            "[71]\tvalidation_0-auc:0.748616\n",
            "[72]\tvalidation_0-auc:0.747993\n",
            "[73]\tvalidation_0-auc:0.747183\n",
            "[74]\tvalidation_0-auc:0.746685\n",
            "[75]\tvalidation_0-auc:0.746115\n",
            "[76]\tvalidation_0-auc:0.747148\n",
            "[77]\tvalidation_0-auc:0.747432\n",
            "Stopping. Best iteration:\n",
            "[27]\tvalidation_0-auc:0.758059\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6944 - accuracy: 0.5182 - val_loss: 0.6774 - val_accuracy: 0.6280\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6877 - accuracy: 0.5436 - val_loss: 0.6615 - val_accuracy: 0.6280\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6709 - accuracy: 0.5841 - val_loss: 0.6275 - val_accuracy: 0.6761\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6255 - accuracy: 0.6596 - val_loss: 0.6152 - val_accuracy: 0.6805\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5985 - accuracy: 0.7062 - val_loss: 0.6149 - val_accuracy: 0.6455\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6763 - accuracy: 0.5710 - val_loss: 0.6349 - val_accuracy: 0.6958\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6185 - accuracy: 0.6706 - val_loss: 0.6167 - val_accuracy: 0.6433\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5869 - accuracy: 0.7035 - val_loss: 0.5892 - val_accuracy: 0.7133\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5804 - accuracy: 0.7145 - val_loss: 0.5646 - val_accuracy: 0.7659\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5691 - accuracy: 0.7255 - val_loss: 0.5970 - val_accuracy: 0.6937\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.663005\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.672382\n",
            "[2]\tvalidation_0-auc:0.687128\n",
            "[3]\tvalidation_0-auc:0.673837\n",
            "[4]\tvalidation_0-auc:0.686954\n",
            "[5]\tvalidation_0-auc:0.687682\n",
            "[6]\tvalidation_0-auc:0.672371\n",
            "[7]\tvalidation_0-auc:0.685581\n",
            "[8]\tvalidation_0-auc:0.685243\n",
            "[9]\tvalidation_0-auc:0.682609\n",
            "[10]\tvalidation_0-auc:0.681943\n",
            "[11]\tvalidation_0-auc:0.691013\n",
            "[12]\tvalidation_0-auc:0.692519\n",
            "[13]\tvalidation_0-auc:0.697223\n",
            "[14]\tvalidation_0-auc:0.696085\n",
            "[15]\tvalidation_0-auc:0.69836\n",
            "[16]\tvalidation_0-auc:0.700881\n",
            "[17]\tvalidation_0-auc:0.701527\n",
            "[18]\tvalidation_0-auc:0.700994\n",
            "[19]\tvalidation_0-auc:0.695142\n",
            "[20]\tvalidation_0-auc:0.695583\n",
            "[21]\tvalidation_0-auc:0.696587\n",
            "[22]\tvalidation_0-auc:0.696782\n",
            "[23]\tvalidation_0-auc:0.697069\n",
            "[24]\tvalidation_0-auc:0.69834\n",
            "[25]\tvalidation_0-auc:0.702244\n",
            "[26]\tvalidation_0-auc:0.701875\n",
            "[27]\tvalidation_0-auc:0.701465\n",
            "[28]\tvalidation_0-auc:0.703638\n",
            "[29]\tvalidation_0-auc:0.703802\n",
            "[30]\tvalidation_0-auc:0.704417\n",
            "[31]\tvalidation_0-auc:0.704837\n",
            "[32]\tvalidation_0-auc:0.704284\n",
            "[33]\tvalidation_0-auc:0.704837\n",
            "[34]\tvalidation_0-auc:0.704817\n",
            "[35]\tvalidation_0-auc:0.704273\n",
            "[36]\tvalidation_0-auc:0.700277\n",
            "[37]\tvalidation_0-auc:0.70206\n",
            "[38]\tvalidation_0-auc:0.703064\n",
            "[39]\tvalidation_0-auc:0.700215\n",
            "[40]\tvalidation_0-auc:0.698934\n",
            "[41]\tvalidation_0-auc:0.698668\n",
            "[42]\tvalidation_0-auc:0.699119\n",
            "[43]\tvalidation_0-auc:0.698955\n",
            "[44]\tvalidation_0-auc:0.697868\n",
            "[45]\tvalidation_0-auc:0.697069\n",
            "[46]\tvalidation_0-auc:0.696557\n",
            "[47]\tvalidation_0-auc:0.697336\n",
            "[48]\tvalidation_0-auc:0.696792\n",
            "[49]\tvalidation_0-auc:0.69588\n",
            "[50]\tvalidation_0-auc:0.694794\n",
            "[51]\tvalidation_0-auc:0.690551\n",
            "[52]\tvalidation_0-auc:0.687149\n",
            "[53]\tvalidation_0-auc:0.686903\n",
            "[54]\tvalidation_0-auc:0.686965\n",
            "[55]\tvalidation_0-auc:0.68762\n",
            "[56]\tvalidation_0-auc:0.68883\n",
            "[57]\tvalidation_0-auc:0.68844\n",
            "[58]\tvalidation_0-auc:0.688727\n",
            "[59]\tvalidation_0-auc:0.687333\n",
            "[60]\tvalidation_0-auc:0.687733\n",
            "[61]\tvalidation_0-auc:0.687159\n",
            "[62]\tvalidation_0-auc:0.6872\n",
            "[63]\tvalidation_0-auc:0.686442\n",
            "[64]\tvalidation_0-auc:0.686196\n",
            "[65]\tvalidation_0-auc:0.6847\n",
            "[66]\tvalidation_0-auc:0.684782\n",
            "[67]\tvalidation_0-auc:0.684075\n",
            "[68]\tvalidation_0-auc:0.683296\n",
            "[69]\tvalidation_0-auc:0.68473\n",
            "[70]\tvalidation_0-auc:0.686329\n",
            "[71]\tvalidation_0-auc:0.686514\n",
            "[72]\tvalidation_0-auc:0.685899\n",
            "[73]\tvalidation_0-auc:0.685468\n",
            "[74]\tvalidation_0-auc:0.683788\n",
            "[75]\tvalidation_0-auc:0.683398\n",
            "[76]\tvalidation_0-auc:0.685571\n",
            "[77]\tvalidation_0-auc:0.685243\n",
            "[78]\tvalidation_0-auc:0.684505\n",
            "[79]\tvalidation_0-auc:0.683296\n",
            "[80]\tvalidation_0-auc:0.683378\n",
            "[81]\tvalidation_0-auc:0.683234\n",
            "Stopping. Best iteration:\n",
            "[31]\tvalidation_0-auc:0.704837\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.7163265306122449 | 0.6111111111111112 | 0.6612021857923497 | 0.6351706036745406 |\n",
            "|     GRU 0.1      | 0.6795918367346939 | 0.5524193548387096 | 0.7486338797814208 | 0.6357308584686775 |\n",
            "|   XGBoost 0.1    | 0.6857142857142857 | 0.5627705627705628 | 0.7103825136612022 | 0.6280193236714976 |\n",
            "|    Logreg 0.1    | 0.7183673469387755 | 0.6027397260273972 | 0.7213114754098361 | 0.6567164179104478 |\n",
            "|     SVM 0.1      | 0.6591836734693878 | 0.5341880341880342 | 0.6830601092896175 | 0.5995203836930456 |\n",
            "|  LSTM beta 0.1   | 0.6455142231947484 | 0.515748031496063  | 0.7705882352941177 | 0.6179245283018868 |\n",
            "|   GRU beta 0.1   | 0.6936542669584245 | 0.5707547169811321 | 0.711764705882353  | 0.6335078534031414 |\n",
            "| XGBoost beta 0.1 | 0.6411378555798687 | 0.512396694214876  | 0.7294117647058823 | 0.6019417475728155 |\n",
            "| logreg beta 0.1  | 0.7024070021881839 | 0.580952380952381  | 0.7176470588235294 | 0.6421052631578948 |\n",
            "|   svm beta 0.1   | 0.649890590809628  | 0.5210084033613446 | 0.7294117647058823 | 0.6078431372549019 |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6855 - accuracy: 0.5584 - val_loss: 0.6400 - val_accuracy: 0.6388\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6268 - accuracy: 0.6477 - val_loss: 0.6396 - val_accuracy: 0.7184\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6014 - accuracy: 0.6799 - val_loss: 0.6358 - val_accuracy: 0.7204\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5999 - accuracy: 0.6758 - val_loss: 0.6070 - val_accuracy: 0.7286\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5803 - accuracy: 0.6886 - val_loss: 0.6344 - val_accuracy: 0.6694\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6340 - accuracy: 0.6336 - val_loss: 0.6285 - val_accuracy: 0.6980\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5780 - accuracy: 0.6913 - val_loss: 0.6440 - val_accuracy: 0.6224\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5667 - accuracy: 0.6946 - val_loss: 0.6344 - val_accuracy: 0.6429\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5638 - accuracy: 0.7013 - val_loss: 0.6693 - val_accuracy: 0.5918\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5666 - accuracy: 0.6852 - val_loss: 0.6366 - val_accuracy: 0.6388\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.684988\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.669591\n",
            "[2]\tvalidation_0-auc:0.672984\n",
            "[3]\tvalidation_0-auc:0.672361\n",
            "[4]\tvalidation_0-auc:0.669455\n",
            "[5]\tvalidation_0-auc:0.66645\n",
            "[6]\tvalidation_0-auc:0.677082\n",
            "[7]\tvalidation_0-auc:0.678138\n",
            "[8]\tvalidation_0-auc:0.677912\n",
            "[9]\tvalidation_0-auc:0.67589\n",
            "[10]\tvalidation_0-auc:0.676233\n",
            "[11]\tvalidation_0-auc:0.674293\n",
            "[12]\tvalidation_0-auc:0.681405\n",
            "[13]\tvalidation_0-auc:0.68006\n",
            "[14]\tvalidation_0-auc:0.67793\n",
            "[15]\tvalidation_0-auc:0.677975\n",
            "[16]\tvalidation_0-auc:0.676901\n",
            "[17]\tvalidation_0-auc:0.680448\n",
            "[18]\tvalidation_0-auc:0.680096\n",
            "[19]\tvalidation_0-auc:0.678499\n",
            "[20]\tvalidation_0-auc:0.678472\n",
            "[21]\tvalidation_0-auc:0.679121\n",
            "[22]\tvalidation_0-auc:0.678544\n",
            "[23]\tvalidation_0-auc:0.679491\n",
            "[24]\tvalidation_0-auc:0.679148\n",
            "[25]\tvalidation_0-auc:0.676197\n",
            "[26]\tvalidation_0-auc:0.674879\n",
            "[27]\tvalidation_0-auc:0.674726\n",
            "[28]\tvalidation_0-auc:0.675132\n",
            "[29]\tvalidation_0-auc:0.674185\n",
            "[30]\tvalidation_0-auc:0.673941\n",
            "[31]\tvalidation_0-auc:0.673661\n",
            "[32]\tvalidation_0-auc:0.672957\n",
            "[33]\tvalidation_0-auc:0.672343\n",
            "[34]\tvalidation_0-auc:0.672777\n",
            "[35]\tvalidation_0-auc:0.670728\n",
            "[36]\tvalidation_0-auc:0.670186\n",
            "[37]\tvalidation_0-auc:0.669437\n",
            "[38]\tvalidation_0-auc:0.670223\n",
            "[39]\tvalidation_0-auc:0.669356\n",
            "[40]\tvalidation_0-auc:0.668598\n",
            "[41]\tvalidation_0-auc:0.668869\n",
            "[42]\tvalidation_0-auc:0.668905\n",
            "[43]\tvalidation_0-auc:0.66886\n",
            "[44]\tvalidation_0-auc:0.668481\n",
            "[45]\tvalidation_0-auc:0.668499\n",
            "[46]\tvalidation_0-auc:0.668751\n",
            "[47]\tvalidation_0-auc:0.66895\n",
            "[48]\tvalidation_0-auc:0.669076\n",
            "[49]\tvalidation_0-auc:0.667605\n",
            "[50]\tvalidation_0-auc:0.667641\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.684988\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6684 - accuracy: 0.6012 - val_loss: 0.6469 - val_accuracy: 0.6433\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6134 - accuracy: 0.6836 - val_loss: 0.6526 - val_accuracy: 0.6521\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5671 - accuracy: 0.7069 - val_loss: 0.6726 - val_accuracy: 0.6280\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5540 - accuracy: 0.7193 - val_loss: 0.6302 - val_accuracy: 0.6915\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5528 - accuracy: 0.7323 - val_loss: 0.6357 - val_accuracy: 0.6565\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 2s 13ms/step - loss: 0.6222 - accuracy: 0.6589 - val_loss: 0.6782 - val_accuracy: 0.6039\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5428 - accuracy: 0.7358 - val_loss: 0.7048 - val_accuracy: 0.5427\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5324 - accuracy: 0.7289 - val_loss: 0.6467 - val_accuracy: 0.6521\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5291 - accuracy: 0.7406 - val_loss: 0.6485 - val_accuracy: 0.6389\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5066 - accuracy: 0.7502 - val_loss: 0.6536 - val_accuracy: 0.6039\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.599647\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.613114\n",
            "[2]\tvalidation_0-auc:0.628226\n",
            "[3]\tvalidation_0-auc:0.620521\n",
            "[4]\tvalidation_0-auc:0.627917\n",
            "[5]\tvalidation_0-auc:0.641354\n",
            "[6]\tvalidation_0-auc:0.636906\n",
            "[7]\tvalidation_0-auc:0.641139\n",
            "[8]\tvalidation_0-auc:0.644313\n",
            "[9]\tvalidation_0-auc:0.643676\n",
            "[10]\tvalidation_0-auc:0.638632\n",
            "[11]\tvalidation_0-auc:0.642063\n",
            "[12]\tvalidation_0-auc:0.648874\n",
            "[13]\tvalidation_0-auc:0.644138\n",
            "[14]\tvalidation_0-auc:0.648278\n",
            "[15]\tvalidation_0-auc:0.648895\n",
            "[16]\tvalidation_0-auc:0.648864\n",
            "[17]\tvalidation_0-auc:0.645351\n",
            "[18]\tvalidation_0-auc:0.646296\n",
            "[19]\tvalidation_0-auc:0.645597\n",
            "[20]\tvalidation_0-auc:0.648895\n",
            "[21]\tvalidation_0-auc:0.647929\n",
            "[22]\tvalidation_0-auc:0.646306\n",
            "[23]\tvalidation_0-auc:0.649942\n",
            "[24]\tvalidation_0-auc:0.644847\n",
            "[25]\tvalidation_0-auc:0.643419\n",
            "[26]\tvalidation_0-auc:0.640163\n",
            "[27]\tvalidation_0-auc:0.638735\n",
            "[28]\tvalidation_0-auc:0.639516\n",
            "[29]\tvalidation_0-auc:0.634924\n",
            "[30]\tvalidation_0-auc:0.635067\n",
            "[31]\tvalidation_0-auc:0.63291\n",
            "[32]\tvalidation_0-auc:0.633794\n",
            "[33]\tvalidation_0-auc:0.633609\n",
            "[34]\tvalidation_0-auc:0.62991\n",
            "[35]\tvalidation_0-auc:0.63104\n",
            "[36]\tvalidation_0-auc:0.633187\n",
            "[37]\tvalidation_0-auc:0.633013\n",
            "[38]\tvalidation_0-auc:0.634461\n",
            "[39]\tvalidation_0-auc:0.630979\n",
            "[40]\tvalidation_0-auc:0.630773\n",
            "[41]\tvalidation_0-auc:0.632592\n",
            "[42]\tvalidation_0-auc:0.632797\n",
            "[43]\tvalidation_0-auc:0.633229\n",
            "[44]\tvalidation_0-auc:0.633639\n",
            "[45]\tvalidation_0-auc:0.634934\n",
            "[46]\tvalidation_0-auc:0.635139\n",
            "[47]\tvalidation_0-auc:0.635026\n",
            "[48]\tvalidation_0-auc:0.636516\n",
            "[49]\tvalidation_0-auc:0.635406\n",
            "[50]\tvalidation_0-auc:0.633044\n",
            "[51]\tvalidation_0-auc:0.63103\n",
            "[52]\tvalidation_0-auc:0.630681\n",
            "[53]\tvalidation_0-auc:0.627373\n",
            "[54]\tvalidation_0-auc:0.627568\n",
            "[55]\tvalidation_0-auc:0.628236\n",
            "[56]\tvalidation_0-auc:0.630866\n",
            "[57]\tvalidation_0-auc:0.631914\n",
            "[58]\tvalidation_0-auc:0.632468\n",
            "[59]\tvalidation_0-auc:0.630506\n",
            "[60]\tvalidation_0-auc:0.632972\n",
            "[61]\tvalidation_0-auc:0.633074\n",
            "[62]\tvalidation_0-auc:0.632848\n",
            "[63]\tvalidation_0-auc:0.631102\n",
            "[64]\tvalidation_0-auc:0.629849\n",
            "[65]\tvalidation_0-auc:0.630095\n",
            "[66]\tvalidation_0-auc:0.630362\n",
            "[67]\tvalidation_0-auc:0.632078\n",
            "[68]\tvalidation_0-auc:0.630886\n",
            "[69]\tvalidation_0-auc:0.629962\n",
            "[70]\tvalidation_0-auc:0.630999\n",
            "[71]\tvalidation_0-auc:0.631657\n",
            "[72]\tvalidation_0-auc:0.632396\n",
            "[73]\tvalidation_0-auc:0.631862\n",
            "Stopping. Best iteration:\n",
            "[23]\tvalidation_0-auc:0.649942\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.2     | 0.6693877551020408 | 0.5336322869955157 |  0.672316384180791  |        0.595        |\n",
            "|     GRU 0.2      | 0.6387755102040816 |        0.5         |  0.7175141242937854 |  0.5893271461716937 |\n",
            "|   XGBoost 0.2    | 0.6346938775510204 | 0.4955357142857143 |  0.6271186440677966 |  0.5536159600997507 |\n",
            "|    Logreg 0.2    | 0.6775510204081633 | 0.5487179487179488 |  0.6045197740112994 |  0.575268817204301  |\n",
            "|     SVM 0.2      | 0.6530612244897959 | 0.5169082125603864 |  0.6045197740112994 |  0.5572916666666666 |\n",
            "|  LSTM beta 0.2   | 0.6564551422319475 | 0.5454545454545454 |  0.4260355029585799 | 0.47840531561461797 |\n",
            "|   GRU beta 0.2   | 0.6039387308533917 |       0.476        |  0.7041420118343196 |  0.568019093078759  |\n",
            "| XGBoost beta 0.2 | 0.6389496717724289 | 0.5095238095238095 |  0.6331360946745562 |  0.5646437994722955 |\n",
            "| logreg beta 0.2  | 0.6345733041575492 | 0.5047619047619047 |  0.6272189349112426 |  0.5593667546174141 |\n",
            "|   svm beta 0.2   | 0.6258205689277899 | 0.4936708860759494 | 0.46153846153846156 |  0.4770642201834862 |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6777 - accuracy: 0.5866 - val_loss: 0.6387 - val_accuracy: 0.6510\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6136 - accuracy: 0.6591 - val_loss: 0.6097 - val_accuracy: 0.7286\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5921 - accuracy: 0.6879 - val_loss: 0.6250 - val_accuracy: 0.6959\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5721 - accuracy: 0.7107 - val_loss: 0.6077 - val_accuracy: 0.7184\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5521 - accuracy: 0.7107 - val_loss: 0.6654 - val_accuracy: 0.6041\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6396 - accuracy: 0.6141 - val_loss: 0.6126 - val_accuracy: 0.7286\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5600 - accuracy: 0.7054 - val_loss: 0.5972 - val_accuracy: 0.7020\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5490 - accuracy: 0.7181 - val_loss: 0.5979 - val_accuracy: 0.6918\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5417 - accuracy: 0.7161 - val_loss: 0.5993 - val_accuracy: 0.7122\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5355 - accuracy: 0.7329 - val_loss: 0.5841 - val_accuracy: 0.7449\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.677632\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.673508\n",
            "[2]\tvalidation_0-auc:0.671387\n",
            "[3]\tvalidation_0-auc:0.673941\n",
            "[4]\tvalidation_0-auc:0.669997\n",
            "[5]\tvalidation_0-auc:0.670475\n",
            "[6]\tvalidation_0-auc:0.679338\n",
            "[7]\tvalidation_0-auc:0.67645\n",
            "[8]\tvalidation_0-auc:0.680304\n",
            "[9]\tvalidation_0-auc:0.680313\n",
            "[10]\tvalidation_0-auc:0.679961\n",
            "[11]\tvalidation_0-auc:0.680177\n",
            "[12]\tvalidation_0-auc:0.682858\n",
            "[13]\tvalidation_0-auc:0.683914\n",
            "[14]\tvalidation_0-auc:0.681305\n",
            "[15]\tvalidation_0-auc:0.682939\n",
            "[16]\tvalidation_0-auc:0.684248\n",
            "[17]\tvalidation_0-auc:0.682957\n",
            "[18]\tvalidation_0-auc:0.684076\n",
            "[19]\tvalidation_0-auc:0.685295\n",
            "[20]\tvalidation_0-auc:0.685222\n",
            "[21]\tvalidation_0-auc:0.684275\n",
            "[22]\tvalidation_0-auc:0.682749\n",
            "[23]\tvalidation_0-auc:0.683896\n",
            "[24]\tvalidation_0-auc:0.683914\n",
            "[25]\tvalidation_0-auc:0.68348\n",
            "[26]\tvalidation_0-auc:0.684933\n",
            "[27]\tvalidation_0-auc:0.685195\n",
            "[28]\tvalidation_0-auc:0.685033\n",
            "[29]\tvalidation_0-auc:0.685132\n",
            "[30]\tvalidation_0-auc:0.685367\n",
            "[31]\tvalidation_0-auc:0.684789\n",
            "[32]\tvalidation_0-auc:0.685204\n",
            "[33]\tvalidation_0-auc:0.685033\n",
            "[34]\tvalidation_0-auc:0.68422\n",
            "[35]\tvalidation_0-auc:0.682903\n",
            "[36]\tvalidation_0-auc:0.682632\n",
            "[37]\tvalidation_0-auc:0.683291\n",
            "[38]\tvalidation_0-auc:0.683887\n",
            "[39]\tvalidation_0-auc:0.683237\n",
            "[40]\tvalidation_0-auc:0.683092\n",
            "[41]\tvalidation_0-auc:0.682506\n",
            "[42]\tvalidation_0-auc:0.682343\n",
            "[43]\tvalidation_0-auc:0.682722\n",
            "[44]\tvalidation_0-auc:0.682271\n",
            "[45]\tvalidation_0-auc:0.681305\n",
            "[46]\tvalidation_0-auc:0.68154\n",
            "[47]\tvalidation_0-auc:0.681341\n",
            "[48]\tvalidation_0-auc:0.679681\n",
            "[49]\tvalidation_0-auc:0.680268\n",
            "[50]\tvalidation_0-auc:0.680177\n",
            "[51]\tvalidation_0-auc:0.679798\n",
            "[52]\tvalidation_0-auc:0.679365\n",
            "[53]\tvalidation_0-auc:0.678517\n",
            "[54]\tvalidation_0-auc:0.679293\n",
            "[55]\tvalidation_0-auc:0.680015\n",
            "[56]\tvalidation_0-auc:0.679654\n",
            "[57]\tvalidation_0-auc:0.678688\n",
            "[58]\tvalidation_0-auc:0.678056\n",
            "[59]\tvalidation_0-auc:0.678291\n",
            "[60]\tvalidation_0-auc:0.677912\n",
            "[61]\tvalidation_0-auc:0.677713\n",
            "[62]\tvalidation_0-auc:0.67849\n",
            "[63]\tvalidation_0-auc:0.678679\n",
            "[64]\tvalidation_0-auc:0.677524\n",
            "[65]\tvalidation_0-auc:0.677235\n",
            "[66]\tvalidation_0-auc:0.675701\n",
            "[67]\tvalidation_0-auc:0.67608\n",
            "[68]\tvalidation_0-auc:0.676739\n",
            "[69]\tvalidation_0-auc:0.67654\n",
            "[70]\tvalidation_0-auc:0.6758\n",
            "[71]\tvalidation_0-auc:0.675439\n",
            "[72]\tvalidation_0-auc:0.675105\n",
            "[73]\tvalidation_0-auc:0.674564\n",
            "[74]\tvalidation_0-auc:0.674347\n",
            "[75]\tvalidation_0-auc:0.67404\n",
            "[76]\tvalidation_0-auc:0.673824\n",
            "[77]\tvalidation_0-auc:0.674419\n",
            "[78]\tvalidation_0-auc:0.674852\n",
            "[79]\tvalidation_0-auc:0.674672\n",
            "[80]\tvalidation_0-auc:0.673787\n",
            "Stopping. Best iteration:\n",
            "[30]\tvalidation_0-auc:0.685367\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6744 - accuracy: 0.5841 - val_loss: 0.6555 - val_accuracy: 0.6783\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6459 - accuracy: 0.6307 - val_loss: 0.6397 - val_accuracy: 0.6761\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5841 - accuracy: 0.7111 - val_loss: 0.6024 - val_accuracy: 0.6893\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5719 - accuracy: 0.7014 - val_loss: 0.6311 - val_accuracy: 0.6324\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5565 - accuracy: 0.7255 - val_loss: 0.6084 - val_accuracy: 0.6740\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6390 - accuracy: 0.6404 - val_loss: 0.6484 - val_accuracy: 0.6280\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5602 - accuracy: 0.7069 - val_loss: 0.5936 - val_accuracy: 0.7002\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5530 - accuracy: 0.7282 - val_loss: 0.6179 - val_accuracy: 0.6608\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5539 - accuracy: 0.7248 - val_loss: 0.6149 - val_accuracy: 0.6608\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5388 - accuracy: 0.7461 - val_loss: 0.6028 - val_accuracy: 0.7002\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.54708\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.575721\n",
            "[2]\tvalidation_0-auc:0.609622\n",
            "[3]\tvalidation_0-auc:0.604413\n",
            "[4]\tvalidation_0-auc:0.628143\n",
            "[5]\tvalidation_0-auc:0.638447\n",
            "[6]\tvalidation_0-auc:0.624743\n",
            "[7]\tvalidation_0-auc:0.629602\n",
            "[8]\tvalidation_0-auc:0.637533\n",
            "[9]\tvalidation_0-auc:0.635725\n",
            "[10]\tvalidation_0-auc:0.641344\n",
            "[11]\tvalidation_0-auc:0.640594\n",
            "[12]\tvalidation_0-auc:0.647693\n",
            "[13]\tvalidation_0-auc:0.656959\n",
            "[14]\tvalidation_0-auc:0.653209\n",
            "[15]\tvalidation_0-auc:0.65325\n",
            "[16]\tvalidation_0-auc:0.656887\n",
            "[17]\tvalidation_0-auc:0.658818\n",
            "[18]\tvalidation_0-auc:0.662424\n",
            "[19]\tvalidation_0-auc:0.66341\n",
            "[20]\tvalidation_0-auc:0.667057\n",
            "[21]\tvalidation_0-auc:0.665835\n",
            "[22]\tvalidation_0-auc:0.666379\n",
            "[23]\tvalidation_0-auc:0.671084\n",
            "[24]\tvalidation_0-auc:0.669461\n",
            "[25]\tvalidation_0-auc:0.670067\n",
            "[26]\tvalidation_0-auc:0.669389\n",
            "[27]\tvalidation_0-auc:0.669327\n",
            "[28]\tvalidation_0-auc:0.665434\n",
            "[29]\tvalidation_0-auc:0.668855\n",
            "[30]\tvalidation_0-auc:0.667807\n",
            "[31]\tvalidation_0-auc:0.666461\n",
            "[32]\tvalidation_0-auc:0.669122\n",
            "[33]\tvalidation_0-auc:0.668125\n",
            "[34]\tvalidation_0-auc:0.666605\n",
            "[35]\tvalidation_0-auc:0.666009\n",
            "[36]\tvalidation_0-auc:0.666348\n",
            "[37]\tvalidation_0-auc:0.666759\n",
            "[38]\tvalidation_0-auc:0.666533\n",
            "[39]\tvalidation_0-auc:0.666646\n",
            "[40]\tvalidation_0-auc:0.667201\n",
            "[41]\tvalidation_0-auc:0.665413\n",
            "[42]\tvalidation_0-auc:0.667776\n",
            "[43]\tvalidation_0-auc:0.666646\n",
            "[44]\tvalidation_0-auc:0.667139\n",
            "[45]\tvalidation_0-auc:0.665711\n",
            "[46]\tvalidation_0-auc:0.66793\n",
            "[47]\tvalidation_0-auc:0.6698\n",
            "[48]\tvalidation_0-auc:0.672327\n",
            "[49]\tvalidation_0-auc:0.671793\n",
            "[50]\tvalidation_0-auc:0.670478\n",
            "[51]\tvalidation_0-auc:0.670447\n",
            "[52]\tvalidation_0-auc:0.670139\n",
            "[53]\tvalidation_0-auc:0.670858\n",
            "[54]\tvalidation_0-auc:0.6717\n",
            "[55]\tvalidation_0-auc:0.673529\n",
            "[56]\tvalidation_0-auc:0.673837\n",
            "[57]\tvalidation_0-auc:0.674721\n",
            "[58]\tvalidation_0-auc:0.673549\n",
            "[59]\tvalidation_0-auc:0.672235\n",
            "[60]\tvalidation_0-auc:0.673056\n",
            "[61]\tvalidation_0-auc:0.674803\n",
            "[62]\tvalidation_0-auc:0.674988\n",
            "[63]\tvalidation_0-auc:0.672995\n",
            "[64]\tvalidation_0-auc:0.672461\n",
            "[65]\tvalidation_0-auc:0.669831\n",
            "[66]\tvalidation_0-auc:0.66942\n",
            "[67]\tvalidation_0-auc:0.670632\n",
            "[68]\tvalidation_0-auc:0.669317\n",
            "[69]\tvalidation_0-auc:0.66942\n",
            "[70]\tvalidation_0-auc:0.671618\n",
            "[71]\tvalidation_0-auc:0.669564\n",
            "[72]\tvalidation_0-auc:0.670406\n",
            "[73]\tvalidation_0-auc:0.670519\n",
            "[74]\tvalidation_0-auc:0.673015\n",
            "[75]\tvalidation_0-auc:0.671063\n",
            "[76]\tvalidation_0-auc:0.669954\n",
            "[77]\tvalidation_0-auc:0.669769\n",
            "[78]\tvalidation_0-auc:0.671105\n",
            "[79]\tvalidation_0-auc:0.66905\n",
            "[80]\tvalidation_0-auc:0.670632\n",
            "[81]\tvalidation_0-auc:0.671187\n",
            "[82]\tvalidation_0-auc:0.66869\n",
            "[83]\tvalidation_0-auc:0.666985\n",
            "[84]\tvalidation_0-auc:0.667889\n",
            "[85]\tvalidation_0-auc:0.665629\n",
            "[86]\tvalidation_0-auc:0.664622\n",
            "[87]\tvalidation_0-auc:0.663739\n",
            "[88]\tvalidation_0-auc:0.663451\n",
            "[89]\tvalidation_0-auc:0.664684\n",
            "[90]\tvalidation_0-auc:0.663225\n",
            "[91]\tvalidation_0-auc:0.662979\n",
            "[92]\tvalidation_0-auc:0.661869\n",
            "[93]\tvalidation_0-auc:0.66303\n",
            "[94]\tvalidation_0-auc:0.661551\n",
            "[95]\tvalidation_0-auc:0.661674\n",
            "[96]\tvalidation_0-auc:0.66115\n",
            "[97]\tvalidation_0-auc:0.66002\n",
            "[98]\tvalidation_0-auc:0.660143\n",
            "[99]\tvalidation_0-auc:0.660821\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+---------------------+--------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |        Recall       |      F1 score      |\n",
            "+-------------------+--------------------+---------------------+---------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.6040816326530613 | 0.46863468634686345 |  0.7175141242937854 | 0.5669642857142857 |\n",
            "|      GRU 0.15     | 0.7448979591836735 |  0.7131147540983607 |  0.4915254237288136 | 0.5819397993311036 |\n",
            "|    XGBoost 0.15   | 0.6795918367346939 |  0.554945054945055  |  0.5706214689265536 | 0.5626740947075209 |\n",
            "|    Logreg 0.15    | 0.6959183673469388 |  0.5843373493975904 |  0.5480225988700564 | 0.565597667638484  |\n",
            "|      SVM 0.15     | 0.6979591836734694 |  0.5857988165680473 |  0.559322033898305  | 0.5722543352601156 |\n",
            "|   LSTM beta 0.15  | 0.6739606126914661 |  0.5568181818181818 |  0.5798816568047337 | 0.5681159420289854 |\n",
            "|   GRU beta 0.15   | 0.700218818380744  |  0.6025641025641025 |  0.5562130177514792 | 0.5784615384615385 |\n",
            "| XGBoost beta 0.15 | 0.6717724288840262 |  0.5542857142857143 |  0.5739644970414202 | 0.5639534883720931 |\n",
            "|  logreg beta 0.15 | 0.687089715536105  |  0.5730337078651685 |  0.6035502958579881 | 0.5878962536023056 |\n",
            "|   svm beta 0.15   | 0.6258205689277899 | 0.49382716049382713 | 0.47337278106508873 | 0.4833836858006042 |\n",
            "+-------------------+--------------------+---------------------+---------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "lztUIjcddMmo",
        "outputId": "8dea912e-610a-422b-cb8a-186877af8011"
      },
      "source": [
        "Result_cross.to_csv('AMD_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.611111</td>\n",
              "      <td>0.716327</td>\n",
              "      <td>0.635171</td>\n",
              "      <td>0.661202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.552419</td>\n",
              "      <td>0.679592</td>\n",
              "      <td>0.635731</td>\n",
              "      <td>0.748634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.562771</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.628019</td>\n",
              "      <td>0.710383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.602740</td>\n",
              "      <td>0.718367</td>\n",
              "      <td>0.656716</td>\n",
              "      <td>0.721311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.534188</td>\n",
              "      <td>0.659184</td>\n",
              "      <td>0.599520</td>\n",
              "      <td>0.683060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.515748</td>\n",
              "      <td>0.645514</td>\n",
              "      <td>0.617925</td>\n",
              "      <td>0.770588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.570755</td>\n",
              "      <td>0.693654</td>\n",
              "      <td>0.633508</td>\n",
              "      <td>0.711765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.512397</td>\n",
              "      <td>0.641138</td>\n",
              "      <td>0.601942</td>\n",
              "      <td>0.729412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.580952</td>\n",
              "      <td>0.702407</td>\n",
              "      <td>0.642105</td>\n",
              "      <td>0.717647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.521008</td>\n",
              "      <td>0.649891</td>\n",
              "      <td>0.607843</td>\n",
              "      <td>0.729412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.533632</td>\n",
              "      <td>0.669388</td>\n",
              "      <td>0.595000</td>\n",
              "      <td>0.672316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.638776</td>\n",
              "      <td>0.589327</td>\n",
              "      <td>0.717514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.495536</td>\n",
              "      <td>0.634694</td>\n",
              "      <td>0.553616</td>\n",
              "      <td>0.627119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.548718</td>\n",
              "      <td>0.677551</td>\n",
              "      <td>0.575269</td>\n",
              "      <td>0.604520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.516908</td>\n",
              "      <td>0.653061</td>\n",
              "      <td>0.557292</td>\n",
              "      <td>0.604520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.656455</td>\n",
              "      <td>0.478405</td>\n",
              "      <td>0.426036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.476000</td>\n",
              "      <td>0.603939</td>\n",
              "      <td>0.568019</td>\n",
              "      <td>0.704142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.509524</td>\n",
              "      <td>0.638950</td>\n",
              "      <td>0.564644</td>\n",
              "      <td>0.633136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.504762</td>\n",
              "      <td>0.634573</td>\n",
              "      <td>0.559367</td>\n",
              "      <td>0.627219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.493671</td>\n",
              "      <td>0.625821</td>\n",
              "      <td>0.477064</td>\n",
              "      <td>0.461538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.468635</td>\n",
              "      <td>0.604082</td>\n",
              "      <td>0.566964</td>\n",
              "      <td>0.717514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.713115</td>\n",
              "      <td>0.744898</td>\n",
              "      <td>0.581940</td>\n",
              "      <td>0.491525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.554945</td>\n",
              "      <td>0.679592</td>\n",
              "      <td>0.562674</td>\n",
              "      <td>0.570621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.584337</td>\n",
              "      <td>0.695918</td>\n",
              "      <td>0.565598</td>\n",
              "      <td>0.548023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.585799</td>\n",
              "      <td>0.697959</td>\n",
              "      <td>0.572254</td>\n",
              "      <td>0.559322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.556818</td>\n",
              "      <td>0.673961</td>\n",
              "      <td>0.568116</td>\n",
              "      <td>0.579882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.602564</td>\n",
              "      <td>0.700219</td>\n",
              "      <td>0.578462</td>\n",
              "      <td>0.556213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.554286</td>\n",
              "      <td>0.671772</td>\n",
              "      <td>0.563953</td>\n",
              "      <td>0.573964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.573034</td>\n",
              "      <td>0.687090</td>\n",
              "      <td>0.587896</td>\n",
              "      <td>0.603550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.493827</td>\n",
              "      <td>0.625821</td>\n",
              "      <td>0.483384</td>\n",
              "      <td>0.473373</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  AMD  0.611111  0.716327  0.635171  0.661202\n",
              "1            GRU 0.1  AMD  0.552419  0.679592  0.635731  0.748634\n",
              "2        XGBoost 0.1  AMD  0.562771  0.685714  0.628019  0.710383\n",
              "3         Logreg 0.1  AMD  0.602740  0.718367  0.656716  0.721311\n",
              "4            SVM 0.1  AMD  0.534188  0.659184  0.599520  0.683060\n",
              "5      LSTM beta 0.1  AMD  0.515748  0.645514  0.617925  0.770588\n",
              "6       GRU beta 0.1  AMD  0.570755  0.693654  0.633508  0.711765\n",
              "7   XGBoost beta 0.1  AMD  0.512397  0.641138  0.601942  0.729412\n",
              "8    logreg beta 0.1  AMD  0.580952  0.702407  0.642105  0.717647\n",
              "9       svm beta 0.1  AMD  0.521008  0.649891  0.607843  0.729412\n",
              "0           LSTM 0.2  AMD  0.533632  0.669388  0.595000  0.672316\n",
              "1            GRU 0.2  AMD  0.500000  0.638776  0.589327  0.717514\n",
              "2        XGBoost 0.2  AMD  0.495536  0.634694  0.553616  0.627119\n",
              "3         Logreg 0.2  AMD  0.548718  0.677551  0.575269  0.604520\n",
              "4            SVM 0.2  AMD  0.516908  0.653061  0.557292  0.604520\n",
              "5      LSTM beta 0.2  AMD  0.545455  0.656455  0.478405  0.426036\n",
              "6       GRU beta 0.2  AMD  0.476000  0.603939  0.568019  0.704142\n",
              "7   XGBoost beta 0.2  AMD  0.509524  0.638950  0.564644  0.633136\n",
              "8    logreg beta 0.2  AMD  0.504762  0.634573  0.559367  0.627219\n",
              "9       svm beta 0.2  AMD  0.493671  0.625821  0.477064  0.461538\n",
              "0          LSTM 0.15  AMD  0.468635  0.604082  0.566964  0.717514\n",
              "1           GRU 0.15  AMD  0.713115  0.744898  0.581940  0.491525\n",
              "2       XGBoost 0.15  AMD  0.554945  0.679592  0.562674  0.570621\n",
              "3        Logreg 0.15  AMD  0.584337  0.695918  0.565598  0.548023\n",
              "4           SVM 0.15  AMD  0.585799  0.697959  0.572254  0.559322\n",
              "5     LSTM beta 0.15  AMD  0.556818  0.673961  0.568116  0.579882\n",
              "6      GRU beta 0.15  AMD  0.602564  0.700219  0.578462  0.556213\n",
              "7  XGBoost beta 0.15  AMD  0.554286  0.671772  0.563953  0.573964\n",
              "8   logreg beta 0.15  AMD  0.573034  0.687090  0.587896  0.603550\n",
              "9      svm beta 0.15  AMD  0.493827  0.625821  0.483384  0.473373"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moW7IK21LwrN"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJFEttzELwrP"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGHuEhB-LwrP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f927023f-0cce-43fd-882a-9719c9ecf8d7"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"AMD\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6909 - accuracy: 0.5376 - val_loss: 0.6635 - val_accuracy: 0.6265\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6731 - accuracy: 0.5940 - val_loss: 0.6398 - val_accuracy: 0.6857\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6316 - accuracy: 0.6597 - val_loss: 0.6079 - val_accuracy: 0.7531\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5731 - accuracy: 0.7067 - val_loss: 0.6168 - val_accuracy: 0.6367\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5594 - accuracy: 0.7141 - val_loss: 0.5843 - val_accuracy: 0.6878\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6679 - accuracy: 0.5919 - val_loss: 0.6610 - val_accuracy: 0.5531\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5583 - accuracy: 0.7315 - val_loss: 0.5490 - val_accuracy: 0.7469\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5492 - accuracy: 0.7369 - val_loss: 0.5716 - val_accuracy: 0.7102\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5349 - accuracy: 0.7336 - val_loss: 0.6088 - val_accuracy: 0.6551\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5296 - accuracy: 0.7483 - val_loss: 0.5861 - val_accuracy: 0.6959\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.739351\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.741701\n",
            "[2]\tvalidation_0-auc:0.734661\n",
            "[3]\tvalidation_0-auc:0.730932\n",
            "[4]\tvalidation_0-auc:0.726456\n",
            "[5]\tvalidation_0-auc:0.733905\n",
            "[6]\tvalidation_0-auc:0.734154\n",
            "[7]\tvalidation_0-auc:0.7375\n",
            "[8]\tvalidation_0-auc:0.742217\n",
            "[9]\tvalidation_0-auc:0.745296\n",
            "[10]\tvalidation_0-auc:0.747521\n",
            "[11]\tvalidation_0-auc:0.746525\n",
            "[12]\tvalidation_0-auc:0.744923\n",
            "[13]\tvalidation_0-auc:0.748598\n",
            "[14]\tvalidation_0-auc:0.749408\n",
            "[15]\tvalidation_0-auc:0.74996\n",
            "[16]\tvalidation_0-auc:0.75182\n",
            "[17]\tvalidation_0-auc:0.750672\n",
            "[18]\tvalidation_0-auc:0.751606\n",
            "[19]\tvalidation_0-auc:0.752283\n",
            "[20]\tvalidation_0-auc:0.751615\n",
            "[21]\tvalidation_0-auc:0.755309\n",
            "[22]\tvalidation_0-auc:0.755166\n",
            "[23]\tvalidation_0-auc:0.75578\n",
            "[24]\tvalidation_0-auc:0.7574\n",
            "[25]\tvalidation_0-auc:0.757026\n",
            "[26]\tvalidation_0-auc:0.756973\n",
            "[27]\tvalidation_0-auc:0.758059\n",
            "[28]\tvalidation_0-auc:0.757845\n",
            "[29]\tvalidation_0-auc:0.757774\n",
            "[30]\tvalidation_0-auc:0.757934\n",
            "[31]\tvalidation_0-auc:0.756492\n",
            "[32]\tvalidation_0-auc:0.754072\n",
            "[33]\tvalidation_0-auc:0.754534\n",
            "[34]\tvalidation_0-auc:0.754143\n",
            "[35]\tvalidation_0-auc:0.75384\n",
            "[36]\tvalidation_0-auc:0.755273\n",
            "[37]\tvalidation_0-auc:0.754205\n",
            "[38]\tvalidation_0-auc:0.753146\n",
            "[39]\tvalidation_0-auc:0.752755\n",
            "[40]\tvalidation_0-auc:0.752772\n",
            "[41]\tvalidation_0-auc:0.753858\n",
            "[42]\tvalidation_0-auc:0.753769\n",
            "[43]\tvalidation_0-auc:0.754196\n",
            "[44]\tvalidation_0-auc:0.753555\n",
            "[45]\tvalidation_0-auc:0.753413\n",
            "[46]\tvalidation_0-auc:0.753289\n",
            "[47]\tvalidation_0-auc:0.753315\n",
            "[48]\tvalidation_0-auc:0.752674\n",
            "[49]\tvalidation_0-auc:0.751277\n",
            "[50]\tvalidation_0-auc:0.751927\n",
            "[51]\tvalidation_0-auc:0.751553\n",
            "[52]\tvalidation_0-auc:0.751989\n",
            "[53]\tvalidation_0-auc:0.751526\n",
            "[54]\tvalidation_0-auc:0.751651\n",
            "[55]\tvalidation_0-auc:0.7519\n",
            "[56]\tvalidation_0-auc:0.750458\n",
            "[57]\tvalidation_0-auc:0.750494\n",
            "[58]\tvalidation_0-auc:0.75012\n",
            "[59]\tvalidation_0-auc:0.750058\n",
            "[60]\tvalidation_0-auc:0.751535\n",
            "[61]\tvalidation_0-auc:0.751055\n",
            "[62]\tvalidation_0-auc:0.751126\n",
            "[63]\tvalidation_0-auc:0.751197\n",
            "[64]\tvalidation_0-auc:0.750378\n",
            "[65]\tvalidation_0-auc:0.749292\n",
            "[66]\tvalidation_0-auc:0.748741\n",
            "[67]\tvalidation_0-auc:0.748491\n",
            "[68]\tvalidation_0-auc:0.747868\n",
            "[69]\tvalidation_0-auc:0.748313\n",
            "[70]\tvalidation_0-auc:0.747424\n",
            "[71]\tvalidation_0-auc:0.748616\n",
            "[72]\tvalidation_0-auc:0.747993\n",
            "[73]\tvalidation_0-auc:0.747183\n",
            "[74]\tvalidation_0-auc:0.746685\n",
            "[75]\tvalidation_0-auc:0.746115\n",
            "[76]\tvalidation_0-auc:0.747148\n",
            "[77]\tvalidation_0-auc:0.747432\n",
            "Stopping. Best iteration:\n",
            "[27]\tvalidation_0-auc:0.758059\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6934 - accuracy: 0.5381 - val_loss: 0.6942 - val_accuracy: 0.3829\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6895 - accuracy: 0.5395 - val_loss: 0.6646 - val_accuracy: 0.6521\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6600 - accuracy: 0.5992 - val_loss: 0.6231 - val_accuracy: 0.7352\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6178 - accuracy: 0.6733 - val_loss: 0.6033 - val_accuracy: 0.6849\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5953 - accuracy: 0.6850 - val_loss: 0.6196 - val_accuracy: 0.6258\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6864 - accuracy: 0.5532 - val_loss: 0.6385 - val_accuracy: 0.7002\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6352 - accuracy: 0.6479 - val_loss: 0.5870 - val_accuracy: 0.7352\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5876 - accuracy: 0.6994 - val_loss: 0.5733 - val_accuracy: 0.7352\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5909 - accuracy: 0.7035 - val_loss: 0.5866 - val_accuracy: 0.7396\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5708 - accuracy: 0.7207 - val_loss: 0.5841 - val_accuracy: 0.7199\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.663005\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.672382\n",
            "[2]\tvalidation_0-auc:0.687128\n",
            "[3]\tvalidation_0-auc:0.673837\n",
            "[4]\tvalidation_0-auc:0.686954\n",
            "[5]\tvalidation_0-auc:0.687682\n",
            "[6]\tvalidation_0-auc:0.672371\n",
            "[7]\tvalidation_0-auc:0.685581\n",
            "[8]\tvalidation_0-auc:0.685243\n",
            "[9]\tvalidation_0-auc:0.682609\n",
            "[10]\tvalidation_0-auc:0.681943\n",
            "[11]\tvalidation_0-auc:0.691013\n",
            "[12]\tvalidation_0-auc:0.692519\n",
            "[13]\tvalidation_0-auc:0.697223\n",
            "[14]\tvalidation_0-auc:0.696085\n",
            "[15]\tvalidation_0-auc:0.69836\n",
            "[16]\tvalidation_0-auc:0.700881\n",
            "[17]\tvalidation_0-auc:0.701527\n",
            "[18]\tvalidation_0-auc:0.700994\n",
            "[19]\tvalidation_0-auc:0.695142\n",
            "[20]\tvalidation_0-auc:0.695583\n",
            "[21]\tvalidation_0-auc:0.696587\n",
            "[22]\tvalidation_0-auc:0.696782\n",
            "[23]\tvalidation_0-auc:0.697069\n",
            "[24]\tvalidation_0-auc:0.69834\n",
            "[25]\tvalidation_0-auc:0.702244\n",
            "[26]\tvalidation_0-auc:0.701875\n",
            "[27]\tvalidation_0-auc:0.701465\n",
            "[28]\tvalidation_0-auc:0.703638\n",
            "[29]\tvalidation_0-auc:0.703802\n",
            "[30]\tvalidation_0-auc:0.704417\n",
            "[31]\tvalidation_0-auc:0.704837\n",
            "[32]\tvalidation_0-auc:0.704284\n",
            "[33]\tvalidation_0-auc:0.704837\n",
            "[34]\tvalidation_0-auc:0.704817\n",
            "[35]\tvalidation_0-auc:0.704273\n",
            "[36]\tvalidation_0-auc:0.700277\n",
            "[37]\tvalidation_0-auc:0.70206\n",
            "[38]\tvalidation_0-auc:0.703064\n",
            "[39]\tvalidation_0-auc:0.700215\n",
            "[40]\tvalidation_0-auc:0.698934\n",
            "[41]\tvalidation_0-auc:0.698668\n",
            "[42]\tvalidation_0-auc:0.699119\n",
            "[43]\tvalidation_0-auc:0.698955\n",
            "[44]\tvalidation_0-auc:0.697868\n",
            "[45]\tvalidation_0-auc:0.697069\n",
            "[46]\tvalidation_0-auc:0.696557\n",
            "[47]\tvalidation_0-auc:0.697336\n",
            "[48]\tvalidation_0-auc:0.696792\n",
            "[49]\tvalidation_0-auc:0.69588\n",
            "[50]\tvalidation_0-auc:0.694794\n",
            "[51]\tvalidation_0-auc:0.690551\n",
            "[52]\tvalidation_0-auc:0.687149\n",
            "[53]\tvalidation_0-auc:0.686903\n",
            "[54]\tvalidation_0-auc:0.686965\n",
            "[55]\tvalidation_0-auc:0.68762\n",
            "[56]\tvalidation_0-auc:0.68883\n",
            "[57]\tvalidation_0-auc:0.68844\n",
            "[58]\tvalidation_0-auc:0.688727\n",
            "[59]\tvalidation_0-auc:0.687333\n",
            "[60]\tvalidation_0-auc:0.687733\n",
            "[61]\tvalidation_0-auc:0.687159\n",
            "[62]\tvalidation_0-auc:0.6872\n",
            "[63]\tvalidation_0-auc:0.686442\n",
            "[64]\tvalidation_0-auc:0.686196\n",
            "[65]\tvalidation_0-auc:0.6847\n",
            "[66]\tvalidation_0-auc:0.684782\n",
            "[67]\tvalidation_0-auc:0.684075\n",
            "[68]\tvalidation_0-auc:0.683296\n",
            "[69]\tvalidation_0-auc:0.68473\n",
            "[70]\tvalidation_0-auc:0.686329\n",
            "[71]\tvalidation_0-auc:0.686514\n",
            "[72]\tvalidation_0-auc:0.685899\n",
            "[73]\tvalidation_0-auc:0.685468\n",
            "[74]\tvalidation_0-auc:0.683788\n",
            "[75]\tvalidation_0-auc:0.683398\n",
            "[76]\tvalidation_0-auc:0.685571\n",
            "[77]\tvalidation_0-auc:0.685243\n",
            "[78]\tvalidation_0-auc:0.684505\n",
            "[79]\tvalidation_0-auc:0.683296\n",
            "[80]\tvalidation_0-auc:0.683378\n",
            "[81]\tvalidation_0-auc:0.683234\n",
            "Stopping. Best iteration:\n",
            "[31]\tvalidation_0-auc:0.704837\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.6877551020408164 |        0.5625       | 0.7377049180327869 | 0.6382978723404255 |\n",
            "|     GRU 0.1      | 0.6959183673469388 |  0.5726495726495726 |  0.73224043715847  | 0.6426858513189447 |\n",
            "|   XGBoost 0.1    | 0.6857142857142857 |  0.5627705627705628 | 0.7103825136612022 | 0.6280193236714976 |\n",
            "|    Logreg 0.1    | 0.7183673469387755 |  0.6027397260273972 | 0.7213114754098361 | 0.6567164179104478 |\n",
            "|     SVM 0.1      | 0.6591836734693878 |  0.5341880341880342 | 0.6830601092896175 | 0.5995203836930456 |\n",
            "|  LSTM beta 0.1   | 0.6258205689277899 | 0.49814126394052044 | 0.788235294117647  | 0.6104783599088838 |\n",
            "|   GRU beta 0.1   | 0.7199124726477024 |  0.6082474226804123 | 0.6941176470588235 | 0.6483516483516484 |\n",
            "| XGBoost beta 0.1 | 0.6411378555798687 |  0.512396694214876  | 0.7294117647058823 | 0.6019417475728155 |\n",
            "| logreg beta 0.1  | 0.7024070021881839 |  0.580952380952381  | 0.7176470588235294 | 0.6421052631578948 |\n",
            "|   svm beta 0.1   | 0.649890590809628  |  0.5210084033613446 | 0.7294117647058823 | 0.6078431372549019 |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6698 - accuracy: 0.5926 - val_loss: 0.6783 - val_accuracy: 0.6041\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6342 - accuracy: 0.6349 - val_loss: 0.7125 - val_accuracy: 0.4449\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6130 - accuracy: 0.6611 - val_loss: 0.6227 - val_accuracy: 0.7306\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5926 - accuracy: 0.6866 - val_loss: 0.6402 - val_accuracy: 0.6816\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5804 - accuracy: 0.7000 - val_loss: 0.6056 - val_accuracy: 0.7306\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6494 - accuracy: 0.6094 - val_loss: 0.6171 - val_accuracy: 0.7143\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5903 - accuracy: 0.6779 - val_loss: 0.7099 - val_accuracy: 0.5469\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5789 - accuracy: 0.6926 - val_loss: 0.6443 - val_accuracy: 0.6673\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5730 - accuracy: 0.6946 - val_loss: 0.6387 - val_accuracy: 0.6449\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5692 - accuracy: 0.6738 - val_loss: 0.6319 - val_accuracy: 0.6306\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.684988\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.669591\n",
            "[2]\tvalidation_0-auc:0.672984\n",
            "[3]\tvalidation_0-auc:0.672361\n",
            "[4]\tvalidation_0-auc:0.669455\n",
            "[5]\tvalidation_0-auc:0.66645\n",
            "[6]\tvalidation_0-auc:0.677082\n",
            "[7]\tvalidation_0-auc:0.678138\n",
            "[8]\tvalidation_0-auc:0.677912\n",
            "[9]\tvalidation_0-auc:0.67589\n",
            "[10]\tvalidation_0-auc:0.676233\n",
            "[11]\tvalidation_0-auc:0.674293\n",
            "[12]\tvalidation_0-auc:0.681405\n",
            "[13]\tvalidation_0-auc:0.68006\n",
            "[14]\tvalidation_0-auc:0.67793\n",
            "[15]\tvalidation_0-auc:0.677975\n",
            "[16]\tvalidation_0-auc:0.676901\n",
            "[17]\tvalidation_0-auc:0.680448\n",
            "[18]\tvalidation_0-auc:0.680096\n",
            "[19]\tvalidation_0-auc:0.678499\n",
            "[20]\tvalidation_0-auc:0.678472\n",
            "[21]\tvalidation_0-auc:0.679121\n",
            "[22]\tvalidation_0-auc:0.678544\n",
            "[23]\tvalidation_0-auc:0.679491\n",
            "[24]\tvalidation_0-auc:0.679148\n",
            "[25]\tvalidation_0-auc:0.676197\n",
            "[26]\tvalidation_0-auc:0.674879\n",
            "[27]\tvalidation_0-auc:0.674726\n",
            "[28]\tvalidation_0-auc:0.675132\n",
            "[29]\tvalidation_0-auc:0.674185\n",
            "[30]\tvalidation_0-auc:0.673941\n",
            "[31]\tvalidation_0-auc:0.673661\n",
            "[32]\tvalidation_0-auc:0.672957\n",
            "[33]\tvalidation_0-auc:0.672343\n",
            "[34]\tvalidation_0-auc:0.672777\n",
            "[35]\tvalidation_0-auc:0.670728\n",
            "[36]\tvalidation_0-auc:0.670186\n",
            "[37]\tvalidation_0-auc:0.669437\n",
            "[38]\tvalidation_0-auc:0.670223\n",
            "[39]\tvalidation_0-auc:0.669356\n",
            "[40]\tvalidation_0-auc:0.668598\n",
            "[41]\tvalidation_0-auc:0.668869\n",
            "[42]\tvalidation_0-auc:0.668905\n",
            "[43]\tvalidation_0-auc:0.66886\n",
            "[44]\tvalidation_0-auc:0.668481\n",
            "[45]\tvalidation_0-auc:0.668499\n",
            "[46]\tvalidation_0-auc:0.668751\n",
            "[47]\tvalidation_0-auc:0.66895\n",
            "[48]\tvalidation_0-auc:0.669076\n",
            "[49]\tvalidation_0-auc:0.667605\n",
            "[50]\tvalidation_0-auc:0.667641\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.684988\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6801 - accuracy: 0.5532 - val_loss: 0.6933 - val_accuracy: 0.5383\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6220 - accuracy: 0.6699 - val_loss: 0.6595 - val_accuracy: 0.6105\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5631 - accuracy: 0.7207 - val_loss: 0.6638 - val_accuracy: 0.6565\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5421 - accuracy: 0.7330 - val_loss: 0.6325 - val_accuracy: 0.6740\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5313 - accuracy: 0.7392 - val_loss: 0.6597 - val_accuracy: 0.6280\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6128 - accuracy: 0.6541 - val_loss: 0.6464 - val_accuracy: 0.6368\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5286 - accuracy: 0.7323 - val_loss: 0.6814 - val_accuracy: 0.6302\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5291 - accuracy: 0.7632 - val_loss: 0.7041 - val_accuracy: 0.6171\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5303 - accuracy: 0.7461 - val_loss: 0.6327 - val_accuracy: 0.6740\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5163 - accuracy: 0.7529 - val_loss: 0.5822 - val_accuracy: 0.7221\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.599647\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.613114\n",
            "[2]\tvalidation_0-auc:0.628226\n",
            "[3]\tvalidation_0-auc:0.620521\n",
            "[4]\tvalidation_0-auc:0.627917\n",
            "[5]\tvalidation_0-auc:0.641354\n",
            "[6]\tvalidation_0-auc:0.636906\n",
            "[7]\tvalidation_0-auc:0.641139\n",
            "[8]\tvalidation_0-auc:0.644313\n",
            "[9]\tvalidation_0-auc:0.643676\n",
            "[10]\tvalidation_0-auc:0.638632\n",
            "[11]\tvalidation_0-auc:0.642063\n",
            "[12]\tvalidation_0-auc:0.648874\n",
            "[13]\tvalidation_0-auc:0.644138\n",
            "[14]\tvalidation_0-auc:0.648278\n",
            "[15]\tvalidation_0-auc:0.648895\n",
            "[16]\tvalidation_0-auc:0.648864\n",
            "[17]\tvalidation_0-auc:0.645351\n",
            "[18]\tvalidation_0-auc:0.646296\n",
            "[19]\tvalidation_0-auc:0.645597\n",
            "[20]\tvalidation_0-auc:0.648895\n",
            "[21]\tvalidation_0-auc:0.647929\n",
            "[22]\tvalidation_0-auc:0.646306\n",
            "[23]\tvalidation_0-auc:0.649942\n",
            "[24]\tvalidation_0-auc:0.644847\n",
            "[25]\tvalidation_0-auc:0.643419\n",
            "[26]\tvalidation_0-auc:0.640163\n",
            "[27]\tvalidation_0-auc:0.638735\n",
            "[28]\tvalidation_0-auc:0.639516\n",
            "[29]\tvalidation_0-auc:0.634924\n",
            "[30]\tvalidation_0-auc:0.635067\n",
            "[31]\tvalidation_0-auc:0.63291\n",
            "[32]\tvalidation_0-auc:0.633794\n",
            "[33]\tvalidation_0-auc:0.633609\n",
            "[34]\tvalidation_0-auc:0.62991\n",
            "[35]\tvalidation_0-auc:0.63104\n",
            "[36]\tvalidation_0-auc:0.633187\n",
            "[37]\tvalidation_0-auc:0.633013\n",
            "[38]\tvalidation_0-auc:0.634461\n",
            "[39]\tvalidation_0-auc:0.630979\n",
            "[40]\tvalidation_0-auc:0.630773\n",
            "[41]\tvalidation_0-auc:0.632592\n",
            "[42]\tvalidation_0-auc:0.632797\n",
            "[43]\tvalidation_0-auc:0.633229\n",
            "[44]\tvalidation_0-auc:0.633639\n",
            "[45]\tvalidation_0-auc:0.634934\n",
            "[46]\tvalidation_0-auc:0.635139\n",
            "[47]\tvalidation_0-auc:0.635026\n",
            "[48]\tvalidation_0-auc:0.636516\n",
            "[49]\tvalidation_0-auc:0.635406\n",
            "[50]\tvalidation_0-auc:0.633044\n",
            "[51]\tvalidation_0-auc:0.63103\n",
            "[52]\tvalidation_0-auc:0.630681\n",
            "[53]\tvalidation_0-auc:0.627373\n",
            "[54]\tvalidation_0-auc:0.627568\n",
            "[55]\tvalidation_0-auc:0.628236\n",
            "[56]\tvalidation_0-auc:0.630866\n",
            "[57]\tvalidation_0-auc:0.631914\n",
            "[58]\tvalidation_0-auc:0.632468\n",
            "[59]\tvalidation_0-auc:0.630506\n",
            "[60]\tvalidation_0-auc:0.632972\n",
            "[61]\tvalidation_0-auc:0.633074\n",
            "[62]\tvalidation_0-auc:0.632848\n",
            "[63]\tvalidation_0-auc:0.631102\n",
            "[64]\tvalidation_0-auc:0.629849\n",
            "[65]\tvalidation_0-auc:0.630095\n",
            "[66]\tvalidation_0-auc:0.630362\n",
            "[67]\tvalidation_0-auc:0.632078\n",
            "[68]\tvalidation_0-auc:0.630886\n",
            "[69]\tvalidation_0-auc:0.629962\n",
            "[70]\tvalidation_0-auc:0.630999\n",
            "[71]\tvalidation_0-auc:0.631657\n",
            "[72]\tvalidation_0-auc:0.632396\n",
            "[73]\tvalidation_0-auc:0.631862\n",
            "Stopping. Best iteration:\n",
            "[23]\tvalidation_0-auc:0.649942\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+---------------------+--------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall       |      F1 score      |\n",
            "+------------------+--------------------+---------------------+---------------------+--------------------+\n",
            "|     LSTM 0.2     | 0.7306122448979592 |  0.6510067114093959 |  0.5480225988700564 | 0.5950920245398773 |\n",
            "|     GRU 0.2      | 0.6306122448979592 |      0.4921875      |  0.711864406779661  | 0.581986143187067  |\n",
            "|   XGBoost 0.2    | 0.6346938775510204 |  0.4955357142857143 |  0.6271186440677966 | 0.5536159600997507 |\n",
            "|    Logreg 0.2    | 0.6775510204081633 |  0.5487179487179488 |  0.6045197740112994 | 0.575268817204301  |\n",
            "|     SVM 0.2      | 0.6530612244897959 |  0.5169082125603864 |  0.6045197740112994 | 0.5572916666666666 |\n",
            "|  LSTM beta 0.2   | 0.6280087527352297 | 0.49748743718592964 |  0.5857988165680473 | 0.5380434782608695 |\n",
            "|   GRU beta 0.2   | 0.7221006564551422 |  0.6381578947368421 |  0.5739644970414202 | 0.604361370716511  |\n",
            "| XGBoost beta 0.2 | 0.6389496717724289 |  0.5095238095238095 |  0.6331360946745562 | 0.5646437994722955 |\n",
            "| logreg beta 0.2  | 0.6345733041575492 |  0.5047619047619047 |  0.6272189349112426 | 0.5593667546174141 |\n",
            "|   svm beta 0.2   | 0.6258205689277899 |  0.4936708860759494 | 0.46153846153846156 | 0.4770642201834862 |\n",
            "+------------------+--------------------+---------------------+---------------------+--------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6799 - accuracy: 0.5960 - val_loss: 0.6467 - val_accuracy: 0.6388\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6216 - accuracy: 0.6604 - val_loss: 0.6214 - val_accuracy: 0.6571\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6122 - accuracy: 0.6651 - val_loss: 0.6146 - val_accuracy: 0.7265\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5865 - accuracy: 0.6879 - val_loss: 0.6539 - val_accuracy: 0.5980\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5793 - accuracy: 0.7141 - val_loss: 0.6213 - val_accuracy: 0.6592\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6228 - accuracy: 0.6624 - val_loss: 0.5984 - val_accuracy: 0.7347\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5527 - accuracy: 0.7114 - val_loss: 0.5826 - val_accuracy: 0.7408\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5517 - accuracy: 0.7067 - val_loss: 0.5837 - val_accuracy: 0.7388\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5452 - accuracy: 0.7174 - val_loss: 0.6037 - val_accuracy: 0.7082\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5373 - accuracy: 0.7201 - val_loss: 0.5941 - val_accuracy: 0.7163\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.677632\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.673508\n",
            "[2]\tvalidation_0-auc:0.671387\n",
            "[3]\tvalidation_0-auc:0.673941\n",
            "[4]\tvalidation_0-auc:0.669997\n",
            "[5]\tvalidation_0-auc:0.670475\n",
            "[6]\tvalidation_0-auc:0.679338\n",
            "[7]\tvalidation_0-auc:0.67645\n",
            "[8]\tvalidation_0-auc:0.680304\n",
            "[9]\tvalidation_0-auc:0.680313\n",
            "[10]\tvalidation_0-auc:0.679961\n",
            "[11]\tvalidation_0-auc:0.680177\n",
            "[12]\tvalidation_0-auc:0.682858\n",
            "[13]\tvalidation_0-auc:0.683914\n",
            "[14]\tvalidation_0-auc:0.681305\n",
            "[15]\tvalidation_0-auc:0.682939\n",
            "[16]\tvalidation_0-auc:0.684248\n",
            "[17]\tvalidation_0-auc:0.682957\n",
            "[18]\tvalidation_0-auc:0.684076\n",
            "[19]\tvalidation_0-auc:0.685295\n",
            "[20]\tvalidation_0-auc:0.685222\n",
            "[21]\tvalidation_0-auc:0.684275\n",
            "[22]\tvalidation_0-auc:0.682749\n",
            "[23]\tvalidation_0-auc:0.683896\n",
            "[24]\tvalidation_0-auc:0.683914\n",
            "[25]\tvalidation_0-auc:0.68348\n",
            "[26]\tvalidation_0-auc:0.684933\n",
            "[27]\tvalidation_0-auc:0.685195\n",
            "[28]\tvalidation_0-auc:0.685033\n",
            "[29]\tvalidation_0-auc:0.685132\n",
            "[30]\tvalidation_0-auc:0.685367\n",
            "[31]\tvalidation_0-auc:0.684789\n",
            "[32]\tvalidation_0-auc:0.685204\n",
            "[33]\tvalidation_0-auc:0.685033\n",
            "[34]\tvalidation_0-auc:0.68422\n",
            "[35]\tvalidation_0-auc:0.682903\n",
            "[36]\tvalidation_0-auc:0.682632\n",
            "[37]\tvalidation_0-auc:0.683291\n",
            "[38]\tvalidation_0-auc:0.683887\n",
            "[39]\tvalidation_0-auc:0.683237\n",
            "[40]\tvalidation_0-auc:0.683092\n",
            "[41]\tvalidation_0-auc:0.682506\n",
            "[42]\tvalidation_0-auc:0.682343\n",
            "[43]\tvalidation_0-auc:0.682722\n",
            "[44]\tvalidation_0-auc:0.682271\n",
            "[45]\tvalidation_0-auc:0.681305\n",
            "[46]\tvalidation_0-auc:0.68154\n",
            "[47]\tvalidation_0-auc:0.681341\n",
            "[48]\tvalidation_0-auc:0.679681\n",
            "[49]\tvalidation_0-auc:0.680268\n",
            "[50]\tvalidation_0-auc:0.680177\n",
            "[51]\tvalidation_0-auc:0.679798\n",
            "[52]\tvalidation_0-auc:0.679365\n",
            "[53]\tvalidation_0-auc:0.678517\n",
            "[54]\tvalidation_0-auc:0.679293\n",
            "[55]\tvalidation_0-auc:0.680015\n",
            "[56]\tvalidation_0-auc:0.679654\n",
            "[57]\tvalidation_0-auc:0.678688\n",
            "[58]\tvalidation_0-auc:0.678056\n",
            "[59]\tvalidation_0-auc:0.678291\n",
            "[60]\tvalidation_0-auc:0.677912\n",
            "[61]\tvalidation_0-auc:0.677713\n",
            "[62]\tvalidation_0-auc:0.67849\n",
            "[63]\tvalidation_0-auc:0.678679\n",
            "[64]\tvalidation_0-auc:0.677524\n",
            "[65]\tvalidation_0-auc:0.677235\n",
            "[66]\tvalidation_0-auc:0.675701\n",
            "[67]\tvalidation_0-auc:0.67608\n",
            "[68]\tvalidation_0-auc:0.676739\n",
            "[69]\tvalidation_0-auc:0.67654\n",
            "[70]\tvalidation_0-auc:0.6758\n",
            "[71]\tvalidation_0-auc:0.675439\n",
            "[72]\tvalidation_0-auc:0.675105\n",
            "[73]\tvalidation_0-auc:0.674564\n",
            "[74]\tvalidation_0-auc:0.674347\n",
            "[75]\tvalidation_0-auc:0.67404\n",
            "[76]\tvalidation_0-auc:0.673824\n",
            "[77]\tvalidation_0-auc:0.674419\n",
            "[78]\tvalidation_0-auc:0.674852\n",
            "[79]\tvalidation_0-auc:0.674672\n",
            "[80]\tvalidation_0-auc:0.673787\n",
            "Stopping. Best iteration:\n",
            "[30]\tvalidation_0-auc:0.685367\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6835 - accuracy: 0.5649 - val_loss: 0.6609 - val_accuracy: 0.6302\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6685 - accuracy: 0.5930 - val_loss: 0.6594 - val_accuracy: 0.6302\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6442 - accuracy: 0.6280 - val_loss: 0.6269 - val_accuracy: 0.6718\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6184 - accuracy: 0.6706 - val_loss: 0.6316 - val_accuracy: 0.6324\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5719 - accuracy: 0.7062 - val_loss: 0.6440 - val_accuracy: 0.6280\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6460 - accuracy: 0.6342 - val_loss: 0.6198 - val_accuracy: 0.6543\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5735 - accuracy: 0.7303 - val_loss: 0.6990 - val_accuracy: 0.5427\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5643 - accuracy: 0.7186 - val_loss: 0.5964 - val_accuracy: 0.7046\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5516 - accuracy: 0.7289 - val_loss: 0.5792 - val_accuracy: 0.7221\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5388 - accuracy: 0.7474 - val_loss: 0.5996 - val_accuracy: 0.6827\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.54708\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.575721\n",
            "[2]\tvalidation_0-auc:0.609622\n",
            "[3]\tvalidation_0-auc:0.604413\n",
            "[4]\tvalidation_0-auc:0.628143\n",
            "[5]\tvalidation_0-auc:0.638447\n",
            "[6]\tvalidation_0-auc:0.624743\n",
            "[7]\tvalidation_0-auc:0.629602\n",
            "[8]\tvalidation_0-auc:0.637533\n",
            "[9]\tvalidation_0-auc:0.635725\n",
            "[10]\tvalidation_0-auc:0.641344\n",
            "[11]\tvalidation_0-auc:0.640594\n",
            "[12]\tvalidation_0-auc:0.647693\n",
            "[13]\tvalidation_0-auc:0.656959\n",
            "[14]\tvalidation_0-auc:0.653209\n",
            "[15]\tvalidation_0-auc:0.65325\n",
            "[16]\tvalidation_0-auc:0.656887\n",
            "[17]\tvalidation_0-auc:0.658818\n",
            "[18]\tvalidation_0-auc:0.662424\n",
            "[19]\tvalidation_0-auc:0.66341\n",
            "[20]\tvalidation_0-auc:0.667057\n",
            "[21]\tvalidation_0-auc:0.665835\n",
            "[22]\tvalidation_0-auc:0.666379\n",
            "[23]\tvalidation_0-auc:0.671084\n",
            "[24]\tvalidation_0-auc:0.669461\n",
            "[25]\tvalidation_0-auc:0.670067\n",
            "[26]\tvalidation_0-auc:0.669389\n",
            "[27]\tvalidation_0-auc:0.669327\n",
            "[28]\tvalidation_0-auc:0.665434\n",
            "[29]\tvalidation_0-auc:0.668855\n",
            "[30]\tvalidation_0-auc:0.667807\n",
            "[31]\tvalidation_0-auc:0.666461\n",
            "[32]\tvalidation_0-auc:0.669122\n",
            "[33]\tvalidation_0-auc:0.668125\n",
            "[34]\tvalidation_0-auc:0.666605\n",
            "[35]\tvalidation_0-auc:0.666009\n",
            "[36]\tvalidation_0-auc:0.666348\n",
            "[37]\tvalidation_0-auc:0.666759\n",
            "[38]\tvalidation_0-auc:0.666533\n",
            "[39]\tvalidation_0-auc:0.666646\n",
            "[40]\tvalidation_0-auc:0.667201\n",
            "[41]\tvalidation_0-auc:0.665413\n",
            "[42]\tvalidation_0-auc:0.667776\n",
            "[43]\tvalidation_0-auc:0.666646\n",
            "[44]\tvalidation_0-auc:0.667139\n",
            "[45]\tvalidation_0-auc:0.665711\n",
            "[46]\tvalidation_0-auc:0.66793\n",
            "[47]\tvalidation_0-auc:0.6698\n",
            "[48]\tvalidation_0-auc:0.672327\n",
            "[49]\tvalidation_0-auc:0.671793\n",
            "[50]\tvalidation_0-auc:0.670478\n",
            "[51]\tvalidation_0-auc:0.670447\n",
            "[52]\tvalidation_0-auc:0.670139\n",
            "[53]\tvalidation_0-auc:0.670858\n",
            "[54]\tvalidation_0-auc:0.6717\n",
            "[55]\tvalidation_0-auc:0.673529\n",
            "[56]\tvalidation_0-auc:0.673837\n",
            "[57]\tvalidation_0-auc:0.674721\n",
            "[58]\tvalidation_0-auc:0.673549\n",
            "[59]\tvalidation_0-auc:0.672235\n",
            "[60]\tvalidation_0-auc:0.673056\n",
            "[61]\tvalidation_0-auc:0.674803\n",
            "[62]\tvalidation_0-auc:0.674988\n",
            "[63]\tvalidation_0-auc:0.672995\n",
            "[64]\tvalidation_0-auc:0.672461\n",
            "[65]\tvalidation_0-auc:0.669831\n",
            "[66]\tvalidation_0-auc:0.66942\n",
            "[67]\tvalidation_0-auc:0.670632\n",
            "[68]\tvalidation_0-auc:0.669317\n",
            "[69]\tvalidation_0-auc:0.66942\n",
            "[70]\tvalidation_0-auc:0.671618\n",
            "[71]\tvalidation_0-auc:0.669564\n",
            "[72]\tvalidation_0-auc:0.670406\n",
            "[73]\tvalidation_0-auc:0.670519\n",
            "[74]\tvalidation_0-auc:0.673015\n",
            "[75]\tvalidation_0-auc:0.671063\n",
            "[76]\tvalidation_0-auc:0.669954\n",
            "[77]\tvalidation_0-auc:0.669769\n",
            "[78]\tvalidation_0-auc:0.671105\n",
            "[79]\tvalidation_0-auc:0.66905\n",
            "[80]\tvalidation_0-auc:0.670632\n",
            "[81]\tvalidation_0-auc:0.671187\n",
            "[82]\tvalidation_0-auc:0.66869\n",
            "[83]\tvalidation_0-auc:0.666985\n",
            "[84]\tvalidation_0-auc:0.667889\n",
            "[85]\tvalidation_0-auc:0.665629\n",
            "[86]\tvalidation_0-auc:0.664622\n",
            "[87]\tvalidation_0-auc:0.663739\n",
            "[88]\tvalidation_0-auc:0.663451\n",
            "[89]\tvalidation_0-auc:0.664684\n",
            "[90]\tvalidation_0-auc:0.663225\n",
            "[91]\tvalidation_0-auc:0.662979\n",
            "[92]\tvalidation_0-auc:0.661869\n",
            "[93]\tvalidation_0-auc:0.66303\n",
            "[94]\tvalidation_0-auc:0.661551\n",
            "[95]\tvalidation_0-auc:0.661674\n",
            "[96]\tvalidation_0-auc:0.66115\n",
            "[97]\tvalidation_0-auc:0.66002\n",
            "[98]\tvalidation_0-auc:0.660143\n",
            "[99]\tvalidation_0-auc:0.660821\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+---------------------+--------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |        Recall       |      F1 score      |\n",
            "+-------------------+--------------------+---------------------+---------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.6591836734693878 |  0.5219298245614035 |  0.672316384180791  | 0.5876543209876544 |\n",
            "|      GRU 0.15     | 0.7163265306122449 |  0.620253164556962  |  0.5536723163841808 | 0.5850746268656716 |\n",
            "|    XGBoost 0.15   | 0.6795918367346939 |  0.554945054945055  |  0.5706214689265536 | 0.5626740947075209 |\n",
            "|    Logreg 0.15    | 0.6959183673469388 |  0.5843373493975904 |  0.5480225988700564 | 0.565597667638484  |\n",
            "|      SVM 0.15     | 0.6979591836734694 |  0.5857988165680473 |  0.559322033898305  | 0.5722543352601156 |\n",
            "|   LSTM beta 0.15  | 0.6280087527352297 |  0.4973821989528796 |  0.5621301775147929 | 0.5277777777777778 |\n",
            "|   GRU beta 0.15   | 0.6827133479212254 |  0.5645161290322581 |  0.621301775147929  | 0.5915492957746479 |\n",
            "| XGBoost beta 0.15 | 0.6717724288840262 |  0.5542857142857143 |  0.5739644970414202 | 0.5639534883720931 |\n",
            "|  logreg beta 0.15 | 0.687089715536105  |  0.5730337078651685 |  0.6035502958579881 | 0.5878962536023056 |\n",
            "|   svm beta 0.15   | 0.6258205689277899 | 0.49382716049382713 | 0.47337278106508873 | 0.4833836858006042 |\n",
            "+-------------------+--------------------+---------------------+---------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QU7y-UXa7BuB",
        "outputId": "f5b0c217-a94b-4f37-aaa2-4df00ca70aff"
      },
      "source": [
        "Result_purging.to_csv('AMD_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.611940</td>\n",
              "      <td>0.718367</td>\n",
              "      <td>0.640625</td>\n",
              "      <td>0.672131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.543210</td>\n",
              "      <td>0.669388</td>\n",
              "      <td>0.619718</td>\n",
              "      <td>0.721311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.562771</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.628019</td>\n",
              "      <td>0.710383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.602740</td>\n",
              "      <td>0.718367</td>\n",
              "      <td>0.656716</td>\n",
              "      <td>0.721311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.534188</td>\n",
              "      <td>0.659184</td>\n",
              "      <td>0.599520</td>\n",
              "      <td>0.683060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.587678</td>\n",
              "      <td>0.708972</td>\n",
              "      <td>0.650919</td>\n",
              "      <td>0.729412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.733871</td>\n",
              "      <td>0.754923</td>\n",
              "      <td>0.619048</td>\n",
              "      <td>0.535294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.512397</td>\n",
              "      <td>0.641138</td>\n",
              "      <td>0.601942</td>\n",
              "      <td>0.729412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.580952</td>\n",
              "      <td>0.702407</td>\n",
              "      <td>0.642105</td>\n",
              "      <td>0.717647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.521008</td>\n",
              "      <td>0.649891</td>\n",
              "      <td>0.607843</td>\n",
              "      <td>0.729412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.591398</td>\n",
              "      <td>0.708163</td>\n",
              "      <td>0.606061</td>\n",
              "      <td>0.621469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>0.712245</td>\n",
              "      <td>0.617886</td>\n",
              "      <td>0.644068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.495536</td>\n",
              "      <td>0.634694</td>\n",
              "      <td>0.553616</td>\n",
              "      <td>0.627119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.548718</td>\n",
              "      <td>0.677551</td>\n",
              "      <td>0.575269</td>\n",
              "      <td>0.604520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.516908</td>\n",
              "      <td>0.653061</td>\n",
              "      <td>0.557292</td>\n",
              "      <td>0.604520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.463158</td>\n",
              "      <td>0.584245</td>\n",
              "      <td>0.581498</td>\n",
              "      <td>0.781065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.630197</td>\n",
              "      <td>0.563307</td>\n",
              "      <td>0.644970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.509524</td>\n",
              "      <td>0.638950</td>\n",
              "      <td>0.564644</td>\n",
              "      <td>0.633136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.504762</td>\n",
              "      <td>0.634573</td>\n",
              "      <td>0.559367</td>\n",
              "      <td>0.627219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.493671</td>\n",
              "      <td>0.625821</td>\n",
              "      <td>0.477064</td>\n",
              "      <td>0.461538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.692913</td>\n",
              "      <td>0.738776</td>\n",
              "      <td>0.578947</td>\n",
              "      <td>0.497175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.476364</td>\n",
              "      <td>0.612245</td>\n",
              "      <td>0.579646</td>\n",
              "      <td>0.740113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.554945</td>\n",
              "      <td>0.679592</td>\n",
              "      <td>0.562674</td>\n",
              "      <td>0.570621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.584337</td>\n",
              "      <td>0.695918</td>\n",
              "      <td>0.565598</td>\n",
              "      <td>0.548023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.585799</td>\n",
              "      <td>0.697959</td>\n",
              "      <td>0.572254</td>\n",
              "      <td>0.559322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.530516</td>\n",
              "      <td>0.658643</td>\n",
              "      <td>0.591623</td>\n",
              "      <td>0.668639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.561856</td>\n",
              "      <td>0.682713</td>\n",
              "      <td>0.600551</td>\n",
              "      <td>0.644970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.554286</td>\n",
              "      <td>0.671772</td>\n",
              "      <td>0.563953</td>\n",
              "      <td>0.573964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.573034</td>\n",
              "      <td>0.687090</td>\n",
              "      <td>0.587896</td>\n",
              "      <td>0.603550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.493827</td>\n",
              "      <td>0.625821</td>\n",
              "      <td>0.483384</td>\n",
              "      <td>0.473373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.687755</td>\n",
              "      <td>0.638298</td>\n",
              "      <td>0.737705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.572650</td>\n",
              "      <td>0.695918</td>\n",
              "      <td>0.642686</td>\n",
              "      <td>0.732240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.562771</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.628019</td>\n",
              "      <td>0.710383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.602740</td>\n",
              "      <td>0.718367</td>\n",
              "      <td>0.656716</td>\n",
              "      <td>0.721311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.534188</td>\n",
              "      <td>0.659184</td>\n",
              "      <td>0.599520</td>\n",
              "      <td>0.683060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.498141</td>\n",
              "      <td>0.625821</td>\n",
              "      <td>0.610478</td>\n",
              "      <td>0.788235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.608247</td>\n",
              "      <td>0.719912</td>\n",
              "      <td>0.648352</td>\n",
              "      <td>0.694118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.512397</td>\n",
              "      <td>0.641138</td>\n",
              "      <td>0.601942</td>\n",
              "      <td>0.729412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.580952</td>\n",
              "      <td>0.702407</td>\n",
              "      <td>0.642105</td>\n",
              "      <td>0.717647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.521008</td>\n",
              "      <td>0.649891</td>\n",
              "      <td>0.607843</td>\n",
              "      <td>0.729412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.651007</td>\n",
              "      <td>0.730612</td>\n",
              "      <td>0.595092</td>\n",
              "      <td>0.548023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.492188</td>\n",
              "      <td>0.630612</td>\n",
              "      <td>0.581986</td>\n",
              "      <td>0.711864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.495536</td>\n",
              "      <td>0.634694</td>\n",
              "      <td>0.553616</td>\n",
              "      <td>0.627119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.548718</td>\n",
              "      <td>0.677551</td>\n",
              "      <td>0.575269</td>\n",
              "      <td>0.604520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.516908</td>\n",
              "      <td>0.653061</td>\n",
              "      <td>0.557292</td>\n",
              "      <td>0.604520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.497487</td>\n",
              "      <td>0.628009</td>\n",
              "      <td>0.538043</td>\n",
              "      <td>0.585799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.638158</td>\n",
              "      <td>0.722101</td>\n",
              "      <td>0.604361</td>\n",
              "      <td>0.573964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.509524</td>\n",
              "      <td>0.638950</td>\n",
              "      <td>0.564644</td>\n",
              "      <td>0.633136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.504762</td>\n",
              "      <td>0.634573</td>\n",
              "      <td>0.559367</td>\n",
              "      <td>0.627219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.493671</td>\n",
              "      <td>0.625821</td>\n",
              "      <td>0.477064</td>\n",
              "      <td>0.461538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.521930</td>\n",
              "      <td>0.659184</td>\n",
              "      <td>0.587654</td>\n",
              "      <td>0.672316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.620253</td>\n",
              "      <td>0.716327</td>\n",
              "      <td>0.585075</td>\n",
              "      <td>0.553672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.554945</td>\n",
              "      <td>0.679592</td>\n",
              "      <td>0.562674</td>\n",
              "      <td>0.570621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.584337</td>\n",
              "      <td>0.695918</td>\n",
              "      <td>0.565598</td>\n",
              "      <td>0.548023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.585799</td>\n",
              "      <td>0.697959</td>\n",
              "      <td>0.572254</td>\n",
              "      <td>0.559322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.497382</td>\n",
              "      <td>0.628009</td>\n",
              "      <td>0.527778</td>\n",
              "      <td>0.562130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.564516</td>\n",
              "      <td>0.682713</td>\n",
              "      <td>0.591549</td>\n",
              "      <td>0.621302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.554286</td>\n",
              "      <td>0.671772</td>\n",
              "      <td>0.563953</td>\n",
              "      <td>0.573964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.573034</td>\n",
              "      <td>0.687090</td>\n",
              "      <td>0.587896</td>\n",
              "      <td>0.603550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>AMD</td>\n",
              "      <td>0.493827</td>\n",
              "      <td>0.625821</td>\n",
              "      <td>0.483384</td>\n",
              "      <td>0.473373</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  AMD  0.611940  0.718367  0.640625  0.672131\n",
              "1            GRU 0.1  AMD  0.543210  0.669388  0.619718  0.721311\n",
              "2        XGBoost 0.1  AMD  0.562771  0.685714  0.628019  0.710383\n",
              "3         Logreg 0.1  AMD  0.602740  0.718367  0.656716  0.721311\n",
              "4            SVM 0.1  AMD  0.534188  0.659184  0.599520  0.683060\n",
              "5      LSTM beta 0.1  AMD  0.587678  0.708972  0.650919  0.729412\n",
              "6       GRU beta 0.1  AMD  0.733871  0.754923  0.619048  0.535294\n",
              "7   XGBoost beta 0.1  AMD  0.512397  0.641138  0.601942  0.729412\n",
              "8    logreg beta 0.1  AMD  0.580952  0.702407  0.642105  0.717647\n",
              "9       svm beta 0.1  AMD  0.521008  0.649891  0.607843  0.729412\n",
              "0           LSTM 0.2  AMD  0.591398  0.708163  0.606061  0.621469\n",
              "1            GRU 0.2  AMD  0.593750  0.712245  0.617886  0.644068\n",
              "2        XGBoost 0.2  AMD  0.495536  0.634694  0.553616  0.627119\n",
              "3         Logreg 0.2  AMD  0.548718  0.677551  0.575269  0.604520\n",
              "4            SVM 0.2  AMD  0.516908  0.653061  0.557292  0.604520\n",
              "5      LSTM beta 0.2  AMD  0.463158  0.584245  0.581498  0.781065\n",
              "6       GRU beta 0.2  AMD  0.500000  0.630197  0.563307  0.644970\n",
              "7   XGBoost beta 0.2  AMD  0.509524  0.638950  0.564644  0.633136\n",
              "8    logreg beta 0.2  AMD  0.504762  0.634573  0.559367  0.627219\n",
              "9       svm beta 0.2  AMD  0.493671  0.625821  0.477064  0.461538\n",
              "0          LSTM 0.15  AMD  0.692913  0.738776  0.578947  0.497175\n",
              "1           GRU 0.15  AMD  0.476364  0.612245  0.579646  0.740113\n",
              "2       XGBoost 0.15  AMD  0.554945  0.679592  0.562674  0.570621\n",
              "3        Logreg 0.15  AMD  0.584337  0.695918  0.565598  0.548023\n",
              "4           SVM 0.15  AMD  0.585799  0.697959  0.572254  0.559322\n",
              "5     LSTM beta 0.15  AMD  0.530516  0.658643  0.591623  0.668639\n",
              "6      GRU beta 0.15  AMD  0.561856  0.682713  0.600551  0.644970\n",
              "7  XGBoost beta 0.15  AMD  0.554286  0.671772  0.563953  0.573964\n",
              "8   logreg beta 0.15  AMD  0.573034  0.687090  0.587896  0.603550\n",
              "9      svm beta 0.15  AMD  0.493827  0.625821  0.483384  0.473373\n",
              "0           LSTM 0.1  AMD  0.562500  0.687755  0.638298  0.737705\n",
              "1            GRU 0.1  AMD  0.572650  0.695918  0.642686  0.732240\n",
              "2        XGBoost 0.1  AMD  0.562771  0.685714  0.628019  0.710383\n",
              "3         Logreg 0.1  AMD  0.602740  0.718367  0.656716  0.721311\n",
              "4            SVM 0.1  AMD  0.534188  0.659184  0.599520  0.683060\n",
              "5      LSTM beta 0.1  AMD  0.498141  0.625821  0.610478  0.788235\n",
              "6       GRU beta 0.1  AMD  0.608247  0.719912  0.648352  0.694118\n",
              "7   XGBoost beta 0.1  AMD  0.512397  0.641138  0.601942  0.729412\n",
              "8    logreg beta 0.1  AMD  0.580952  0.702407  0.642105  0.717647\n",
              "9       svm beta 0.1  AMD  0.521008  0.649891  0.607843  0.729412\n",
              "0           LSTM 0.2  AMD  0.651007  0.730612  0.595092  0.548023\n",
              "1            GRU 0.2  AMD  0.492188  0.630612  0.581986  0.711864\n",
              "2        XGBoost 0.2  AMD  0.495536  0.634694  0.553616  0.627119\n",
              "3         Logreg 0.2  AMD  0.548718  0.677551  0.575269  0.604520\n",
              "4            SVM 0.2  AMD  0.516908  0.653061  0.557292  0.604520\n",
              "5      LSTM beta 0.2  AMD  0.497487  0.628009  0.538043  0.585799\n",
              "6       GRU beta 0.2  AMD  0.638158  0.722101  0.604361  0.573964\n",
              "7   XGBoost beta 0.2  AMD  0.509524  0.638950  0.564644  0.633136\n",
              "8    logreg beta 0.2  AMD  0.504762  0.634573  0.559367  0.627219\n",
              "9       svm beta 0.2  AMD  0.493671  0.625821  0.477064  0.461538\n",
              "0          LSTM 0.15  AMD  0.521930  0.659184  0.587654  0.672316\n",
              "1           GRU 0.15  AMD  0.620253  0.716327  0.585075  0.553672\n",
              "2       XGBoost 0.15  AMD  0.554945  0.679592  0.562674  0.570621\n",
              "3        Logreg 0.15  AMD  0.584337  0.695918  0.565598  0.548023\n",
              "4           SVM 0.15  AMD  0.585799  0.697959  0.572254  0.559322\n",
              "5     LSTM beta 0.15  AMD  0.497382  0.628009  0.527778  0.562130\n",
              "6      GRU beta 0.15  AMD  0.564516  0.682713  0.591549  0.621302\n",
              "7  XGBoost beta 0.15  AMD  0.554286  0.671772  0.563953  0.573964\n",
              "8   logreg beta 0.15  AMD  0.573034  0.687090  0.587896  0.603550\n",
              "9      svm beta 0.15  AMD  0.493827  0.625821  0.483384  0.473373"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9283uXG7ldX"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMD_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYps671PBh9i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2a8-3zLCs9f"
      },
      "source": [
        "## AAPL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "twW1qvrnCs99",
        "outputId": "fe9da72f-15fd-4b8f-98a8-9acb46dd40f3"
      },
      "source": [
        "dfs = pd.read_csv(\"AAPL.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.AAPL</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>141.89</td>\n",
              "      <td>142.91</td>\n",
              "      <td>139.1300</td>\n",
              "      <td>142.42</td>\n",
              "      <td>4233615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.AAPL</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>143.67</td>\n",
              "      <td>144.37</td>\n",
              "      <td>141.2900</td>\n",
              "      <td>141.36</td>\n",
              "      <td>4102531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.AAPL</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>142.47</td>\n",
              "      <td>144.45</td>\n",
              "      <td>142.0300</td>\n",
              "      <td>142.86</td>\n",
              "      <td>3205269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2763</td>\n",
              "      <td>US1.AAPL</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>143.19</td>\n",
              "      <td>144.75</td>\n",
              "      <td>141.7000</td>\n",
              "      <td>141.95</td>\n",
              "      <td>5602142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2762</td>\n",
              "      <td>US1.AAPL</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>145.40</td>\n",
              "      <td>145.96</td>\n",
              "      <td>143.8300</td>\n",
              "      <td>145.39</td>\n",
              "      <td>3504880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2762</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.AAPL</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>291.83</td>\n",
              "      <td>294.50</td>\n",
              "      <td>290.0000</td>\n",
              "      <td>294.11</td>\n",
              "      <td>21492761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2763</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.AAPL</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>290.26</td>\n",
              "      <td>290.48</td>\n",
              "      <td>286.9100</td>\n",
              "      <td>289.10</td>\n",
              "      <td>13965617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.AAPL</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>289.64</td>\n",
              "      <td>291.99</td>\n",
              "      <td>285.2600</td>\n",
              "      <td>288.96</td>\n",
              "      <td>22791119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.AAPL</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>281.96</td>\n",
              "      <td>289.44</td>\n",
              "      <td>281.8205</td>\n",
              "      <td>289.44</td>\n",
              "      <td>16379352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.AAPL</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>281.60</td>\n",
              "      <td>282.90</td>\n",
              "      <td>277.7700</td>\n",
              "      <td>278.60</td>\n",
              "      <td>14964464</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2767 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  <TICKER> <PER>    <DATE>  ...  <HIGH>     <LOW>  <CLOSE>     <VOL>\n",
              "0      2766  US1.AAPL     D  20211001  ...  142.91  139.1300   142.42   4233615\n",
              "1      2765  US1.AAPL     D  20210930  ...  144.37  141.2900   141.36   4102531\n",
              "2      2764  US1.AAPL     D  20210929  ...  144.45  142.0300   142.86   3205269\n",
              "3      2763  US1.AAPL     D  20210928  ...  144.75  141.7000   141.95   5602142\n",
              "4      2762  US1.AAPL     D  20210927  ...  145.96  143.8300   145.39   3504880\n",
              "...     ...       ...   ...       ...  ...     ...       ...      ...       ...\n",
              "2762      4  US1.AAPL     D  20101008  ...  294.50  290.0000   294.11  21492761\n",
              "2763      3  US1.AAPL     D  20101007  ...  290.48  286.9100   289.10  13965617\n",
              "2764      2  US1.AAPL     D  20101006  ...  291.99  285.2600   288.96  22791119\n",
              "2765      1  US1.AAPL     D  20101005  ...  289.44  281.8205   289.44  16379352\n",
              "2766      0  US1.AAPL     D  20101004  ...  282.90  277.7700   278.60  14964464\n",
              "\n",
              "[2767 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hyemRmnHCs9-",
        "outputId": "dd62ac9c-6303-4e79-916e-db237d2bab83"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"a764ecc2-8b3b-43ef-abd5-4d439dcc5f41\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"a764ecc2-8b3b-43ef-abd5-4d439dcc5f41\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'a764ecc2-8b3b-43ef-abd5-4d439dcc5f41',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [227.01, 220.81, 218.96, 224.58, 223.98, 218.85, 219.89, 220.99, 217.68, 218.68, 217.69, 220.95, 222.77, 220.65, 219.9, 218.8, 223.09, 223.587, 216.77, 214.15, 213.27, 213.27, 209.25, 205.695, 208.74, 209.1, 205.52, 204.14, 206.6, 202.63, 212.45, 212.65, 210.35, 210.34, 206.44, 201.74, 202.75, 208.98, 200.5, 200.93, 203.45, 199.14, 196.98, 193.22, 204.09, 208.4595, 212.78, 208.884, 209.67, 207.75, 207.05, 208.67, 208.825, 207.22, 202.57, 205.66, 203.28, 204.49, 205.2, 203.24, 201.74, 203.21, 201.22, 200.01, 204.25, 204.33, 202.71, 201.68, 197.92, 199.73, 199.8, 195.57, 198.55, 198.8, 199.3794, 197.86, 198.5, 193.95, 192.74, 194.74, 194.19, 194.81, 192.52, 190.14, 185.23, 182.541, 179.67, 173.28, 175.0, 178.25, 177.35, 178.17, 179.06, 179.67, 182.79, 186.59, 183.09, 188.98, 190.14, 190.92, 188.65, 185.72, 197.3, 200.72, 202.9, 202.86, 208.6, 211.78, 209.17, 210.52, 200.57, 204.61, 204.29, 205.24, 207.18, 207.51, 204.64, 203.86, 203.12, 199.25, 199.23, 198.88, 198.94, 200.66, 199.5, 200.08, 196.97, 195.72, 195.35, 194.04, 191.24, 189.94, 188.71, 188.47, 186.79, 188.77, 191.06, 195.05, 188.17, 186.54, 188.03, 186.21, 183.73, 181.72, 180.94, 178.98, 172.92, 172.47, 174.51, 175.51, 175.86, 174.97, 173.2, 174.88, 174.35, 174.26, 172.97, 171.14, 172.09, 170.94, 170.38, 170.79, 170.17, 170.89, 169.45, 170.5, 170.99, 174.28, 174.1, 171.27, 166.53, 166.27, 165.24, 154.638, 156.3, 157.72, 152.71, 153.97, 153.69, 156.82, 155.86, 154.92, 153.05, 149.99, 152.28, 153.8, 153.31, 150.77, 147.89, 148.26, 142.085, 157.92, 157.94, 156.23, 156.53, 157.17, 146.65, 150.76, 156.9, 160.76, 165.89, 164.21, 165.4, 170.84, 169.09, 168.63, 169.61, 168.39, 174.76, 176.71, 184.93, 178.6, 179.55, 180.97, 174.25, 174.62, 171.9705, 176.76, 176.94, 185.85, 193.53, 191.41, 186.8, 192.23, 194.2, 204.47, 208.49, 209.96, 203.78, 201.59, 207.47, 222.06, 218.79, 213.33, 212.55, 216.03, 219.85, 215.08, 222.765, 220.59, 219.3, 216.12, 221.2, 222.13, 217.28, 222.11, 214.45, 216.39, 226.87, 223.76, 224.26, 227.99, 232.1, 229.29, 227.27, 225.76, 225.09, 220.51, 222.19, 220.79, 217.655, 220.04, 218.34, 218.24, 217.89, 223.64, 226.4, 221.06, 223.86, 218.3402, 221.42, 223.12, 226.87, 228.36, 227.51, 225.07, 222.98, 219.71, 217.99, 216.14, 215.58, 215.06, 215.06, 215.45, 217.61, 213.48, 210.3, 209.76, 208.86, 207.55, 208.89, 207.25, 207.11, 209.06, 207.98, 207.38, 201.62, 190.58, 189.94, 190.98, 194.23, 194.82, 192.99, 191.6, 191.67, 191.97, 190.4, 191.47, 190.92, 191.33, 191.04, 187.93, 190.35, 190.61, 187.98, 185.38, 184.0, 187.15, 182.91, 185.51, 184.165, 184.44, 182.18, 184.92, 185.47, 186.5, 185.77, 188.76, 188.92, 190.81, 190.71, 192.25, 191.26, 191.48, 193.48, 194.05, 193.31, 191.81, 190.23, 186.88, 187.5, 187.86, 188.58, 188.21, 188.45, 187.31, 187.61, 186.29, 186.99, 188.19, 186.44, 188.17, 188.59, 190.03, 187.35, 186.04, 185.13, 183.83, 176.89, 176.62, 169.0, 165.26, 162.33, 164.22, 163.7716, 162.94, 165.25, 165.72, 172.8, 177.84, 178.24, 175.82, 174.73, 174.16, 172.44, 173.25, 170.06, 168.35, 172.78, 171.58, 168.39, 166.68, 167.91, 166.48, 168.35, 172.76, 164.96, 168.75, 171.27, 175.25, 175.32, 177.89, 178.65, 178.485, 179.95, 181.73, 179.96, 176.95, 175.02, 176.68, 176.83, 176.28, 174.95, 178.13, 178.34, 178.98, 175.56, 172.61, 171.01, 171.85, 172.45, 173.0, 167.36, 164.3451, 162.75, 156.22, 155.16, 159.52, 163.06, 156.49, 160.41, 167.7, 167.47, 166.96, 167.96, 171.45, 171.1, 174.22, 176.96, 177.01, 178.45, 179.27, 179.1, 176.2, 177.0, 175.27, 174.245, 174.33, 174.34, 174.96, 173.02, 172.18, 172.26, 169.3, 171.07, 170.58, 170.57, 175.05, 175.01, 174.36, 174.53, 176.43, 173.88, 172.22, 172.31, 171.73, 172.66, 169.36, 169.33, 169.05, 169.65, 169.76, 171.0, 171.75, 169.49, 173.0, 174.08, 174.85, 174.94, 173.13, 169.99, 170.11, 171.1, 169.09, 171.34, 173.98, 174.66, 175.87, 176.24, 174.79, 174.32, 172.48, 168.07, 166.87, 169.05, 166.72, 163.03, 157.42, 156.41, 157.09, 156.2, 156.25, 155.98, 159.77, 160.47, 159.87, 157.01, 156.01, 156.54, 155.9, 155.82, 155.29, 155.4, 153.47, 154.47, 153.81, 154.0, 153.26, 154.2, 153.22, 150.56, 151.76, 153.41, 156.05, 158.72, 158.64, 159.88, 158.27, 159.62, 160.85, 161.49, 158.63, 161.27, 161.92, 162.08, 164.04, 163.97, 163.33, 162.92, 161.47, 159.86, 159.31, 159.99, 159.78, 157.16, 157.51, 157.86, 160.92, 161.64, 159.78, 157.49, 155.32, 161.1, 160.1, 158.79, 156.34, 155.51, 157.15, 150.05, 148.77, 149.49, 150.55, 153.5, 152.75, 152.11, 150.3, 150.35, 151.02, 150.08, 149.6, 149.04, 147.78, 145.77, 145.55, 145.09, 144.17, 142.72, 144.09, 143.565, 144.03, 143.67, 145.83, 143.78, 145.8, 146.29, 145.63, 145.87, 145.01, 146.34, 142.25, 144.3, 145.16, 146.59, 145.4, 148.97, 154.92, 155.37, 154.45, 153.93, 155.45, 153.18, 152.77, 153.67, 153.59, 153.91, 153.3, 153.8, 153.96, 153.05, 152.51, 150.2, 155.47, 155.68, 156.1, 153.94, 153.26, 153.98, 153.01, 148.96, 146.54, 147.06, 147.47, 146.56, 143.64, 143.79, 143.66, 144.53, 143.63, 142.27, 142.44, 140.66, 141.22, 141.83, 141.07, 141.79, 141.62, 143.16, 143.34, 143.67, 144.02, 144.76, 143.7, 143.67, 143.92, 144.16, 143.81, 140.88, 140.63, 140.92, 141.43, 139.85, 141.49, 139.91, 140.69, 140.45, 138.98, 139.19, 139.15, 138.68, 139.01, 139.49, 139.32, 139.77, 138.93, 139.79, 136.99, 136.94, 136.64, 136.53, 137.1, 136.69, 135.71, 135.345, 135.51, 135.01, 133.29, 132.12, 132.41, 132.04, 131.51, 130.29, 129.07, 128.51, 128.75, 121.3, 121.56, 121.72, 121.94, 121.9, 119.97, 120.09, 119.99, 119.77, 119.97, 119.99, 119.03, 119.25, 119.73, 119.11, 119.0, 117.91, 116.61, 116.02, 116.14, 115.84, 116.73, 116.75, 117.25, 116.51, 116.29, 117.06, 116.95, 116.63, 115.9725, 115.82, 115.2, 115.19, 113.29, 113.96, 112.11, 111.01, 109.94, 109.1, 109.89, 109.47, 110.52, 111.46, 111.58, 111.79, 111.225, 111.77, 111.73, 110.06, 109.94, 110.01, 107.11, 105.7, 108.42, 107.79, 110.88, 111.06, 110.41, 108.84, 109.82, 111.6, 111.49, 113.56, 113.72, 114.5, 115.59, 118.26, 117.61, 116.53, 117.01, 117.12, 117.45, 117.55, 117.63, 116.99, 117.35, 116.34, 116.05, 114.03, 113.91, 113.04, 113.02, 112.51, 113.03, 112.18, 113.95, 113.11, 112.87, 112.7049, 114.61, 113.53, 113.57, 113.53, 114.91, 115.57, 111.77, 108.02, 105.44, 103.14, 105.51, 108.37, 107.7, 107.73, 106.73, 106.11, 105.99, 106.82, 106.94, 107.59, 108.04, 108.83, 108.5, 109.36, 109.0811, 109.2, 109.38, 109.51, 108.16, 107.93, 108.0, 108.82, 108.37, 107.47, 105.89, 105.8, 104.47, 106.06, 104.195, 104.33, 102.97, 96.6723, 97.32, 98.66, 99.42, 99.93, 99.87, 99.83, 98.76, 98.79, 96.87, 97.42, 96.98, 96.68, 95.94, 95.53, 95.02, 95.9, 95.61, 94.4, 93.59, 92.04, 93.37, 96.09, 95.56, 95.91, 95.1, 95.32, 97.57, 97.14, 97.46, 97.34, 98.84, 99.66, 98.94, 99.04, 98.63, 97.91, 97.7, 98.45, 99.86, 100.34, 100.42, 99.6, 97.9, 96.42, 95.22, 94.2, 94.57, 93.48, 93.87, 90.52, 90.34, 92.51, 93.4, 92.79, 92.72, 93.24, 94.19, 95.18, 93.64, 93.74, 94.82, 97.81, 104.3, 105.08, 105.68, 105.97, 107.13, 106.95, 107.4, 109.814, 112.1, 112.04, 110.44, 109.03, 108.66, 108.54, 110.951, 109.803, 111.1299, 109.96, 108.96, 109.59, 107.68, 105.19, 105.66, 106.14, 106.74, 105.91, 105.91, 105.8, 105.97, 104.58, 102.52, 102.25, 101.17, 101.11, 101.04, 101.87, 103.01, 101.5, 100.73, 100.53, 96.6501, 96.91, 96.76, 96.1, 94.69, 96.87, 96.03, 96.26, 98.11, 96.62, 93.99, 93.66, 94.27, 95.02, 95.01, 94.01, 96.6, 96.34, 94.48, 96.38, 97.14, 94.08, 93.43, 99.98, 99.466, 101.42, 96.26, 96.81, 96.68, 97.09, 99.51, 97.41, 99.96, 98.53, 96.98, 96.54, 100.7, 102.71, 105.33, 105.2299, 107.32, 108.74, 106.82, 108.03, 108.61, 107.21, 107.32, 105.89, 108.9999, 111.34, 110.49, 112.47, 113.17, 116.18, 115.63, 118.24, 118.27, 119.0, 115.2, 116.3, 117.29, 118.37, 117.75, 118.02, 118.91, 117.75, 119.3, 118.78, 117.2814, 113.67, 114.18, 112.33, 115.74, 116.11, 116.78, 120.58, 121.06, 120.91, 122.01, 122.59, 121.19, 119.505, 120.53, 119.3, 114.58, 115.27, 119.09, 115.49, 113.76, 113.8, 111.73, 111.02, 111.85, 110.21, 111.79, 111.5801, 112.12, 109.5, 110.77, 111.33, 110.64, 110.28, 109.57, 110.29, 109.06, 112.46, 114.75, 115.0, 114.34, 113.37, 115.22, 113.4899, 113.93, 116.42, 116.27, 115.3379, 114.2, 112.59, 110.1456, 112.31, 109.17, 110.34, 112.33, 107.73, 112.71, 113.28, 112.92, 109.7, 103.75, 103.1299, 105.77, 112.65, 115.02, 116.5, 117.17, 115.96, 115.17, 115.26, 113.55, 119.72, 115.54, 115.14, 115.39, 114.63, 118.45, 121.46, 122.37, 122.98, 123.39, 122.9, 124.5, 125.16, 125.22, 130.67, 132.07, 129.62, 128.5, 126.83, 125.61, 125.65, 123.26, 120.03, 122.59, 125.69, 125.99, 126.4, 126.58, 125.42, 124.58, 126.78, 127.51, 128.16, 127.02, 127.61, 126.42, 127.84, 127.29, 127.59, 126.93, 127.16, 128.6, 128.87, 127.41, 127.76, 128.65, 129.46, 130.16, 129.96, 130.53, 130.32, 131.77, 132.01, 129.63, 132.57, 131.4, 130.07, 130.07, 130.17, 128.76, 128.95, 126.0, 125.86, 126.32, 127.6, 125.25, 125.085, 125.81, 128.7, 128.95, 125.22, 128.64, 130.56, 132.69, 130.28, 129.68, 128.63, 126.89, 127.59, 124.76, 126.18, 126.78, 126.3, 126.85, 127.1, 126.56, 125.6, 126.005, 127.34, 125.31, 124.23, 124.45, 126.33, 123.24, 124.25, 123.39, 126.69, 127.21, 125.95, 127.49, 128.47, 127.05, 124.94, 123.59, 124.44, 122.25, 124.52, 127.11, 126.59, 126.4, 128.56, 129.38, 129.08, 128.47, 130.41, 128.78, 132.18, 132.975, 129.5, 128.46, 128.72, 127.82, 127.08, 126.43, 124.87, 122.02, 119.72, 118.91, 119.94, 119.55, 118.6, 118.62, 117.16, 118.94, 115.32, 109.15, 113.09, 112.99, 112.39, 109.55, 108.71, 105.98, 106.83, 109.79, 110.225, 109.26, 112.0, 111.87, 107.75, 106.23, 106.25, 109.3, 110.39, 112.58, 113.92, 113.98, 112.0201, 112.54, 112.94, 111.76, 112.65, 109.4, 106.74, 108.23, 109.72, 111.63, 111.96, 114.13, 112.41, 114.99, 115.49, 115.93, 114.62, 115.07, 118.81, 118.99, 117.6, 118.625, 116.46, 116.3, 114.66, 115.48, 114.0, 114.19, 112.82, 111.25, 109.6999, 108.82, 109.0, 108.69, 108.86, 108.6, 109.38, 107.97, 106.98, 107.34, 106.71, 105.1, 105.22, 104.83, 103.0, 102.47, 99.77, 97.66, 96.26, 97.56, 98.79, 99.83, 100.73, 101.02, 100.774, 98.75, 99.625, 99.61, 99.89, 99.19, 100.75, 100.11, 100.73, 97.87, 101.75, 102.64, 101.07, 100.97, 101.7844, 101.58, 100.84, 101.6101, 101.67, 101.42, 100.9801, 97.9999, 98.34, 98.95, 98.11, 98.94, 103.3, 102.5, 102.25, 102.12, 100.89, 101.54, 101.32, 100.59, 100.57, 100.55, 99.14, 97.98, 97.5, 97.23, 95.97, 95.98, 94.73, 94.48, 94.96, 95.14, 95.59, 96.1365, 95.58, 98.13, 98.39, 99.0, 97.68, 97.02, 97.1901, 94.7, 93.94, 94.43, 93.08, 94.78, 95.31, 96.45, 95.22, 95.03, 95.39, 95.33, 95.97, 94.03, 93.47, 93.52, 92.96, 91.96, 90.92, 90.36, 90.29, 90.814, 90.9, 91.84, 92.16, 92.08, 92.17, 91.26, 92.27, 93.87, 94.25, 93.7, 645.57, 647.35, 644.82, 637.54, 628.5, 633.0, 635.28, 624.0, 625.71, 614.14, 607.33, 606.28, 604.61, 604.4, 597.465, 588.82, 593.86, 593.7, 592.77, 585.54, 588.12, 592.34, 594.43, 600.93, 592.58, 591.36, 590.04, 592.44, 594.0705, 571.98, 567.77, 524.74, 531.69, 531.16, 524.99, 518.975, 517.9, 521.694, 519.62, 523.48, 530.305, 523.43, 523.548, 531.83, 538.825, 542.61, 541.74, 536.73, 536.86, 537.425, 539.78, 545.0, 539.15, 531.63, 528.75, 531.26, 531.32, 526.93, 524.66, 530.69, 536.6, 536.1099, 530.92, 530.43, 530.82, 532.36, 531.24, 527.76, 526.3, 527.65, 517.335, 522.07, 527.54, 525.24, 531.1, 537.39, 545.86, 543.88, 544.42, 535.86, 535.96, 528.985, 519.75, 512.46, 512.61, 508.79, 501.63, 500.59, 499.275, 500.54, 506.83, 550.55, 546.2, 556.18, 551.51, 549.0, 540.41, 554.22, 557.36, 546.39, 535.6, 532.92, 536.5, 543.37, 540.03, 543.949, 541.04, 553.23, 561.16, 554.5, 560.09, 563.83, 568.5, 570.23, 548.72, 544.45, 550.75, 554.975, 557.445, 554.49, 560.67, 561.14, 565.64, 566.6, 559.93, 568.1, 564.86, 566.38, 551.3, 556.68, 545.979, 533.6, 523.75, 519.69, 521.02, 515.01, 519.5, 518.65, 524.94, 528.19, 520.72, 520.11, 519.225, 520.59, 512.5, 520.87, 525.5805, 526.7, 519.94, 522.37, 524.88, 517.015, 529.48, 526.0925, 531.875, 524.95, 519.87, 521.385]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a764ecc2-8b3b-43ef-abd5-4d439dcc5f41');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"10297137-05fb-43e7-88d7-f47479ef1f81\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"10297137-05fb-43e7-88d7-f47479ef1f81\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '10297137-05fb-43e7-88d7-f47479ef1f81',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('10297137-05fb-43e7-88d7-f47479ef1f81');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpfPksd5Cs-A"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G4Fo7yNCs-B",
        "outputId": "540bff62-3cf9-4973-cd66-6fb29484e9d0"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .2, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"AAPL\", step_sizes=4, th= th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6013 - accuracy: 0.7221 - val_loss: 0.7548 - val_accuracy: 0.5367\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5861 - accuracy: 0.7248 - val_loss: 0.7240 - val_accuracy: 0.5367\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5487 - accuracy: 0.7383 - val_loss: 0.7983 - val_accuracy: 0.5776\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5488 - accuracy: 0.7564 - val_loss: 0.6498 - val_accuracy: 0.6306\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5399 - accuracy: 0.7483 - val_loss: 0.6907 - val_accuracy: 0.6204\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.5946 - accuracy: 0.7215 - val_loss: 0.7259 - val_accuracy: 0.5265\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5249 - accuracy: 0.7584 - val_loss: 0.6471 - val_accuracy: 0.7041\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4744 - accuracy: 0.8067 - val_loss: 0.6830 - val_accuracy: 0.6490\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4650 - accuracy: 0.7913 - val_loss: 0.6639 - val_accuracy: 0.6878\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4508 - accuracy: 0.8060 - val_loss: 0.7212 - val_accuracy: 0.6898\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.763672\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.767248\n",
            "[2]\tvalidation_0-auc:0.765749\n",
            "[3]\tvalidation_0-auc:0.767031\n",
            "[4]\tvalidation_0-auc:0.771394\n",
            "[5]\tvalidation_0-auc:0.772039\n",
            "[6]\tvalidation_0-auc:0.770716\n",
            "[7]\tvalidation_0-auc:0.775314\n",
            "[8]\tvalidation_0-auc:0.783236\n",
            "[9]\tvalidation_0-auc:0.787717\n",
            "[10]\tvalidation_0-auc:0.788856\n",
            "[11]\tvalidation_0-auc:0.78966\n",
            "[12]\tvalidation_0-auc:0.791536\n",
            "[13]\tvalidation_0-auc:0.793236\n",
            "[14]\tvalidation_0-auc:0.792893\n",
            "[15]\tvalidation_0-auc:0.7909\n",
            "[16]\tvalidation_0-auc:0.7909\n",
            "[17]\tvalidation_0-auc:0.790648\n",
            "[18]\tvalidation_0-auc:0.791637\n",
            "[19]\tvalidation_0-auc:0.793521\n",
            "[20]\tvalidation_0-auc:0.79399\n",
            "[21]\tvalidation_0-auc:0.79069\n",
            "[22]\tvalidation_0-auc:0.79255\n",
            "[23]\tvalidation_0-auc:0.793245\n",
            "[24]\tvalidation_0-auc:0.793948\n",
            "[25]\tvalidation_0-auc:0.794308\n",
            "[26]\tvalidation_0-auc:0.795648\n",
            "[27]\tvalidation_0-auc:0.799015\n",
            "[28]\tvalidation_0-auc:0.800389\n",
            "[29]\tvalidation_0-auc:0.799836\n",
            "[30]\tvalidation_0-auc:0.800564\n",
            "[31]\tvalidation_0-auc:0.800045\n",
            "[32]\tvalidation_0-auc:0.800196\n",
            "[33]\tvalidation_0-auc:0.799484\n",
            "[34]\tvalidation_0-auc:0.799057\n",
            "[35]\tvalidation_0-auc:0.799559\n",
            "[36]\tvalidation_0-auc:0.801402\n",
            "[37]\tvalidation_0-auc:0.800171\n",
            "[38]\tvalidation_0-auc:0.800121\n",
            "[39]\tvalidation_0-auc:0.801092\n",
            "[40]\tvalidation_0-auc:0.802399\n",
            "[41]\tvalidation_0-auc:0.802231\n",
            "[42]\tvalidation_0-auc:0.80203\n",
            "[43]\tvalidation_0-auc:0.802139\n",
            "[44]\tvalidation_0-auc:0.802306\n",
            "[45]\tvalidation_0-auc:0.802156\n",
            "[46]\tvalidation_0-auc:0.802876\n",
            "[47]\tvalidation_0-auc:0.804266\n",
            "[48]\tvalidation_0-auc:0.80461\n",
            "[49]\tvalidation_0-auc:0.804568\n",
            "[50]\tvalidation_0-auc:0.804216\n",
            "[51]\tvalidation_0-auc:0.804199\n",
            "[52]\tvalidation_0-auc:0.804015\n",
            "[53]\tvalidation_0-auc:0.804518\n",
            "[54]\tvalidation_0-auc:0.80564\n",
            "[55]\tvalidation_0-auc:0.805707\n",
            "[56]\tvalidation_0-auc:0.805673\n",
            "[57]\tvalidation_0-auc:0.806335\n",
            "[58]\tvalidation_0-auc:0.805581\n",
            "[59]\tvalidation_0-auc:0.806519\n",
            "[60]\tvalidation_0-auc:0.807089\n",
            "[61]\tvalidation_0-auc:0.807256\n",
            "[62]\tvalidation_0-auc:0.807708\n",
            "[63]\tvalidation_0-auc:0.808286\n",
            "[64]\tvalidation_0-auc:0.808387\n",
            "[65]\tvalidation_0-auc:0.80909\n",
            "[66]\tvalidation_0-auc:0.809777\n",
            "[67]\tvalidation_0-auc:0.809492\n",
            "[68]\tvalidation_0-auc:0.809911\n",
            "[69]\tvalidation_0-auc:0.809861\n",
            "[70]\tvalidation_0-auc:0.809961\n",
            "[71]\tvalidation_0-auc:0.809367\n",
            "[72]\tvalidation_0-auc:0.809651\n",
            "[73]\tvalidation_0-auc:0.809752\n",
            "[74]\tvalidation_0-auc:0.809886\n",
            "[75]\tvalidation_0-auc:0.809534\n",
            "[76]\tvalidation_0-auc:0.808931\n",
            "[77]\tvalidation_0-auc:0.810154\n",
            "[78]\tvalidation_0-auc:0.810774\n",
            "[79]\tvalidation_0-auc:0.810573\n",
            "[80]\tvalidation_0-auc:0.810539\n",
            "[81]\tvalidation_0-auc:0.810606\n",
            "[82]\tvalidation_0-auc:0.810112\n",
            "[83]\tvalidation_0-auc:0.809392\n",
            "[84]\tvalidation_0-auc:0.809392\n",
            "[85]\tvalidation_0-auc:0.809409\n",
            "[86]\tvalidation_0-auc:0.809928\n",
            "[87]\tvalidation_0-auc:0.809945\n",
            "[88]\tvalidation_0-auc:0.810079\n",
            "[89]\tvalidation_0-auc:0.809961\n",
            "[90]\tvalidation_0-auc:0.809593\n",
            "[91]\tvalidation_0-auc:0.809827\n",
            "[92]\tvalidation_0-auc:0.809945\n",
            "[93]\tvalidation_0-auc:0.809995\n",
            "[94]\tvalidation_0-auc:0.810263\n",
            "[95]\tvalidation_0-auc:0.809953\n",
            "[96]\tvalidation_0-auc:0.810037\n",
            "[97]\tvalidation_0-auc:0.810271\n",
            "[98]\tvalidation_0-auc:0.810321\n",
            "[99]\tvalidation_0-auc:0.810455\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6119 - accuracy: 0.7111 - val_loss: 0.7559 - val_accuracy: 0.5295\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5837 - accuracy: 0.7207 - val_loss: 0.7259 - val_accuracy: 0.5055\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5528 - accuracy: 0.7358 - val_loss: 0.7876 - val_accuracy: 0.5164\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5215 - accuracy: 0.7694 - val_loss: 0.7393 - val_accuracy: 0.5930\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5119 - accuracy: 0.7721 - val_loss: 0.6928 - val_accuracy: 0.6827\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6045 - accuracy: 0.7165 - val_loss: 0.7274 - val_accuracy: 0.4748\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5492 - accuracy: 0.7406 - val_loss: 0.7804 - val_accuracy: 0.5492\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5047 - accuracy: 0.7763 - val_loss: 0.8601 - val_accuracy: 0.5777\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4940 - accuracy: 0.7948 - val_loss: 0.7265 - val_accuracy: 0.6411\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4714 - accuracy: 0.7941 - val_loss: 0.6902 - val_accuracy: 0.7571\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.732472\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.751903\n",
            "[2]\tvalidation_0-auc:0.755074\n",
            "[3]\tvalidation_0-auc:0.766577\n",
            "[4]\tvalidation_0-auc:0.762599\n",
            "[5]\tvalidation_0-auc:0.767922\n",
            "[6]\tvalidation_0-auc:0.776254\n",
            "[7]\tvalidation_0-auc:0.781261\n",
            "[8]\tvalidation_0-auc:0.782587\n",
            "[9]\tvalidation_0-auc:0.773275\n",
            "[10]\tvalidation_0-auc:0.773438\n",
            "[11]\tvalidation_0-auc:0.772237\n",
            "[12]\tvalidation_0-auc:0.768854\n",
            "[13]\tvalidation_0-auc:0.768451\n",
            "[14]\tvalidation_0-auc:0.767769\n",
            "[15]\tvalidation_0-auc:0.769277\n",
            "[16]\tvalidation_0-auc:0.765145\n",
            "[17]\tvalidation_0-auc:0.764415\n",
            "[18]\tvalidation_0-auc:0.765443\n",
            "[19]\tvalidation_0-auc:0.764271\n",
            "[20]\tvalidation_0-auc:0.76354\n",
            "[21]\tvalidation_0-auc:0.763569\n",
            "[22]\tvalidation_0-auc:0.761955\n",
            "[23]\tvalidation_0-auc:0.763781\n",
            "[24]\tvalidation_0-auc:0.758956\n",
            "[25]\tvalidation_0-auc:0.757765\n",
            "[26]\tvalidation_0-auc:0.752662\n",
            "[27]\tvalidation_0-auc:0.749327\n",
            "[28]\tvalidation_0-auc:0.749279\n",
            "[29]\tvalidation_0-auc:0.747242\n",
            "[30]\tvalidation_0-auc:0.743456\n",
            "[31]\tvalidation_0-auc:0.743091\n",
            "[32]\tvalidation_0-auc:0.743042\n",
            "[33]\tvalidation_0-auc:0.737834\n",
            "[34]\tvalidation_0-auc:0.738141\n",
            "[35]\tvalidation_0-auc:0.738507\n",
            "[36]\tvalidation_0-auc:0.737007\n",
            "[37]\tvalidation_0-auc:0.734643\n",
            "[38]\tvalidation_0-auc:0.733769\n",
            "[39]\tvalidation_0-auc:0.735374\n",
            "[40]\tvalidation_0-auc:0.733586\n",
            "[41]\tvalidation_0-auc:0.731213\n",
            "[42]\tvalidation_0-auc:0.7293\n",
            "[43]\tvalidation_0-auc:0.72536\n",
            "[44]\tvalidation_0-auc:0.726206\n",
            "[45]\tvalidation_0-auc:0.722919\n",
            "[46]\tvalidation_0-auc:0.726571\n",
            "[47]\tvalidation_0-auc:0.726091\n",
            "[48]\tvalidation_0-auc:0.726984\n",
            "[49]\tvalidation_0-auc:0.727984\n",
            "[50]\tvalidation_0-auc:0.724813\n",
            "[51]\tvalidation_0-auc:0.727273\n",
            "[52]\tvalidation_0-auc:0.728387\n",
            "[53]\tvalidation_0-auc:0.727599\n",
            "[54]\tvalidation_0-auc:0.728791\n",
            "[55]\tvalidation_0-auc:0.729618\n",
            "[56]\tvalidation_0-auc:0.729733\n",
            "[57]\tvalidation_0-auc:0.727965\n",
            "[58]\tvalidation_0-auc:0.729262\n",
            "Stopping. Best iteration:\n",
            "[8]\tvalidation_0-auc:0.782587\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.6204081632653061 | 0.8253968253968254 |  0.2290748898678414 | 0.3586206896551724 |\n",
            "|     GRU 0.1      | 0.689795918367347  | 0.831858407079646  | 0.41409691629955947 | 0.5529411764705882 |\n",
            "|   XGBoost 0.1    | 0.7244897959183674 | 0.8432835820895522 |  0.4977973568281938 | 0.6260387811634349 |\n",
            "|    Logreg 0.1    | 0.7020408163265306 | 0.8521739130434782 | 0.43171806167400884 | 0.5730994152046784 |\n",
            "|     SVM 0.1      | 0.710204081632653  | 0.8455284552845529 |  0.4581497797356828 | 0.5942857142857143 |\n",
            "|  LSTM beta 0.1   | 0.6827133479212254 | 0.7243589743589743 |  0.5255813953488372 | 0.6091644204851752 |\n",
            "|   GRU beta 0.1   | 0.7571115973741794 | 0.7452830188679245 |  0.7348837209302326 | 0.7400468384074942 |\n",
            "| XGBoost beta 0.1 | 0.6958424507658644 | 0.7467532467532467 |  0.5348837209302325 | 0.6233062330623305 |\n",
            "| logreg beta 0.1  | 0.6301969365426696 | 0.6642857142857143 |  0.4325581395348837 | 0.523943661971831  |\n",
            "|   svm beta 0.1   | 0.612691466083151  | 0.6091954022988506 |  0.4930232558139535 | 0.5449871465295629 |\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6043 - accuracy: 0.7168 - val_loss: 0.6263 - val_accuracy: 0.6796\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5937 - accuracy: 0.7208 - val_loss: 0.6263 - val_accuracy: 0.6694\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5752 - accuracy: 0.7242 - val_loss: 0.5801 - val_accuracy: 0.6939\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5578 - accuracy: 0.7396 - val_loss: 0.5712 - val_accuracy: 0.7184\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5414 - accuracy: 0.7477 - val_loss: 0.5755 - val_accuracy: 0.7286\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6090 - accuracy: 0.7188 - val_loss: 0.6274 - val_accuracy: 0.6776\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5866 - accuracy: 0.7242 - val_loss: 0.6132 - val_accuracy: 0.6694\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5525 - accuracy: 0.7530 - val_loss: 0.6175 - val_accuracy: 0.7245\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5346 - accuracy: 0.7597 - val_loss: 0.6220 - val_accuracy: 0.6980\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5219 - accuracy: 0.7691 - val_loss: 0.5837 - val_accuracy: 0.7510\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.710449\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.723035\n",
            "[2]\tvalidation_0-auc:0.736424\n",
            "[3]\tvalidation_0-auc:0.739007\n",
            "[4]\tvalidation_0-auc:0.735946\n",
            "[5]\tvalidation_0-auc:0.740651\n",
            "[6]\tvalidation_0-auc:0.743148\n",
            "[7]\tvalidation_0-auc:0.742497\n",
            "[8]\tvalidation_0-auc:0.74244\n",
            "[9]\tvalidation_0-auc:0.742889\n",
            "[10]\tvalidation_0-auc:0.746131\n",
            "[11]\tvalidation_0-auc:0.746141\n",
            "[12]\tvalidation_0-auc:0.746428\n",
            "[13]\tvalidation_0-auc:0.746696\n",
            "[14]\tvalidation_0-auc:0.746036\n",
            "[15]\tvalidation_0-auc:0.747375\n",
            "[16]\tvalidation_0-auc:0.747499\n",
            "[17]\tvalidation_0-auc:0.748369\n",
            "[18]\tvalidation_0-auc:0.74747\n",
            "[19]\tvalidation_0-auc:0.748962\n",
            "[20]\tvalidation_0-auc:0.747614\n",
            "[21]\tvalidation_0-auc:0.747518\n",
            "[22]\tvalidation_0-auc:0.748675\n",
            "[23]\tvalidation_0-auc:0.748551\n",
            "[24]\tvalidation_0-auc:0.74813\n",
            "[25]\tvalidation_0-auc:0.75121\n",
            "[26]\tvalidation_0-auc:0.750808\n",
            "[27]\tvalidation_0-auc:0.75428\n",
            "[28]\tvalidation_0-auc:0.754538\n",
            "[29]\tvalidation_0-auc:0.756432\n",
            "[30]\tvalidation_0-auc:0.755131\n",
            "[31]\tvalidation_0-auc:0.755858\n",
            "[32]\tvalidation_0-auc:0.757445\n",
            "[33]\tvalidation_0-auc:0.757675\n",
            "[34]\tvalidation_0-auc:0.757962\n",
            "[35]\tvalidation_0-auc:0.757436\n",
            "[36]\tvalidation_0-auc:0.756154\n",
            "[37]\tvalidation_0-auc:0.75602\n",
            "[38]\tvalidation_0-auc:0.757178\n",
            "[39]\tvalidation_0-auc:0.757292\n",
            "[40]\tvalidation_0-auc:0.757331\n",
            "[41]\tvalidation_0-auc:0.759129\n",
            "[42]\tvalidation_0-auc:0.758832\n",
            "[43]\tvalidation_0-auc:0.759865\n",
            "[44]\tvalidation_0-auc:0.759779\n",
            "[45]\tvalidation_0-auc:0.75909\n",
            "[46]\tvalidation_0-auc:0.758765\n",
            "[47]\tvalidation_0-auc:0.759798\n",
            "[48]\tvalidation_0-auc:0.760563\n",
            "[49]\tvalidation_0-auc:0.760735\n",
            "[50]\tvalidation_0-auc:0.759817\n",
            "[51]\tvalidation_0-auc:0.759377\n",
            "[52]\tvalidation_0-auc:0.760993\n",
            "[53]\tvalidation_0-auc:0.760573\n",
            "[54]\tvalidation_0-auc:0.761491\n",
            "[55]\tvalidation_0-auc:0.761338\n",
            "[56]\tvalidation_0-auc:0.761816\n",
            "[57]\tvalidation_0-auc:0.762275\n",
            "[58]\tvalidation_0-auc:0.762983\n",
            "[59]\tvalidation_0-auc:0.762791\n",
            "[60]\tvalidation_0-auc:0.762811\n",
            "[61]\tvalidation_0-auc:0.762791\n",
            "[62]\tvalidation_0-auc:0.763442\n",
            "[63]\tvalidation_0-auc:0.763241\n",
            "[64]\tvalidation_0-auc:0.765096\n",
            "[65]\tvalidation_0-auc:0.765651\n",
            "[66]\tvalidation_0-auc:0.765861\n",
            "[67]\tvalidation_0-auc:0.765326\n",
            "[68]\tvalidation_0-auc:0.765594\n",
            "[69]\tvalidation_0-auc:0.766646\n",
            "[70]\tvalidation_0-auc:0.766894\n",
            "[71]\tvalidation_0-auc:0.766626\n",
            "[72]\tvalidation_0-auc:0.766435\n",
            "[73]\tvalidation_0-auc:0.765651\n",
            "[74]\tvalidation_0-auc:0.765326\n",
            "[75]\tvalidation_0-auc:0.764522\n",
            "[76]\tvalidation_0-auc:0.764561\n",
            "[77]\tvalidation_0-auc:0.764637\n",
            "[78]\tvalidation_0-auc:0.764102\n",
            "[79]\tvalidation_0-auc:0.764159\n",
            "[80]\tvalidation_0-auc:0.764159\n",
            "[81]\tvalidation_0-auc:0.764503\n",
            "[82]\tvalidation_0-auc:0.763719\n",
            "[83]\tvalidation_0-auc:0.762782\n",
            "[84]\tvalidation_0-auc:0.763031\n",
            "[85]\tvalidation_0-auc:0.764389\n",
            "[86]\tvalidation_0-auc:0.764446\n",
            "[87]\tvalidation_0-auc:0.764331\n",
            "[88]\tvalidation_0-auc:0.763337\n",
            "[89]\tvalidation_0-auc:0.762572\n",
            "[90]\tvalidation_0-auc:0.762973\n",
            "[91]\tvalidation_0-auc:0.763987\n",
            "[92]\tvalidation_0-auc:0.763681\n",
            "[93]\tvalidation_0-auc:0.764389\n",
            "[94]\tvalidation_0-auc:0.764025\n",
            "[95]\tvalidation_0-auc:0.763949\n",
            "[96]\tvalidation_0-auc:0.764083\n",
            "[97]\tvalidation_0-auc:0.763853\n",
            "[98]\tvalidation_0-auc:0.7637\n",
            "[99]\tvalidation_0-auc:0.763815\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6182 - accuracy: 0.7090 - val_loss: 0.6254 - val_accuracy: 0.6827\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6005 - accuracy: 0.7145 - val_loss: 0.6152 - val_accuracy: 0.6827\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5677 - accuracy: 0.7138 - val_loss: 0.5323 - val_accuracy: 0.7199\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5415 - accuracy: 0.7536 - val_loss: 0.5806 - val_accuracy: 0.6805\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5239 - accuracy: 0.7618 - val_loss: 0.5392 - val_accuracy: 0.7309\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5950 - accuracy: 0.7179 - val_loss: 0.7116 - val_accuracy: 0.7330\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5267 - accuracy: 0.7639 - val_loss: 0.7404 - val_accuracy: 0.7505\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5136 - accuracy: 0.7790 - val_loss: 0.7495 - val_accuracy: 0.7549\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5006 - accuracy: 0.7879 - val_loss: 0.8049 - val_accuracy: 0.7352\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5041 - accuracy: 0.7852 - val_loss: 0.7322 - val_accuracy: 0.7549\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.78645\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.798143\n",
            "[2]\tvalidation_0-auc:0.803504\n",
            "[3]\tvalidation_0-auc:0.807604\n",
            "[4]\tvalidation_0-auc:0.800298\n",
            "[5]\tvalidation_0-auc:0.808831\n",
            "[6]\tvalidation_0-auc:0.804012\n",
            "[7]\tvalidation_0-auc:0.804797\n",
            "[8]\tvalidation_0-auc:0.809041\n",
            "[9]\tvalidation_0-auc:0.808477\n",
            "[10]\tvalidation_0-auc:0.810986\n",
            "[11]\tvalidation_0-auc:0.814478\n",
            "[12]\tvalidation_0-auc:0.816877\n",
            "[13]\tvalidation_0-auc:0.81912\n",
            "[14]\tvalidation_0-auc:0.820071\n",
            "[15]\tvalidation_0-auc:0.816479\n",
            "[16]\tvalidation_0-auc:0.818568\n",
            "[17]\tvalidation_0-auc:0.818711\n",
            "[18]\tvalidation_0-auc:0.820679\n",
            "[19]\tvalidation_0-auc:0.821286\n",
            "[20]\tvalidation_0-auc:0.822845\n",
            "[21]\tvalidation_0-auc:0.826238\n",
            "[22]\tvalidation_0-auc:0.823806\n",
            "[23]\tvalidation_0-auc:0.824514\n",
            "[24]\tvalidation_0-auc:0.825155\n",
            "[25]\tvalidation_0-auc:0.826724\n",
            "[26]\tvalidation_0-auc:0.826691\n",
            "[27]\tvalidation_0-auc:0.827111\n",
            "[28]\tvalidation_0-auc:0.828537\n",
            "[29]\tvalidation_0-auc:0.827122\n",
            "[30]\tvalidation_0-auc:0.824945\n",
            "[31]\tvalidation_0-auc:0.824348\n",
            "[32]\tvalidation_0-auc:0.825409\n",
            "[33]\tvalidation_0-auc:0.825984\n",
            "[34]\tvalidation_0-auc:0.824878\n",
            "[35]\tvalidation_0-auc:0.823828\n",
            "[36]\tvalidation_0-auc:0.823497\n",
            "[37]\tvalidation_0-auc:0.823232\n",
            "[38]\tvalidation_0-auc:0.824182\n",
            "[39]\tvalidation_0-auc:0.823243\n",
            "[40]\tvalidation_0-auc:0.823685\n",
            "[41]\tvalidation_0-auc:0.822723\n",
            "[42]\tvalidation_0-auc:0.824481\n",
            "[43]\tvalidation_0-auc:0.825431\n",
            "[44]\tvalidation_0-auc:0.824204\n",
            "[45]\tvalidation_0-auc:0.823961\n",
            "[46]\tvalidation_0-auc:0.822922\n",
            "[47]\tvalidation_0-auc:0.823895\n",
            "[48]\tvalidation_0-auc:0.824005\n",
            "[49]\tvalidation_0-auc:0.824226\n",
            "[50]\tvalidation_0-auc:0.82111\n",
            "[51]\tvalidation_0-auc:0.820137\n",
            "[52]\tvalidation_0-auc:0.819297\n",
            "[53]\tvalidation_0-auc:0.819341\n",
            "[54]\tvalidation_0-auc:0.816888\n",
            "[55]\tvalidation_0-auc:0.817131\n",
            "[56]\tvalidation_0-auc:0.816711\n",
            "[57]\tvalidation_0-auc:0.817042\n",
            "[58]\tvalidation_0-auc:0.822436\n",
            "[59]\tvalidation_0-auc:0.822303\n",
            "[60]\tvalidation_0-auc:0.821773\n",
            "[61]\tvalidation_0-auc:0.82248\n",
            "[62]\tvalidation_0-auc:0.821905\n",
            "[63]\tvalidation_0-auc:0.820557\n",
            "[64]\tvalidation_0-auc:0.820778\n",
            "[65]\tvalidation_0-auc:0.818501\n",
            "[66]\tvalidation_0-auc:0.818988\n",
            "[67]\tvalidation_0-auc:0.818965\n",
            "[68]\tvalidation_0-auc:0.820248\n",
            "[69]\tvalidation_0-auc:0.820977\n",
            "[70]\tvalidation_0-auc:0.819098\n",
            "[71]\tvalidation_0-auc:0.818413\n",
            "[72]\tvalidation_0-auc:0.818678\n",
            "[73]\tvalidation_0-auc:0.818767\n",
            "[74]\tvalidation_0-auc:0.821773\n",
            "[75]\tvalidation_0-auc:0.822944\n",
            "[76]\tvalidation_0-auc:0.822657\n",
            "[77]\tvalidation_0-auc:0.82153\n",
            "[78]\tvalidation_0-auc:0.822878\n",
            "Stopping. Best iteration:\n",
            "[28]\tvalidation_0-auc:0.828537\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.2     | 0.7285714285714285 |        0.62        | 0.39490445859872614 |  0.4824902723735409 |\n",
            "|     GRU 0.2      | 0.7510204081632653 | 0.6767676767676768 |  0.4267515923566879 |      0.5234375      |\n",
            "|   XGBoost 0.2    | 0.7551020408163265 | 0.6868686868686869 | 0.43312101910828027 |  0.5312500000000001 |\n",
            "|    Logreg 0.2    | 0.7285714285714285 | 0.6276595744680851 | 0.37579617834394907 | 0.47011952191235057 |\n",
            "|     SVM 0.2      | 0.7224489795918367 | 0.6567164179104478 |  0.2802547770700637 | 0.39285714285714285 |\n",
            "|  LSTM beta 0.2   | 0.7308533916849015 | 0.5887096774193549 |  0.503448275862069  |  0.5427509293680297 |\n",
            "|   GRU beta 0.2   | 0.7549234135667396 | 0.6341463414634146 |  0.5379310344827586 |  0.582089552238806  |\n",
            "| XGBoost beta 0.2 | 0.8096280087527352 | 0.7132352941176471 |  0.6689655172413793 |  0.6903914590747331 |\n",
            "| logreg beta 0.2  | 0.7461706783369803 | 0.5911949685534591 |  0.6482758620689655 |  0.618421052631579  |\n",
            "|   svm beta 0.2   |  0.75054704595186  | 0.6083916083916084 |         0.6         |  0.6041666666666667 |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.5446 - accuracy: 0.7785 - val_loss: 0.7283 - val_accuracy: 0.6082\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5358 - accuracy: 0.7805 - val_loss: 0.6590 - val_accuracy: 0.6082\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4871 - accuracy: 0.7846 - val_loss: 0.6202 - val_accuracy: 0.7020\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4673 - accuracy: 0.8020 - val_loss: 0.6433 - val_accuracy: 0.6510\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4459 - accuracy: 0.8154 - val_loss: 0.6433 - val_accuracy: 0.7000\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.5325 - accuracy: 0.7805 - val_loss: 0.8302 - val_accuracy: 0.6020\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4807 - accuracy: 0.7946 - val_loss: 1.0085 - val_accuracy: 0.6245\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4489 - accuracy: 0.8275 - val_loss: 0.7887 - val_accuracy: 0.6551\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4201 - accuracy: 0.8302 - val_loss: 0.7148 - val_accuracy: 0.6714\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4020 - accuracy: 0.8450 - val_loss: 0.7379 - val_accuracy: 0.7163\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.71293\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.736997\n",
            "[2]\tvalidation_0-auc:0.736866\n",
            "[3]\tvalidation_0-auc:0.732959\n",
            "[4]\tvalidation_0-auc:0.735214\n",
            "[5]\tvalidation_0-auc:0.737521\n",
            "[6]\tvalidation_0-auc:0.742782\n",
            "[7]\tvalidation_0-auc:0.740842\n",
            "[8]\tvalidation_0-auc:0.741183\n",
            "[9]\tvalidation_0-auc:0.742056\n",
            "[10]\tvalidation_0-auc:0.741471\n",
            "[11]\tvalidation_0-auc:0.741017\n",
            "[12]\tvalidation_0-auc:0.740711\n",
            "[13]\tvalidation_0-auc:0.740816\n",
            "[14]\tvalidation_0-auc:0.740143\n",
            "[15]\tvalidation_0-auc:0.740702\n",
            "[16]\tvalidation_0-auc:0.741593\n",
            "[17]\tvalidation_0-auc:0.742362\n",
            "[18]\tvalidation_0-auc:0.743787\n",
            "[19]\tvalidation_0-auc:0.743691\n",
            "[20]\tvalidation_0-auc:0.748978\n",
            "[21]\tvalidation_0-auc:0.749607\n",
            "[22]\tvalidation_0-auc:0.750647\n",
            "[23]\tvalidation_0-auc:0.750367\n",
            "[24]\tvalidation_0-auc:0.749449\n",
            "[25]\tvalidation_0-auc:0.74903\n",
            "[26]\tvalidation_0-auc:0.747693\n",
            "[27]\tvalidation_0-auc:0.749336\n",
            "[28]\tvalidation_0-auc:0.75014\n",
            "[29]\tvalidation_0-auc:0.750157\n",
            "[30]\tvalidation_0-auc:0.750122\n",
            "[31]\tvalidation_0-auc:0.750612\n",
            "[32]\tvalidation_0-auc:0.751066\n",
            "[33]\tvalidation_0-auc:0.752141\n",
            "[34]\tvalidation_0-auc:0.752062\n",
            "[35]\tvalidation_0-auc:0.75256\n",
            "[36]\tvalidation_0-auc:0.752464\n",
            "[37]\tvalidation_0-auc:0.753801\n",
            "[38]\tvalidation_0-auc:0.75409\n",
            "[39]\tvalidation_0-auc:0.754186\n",
            "[40]\tvalidation_0-auc:0.754002\n",
            "[41]\tvalidation_0-auc:0.75423\n",
            "[42]\tvalidation_0-auc:0.753627\n",
            "[43]\tvalidation_0-auc:0.753793\n",
            "[44]\tvalidation_0-auc:0.754404\n",
            "[45]\tvalidation_0-auc:0.754684\n",
            "[46]\tvalidation_0-auc:0.754404\n",
            "[47]\tvalidation_0-auc:0.75374\n",
            "[48]\tvalidation_0-auc:0.753565\n",
            "[49]\tvalidation_0-auc:0.753356\n",
            "[50]\tvalidation_0-auc:0.753897\n",
            "[51]\tvalidation_0-auc:0.754658\n",
            "[52]\tvalidation_0-auc:0.754649\n",
            "[53]\tvalidation_0-auc:0.754841\n",
            "[54]\tvalidation_0-auc:0.754614\n",
            "[55]\tvalidation_0-auc:0.754064\n",
            "[56]\tvalidation_0-auc:0.754448\n",
            "[57]\tvalidation_0-auc:0.754011\n",
            "[58]\tvalidation_0-auc:0.753793\n",
            "[59]\tvalidation_0-auc:0.753513\n",
            "[60]\tvalidation_0-auc:0.753461\n",
            "[61]\tvalidation_0-auc:0.753347\n",
            "[62]\tvalidation_0-auc:0.753749\n",
            "[63]\tvalidation_0-auc:0.753225\n",
            "[64]\tvalidation_0-auc:0.752735\n",
            "[65]\tvalidation_0-auc:0.75284\n",
            "[66]\tvalidation_0-auc:0.752997\n",
            "[67]\tvalidation_0-auc:0.753487\n",
            "[68]\tvalidation_0-auc:0.753592\n",
            "[69]\tvalidation_0-auc:0.753487\n",
            "[70]\tvalidation_0-auc:0.753574\n",
            "[71]\tvalidation_0-auc:0.753871\n",
            "[72]\tvalidation_0-auc:0.753032\n",
            "[73]\tvalidation_0-auc:0.753836\n",
            "[74]\tvalidation_0-auc:0.753801\n",
            "[75]\tvalidation_0-auc:0.754186\n",
            "[76]\tvalidation_0-auc:0.753731\n",
            "[77]\tvalidation_0-auc:0.753557\n",
            "[78]\tvalidation_0-auc:0.752796\n",
            "[79]\tvalidation_0-auc:0.752657\n",
            "[80]\tvalidation_0-auc:0.753094\n",
            "[81]\tvalidation_0-auc:0.752884\n",
            "[82]\tvalidation_0-auc:0.753059\n",
            "[83]\tvalidation_0-auc:0.752927\n",
            "[84]\tvalidation_0-auc:0.752761\n",
            "[85]\tvalidation_0-auc:0.752954\n",
            "[86]\tvalidation_0-auc:0.753085\n",
            "[87]\tvalidation_0-auc:0.753697\n",
            "[88]\tvalidation_0-auc:0.753242\n",
            "[89]\tvalidation_0-auc:0.75305\n",
            "[90]\tvalidation_0-auc:0.752648\n",
            "[91]\tvalidation_0-auc:0.752665\n",
            "[92]\tvalidation_0-auc:0.752386\n",
            "[93]\tvalidation_0-auc:0.751756\n",
            "[94]\tvalidation_0-auc:0.752473\n",
            "[95]\tvalidation_0-auc:0.752718\n",
            "[96]\tvalidation_0-auc:0.752997\n",
            "[97]\tvalidation_0-auc:0.753172\n",
            "[98]\tvalidation_0-auc:0.753015\n",
            "[99]\tvalidation_0-auc:0.75305\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.5534 - accuracy: 0.7701 - val_loss: 0.7569 - val_accuracy: 0.6061\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5358 - accuracy: 0.7756 - val_loss: 0.7165 - val_accuracy: 0.6061\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5110 - accuracy: 0.7763 - val_loss: 0.6506 - val_accuracy: 0.6061\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4906 - accuracy: 0.7824 - val_loss: 0.6400 - val_accuracy: 0.5536\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4765 - accuracy: 0.7852 - val_loss: 0.6176 - val_accuracy: 0.5646\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.5421 - accuracy: 0.7742 - val_loss: 0.7877 - val_accuracy: 0.5427\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4398 - accuracy: 0.8181 - val_loss: 0.8291 - val_accuracy: 0.7024\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4182 - accuracy: 0.8257 - val_loss: 0.9363 - val_accuracy: 0.6630\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4124 - accuracy: 0.8250 - val_loss: 0.8585 - val_accuracy: 0.6827\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.3969 - accuracy: 0.8346 - val_loss: 0.8271 - val_accuracy: 0.6849\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.767399\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.766607\n",
            "[2]\tvalidation_0-auc:0.792619\n",
            "[3]\tvalidation_0-auc:0.791426\n",
            "[4]\tvalidation_0-auc:0.790574\n",
            "[5]\tvalidation_0-auc:0.788648\n",
            "[6]\tvalidation_0-auc:0.791336\n",
            "[7]\tvalidation_0-auc:0.797172\n",
            "[8]\tvalidation_0-auc:0.804814\n",
            "[9]\tvalidation_0-auc:0.802808\n",
            "[10]\tvalidation_0-auc:0.804001\n",
            "[11]\tvalidation_0-auc:0.806378\n",
            "[12]\tvalidation_0-auc:0.806809\n",
            "[13]\tvalidation_0-auc:0.804553\n",
            "[14]\tvalidation_0-auc:0.804202\n",
            "[15]\tvalidation_0-auc:0.809106\n",
            "[16]\tvalidation_0-auc:0.807732\n",
            "[17]\tvalidation_0-auc:0.80708\n",
            "[18]\tvalidation_0-auc:0.805465\n",
            "[19]\tvalidation_0-auc:0.804232\n",
            "[20]\tvalidation_0-auc:0.80357\n",
            "[21]\tvalidation_0-auc:0.804192\n",
            "[22]\tvalidation_0-auc:0.803018\n",
            "[23]\tvalidation_0-auc:0.800391\n",
            "[24]\tvalidation_0-auc:0.798034\n",
            "[25]\tvalidation_0-auc:0.800241\n",
            "[26]\tvalidation_0-auc:0.798255\n",
            "[27]\tvalidation_0-auc:0.800963\n",
            "[28]\tvalidation_0-auc:0.800361\n",
            "[29]\tvalidation_0-auc:0.799669\n",
            "[30]\tvalidation_0-auc:0.801284\n",
            "[31]\tvalidation_0-auc:0.801123\n",
            "[32]\tvalidation_0-auc:0.798195\n",
            "[33]\tvalidation_0-auc:0.799178\n",
            "[34]\tvalidation_0-auc:0.794986\n",
            "[35]\tvalidation_0-auc:0.796831\n",
            "[36]\tvalidation_0-auc:0.792118\n",
            "[37]\tvalidation_0-auc:0.792679\n",
            "[38]\tvalidation_0-auc:0.790674\n",
            "[39]\tvalidation_0-auc:0.79274\n",
            "[40]\tvalidation_0-auc:0.790754\n",
            "[41]\tvalidation_0-auc:0.789892\n",
            "[42]\tvalidation_0-auc:0.789601\n",
            "[43]\tvalidation_0-auc:0.787174\n",
            "[44]\tvalidation_0-auc:0.788618\n",
            "[45]\tvalidation_0-auc:0.787635\n",
            "[46]\tvalidation_0-auc:0.78916\n",
            "[47]\tvalidation_0-auc:0.787335\n",
            "[48]\tvalidation_0-auc:0.786793\n",
            "[49]\tvalidation_0-auc:0.787114\n",
            "[50]\tvalidation_0-auc:0.788317\n",
            "[51]\tvalidation_0-auc:0.787675\n",
            "[52]\tvalidation_0-auc:0.787916\n",
            "[53]\tvalidation_0-auc:0.787936\n",
            "[54]\tvalidation_0-auc:0.788759\n",
            "[55]\tvalidation_0-auc:0.789501\n",
            "[56]\tvalidation_0-auc:0.786994\n",
            "[57]\tvalidation_0-auc:0.787675\n",
            "[58]\tvalidation_0-auc:0.787254\n",
            "[59]\tvalidation_0-auc:0.788598\n",
            "[60]\tvalidation_0-auc:0.788337\n",
            "[61]\tvalidation_0-auc:0.786673\n",
            "[62]\tvalidation_0-auc:0.786111\n",
            "[63]\tvalidation_0-auc:0.786051\n",
            "[64]\tvalidation_0-auc:0.785189\n",
            "[65]\tvalidation_0-auc:0.784587\n",
            "Stopping. Best iteration:\n",
            "[15]\tvalidation_0-auc:0.809106\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+----------------------+----------------------+----------------------+\n",
            "|       Model       |      Accuracy      |      Precision       |        Recall        |       F1 score       |\n",
            "+-------------------+--------------------+----------------------+----------------------+----------------------+\n",
            "|     LSTM 0.15     |        0.7         |  0.7102803738317757  |  0.3958333333333333  |  0.5083612040133778  |\n",
            "|      GRU 0.15     | 0.7163265306122449 |  0.7476635514018691  |  0.4166666666666667  |  0.5351170568561873  |\n",
            "|    XGBoost 0.15   | 0.6979591836734694 |         0.75         |       0.34375        |  0.4714285714285714  |\n",
            "|    Logreg 0.15    | 0.6795918367346939 |  0.7272727272727273  |  0.2916666666666667  |   0.41635687732342   |\n",
            "|      SVM 0.15     | 0.6775510204081633 |  0.7297297297297297  |       0.28125        | 0.40601503759398494  |\n",
            "|   LSTM beta 0.15  | 0.5645514223194749 | 0.047619047619047616 | 0.005555555555555556 | 0.009950248756218907 |\n",
            "|   GRU beta 0.15   | 0.6849015317286652 |        0.6125        |  0.5444444444444444  |  0.5764705882352941  |\n",
            "| XGBoost beta 0.15 | 0.7592997811816192 |  0.8070175438596491  |  0.5111111111111111  |  0.6258503401360545  |\n",
            "|  logreg beta 0.15 | 0.6673960612691466 |  0.6147540983606558  |  0.4166666666666667  |  0.4966887417218544  |\n",
            "|   svm beta 0.15   | 0.6739606126914661 |  0.6115107913669064  |  0.4722222222222222  |  0.5329153605015674  |\n",
            "+-------------------+--------------------+----------------------+----------------------+----------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "6mWRVqUbCs-B",
        "outputId": "943676a6-26b3-49d7-eddf-b8932812a05a"
      },
      "source": [
        "Result_cross.to_csv('AAPL_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.825397</td>\n",
              "      <td>0.620408</td>\n",
              "      <td>0.358621</td>\n",
              "      <td>0.229075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.831858</td>\n",
              "      <td>0.689796</td>\n",
              "      <td>0.552941</td>\n",
              "      <td>0.414097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.843284</td>\n",
              "      <td>0.724490</td>\n",
              "      <td>0.626039</td>\n",
              "      <td>0.497797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.852174</td>\n",
              "      <td>0.702041</td>\n",
              "      <td>0.573099</td>\n",
              "      <td>0.431718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.845528</td>\n",
              "      <td>0.710204</td>\n",
              "      <td>0.594286</td>\n",
              "      <td>0.458150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.724359</td>\n",
              "      <td>0.682713</td>\n",
              "      <td>0.609164</td>\n",
              "      <td>0.525581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.745283</td>\n",
              "      <td>0.757112</td>\n",
              "      <td>0.740047</td>\n",
              "      <td>0.734884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.746753</td>\n",
              "      <td>0.695842</td>\n",
              "      <td>0.623306</td>\n",
              "      <td>0.534884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.630197</td>\n",
              "      <td>0.523944</td>\n",
              "      <td>0.432558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.609195</td>\n",
              "      <td>0.612691</td>\n",
              "      <td>0.544987</td>\n",
              "      <td>0.493023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.620000</td>\n",
              "      <td>0.728571</td>\n",
              "      <td>0.482490</td>\n",
              "      <td>0.394904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.676768</td>\n",
              "      <td>0.751020</td>\n",
              "      <td>0.523438</td>\n",
              "      <td>0.426752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.686869</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.531250</td>\n",
              "      <td>0.433121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.627660</td>\n",
              "      <td>0.728571</td>\n",
              "      <td>0.470120</td>\n",
              "      <td>0.375796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.656716</td>\n",
              "      <td>0.722449</td>\n",
              "      <td>0.392857</td>\n",
              "      <td>0.280255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.588710</td>\n",
              "      <td>0.730853</td>\n",
              "      <td>0.542751</td>\n",
              "      <td>0.503448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.634146</td>\n",
              "      <td>0.754923</td>\n",
              "      <td>0.582090</td>\n",
              "      <td>0.537931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.713235</td>\n",
              "      <td>0.809628</td>\n",
              "      <td>0.690391</td>\n",
              "      <td>0.668966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.591195</td>\n",
              "      <td>0.746171</td>\n",
              "      <td>0.618421</td>\n",
              "      <td>0.648276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.608392</td>\n",
              "      <td>0.750547</td>\n",
              "      <td>0.604167</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.710280</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.508361</td>\n",
              "      <td>0.395833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.747664</td>\n",
              "      <td>0.716327</td>\n",
              "      <td>0.535117</td>\n",
              "      <td>0.416667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.697959</td>\n",
              "      <td>0.471429</td>\n",
              "      <td>0.343750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.679592</td>\n",
              "      <td>0.416357</td>\n",
              "      <td>0.291667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.729730</td>\n",
              "      <td>0.677551</td>\n",
              "      <td>0.406015</td>\n",
              "      <td>0.281250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>0.564551</td>\n",
              "      <td>0.009950</td>\n",
              "      <td>0.005556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.612500</td>\n",
              "      <td>0.684902</td>\n",
              "      <td>0.576471</td>\n",
              "      <td>0.544444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.807018</td>\n",
              "      <td>0.759300</td>\n",
              "      <td>0.625850</td>\n",
              "      <td>0.511111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.614754</td>\n",
              "      <td>0.667396</td>\n",
              "      <td>0.496689</td>\n",
              "      <td>0.416667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.611511</td>\n",
              "      <td>0.673961</td>\n",
              "      <td>0.532915</td>\n",
              "      <td>0.472222</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  AAPL  0.825397  0.620408  0.358621  0.229075\n",
              "1            GRU 0.1  AAPL  0.831858  0.689796  0.552941  0.414097\n",
              "2        XGBoost 0.1  AAPL  0.843284  0.724490  0.626039  0.497797\n",
              "3         Logreg 0.1  AAPL  0.852174  0.702041  0.573099  0.431718\n",
              "4            SVM 0.1  AAPL  0.845528  0.710204  0.594286  0.458150\n",
              "5      LSTM beta 0.1  AAPL  0.724359  0.682713  0.609164  0.525581\n",
              "6       GRU beta 0.1  AAPL  0.745283  0.757112  0.740047  0.734884\n",
              "7   XGBoost beta 0.1  AAPL  0.746753  0.695842  0.623306  0.534884\n",
              "8    logreg beta 0.1  AAPL  0.664286  0.630197  0.523944  0.432558\n",
              "9       svm beta 0.1  AAPL  0.609195  0.612691  0.544987  0.493023\n",
              "0           LSTM 0.2  AAPL  0.620000  0.728571  0.482490  0.394904\n",
              "1            GRU 0.2  AAPL  0.676768  0.751020  0.523438  0.426752\n",
              "2        XGBoost 0.2  AAPL  0.686869  0.755102  0.531250  0.433121\n",
              "3         Logreg 0.2  AAPL  0.627660  0.728571  0.470120  0.375796\n",
              "4            SVM 0.2  AAPL  0.656716  0.722449  0.392857  0.280255\n",
              "5      LSTM beta 0.2  AAPL  0.588710  0.730853  0.542751  0.503448\n",
              "6       GRU beta 0.2  AAPL  0.634146  0.754923  0.582090  0.537931\n",
              "7   XGBoost beta 0.2  AAPL  0.713235  0.809628  0.690391  0.668966\n",
              "8    logreg beta 0.2  AAPL  0.591195  0.746171  0.618421  0.648276\n",
              "9       svm beta 0.2  AAPL  0.608392  0.750547  0.604167  0.600000\n",
              "0          LSTM 0.15  AAPL  0.710280  0.700000  0.508361  0.395833\n",
              "1           GRU 0.15  AAPL  0.747664  0.716327  0.535117  0.416667\n",
              "2       XGBoost 0.15  AAPL  0.750000  0.697959  0.471429  0.343750\n",
              "3        Logreg 0.15  AAPL  0.727273  0.679592  0.416357  0.291667\n",
              "4           SVM 0.15  AAPL  0.729730  0.677551  0.406015  0.281250\n",
              "5     LSTM beta 0.15  AAPL  0.047619  0.564551  0.009950  0.005556\n",
              "6      GRU beta 0.15  AAPL  0.612500  0.684902  0.576471  0.544444\n",
              "7  XGBoost beta 0.15  AAPL  0.807018  0.759300  0.625850  0.511111\n",
              "8   logreg beta 0.15  AAPL  0.614754  0.667396  0.496689  0.416667\n",
              "9      svm beta 0.15  AAPL  0.611511  0.673961  0.532915  0.472222"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhE6Y93kCs-C"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-Cry4swCs-C"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whq58hnjCs-C",
        "outputId": "de60a07d-ecff-4826-dbb3-9ff85c62e8fb"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"AAPL\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6108 - accuracy: 0.7228 - val_loss: 0.8340 - val_accuracy: 0.5367\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6008 - accuracy: 0.7248 - val_loss: 0.7614 - val_accuracy: 0.5367\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5895 - accuracy: 0.7248 - val_loss: 0.7654 - val_accuracy: 0.5367\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5914 - accuracy: 0.7248 - val_loss: 0.7373 - val_accuracy: 0.5367\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5653 - accuracy: 0.7255 - val_loss: 0.6991 - val_accuracy: 0.5367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6095 - accuracy: 0.7248 - val_loss: 0.7107 - val_accuracy: 0.5286\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5349 - accuracy: 0.7584 - val_loss: 0.7216 - val_accuracy: 0.6388\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4787 - accuracy: 0.7993 - val_loss: 0.6566 - val_accuracy: 0.7306\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4659 - accuracy: 0.8020 - val_loss: 0.6337 - val_accuracy: 0.7122\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4576 - accuracy: 0.8081 - val_loss: 0.5624 - val_accuracy: 0.7551\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.763672\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.767248\n",
            "[2]\tvalidation_0-auc:0.765749\n",
            "[3]\tvalidation_0-auc:0.767031\n",
            "[4]\tvalidation_0-auc:0.771394\n",
            "[5]\tvalidation_0-auc:0.772039\n",
            "[6]\tvalidation_0-auc:0.770716\n",
            "[7]\tvalidation_0-auc:0.775314\n",
            "[8]\tvalidation_0-auc:0.783236\n",
            "[9]\tvalidation_0-auc:0.787717\n",
            "[10]\tvalidation_0-auc:0.788856\n",
            "[11]\tvalidation_0-auc:0.78966\n",
            "[12]\tvalidation_0-auc:0.791536\n",
            "[13]\tvalidation_0-auc:0.793236\n",
            "[14]\tvalidation_0-auc:0.792893\n",
            "[15]\tvalidation_0-auc:0.7909\n",
            "[16]\tvalidation_0-auc:0.7909\n",
            "[17]\tvalidation_0-auc:0.790648\n",
            "[18]\tvalidation_0-auc:0.791637\n",
            "[19]\tvalidation_0-auc:0.793521\n",
            "[20]\tvalidation_0-auc:0.79399\n",
            "[21]\tvalidation_0-auc:0.79069\n",
            "[22]\tvalidation_0-auc:0.79255\n",
            "[23]\tvalidation_0-auc:0.793245\n",
            "[24]\tvalidation_0-auc:0.793948\n",
            "[25]\tvalidation_0-auc:0.794308\n",
            "[26]\tvalidation_0-auc:0.795648\n",
            "[27]\tvalidation_0-auc:0.799015\n",
            "[28]\tvalidation_0-auc:0.800389\n",
            "[29]\tvalidation_0-auc:0.799836\n",
            "[30]\tvalidation_0-auc:0.800564\n",
            "[31]\tvalidation_0-auc:0.800045\n",
            "[32]\tvalidation_0-auc:0.800196\n",
            "[33]\tvalidation_0-auc:0.799484\n",
            "[34]\tvalidation_0-auc:0.799057\n",
            "[35]\tvalidation_0-auc:0.799559\n",
            "[36]\tvalidation_0-auc:0.801402\n",
            "[37]\tvalidation_0-auc:0.800171\n",
            "[38]\tvalidation_0-auc:0.800121\n",
            "[39]\tvalidation_0-auc:0.801092\n",
            "[40]\tvalidation_0-auc:0.802399\n",
            "[41]\tvalidation_0-auc:0.802231\n",
            "[42]\tvalidation_0-auc:0.80203\n",
            "[43]\tvalidation_0-auc:0.802139\n",
            "[44]\tvalidation_0-auc:0.802306\n",
            "[45]\tvalidation_0-auc:0.802156\n",
            "[46]\tvalidation_0-auc:0.802876\n",
            "[47]\tvalidation_0-auc:0.804266\n",
            "[48]\tvalidation_0-auc:0.80461\n",
            "[49]\tvalidation_0-auc:0.804568\n",
            "[50]\tvalidation_0-auc:0.804216\n",
            "[51]\tvalidation_0-auc:0.804199\n",
            "[52]\tvalidation_0-auc:0.804015\n",
            "[53]\tvalidation_0-auc:0.804518\n",
            "[54]\tvalidation_0-auc:0.80564\n",
            "[55]\tvalidation_0-auc:0.805707\n",
            "[56]\tvalidation_0-auc:0.805673\n",
            "[57]\tvalidation_0-auc:0.806335\n",
            "[58]\tvalidation_0-auc:0.805581\n",
            "[59]\tvalidation_0-auc:0.806519\n",
            "[60]\tvalidation_0-auc:0.807089\n",
            "[61]\tvalidation_0-auc:0.807256\n",
            "[62]\tvalidation_0-auc:0.807708\n",
            "[63]\tvalidation_0-auc:0.808286\n",
            "[64]\tvalidation_0-auc:0.808387\n",
            "[65]\tvalidation_0-auc:0.80909\n",
            "[66]\tvalidation_0-auc:0.809777\n",
            "[67]\tvalidation_0-auc:0.809492\n",
            "[68]\tvalidation_0-auc:0.809911\n",
            "[69]\tvalidation_0-auc:0.809861\n",
            "[70]\tvalidation_0-auc:0.809961\n",
            "[71]\tvalidation_0-auc:0.809367\n",
            "[72]\tvalidation_0-auc:0.809651\n",
            "[73]\tvalidation_0-auc:0.809752\n",
            "[74]\tvalidation_0-auc:0.809886\n",
            "[75]\tvalidation_0-auc:0.809534\n",
            "[76]\tvalidation_0-auc:0.808931\n",
            "[77]\tvalidation_0-auc:0.810154\n",
            "[78]\tvalidation_0-auc:0.810774\n",
            "[79]\tvalidation_0-auc:0.810573\n",
            "[80]\tvalidation_0-auc:0.810539\n",
            "[81]\tvalidation_0-auc:0.810606\n",
            "[82]\tvalidation_0-auc:0.810112\n",
            "[83]\tvalidation_0-auc:0.809392\n",
            "[84]\tvalidation_0-auc:0.809392\n",
            "[85]\tvalidation_0-auc:0.809409\n",
            "[86]\tvalidation_0-auc:0.809928\n",
            "[87]\tvalidation_0-auc:0.809945\n",
            "[88]\tvalidation_0-auc:0.810079\n",
            "[89]\tvalidation_0-auc:0.809961\n",
            "[90]\tvalidation_0-auc:0.809593\n",
            "[91]\tvalidation_0-auc:0.809827\n",
            "[92]\tvalidation_0-auc:0.809945\n",
            "[93]\tvalidation_0-auc:0.809995\n",
            "[94]\tvalidation_0-auc:0.810263\n",
            "[95]\tvalidation_0-auc:0.809953\n",
            "[96]\tvalidation_0-auc:0.810037\n",
            "[97]\tvalidation_0-auc:0.810271\n",
            "[98]\tvalidation_0-auc:0.810321\n",
            "[99]\tvalidation_0-auc:0.810455\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6101 - accuracy: 0.7172 - val_loss: 0.7768 - val_accuracy: 0.5295\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5863 - accuracy: 0.7159 - val_loss: 0.7368 - val_accuracy: 0.4858\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5596 - accuracy: 0.7310 - val_loss: 0.6884 - val_accuracy: 0.5208\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5307 - accuracy: 0.7522 - val_loss: 0.6553 - val_accuracy: 0.6433\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4971 - accuracy: 0.7742 - val_loss: 0.6359 - val_accuracy: 0.7002\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.5758 - accuracy: 0.7227 - val_loss: 0.8105 - val_accuracy: 0.5492\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5314 - accuracy: 0.7557 - val_loss: 0.8707 - val_accuracy: 0.6105\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5076 - accuracy: 0.7721 - val_loss: 0.8456 - val_accuracy: 0.6433\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4768 - accuracy: 0.7982 - val_loss: 0.8403 - val_accuracy: 0.6761\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4669 - accuracy: 0.7948 - val_loss: 0.8327 - val_accuracy: 0.6718\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.732472\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.751903\n",
            "[2]\tvalidation_0-auc:0.755074\n",
            "[3]\tvalidation_0-auc:0.766577\n",
            "[4]\tvalidation_0-auc:0.762599\n",
            "[5]\tvalidation_0-auc:0.767922\n",
            "[6]\tvalidation_0-auc:0.776254\n",
            "[7]\tvalidation_0-auc:0.781261\n",
            "[8]\tvalidation_0-auc:0.782587\n",
            "[9]\tvalidation_0-auc:0.773275\n",
            "[10]\tvalidation_0-auc:0.773438\n",
            "[11]\tvalidation_0-auc:0.772237\n",
            "[12]\tvalidation_0-auc:0.768854\n",
            "[13]\tvalidation_0-auc:0.768451\n",
            "[14]\tvalidation_0-auc:0.767769\n",
            "[15]\tvalidation_0-auc:0.769277\n",
            "[16]\tvalidation_0-auc:0.765145\n",
            "[17]\tvalidation_0-auc:0.764415\n",
            "[18]\tvalidation_0-auc:0.765443\n",
            "[19]\tvalidation_0-auc:0.764271\n",
            "[20]\tvalidation_0-auc:0.76354\n",
            "[21]\tvalidation_0-auc:0.763569\n",
            "[22]\tvalidation_0-auc:0.761955\n",
            "[23]\tvalidation_0-auc:0.763781\n",
            "[24]\tvalidation_0-auc:0.758956\n",
            "[25]\tvalidation_0-auc:0.757765\n",
            "[26]\tvalidation_0-auc:0.752662\n",
            "[27]\tvalidation_0-auc:0.749327\n",
            "[28]\tvalidation_0-auc:0.749279\n",
            "[29]\tvalidation_0-auc:0.747242\n",
            "[30]\tvalidation_0-auc:0.743456\n",
            "[31]\tvalidation_0-auc:0.743091\n",
            "[32]\tvalidation_0-auc:0.743042\n",
            "[33]\tvalidation_0-auc:0.737834\n",
            "[34]\tvalidation_0-auc:0.738141\n",
            "[35]\tvalidation_0-auc:0.738507\n",
            "[36]\tvalidation_0-auc:0.737007\n",
            "[37]\tvalidation_0-auc:0.734643\n",
            "[38]\tvalidation_0-auc:0.733769\n",
            "[39]\tvalidation_0-auc:0.735374\n",
            "[40]\tvalidation_0-auc:0.733586\n",
            "[41]\tvalidation_0-auc:0.731213\n",
            "[42]\tvalidation_0-auc:0.7293\n",
            "[43]\tvalidation_0-auc:0.72536\n",
            "[44]\tvalidation_0-auc:0.726206\n",
            "[45]\tvalidation_0-auc:0.722919\n",
            "[46]\tvalidation_0-auc:0.726571\n",
            "[47]\tvalidation_0-auc:0.726091\n",
            "[48]\tvalidation_0-auc:0.726984\n",
            "[49]\tvalidation_0-auc:0.727984\n",
            "[50]\tvalidation_0-auc:0.724813\n",
            "[51]\tvalidation_0-auc:0.727273\n",
            "[52]\tvalidation_0-auc:0.728387\n",
            "[53]\tvalidation_0-auc:0.727599\n",
            "[54]\tvalidation_0-auc:0.728791\n",
            "[55]\tvalidation_0-auc:0.729618\n",
            "[56]\tvalidation_0-auc:0.729733\n",
            "[57]\tvalidation_0-auc:0.727965\n",
            "[58]\tvalidation_0-auc:0.729262\n",
            "Stopping. Best iteration:\n",
            "[8]\tvalidation_0-auc:0.782587\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.536734693877551  |        0.0         |         0.0         |        0.0         |\n",
            "|     GRU 0.1      | 0.7551020408163265 | 0.8322981366459627 |  0.5903083700440529 | 0.6907216494845361 |\n",
            "|   XGBoost 0.1    | 0.7244897959183674 | 0.8432835820895522 |  0.4977973568281938 | 0.6260387811634349 |\n",
            "|    Logreg 0.1    | 0.7020408163265306 | 0.8521739130434782 | 0.43171806167400884 | 0.5730994152046784 |\n",
            "|     SVM 0.1      | 0.710204081632653  | 0.8455284552845529 |  0.4581497797356828 | 0.5942857142857143 |\n",
            "|  LSTM beta 0.1   | 0.700218818380744  | 0.7241379310344828 |  0.586046511627907  | 0.6478149100257069 |\n",
            "|   GRU beta 0.1   | 0.6717724288840262 | 0.7152317880794702 |  0.5023255813953489 | 0.5901639344262296 |\n",
            "| XGBoost beta 0.1 | 0.6958424507658644 | 0.7467532467532467 |  0.5348837209302325 | 0.6233062330623305 |\n",
            "| logreg beta 0.1  | 0.6301969365426696 | 0.6642857142857143 |  0.4325581395348837 | 0.523943661971831  |\n",
            "|   svm beta 0.1   | 0.612691466083151  | 0.6091954022988506 |  0.4930232558139535 | 0.5449871465295629 |\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6057 - accuracy: 0.7134 - val_loss: 0.6279 - val_accuracy: 0.6796\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5968 - accuracy: 0.7208 - val_loss: 0.6236 - val_accuracy: 0.6796\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5823 - accuracy: 0.7309 - val_loss: 0.5750 - val_accuracy: 0.7388\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5571 - accuracy: 0.7329 - val_loss: 0.6192 - val_accuracy: 0.6694\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5474 - accuracy: 0.7450 - val_loss: 0.5944 - val_accuracy: 0.6878\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6050 - accuracy: 0.7107 - val_loss: 0.6217 - val_accuracy: 0.6714\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5607 - accuracy: 0.7329 - val_loss: 0.6057 - val_accuracy: 0.7286\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5283 - accuracy: 0.7477 - val_loss: 0.5977 - val_accuracy: 0.7449\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5204 - accuracy: 0.7644 - val_loss: 0.6331 - val_accuracy: 0.7041\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5189 - accuracy: 0.7638 - val_loss: 0.5851 - val_accuracy: 0.7327\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.710449\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.723035\n",
            "[2]\tvalidation_0-auc:0.736424\n",
            "[3]\tvalidation_0-auc:0.739007\n",
            "[4]\tvalidation_0-auc:0.735946\n",
            "[5]\tvalidation_0-auc:0.740651\n",
            "[6]\tvalidation_0-auc:0.743148\n",
            "[7]\tvalidation_0-auc:0.742497\n",
            "[8]\tvalidation_0-auc:0.74244\n",
            "[9]\tvalidation_0-auc:0.742889\n",
            "[10]\tvalidation_0-auc:0.746131\n",
            "[11]\tvalidation_0-auc:0.746141\n",
            "[12]\tvalidation_0-auc:0.746428\n",
            "[13]\tvalidation_0-auc:0.746696\n",
            "[14]\tvalidation_0-auc:0.746036\n",
            "[15]\tvalidation_0-auc:0.747375\n",
            "[16]\tvalidation_0-auc:0.747499\n",
            "[17]\tvalidation_0-auc:0.748369\n",
            "[18]\tvalidation_0-auc:0.74747\n",
            "[19]\tvalidation_0-auc:0.748962\n",
            "[20]\tvalidation_0-auc:0.747614\n",
            "[21]\tvalidation_0-auc:0.747518\n",
            "[22]\tvalidation_0-auc:0.748675\n",
            "[23]\tvalidation_0-auc:0.748551\n",
            "[24]\tvalidation_0-auc:0.74813\n",
            "[25]\tvalidation_0-auc:0.75121\n",
            "[26]\tvalidation_0-auc:0.750808\n",
            "[27]\tvalidation_0-auc:0.75428\n",
            "[28]\tvalidation_0-auc:0.754538\n",
            "[29]\tvalidation_0-auc:0.756432\n",
            "[30]\tvalidation_0-auc:0.755131\n",
            "[31]\tvalidation_0-auc:0.755858\n",
            "[32]\tvalidation_0-auc:0.757445\n",
            "[33]\tvalidation_0-auc:0.757675\n",
            "[34]\tvalidation_0-auc:0.757962\n",
            "[35]\tvalidation_0-auc:0.757436\n",
            "[36]\tvalidation_0-auc:0.756154\n",
            "[37]\tvalidation_0-auc:0.75602\n",
            "[38]\tvalidation_0-auc:0.757178\n",
            "[39]\tvalidation_0-auc:0.757292\n",
            "[40]\tvalidation_0-auc:0.757331\n",
            "[41]\tvalidation_0-auc:0.759129\n",
            "[42]\tvalidation_0-auc:0.758832\n",
            "[43]\tvalidation_0-auc:0.759865\n",
            "[44]\tvalidation_0-auc:0.759779\n",
            "[45]\tvalidation_0-auc:0.75909\n",
            "[46]\tvalidation_0-auc:0.758765\n",
            "[47]\tvalidation_0-auc:0.759798\n",
            "[48]\tvalidation_0-auc:0.760563\n",
            "[49]\tvalidation_0-auc:0.760735\n",
            "[50]\tvalidation_0-auc:0.759817\n",
            "[51]\tvalidation_0-auc:0.759377\n",
            "[52]\tvalidation_0-auc:0.760993\n",
            "[53]\tvalidation_0-auc:0.760573\n",
            "[54]\tvalidation_0-auc:0.761491\n",
            "[55]\tvalidation_0-auc:0.761338\n",
            "[56]\tvalidation_0-auc:0.761816\n",
            "[57]\tvalidation_0-auc:0.762275\n",
            "[58]\tvalidation_0-auc:0.762983\n",
            "[59]\tvalidation_0-auc:0.762791\n",
            "[60]\tvalidation_0-auc:0.762811\n",
            "[61]\tvalidation_0-auc:0.762791\n",
            "[62]\tvalidation_0-auc:0.763442\n",
            "[63]\tvalidation_0-auc:0.763241\n",
            "[64]\tvalidation_0-auc:0.765096\n",
            "[65]\tvalidation_0-auc:0.765651\n",
            "[66]\tvalidation_0-auc:0.765861\n",
            "[67]\tvalidation_0-auc:0.765326\n",
            "[68]\tvalidation_0-auc:0.765594\n",
            "[69]\tvalidation_0-auc:0.766646\n",
            "[70]\tvalidation_0-auc:0.766894\n",
            "[71]\tvalidation_0-auc:0.766626\n",
            "[72]\tvalidation_0-auc:0.766435\n",
            "[73]\tvalidation_0-auc:0.765651\n",
            "[74]\tvalidation_0-auc:0.765326\n",
            "[75]\tvalidation_0-auc:0.764522\n",
            "[76]\tvalidation_0-auc:0.764561\n",
            "[77]\tvalidation_0-auc:0.764637\n",
            "[78]\tvalidation_0-auc:0.764102\n",
            "[79]\tvalidation_0-auc:0.764159\n",
            "[80]\tvalidation_0-auc:0.764159\n",
            "[81]\tvalidation_0-auc:0.764503\n",
            "[82]\tvalidation_0-auc:0.763719\n",
            "[83]\tvalidation_0-auc:0.762782\n",
            "[84]\tvalidation_0-auc:0.763031\n",
            "[85]\tvalidation_0-auc:0.764389\n",
            "[86]\tvalidation_0-auc:0.764446\n",
            "[87]\tvalidation_0-auc:0.764331\n",
            "[88]\tvalidation_0-auc:0.763337\n",
            "[89]\tvalidation_0-auc:0.762572\n",
            "[90]\tvalidation_0-auc:0.762973\n",
            "[91]\tvalidation_0-auc:0.763987\n",
            "[92]\tvalidation_0-auc:0.763681\n",
            "[93]\tvalidation_0-auc:0.764389\n",
            "[94]\tvalidation_0-auc:0.764025\n",
            "[95]\tvalidation_0-auc:0.763949\n",
            "[96]\tvalidation_0-auc:0.764083\n",
            "[97]\tvalidation_0-auc:0.763853\n",
            "[98]\tvalidation_0-auc:0.7637\n",
            "[99]\tvalidation_0-auc:0.763815\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6158 - accuracy: 0.7117 - val_loss: 0.6274 - val_accuracy: 0.6827\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5967 - accuracy: 0.7145 - val_loss: 0.5608 - val_accuracy: 0.6477\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5494 - accuracy: 0.7392 - val_loss: 0.5804 - val_accuracy: 0.6871\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5406 - accuracy: 0.7563 - val_loss: 0.5965 - val_accuracy: 0.7330\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5258 - accuracy: 0.7625 - val_loss: 0.6459 - val_accuracy: 0.6871\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5925 - accuracy: 0.7186 - val_loss: 0.6782 - val_accuracy: 0.6937\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5201 - accuracy: 0.7694 - val_loss: 0.7252 - val_accuracy: 0.7352\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5008 - accuracy: 0.7797 - val_loss: 0.7262 - val_accuracy: 0.7418\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4960 - accuracy: 0.7859 - val_loss: 0.7126 - val_accuracy: 0.7505\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4989 - accuracy: 0.7824 - val_loss: 0.6779 - val_accuracy: 0.7505\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.78645\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.798143\n",
            "[2]\tvalidation_0-auc:0.803504\n",
            "[3]\tvalidation_0-auc:0.807604\n",
            "[4]\tvalidation_0-auc:0.800298\n",
            "[5]\tvalidation_0-auc:0.808831\n",
            "[6]\tvalidation_0-auc:0.804012\n",
            "[7]\tvalidation_0-auc:0.804797\n",
            "[8]\tvalidation_0-auc:0.809041\n",
            "[9]\tvalidation_0-auc:0.808477\n",
            "[10]\tvalidation_0-auc:0.810986\n",
            "[11]\tvalidation_0-auc:0.814478\n",
            "[12]\tvalidation_0-auc:0.816877\n",
            "[13]\tvalidation_0-auc:0.81912\n",
            "[14]\tvalidation_0-auc:0.820071\n",
            "[15]\tvalidation_0-auc:0.816479\n",
            "[16]\tvalidation_0-auc:0.818568\n",
            "[17]\tvalidation_0-auc:0.818711\n",
            "[18]\tvalidation_0-auc:0.820679\n",
            "[19]\tvalidation_0-auc:0.821286\n",
            "[20]\tvalidation_0-auc:0.822845\n",
            "[21]\tvalidation_0-auc:0.826238\n",
            "[22]\tvalidation_0-auc:0.823806\n",
            "[23]\tvalidation_0-auc:0.824514\n",
            "[24]\tvalidation_0-auc:0.825155\n",
            "[25]\tvalidation_0-auc:0.826724\n",
            "[26]\tvalidation_0-auc:0.826691\n",
            "[27]\tvalidation_0-auc:0.827111\n",
            "[28]\tvalidation_0-auc:0.828537\n",
            "[29]\tvalidation_0-auc:0.827122\n",
            "[30]\tvalidation_0-auc:0.824945\n",
            "[31]\tvalidation_0-auc:0.824348\n",
            "[32]\tvalidation_0-auc:0.825409\n",
            "[33]\tvalidation_0-auc:0.825984\n",
            "[34]\tvalidation_0-auc:0.824878\n",
            "[35]\tvalidation_0-auc:0.823828\n",
            "[36]\tvalidation_0-auc:0.823497\n",
            "[37]\tvalidation_0-auc:0.823232\n",
            "[38]\tvalidation_0-auc:0.824182\n",
            "[39]\tvalidation_0-auc:0.823243\n",
            "[40]\tvalidation_0-auc:0.823685\n",
            "[41]\tvalidation_0-auc:0.822723\n",
            "[42]\tvalidation_0-auc:0.824481\n",
            "[43]\tvalidation_0-auc:0.825431\n",
            "[44]\tvalidation_0-auc:0.824204\n",
            "[45]\tvalidation_0-auc:0.823961\n",
            "[46]\tvalidation_0-auc:0.822922\n",
            "[47]\tvalidation_0-auc:0.823895\n",
            "[48]\tvalidation_0-auc:0.824005\n",
            "[49]\tvalidation_0-auc:0.824226\n",
            "[50]\tvalidation_0-auc:0.82111\n",
            "[51]\tvalidation_0-auc:0.820137\n",
            "[52]\tvalidation_0-auc:0.819297\n",
            "[53]\tvalidation_0-auc:0.819341\n",
            "[54]\tvalidation_0-auc:0.816888\n",
            "[55]\tvalidation_0-auc:0.817131\n",
            "[56]\tvalidation_0-auc:0.816711\n",
            "[57]\tvalidation_0-auc:0.817042\n",
            "[58]\tvalidation_0-auc:0.822436\n",
            "[59]\tvalidation_0-auc:0.822303\n",
            "[60]\tvalidation_0-auc:0.821773\n",
            "[61]\tvalidation_0-auc:0.82248\n",
            "[62]\tvalidation_0-auc:0.821905\n",
            "[63]\tvalidation_0-auc:0.820557\n",
            "[64]\tvalidation_0-auc:0.820778\n",
            "[65]\tvalidation_0-auc:0.818501\n",
            "[66]\tvalidation_0-auc:0.818988\n",
            "[67]\tvalidation_0-auc:0.818965\n",
            "[68]\tvalidation_0-auc:0.820248\n",
            "[69]\tvalidation_0-auc:0.820977\n",
            "[70]\tvalidation_0-auc:0.819098\n",
            "[71]\tvalidation_0-auc:0.818413\n",
            "[72]\tvalidation_0-auc:0.818678\n",
            "[73]\tvalidation_0-auc:0.818767\n",
            "[74]\tvalidation_0-auc:0.821773\n",
            "[75]\tvalidation_0-auc:0.822944\n",
            "[76]\tvalidation_0-auc:0.822657\n",
            "[77]\tvalidation_0-auc:0.82153\n",
            "[78]\tvalidation_0-auc:0.822878\n",
            "Stopping. Best iteration:\n",
            "[28]\tvalidation_0-auc:0.828537\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.2     | 0.6877551020408164 | 0.5588235294117647 | 0.12101910828025478 | 0.19895287958115182 |\n",
            "|     GRU 0.2      | 0.7326530612244898 | 0.6382978723404256 |  0.3821656050955414 |  0.4780876494023904 |\n",
            "|   XGBoost 0.2    | 0.7551020408163265 | 0.6868686868686869 | 0.43312101910828027 |  0.5312500000000001 |\n",
            "|    Logreg 0.2    | 0.7285714285714285 | 0.6276595744680851 | 0.37579617834394907 | 0.47011952191235057 |\n",
            "|     SVM 0.2      | 0.7224489795918367 | 0.6567164179104478 |  0.2802547770700637 | 0.39285714285714285 |\n",
            "|  LSTM beta 0.2   | 0.687089715536105  | 0.5131578947368421 |  0.2689655172413793 | 0.35294117647058826 |\n",
            "|   GRU beta 0.2   |  0.75054704595186  | 0.6148148148148148 |  0.5724137931034483 |  0.592857142857143  |\n",
            "| XGBoost beta 0.2 | 0.8096280087527352 | 0.7132352941176471 |  0.6689655172413793 |  0.6903914590747331 |\n",
            "| logreg beta 0.2  | 0.7461706783369803 | 0.5911949685534591 |  0.6482758620689655 |  0.618421052631579  |\n",
            "|   svm beta 0.2   |  0.75054704595186  | 0.6083916083916084 |         0.6         |  0.6041666666666667 |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.5421 - accuracy: 0.7779 - val_loss: 0.7473 - val_accuracy: 0.6082\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5234 - accuracy: 0.7826 - val_loss: 0.7113 - val_accuracy: 0.5980\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4806 - accuracy: 0.7913 - val_loss: 0.7424 - val_accuracy: 0.5939\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4681 - accuracy: 0.8020 - val_loss: 0.6539 - val_accuracy: 0.6122\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4404 - accuracy: 0.8074 - val_loss: 0.7538 - val_accuracy: 0.7020\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.5495 - accuracy: 0.7758 - val_loss: 0.7520 - val_accuracy: 0.6020\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4864 - accuracy: 0.7960 - val_loss: 0.6764 - val_accuracy: 0.7143\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4294 - accuracy: 0.8282 - val_loss: 0.6938 - val_accuracy: 0.6898\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4169 - accuracy: 0.8289 - val_loss: 0.6821 - val_accuracy: 0.6878\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4015 - accuracy: 0.8423 - val_loss: 0.6657 - val_accuracy: 0.7143\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.71293\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.736997\n",
            "[2]\tvalidation_0-auc:0.736866\n",
            "[3]\tvalidation_0-auc:0.732959\n",
            "[4]\tvalidation_0-auc:0.735214\n",
            "[5]\tvalidation_0-auc:0.737521\n",
            "[6]\tvalidation_0-auc:0.742782\n",
            "[7]\tvalidation_0-auc:0.740842\n",
            "[8]\tvalidation_0-auc:0.741183\n",
            "[9]\tvalidation_0-auc:0.742056\n",
            "[10]\tvalidation_0-auc:0.741471\n",
            "[11]\tvalidation_0-auc:0.741017\n",
            "[12]\tvalidation_0-auc:0.740711\n",
            "[13]\tvalidation_0-auc:0.740816\n",
            "[14]\tvalidation_0-auc:0.740143\n",
            "[15]\tvalidation_0-auc:0.740702\n",
            "[16]\tvalidation_0-auc:0.741593\n",
            "[17]\tvalidation_0-auc:0.742362\n",
            "[18]\tvalidation_0-auc:0.743787\n",
            "[19]\tvalidation_0-auc:0.743691\n",
            "[20]\tvalidation_0-auc:0.748978\n",
            "[21]\tvalidation_0-auc:0.749607\n",
            "[22]\tvalidation_0-auc:0.750647\n",
            "[23]\tvalidation_0-auc:0.750367\n",
            "[24]\tvalidation_0-auc:0.749449\n",
            "[25]\tvalidation_0-auc:0.74903\n",
            "[26]\tvalidation_0-auc:0.747693\n",
            "[27]\tvalidation_0-auc:0.749336\n",
            "[28]\tvalidation_0-auc:0.75014\n",
            "[29]\tvalidation_0-auc:0.750157\n",
            "[30]\tvalidation_0-auc:0.750122\n",
            "[31]\tvalidation_0-auc:0.750612\n",
            "[32]\tvalidation_0-auc:0.751066\n",
            "[33]\tvalidation_0-auc:0.752141\n",
            "[34]\tvalidation_0-auc:0.752062\n",
            "[35]\tvalidation_0-auc:0.75256\n",
            "[36]\tvalidation_0-auc:0.752464\n",
            "[37]\tvalidation_0-auc:0.753801\n",
            "[38]\tvalidation_0-auc:0.75409\n",
            "[39]\tvalidation_0-auc:0.754186\n",
            "[40]\tvalidation_0-auc:0.754002\n",
            "[41]\tvalidation_0-auc:0.75423\n",
            "[42]\tvalidation_0-auc:0.753627\n",
            "[43]\tvalidation_0-auc:0.753793\n",
            "[44]\tvalidation_0-auc:0.754404\n",
            "[45]\tvalidation_0-auc:0.754684\n",
            "[46]\tvalidation_0-auc:0.754404\n",
            "[47]\tvalidation_0-auc:0.75374\n",
            "[48]\tvalidation_0-auc:0.753565\n",
            "[49]\tvalidation_0-auc:0.753356\n",
            "[50]\tvalidation_0-auc:0.753897\n",
            "[51]\tvalidation_0-auc:0.754658\n",
            "[52]\tvalidation_0-auc:0.754649\n",
            "[53]\tvalidation_0-auc:0.754841\n",
            "[54]\tvalidation_0-auc:0.754614\n",
            "[55]\tvalidation_0-auc:0.754064\n",
            "[56]\tvalidation_0-auc:0.754448\n",
            "[57]\tvalidation_0-auc:0.754011\n",
            "[58]\tvalidation_0-auc:0.753793\n",
            "[59]\tvalidation_0-auc:0.753513\n",
            "[60]\tvalidation_0-auc:0.753461\n",
            "[61]\tvalidation_0-auc:0.753347\n",
            "[62]\tvalidation_0-auc:0.753749\n",
            "[63]\tvalidation_0-auc:0.753225\n",
            "[64]\tvalidation_0-auc:0.752735\n",
            "[65]\tvalidation_0-auc:0.75284\n",
            "[66]\tvalidation_0-auc:0.752997\n",
            "[67]\tvalidation_0-auc:0.753487\n",
            "[68]\tvalidation_0-auc:0.753592\n",
            "[69]\tvalidation_0-auc:0.753487\n",
            "[70]\tvalidation_0-auc:0.753574\n",
            "[71]\tvalidation_0-auc:0.753871\n",
            "[72]\tvalidation_0-auc:0.753032\n",
            "[73]\tvalidation_0-auc:0.753836\n",
            "[74]\tvalidation_0-auc:0.753801\n",
            "[75]\tvalidation_0-auc:0.754186\n",
            "[76]\tvalidation_0-auc:0.753731\n",
            "[77]\tvalidation_0-auc:0.753557\n",
            "[78]\tvalidation_0-auc:0.752796\n",
            "[79]\tvalidation_0-auc:0.752657\n",
            "[80]\tvalidation_0-auc:0.753094\n",
            "[81]\tvalidation_0-auc:0.752884\n",
            "[82]\tvalidation_0-auc:0.753059\n",
            "[83]\tvalidation_0-auc:0.752927\n",
            "[84]\tvalidation_0-auc:0.752761\n",
            "[85]\tvalidation_0-auc:0.752954\n",
            "[86]\tvalidation_0-auc:0.753085\n",
            "[87]\tvalidation_0-auc:0.753697\n",
            "[88]\tvalidation_0-auc:0.753242\n",
            "[89]\tvalidation_0-auc:0.75305\n",
            "[90]\tvalidation_0-auc:0.752648\n",
            "[91]\tvalidation_0-auc:0.752665\n",
            "[92]\tvalidation_0-auc:0.752386\n",
            "[93]\tvalidation_0-auc:0.751756\n",
            "[94]\tvalidation_0-auc:0.752473\n",
            "[95]\tvalidation_0-auc:0.752718\n",
            "[96]\tvalidation_0-auc:0.752997\n",
            "[97]\tvalidation_0-auc:0.753172\n",
            "[98]\tvalidation_0-auc:0.753015\n",
            "[99]\tvalidation_0-auc:0.75305\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.5545 - accuracy: 0.7728 - val_loss: 0.7238 - val_accuracy: 0.6061\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5388 - accuracy: 0.7756 - val_loss: 0.7078 - val_accuracy: 0.6061\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5035 - accuracy: 0.7769 - val_loss: 0.7010 - val_accuracy: 0.6061\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4870 - accuracy: 0.7769 - val_loss: 0.6159 - val_accuracy: 0.6039\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4623 - accuracy: 0.7955 - val_loss: 0.6280 - val_accuracy: 0.6149\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5255 - accuracy: 0.7763 - val_loss: 0.7411 - val_accuracy: 0.6368\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4388 - accuracy: 0.8222 - val_loss: 0.8812 - val_accuracy: 0.6718\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4195 - accuracy: 0.8332 - val_loss: 0.8816 - val_accuracy: 0.6543\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4277 - accuracy: 0.8270 - val_loss: 0.7948 - val_accuracy: 0.6915\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.3991 - accuracy: 0.8387 - val_loss: 0.8439 - val_accuracy: 0.6805\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.767399\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.766607\n",
            "[2]\tvalidation_0-auc:0.792619\n",
            "[3]\tvalidation_0-auc:0.791426\n",
            "[4]\tvalidation_0-auc:0.790574\n",
            "[5]\tvalidation_0-auc:0.788648\n",
            "[6]\tvalidation_0-auc:0.791336\n",
            "[7]\tvalidation_0-auc:0.797172\n",
            "[8]\tvalidation_0-auc:0.804814\n",
            "[9]\tvalidation_0-auc:0.802808\n",
            "[10]\tvalidation_0-auc:0.804001\n",
            "[11]\tvalidation_0-auc:0.806378\n",
            "[12]\tvalidation_0-auc:0.806809\n",
            "[13]\tvalidation_0-auc:0.804553\n",
            "[14]\tvalidation_0-auc:0.804202\n",
            "[15]\tvalidation_0-auc:0.809106\n",
            "[16]\tvalidation_0-auc:0.807732\n",
            "[17]\tvalidation_0-auc:0.80708\n",
            "[18]\tvalidation_0-auc:0.805465\n",
            "[19]\tvalidation_0-auc:0.804232\n",
            "[20]\tvalidation_0-auc:0.80357\n",
            "[21]\tvalidation_0-auc:0.804192\n",
            "[22]\tvalidation_0-auc:0.803018\n",
            "[23]\tvalidation_0-auc:0.800391\n",
            "[24]\tvalidation_0-auc:0.798034\n",
            "[25]\tvalidation_0-auc:0.800241\n",
            "[26]\tvalidation_0-auc:0.798255\n",
            "[27]\tvalidation_0-auc:0.800963\n",
            "[28]\tvalidation_0-auc:0.800361\n",
            "[29]\tvalidation_0-auc:0.799669\n",
            "[30]\tvalidation_0-auc:0.801284\n",
            "[31]\tvalidation_0-auc:0.801123\n",
            "[32]\tvalidation_0-auc:0.798195\n",
            "[33]\tvalidation_0-auc:0.799178\n",
            "[34]\tvalidation_0-auc:0.794986\n",
            "[35]\tvalidation_0-auc:0.796831\n",
            "[36]\tvalidation_0-auc:0.792118\n",
            "[37]\tvalidation_0-auc:0.792679\n",
            "[38]\tvalidation_0-auc:0.790674\n",
            "[39]\tvalidation_0-auc:0.79274\n",
            "[40]\tvalidation_0-auc:0.790754\n",
            "[41]\tvalidation_0-auc:0.789892\n",
            "[42]\tvalidation_0-auc:0.789601\n",
            "[43]\tvalidation_0-auc:0.787174\n",
            "[44]\tvalidation_0-auc:0.788618\n",
            "[45]\tvalidation_0-auc:0.787635\n",
            "[46]\tvalidation_0-auc:0.78916\n",
            "[47]\tvalidation_0-auc:0.787335\n",
            "[48]\tvalidation_0-auc:0.786793\n",
            "[49]\tvalidation_0-auc:0.787114\n",
            "[50]\tvalidation_0-auc:0.788317\n",
            "[51]\tvalidation_0-auc:0.787675\n",
            "[52]\tvalidation_0-auc:0.787916\n",
            "[53]\tvalidation_0-auc:0.787936\n",
            "[54]\tvalidation_0-auc:0.788759\n",
            "[55]\tvalidation_0-auc:0.789501\n",
            "[56]\tvalidation_0-auc:0.786994\n",
            "[57]\tvalidation_0-auc:0.787675\n",
            "[58]\tvalidation_0-auc:0.787254\n",
            "[59]\tvalidation_0-auc:0.788598\n",
            "[60]\tvalidation_0-auc:0.788337\n",
            "[61]\tvalidation_0-auc:0.786673\n",
            "[62]\tvalidation_0-auc:0.786111\n",
            "[63]\tvalidation_0-auc:0.786051\n",
            "[64]\tvalidation_0-auc:0.785189\n",
            "[65]\tvalidation_0-auc:0.784587\n",
            "Stopping. Best iteration:\n",
            "[15]\tvalidation_0-auc:0.809106\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+---------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |       F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.7020408163265306 | 0.7129629629629629 | 0.4010416666666667 |  0.5133333333333334 |\n",
            "|      GRU 0.15     | 0.7142857142857143 | 0.7452830188679245 | 0.4114583333333333 |  0.5302013422818792 |\n",
            "|    XGBoost 0.15   | 0.6979591836734694 |        0.75        |      0.34375       |  0.4714285714285714 |\n",
            "|    Logreg 0.15    | 0.6795918367346939 | 0.7272727272727273 | 0.2916666666666667 |   0.41635687732342  |\n",
            "|      SVM 0.15     | 0.6775510204081633 | 0.7297297297297297 |      0.28125       | 0.40601503759398494 |\n",
            "|   LSTM beta 0.15  | 0.6148796498905909 | 0.5294117647058824 |        0.2         |  0.2903225806451613 |\n",
            "|   GRU beta 0.15   | 0.6805251641137856 |     0.6328125      |        0.45        |  0.525974025974026  |\n",
            "| XGBoost beta 0.15 | 0.7592997811816192 | 0.8070175438596491 | 0.5111111111111111 |  0.6258503401360545 |\n",
            "|  logreg beta 0.15 | 0.6673960612691466 | 0.6147540983606558 | 0.4166666666666667 |  0.4966887417218544 |\n",
            "|   svm beta 0.15   | 0.6739606126914661 | 0.6115107913669064 | 0.4722222222222222 |  0.5329153605015674 |\n",
            "+-------------------+--------------------+--------------------+--------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "F57CidjyCs-D",
        "outputId": "1648f9ce-8f7f-48f8-9544-8f88a51d7ce7"
      },
      "source": [
        "Result_purging.to_csv('AAPL_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.536735</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.832298</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.690722</td>\n",
              "      <td>0.590308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.843284</td>\n",
              "      <td>0.724490</td>\n",
              "      <td>0.626039</td>\n",
              "      <td>0.497797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.852174</td>\n",
              "      <td>0.702041</td>\n",
              "      <td>0.573099</td>\n",
              "      <td>0.431718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.845528</td>\n",
              "      <td>0.710204</td>\n",
              "      <td>0.594286</td>\n",
              "      <td>0.458150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.724138</td>\n",
              "      <td>0.700219</td>\n",
              "      <td>0.647815</td>\n",
              "      <td>0.586047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.715232</td>\n",
              "      <td>0.671772</td>\n",
              "      <td>0.590164</td>\n",
              "      <td>0.502326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.746753</td>\n",
              "      <td>0.695842</td>\n",
              "      <td>0.623306</td>\n",
              "      <td>0.534884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.630197</td>\n",
              "      <td>0.523944</td>\n",
              "      <td>0.432558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.609195</td>\n",
              "      <td>0.612691</td>\n",
              "      <td>0.544987</td>\n",
              "      <td>0.493023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.558824</td>\n",
              "      <td>0.687755</td>\n",
              "      <td>0.198953</td>\n",
              "      <td>0.121019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.638298</td>\n",
              "      <td>0.732653</td>\n",
              "      <td>0.478088</td>\n",
              "      <td>0.382166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.686869</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.531250</td>\n",
              "      <td>0.433121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.627660</td>\n",
              "      <td>0.728571</td>\n",
              "      <td>0.470120</td>\n",
              "      <td>0.375796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.656716</td>\n",
              "      <td>0.722449</td>\n",
              "      <td>0.392857</td>\n",
              "      <td>0.280255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.513158</td>\n",
              "      <td>0.687090</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>0.268966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.614815</td>\n",
              "      <td>0.750547</td>\n",
              "      <td>0.592857</td>\n",
              "      <td>0.572414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.713235</td>\n",
              "      <td>0.809628</td>\n",
              "      <td>0.690391</td>\n",
              "      <td>0.668966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.591195</td>\n",
              "      <td>0.746171</td>\n",
              "      <td>0.618421</td>\n",
              "      <td>0.648276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.608392</td>\n",
              "      <td>0.750547</td>\n",
              "      <td>0.604167</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.712963</td>\n",
              "      <td>0.702041</td>\n",
              "      <td>0.513333</td>\n",
              "      <td>0.401042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.745283</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.530201</td>\n",
              "      <td>0.411458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.697959</td>\n",
              "      <td>0.471429</td>\n",
              "      <td>0.343750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.679592</td>\n",
              "      <td>0.416357</td>\n",
              "      <td>0.291667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.729730</td>\n",
              "      <td>0.677551</td>\n",
              "      <td>0.406015</td>\n",
              "      <td>0.281250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.529412</td>\n",
              "      <td>0.614880</td>\n",
              "      <td>0.290323</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.632812</td>\n",
              "      <td>0.680525</td>\n",
              "      <td>0.525974</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.807018</td>\n",
              "      <td>0.759300</td>\n",
              "      <td>0.625850</td>\n",
              "      <td>0.511111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.614754</td>\n",
              "      <td>0.667396</td>\n",
              "      <td>0.496689</td>\n",
              "      <td>0.416667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.611511</td>\n",
              "      <td>0.673961</td>\n",
              "      <td>0.532915</td>\n",
              "      <td>0.472222</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  AAPL  0.000000  0.536735  0.000000  0.000000\n",
              "1            GRU 0.1  AAPL  0.832298  0.755102  0.690722  0.590308\n",
              "2        XGBoost 0.1  AAPL  0.843284  0.724490  0.626039  0.497797\n",
              "3         Logreg 0.1  AAPL  0.852174  0.702041  0.573099  0.431718\n",
              "4            SVM 0.1  AAPL  0.845528  0.710204  0.594286  0.458150\n",
              "5      LSTM beta 0.1  AAPL  0.724138  0.700219  0.647815  0.586047\n",
              "6       GRU beta 0.1  AAPL  0.715232  0.671772  0.590164  0.502326\n",
              "7   XGBoost beta 0.1  AAPL  0.746753  0.695842  0.623306  0.534884\n",
              "8    logreg beta 0.1  AAPL  0.664286  0.630197  0.523944  0.432558\n",
              "9       svm beta 0.1  AAPL  0.609195  0.612691  0.544987  0.493023\n",
              "0           LSTM 0.2  AAPL  0.558824  0.687755  0.198953  0.121019\n",
              "1            GRU 0.2  AAPL  0.638298  0.732653  0.478088  0.382166\n",
              "2        XGBoost 0.2  AAPL  0.686869  0.755102  0.531250  0.433121\n",
              "3         Logreg 0.2  AAPL  0.627660  0.728571  0.470120  0.375796\n",
              "4            SVM 0.2  AAPL  0.656716  0.722449  0.392857  0.280255\n",
              "5      LSTM beta 0.2  AAPL  0.513158  0.687090  0.352941  0.268966\n",
              "6       GRU beta 0.2  AAPL  0.614815  0.750547  0.592857  0.572414\n",
              "7   XGBoost beta 0.2  AAPL  0.713235  0.809628  0.690391  0.668966\n",
              "8    logreg beta 0.2  AAPL  0.591195  0.746171  0.618421  0.648276\n",
              "9       svm beta 0.2  AAPL  0.608392  0.750547  0.604167  0.600000\n",
              "0          LSTM 0.15  AAPL  0.712963  0.702041  0.513333  0.401042\n",
              "1           GRU 0.15  AAPL  0.745283  0.714286  0.530201  0.411458\n",
              "2       XGBoost 0.15  AAPL  0.750000  0.697959  0.471429  0.343750\n",
              "3        Logreg 0.15  AAPL  0.727273  0.679592  0.416357  0.291667\n",
              "4           SVM 0.15  AAPL  0.729730  0.677551  0.406015  0.281250\n",
              "5     LSTM beta 0.15  AAPL  0.529412  0.614880  0.290323  0.200000\n",
              "6      GRU beta 0.15  AAPL  0.632812  0.680525  0.525974  0.450000\n",
              "7  XGBoost beta 0.15  AAPL  0.807018  0.759300  0.625850  0.511111\n",
              "8   logreg beta 0.15  AAPL  0.614754  0.667396  0.496689  0.416667\n",
              "9      svm beta 0.15  AAPL  0.611511  0.673961  0.532915  0.472222"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH__8B42Cs-G"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AAPL_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK9sWU9XFn7o"
      },
      "source": [
        "## AMT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "o-uELrKBFn7y",
        "outputId": "daace9e7-2d7f-46c8-c2b2-ead03ae31eb0"
      },
      "source": [
        "dfs = pd.read_csv(\"AMT.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.AMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>265.44</td>\n",
              "      <td>268.98</td>\n",
              "      <td>265.440</td>\n",
              "      <td>266.440</td>\n",
              "      <td>33744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.AMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>271.80</td>\n",
              "      <td>272.14</td>\n",
              "      <td>265.300</td>\n",
              "      <td>265.640</td>\n",
              "      <td>50333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.AMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>269.10</td>\n",
              "      <td>271.24</td>\n",
              "      <td>268.440</td>\n",
              "      <td>269.810</td>\n",
              "      <td>43260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2763</td>\n",
              "      <td>US1.AMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>271.08</td>\n",
              "      <td>271.81</td>\n",
              "      <td>266.860</td>\n",
              "      <td>268.645</td>\n",
              "      <td>76212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2762</td>\n",
              "      <td>US1.AMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>280.62</td>\n",
              "      <td>280.71</td>\n",
              "      <td>273.670</td>\n",
              "      <td>273.790</td>\n",
              "      <td>59047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2762</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.AMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>50.53</td>\n",
              "      <td>50.56</td>\n",
              "      <td>50.130</td>\n",
              "      <td>50.360</td>\n",
              "      <td>2086997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2763</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.AMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>51.03</td>\n",
              "      <td>51.63</td>\n",
              "      <td>50.310</td>\n",
              "      <td>50.390</td>\n",
              "      <td>3668859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.AMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>51.94</td>\n",
              "      <td>52.05</td>\n",
              "      <td>50.310</td>\n",
              "      <td>50.750</td>\n",
              "      <td>3568106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.AMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>51.43</td>\n",
              "      <td>52.34</td>\n",
              "      <td>51.390</td>\n",
              "      <td>51.850</td>\n",
              "      <td>2281804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.AMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>51.59</td>\n",
              "      <td>51.84</td>\n",
              "      <td>50.855</td>\n",
              "      <td>51.070</td>\n",
              "      <td>2776426</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2767 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...  <HIGH>    <LOW>  <CLOSE>    <VOL>\n",
              "0      2766  US1.AMT     D  20211001  ...  268.98  265.440  266.440    33744\n",
              "1      2765  US1.AMT     D  20210930  ...  272.14  265.300  265.640    50333\n",
              "2      2764  US1.AMT     D  20210929  ...  271.24  268.440  269.810    43260\n",
              "3      2763  US1.AMT     D  20210928  ...  271.81  266.860  268.645    76212\n",
              "4      2762  US1.AMT     D  20210927  ...  280.71  273.670  273.790    59047\n",
              "...     ...      ...   ...       ...  ...     ...      ...      ...      ...\n",
              "2762      4  US1.AMT     D  20101008  ...   50.56   50.130   50.360  2086997\n",
              "2763      3  US1.AMT     D  20101007  ...   51.63   50.310   50.390  3668859\n",
              "2764      2  US1.AMT     D  20101006  ...   52.05   50.310   50.750  3568106\n",
              "2765      1  US1.AMT     D  20101005  ...   52.34   51.390   51.850  2281804\n",
              "2766      0  US1.AMT     D  20101004  ...   51.84   50.855   51.070  2776426\n",
              "\n",
              "[2767 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fFsUev3JFn7z",
        "outputId": "455c1dad-9dc9-4993-c7e5-c47f5a3c6e1d"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"30cf600f-5238-4a18-ab3c-edf4c0ccfdf5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"30cf600f-5238-4a18-ab3c-edf4c0ccfdf5\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '30cf600f-5238-4a18-ab3c-edf4c0ccfdf5',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [225.9, 223.98, 219.99, 220.88, 221.06, 219.02, 223.3, 222.07, 225.22, 225.43, 225.57, 227.08, 221.5, 221.29, 216.77, 215.35, 218.86, 212.7, 216.25, 224.17, 232.53, 235.79, 240.99, 237.8, 230.1, 230.41, 230.19, 229.79, 230.14, 226.73, 228.52, 227.38, 225.68, 225.15, 223.04, 224.19, 218.88, 221.09, 222.3, 222.26, 221.07, 217.51, 216.93, 213.96, 215.47, 211.47, 211.58, 208.64, 205.57, 204.64, 205.47, 204.86, 206.77, 206.68, 205.65, 208.68, 208.74, 209.3, 210.7, 208.98, 209.24, 211.52, 211.28, 208.99, 209.05, 209.99, 205.67, 202.3, 204.33, 205.85, 205.82, 210.81, 214.57, 215.25, 217.65, 216.19, 214.4, 214.09, 211.23, 209.02, 212.0, 208.94, 209.83, 212.41, 212.74, 212.71, 207.98, 209.2, 208.78, 204.74, 201.1, 202.5, 203.04, 202.93, 200.69, 198.17, 198.23, 202.38, 202.02, 200.5, 198.08, 196.87, 195.68, 192.52, 191.72, 189.86, 192.19, 192.91, 192.8, 192.8, 195.23, 192.44, 193.21, 192.31, 193.23, 193.51, 192.32, 192.94, 191.13, 192.19, 196.08, 198.01, 196.64, 196.62, 196.75, 196.23, 195.78, 193.04, 194.29, 195.13, 192.51, 197.03, 194.9, 194.12, 195.03, 195.69, 193.67, 194.71, 191.54, 189.91, 188.89, 188.97, 187.98, 187.26, 185.83, 185.24, 181.61, 181.76, 181.68, 181.1, 179.67, 177.98, 176.09, 173.2, 173.98, 174.24, 176.63, 176.4503, 176.69, 177.62, 177.85, 175.78, 173.68, 172.59, 172.99, 172.43, 172.02, 170.3, 172.39, 170.91, 170.44, 173.11, 170.51, 169.44, 168.05, 167.41, 165.66, 165.77, 164.7, 164.62, 164.38, 163.72, 163.1, 161.36, 161.39, 161.81, 159.65, 161.06, 159.27, 158.26, 157.6, 156.73, 158.23, 158.81, 158.62, 157.49, 153.58, 158.08, 161.48, 162.61, 162.3, 160.98, 166.6, 166.81, 164.71, 166.63, 166.16, 166.4, 167.62, 163.52, 163.12, 164.43, 162.78, 162.03, 162.09, 161.59, 160.39, 160.37, 160.64, 166.23, 163.99, 162.16, 163.41, 161.31, 161.04, 160.04, 159.36, 156.96, 156.13, 155.67, 152.07, 152.98, 155.8, 160.93, 152.91, 149.6, 153.51, 151.95, 150.83, 149.26, 150.14, 146.83, 146.72, 145.71, 142.75, 141.92, 140.7, 145.54, 147.63, 145.8, 144.81, 144.43, 143.87, 144.09, 144.03, 145.3, 143.39, 143.96, 145.7, 146.38, 148.49, 148.74, 147.58, 147.46, 147.74, 147.18, 148.68, 147.31, 147.03, 147.35, 146.54, 148.09, 147.6, 148.14, 149.1, 148.96, 150.44, 150.09, 148.37, 148.09, 147.39, 148.12, 149.22, 150.74, 151.17, 149.73, 149.55, 149.11, 149.0, 149.18, 150.56, 149.36, 150.02, 150.9, 151.02, 148.25, 150.19, 148.22, 142.72, 144.37, 144.51, 142.92, 141.56, 140.83, 140.4, 141.67, 141.44, 142.15, 141.75, 143.21, 143.76, 143.4, 143.53, 142.19, 145.79, 145.67, 143.03, 144.67, 144.18, 144.24, 143.39, 143.16, 142.12, 142.97, 141.67, 140.57, 139.07, 138.23, 138.58, 138.36, 136.44, 141.09, 139.65, 138.56, 138.14, 139.03, 138.9, 140.05, 138.48, 138.23, 137.85, 135.99, 136.33, 135.5, 136.83, 137.13, 137.37, 137.04, 136.87, 136.45, 136.54, 138.29, 138.81, 139.89, 137.94, 137.0, 138.13, 138.87, 137.61, 136.46, 135.15, 136.41, 135.23, 136.85, 135.76, 136.94, 138.19, 136.65, 137.38, 139.83, 141.02, 138.56, 138.89, 138.38, 139.92, 137.35, 143.24, 142.13, 144.09, 142.99, 141.49, 143.74, 145.29, 144.3, 144.5, 145.23, 142.22, 143.03, 142.27, 146.32, 145.14, 146.44, 146.64, 146.53, 146.26, 145.09, 143.56, 142.73, 140.03, 138.88, 136.77, 133.98, 135.28, 139.33, 140.61, 142.75, 141.87, 139.01, 137.47, 139.23, 140.03, 136.26, 135.39, 136.34, 136.22, 136.05, 133.57, 139.99, 140.2, 142.03, 145.18, 147.05, 147.72, 145.16, 144.33, 145.12, 142.99, 141.75, 141.17, 139.22, 137.6, 135.17, 136.62, 135.62, 133.01, 134.95, 137.58, 140.98, 142.03, 140.49, 139.63, 141.57, 141.105, 142.69, 142.91, 141.55, 140.88, 140.66, 139.58, 140.2, 141.85, 142.85, 142.75, 143.65, 143.75, 143.5, 144.04, 142.5, 144.11, 141.82, 140.76, 138.97, 143.58, 143.95, 146.29, 148.63, 148.24, 147.98, 145.32, 146.63, 146.24, 145.9, 148.4, 148.95, 149.49, 150.05, 150.56, 151.8, 152.75, 150.62, 150.16, 142.12, 142.43, 140.78, 143.67, 142.86, 139.06, 138.45, 137.32, 138.04, 138.77, 139.04, 138.45, 137.49, 139.07, 139.57, 140.01, 140.02, 137.93, 137.09, 137.91, 139.05, 139.35, 138.87, 136.7, 135.83, 136.65, 136.06, 134.74, 136.44, 137.41, 137.66, 139.43, 140.38, 141.0, 144.78, 145.01, 142.87, 143.65, 144.86, 146.14, 145.0, 146.99, 146.14, 146.95, 145.73, 148.06, 147.5, 145.89, 145.82, 144.25, 143.85, 142.84, 142.09, 141.21, 140.0, 139.74, 141.09, 139.64, 139.9, 136.68, 136.85, 137.86, 136.64, 138.0, 137.56, 137.15, 138.06, 136.05, 136.45, 138.41, 134.91, 137.79, 136.61, 137.19, 136.9, 136.25, 135.42, 136.21, 135.4, 136.04, 134.14, 133.94, 131.05, 131.22, 133.11, 131.62, 132.25, 131.41, 132.3, 132.85, 134.54, 135.79, 132.44, 131.84, 130.88, 130.65, 131.32, 132.34, 131.97, 130.88, 130.49, 130.16, 129.18, 130.21, 130.93, 131.72, 131.63, 131.61, 132.41, 131.8, 131.2, 132.12, 131.09, 130.97, 129.28, 128.75, 128.72, 128.31, 127.98, 130.78, 128.37, 127.92, 126.06, 126.27, 127.09, 128.07, 128.84, 128.64, 127.39, 126.78, 127.9, 127.6, 125.97, 125.89, 122.8, 123.92, 123.83, 124.42, 124.31, 124.67, 124.86, 124.25, 122.61, 122.32, 122.75, 122.47, 122.12, 121.96, 120.64, 120.75, 121.2, 121.54, 120.91, 121.06, 120.4, 119.95, 119.36, 118.94, 118.3, 116.73, 115.98, 115.93, 115.09, 115.28, 114.02, 114.27, 113.62, 112.97, 113.25, 114.7, 114.75, 115.64, 114.79, 113.86, 114.765, 113.29, 112.8, 112.19, 111.0, 111.15, 108.1, 107.71, 107.12, 106.77, 107.56, 105.51, 105.77, 104.29, 103.47, 103.62, 105.24, 104.12, 103.28, 103.5, 103.0, 103.82, 104.31, 103.9, 104.58, 104.78, 104.17, 104.45, 105.66, 105.13, 103.46, 104.17, 103.84, 103.51, 105.02, 105.28, 105.97, 106.34, 106.15, 105.69, 106.39, 105.84, 106.28, 106.03, 106.61, 106.6, 106.99, 106.97, 105.79, 106.08, 106.5, 106.12, 103.84, 102.93, 102.375, 102.215, 100.845, 102.03, 102.59, 100.96, 102.2893, 106.09, 106.14, 107.3, 106.78, 108.02, 105.91, 105.1, 105.54, 105.43, 103.52, 102.57, 105.72, 105.72, 109.2, 115.93, 115.18, 112.99, 113.28, 112.91, 115.19, 117.23, 116.14, 115.425, 115.71, 116.75, 116.72, 115.71, 114.81, 115.31, 114.145, 113.33, 113.35, 112.69, 111.53, 108.94, 109.43, 108.51, 108.51, 107.47, 110.74, 112.25, 113.33, 113.5, 112.635, 112.57, 112.73, 112.99, 112.27, 110.09, 108.62, 109.24, 108.05, 109.11, 108.32, 108.11, 111.27, 110.22, 116.23, 116.85, 117.11, 115.1, 113.82, 113.38, 113.22, 113.86, 113.28, 114.27, 114.56, 115.2, 114.61, 113.61, 114.35, 114.54, 114.12, 115.81, 116.51, 115.91, 116.21, 115.93, 115.73, 116.58, 116.4399, 115.72, 116.77, 117.04, 115.76, 116.65, 117.56, 116.99, 116.89, 117.83, 117.52, 117.74, 117.36, 116.28, 116.81, 116.98, 116.0, 116.11, 114.64, 113.65, 112.19, 112.38, 113.81, 113.57, 113.615, 112.99, 110.78, 108.89, 108.27, 111.45, 109.38, 109.45, 107.45, 106.99, 107.69, 107.28, 107.63, 108.96, 108.79, 109.1, 107.57, 107.88, 107.19, 106.98, 106.54, 105.47, 105.78, 106.06, 105.3, 106.17, 105.29, 104.19, 104.0, 103.76, 104.55, 105.37, 106.47, 105.73, 106.6, 105.74, 107.17, 106.66, 106.57, 106.28, 105.51, 104.17, 105.52, 104.89, 104.0899, 104.5, 104.76, 104.16, 104.2, 102.56, 104.83, 105.48, 105.5, 105.77, 105.18, 106.23, 106.16, 106.0, 105.76, 105.0, 105.71, 104.06, 104.73, 104.5, 102.4, 102.32, 101.95, 100.27, 100.35, 99.59, 99.95, 100.32, 100.51, 101.73, 99.82, 98.43, 98.05, 98.44, 94.7, 94.55, 94.6, 95.59, 95.92, 95.3, 94.61, 94.68, 92.18, 90.0, 89.45, 88.99, 88.01, 89.245, 87.28, 87.49, 86.19, 86.41, 85.66, 83.62, 85.84, 85.5999, 85.41, 88.8, 91.11, 90.5, 91.35, 93.1, 94.37, 92.46, 92.49, 92.41, 91.15, 93.24, 90.02, 89.0, 90.87, 88.38, 92.17, 92.15, 94.5, 94.4973, 94.19, 95.79, 98.26, 98.62, 96.76, 96.95, 98.08, 99.39, 98.39, 98.14, 98.51, 96.4, 95.58, 94.61, 96.11, 97.5, 95.6, 94.08, 93.66, 95.06, 95.55, 96.88, 97.71, 97.5, 97.86, 99.47, 101.16, 99.42, 99.68, 99.07, 99.27, 99.93, 100.47, 99.62, 99.69, 98.36, 98.05, 96.4, 97.37, 97.62, 97.17, 97.13, 98.02, 100.87, 102.28, 103.3, 104.06, 102.24, 101.96, 100.86, 99.29, 99.47, 99.18, 99.46, 96.05, 96.93, 96.77, 96.15, 94.66, 93.06, 93.71, 94.78, 94.96, 95.57, 94.23, 93.38, 93.45, 89.72, 88.37, 87.95, 87.02, 87.22, 88.82, 88.0, 88.72, 89.04, 90.43, 90.47, 91.53, 90.6, 89.55, 88.84, 89.25, 88.31, 88.72, 90.01, 89.03, 91.0, 91.15, 90.04, 92.22, 93.43, 93.85, 91.52, 89.54, 91.94, 96.68, 98.86, 100.41, 100.48, 101.46, 101.1, 100.25, 99.98, 101.01, 100.48, 99.65, 98.39, 96.68, 96.13, 95.35, 95.14, 94.95, 96.34, 96.68, 96.51, 96.8, 98.0, 96.55, 96.93, 97.05, 96.94, 97.82, 95.95, 95.49, 95.69, 95.43, 94.37, 94.53, 95.64, 94.98, 94.9, 94.01, 93.29, 92.96, 94.23, 94.75, 95.12, 96.26, 95.81, 95.98, 96.18, 94.13, 93.3, 92.27, 93.19, 93.46, 92.7, 92.07, 92.85, 92.53, 94.71, 92.71, 94.15, 94.2, 92.82, 94.58, 95.19, 94.6, 94.34, 93.88, 94.32, 95.45, 95.17, 95.45, 94.8, 92.93, 93.55, 93.62, 94.94, 93.63, 92.89, 93.0, 94.63, 93.43, 94.5299, 93.51, 95.76, 95.22, 95.33, 96.07, 95.92, 95.02, 95.82, 96.02, 96.76, 95.16, 94.5, 94.79, 95.08, 95.32, 96.56, 96.86, 98.04, 96.78, 96.5, 94.22, 96.15, 96.03, 95.2, 95.49, 97.19, 97.64, 97.7, 94.6, 95.48, 94.11, 94.6, 94.23, 96.26, 94.64, 94.57, 96.6, 96.6, 99.23, 98.56, 98.27, 99.39, 99.16, 98.5, 97.86, 94.6, 97.32, 96.52, 95.25, 96.22, 96.23, 96.4, 95.95, 95.94, 96.36, 96.7, 95.72, 99.59, 97.85, 99.07, 97.84, 96.97, 98.44, 98.9, 100.4, 101.75, 99.6, 98.81, 96.73, 97.14, 97.4, 96.3, 97.27, 97.88, 99.0, 99.43, 99.88, 98.96, 97.93, 98.27, 99.65, 98.87, 100.35, 100.62, 100.64, 100.75, 100.65, 100.67, 99.47, 98.6, 97.14, 95.81, 97.45, 98.6, 99.94, 100.21, 100.78, 100.78, 100.53, 101.6, 101.7, 101.95, 103.48, 105.02, 104.7, 104.2, 102.48, 102.1, 100.68, 100.83, 100.67, 99.59, 99.55, 99.96, 99.72, 99.48, 99.33, 99.46, 99.55, 97.65, 96.88, 96.82, 97.5275, 98.93, 95.08, 96.38, 96.2, 96.19, 95.83, 94.97, 95.57, 94.08, 93.16, 92.4301, 91.95, 93.35, 93.06, 94.71, 94.8, 95.29, 93.31, 93.88, 93.88, 93.5, 93.45, 93.64, 93.55, 93.45, 93.13, 94.2, 93.93, 94.31, 95.16, 95.87, 95.6, 95.47, 95.17, 96.01, 97.82, 97.49, 98.54, 99.34, 99.61, 98.6, 98.12, 98.78, 98.59, 98.79, 98.77, 98.62, 98.93, 98.13, 98.93, 98.64, 99.05, 99.77, 98.22, 97.56, 97.18, 96.72, 98.02, 98.105, 97.24, 95.65, 94.74, 95.55, 94.38, 94.4, 96.28, 92.37, 92.4, 92.21, 92.39, 92.97, 92.85, 91.86, 92.46, 90.84, 92.09, 91.63, 91.97, 91.65, 91.4188, 90.88, 90.35, 90.71, 90.01, 89.73, 89.7, 89.97, 89.03, 88.63, 89.27, 89.25, 89.08, 89.11, 88.74, 88.07, 88.23, 87.91, 88.44, 88.22, 88.78, 89.33, 89.67, 90.42, 89.725, 89.01, 88.71, 88.39, 89.66, 89.37, 89.18, 89.0, 88.41, 88.45, 88.37, 88.01, 87.84, 88.86, 88.67, 88.77, 88.53, 88.28, 88.64, 88.25, 88.36, 87.22, 88.39, 87.01, 86.24, 83.51, 83.0, 83.46, 83.07, 83.57, 84.03, 83.93, 83.05, 82.66, 82.38, 81.68, 81.3, 80.56, 81.55, 81.86, 81.5, 80.68, 80.76, 81.25, 81.24, 81.58, 81.88, 82.1, 80.66, 80.81, 81.73, 80.89, 81.08, 81.24, 80.68, 81.78, 81.73, 80.4, 80.38, 80.82, 81.71, 81.17, 81.27, 81.93, 80.8, 81.53, 81.45, 81.49, 81.47, 80.83, 80.14, 82.99, 84.05, 84.51, 83.75, 83.53, 83.17, 82.45, 81.79, 81.88, 81.51, 80.38, 79.76, 79.01, 79.15, 78.83, 80.85, 81.17, 79.56, 80.56, 79.25, 79.62, 83.28, 83.46, 83.63, 83.13, 83.27, 83.25, 82.64, 82.09, 82.65, 82.11, 81.94, 81.27, 80.1899, 79.67, 79.44, 79.79, 79.8, 78.82, 78.84, 79.47, 78.41, 77.98, 78.1, 78.19, 76.39, 76.09, 76.76, 76.69, 77.78, 79.2, 78.89, 78.05, 77.71, 77.41, 77.71, 78.21, 77.77, 78.37, 78.03, 77.0, 77.27, 77.59, 78.11, 78.08, 78.58, 79.27, 79.11, 78.15, 77.95, 77.45, 77.76, 78.8, 79.53, 78.6, 79.0, 79.39, 79.34, 80.69, 80.29, 80.33, 80.82, 80.4, 79.015, 80.13, 79.5]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('30cf600f-5238-4a18-ab3c-edf4c0ccfdf5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"bdf76711-2627-4ade-9feb-b5c971d49cfd\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"bdf76711-2627-4ade-9feb-b5c971d49cfd\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'bdf76711-2627-4ade-9feb-b5c971d49cfd',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('bdf76711-2627-4ade-9feb-b5c971d49cfd');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyScHKrhFn7z"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anqa-UqNFn7z",
        "outputId": "52bacb14-3ac7-4007-e192-4ced157b1cdb"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .2, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"AMT\", step_sizes=4, th= th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6643 - accuracy: 0.6396 - val_loss: 0.5680 - val_accuracy: 0.8735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6565 - accuracy: 0.6430 - val_loss: 0.5192 - val_accuracy: 0.8735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6550 - accuracy: 0.6430 - val_loss: 0.5328 - val_accuracy: 0.8735\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6514 - accuracy: 0.6430 - val_loss: 0.5421 - val_accuracy: 0.8735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6512 - accuracy: 0.6430 - val_loss: 0.4917 - val_accuracy: 0.8735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6600 - accuracy: 0.6376 - val_loss: 0.5259 - val_accuracy: 0.8735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6532 - accuracy: 0.6436 - val_loss: 0.5439 - val_accuracy: 0.8735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6313 - accuracy: 0.6617 - val_loss: 0.4134 - val_accuracy: 0.8837\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6149 - accuracy: 0.6805 - val_loss: 0.4707 - val_accuracy: 0.8735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6032 - accuracy: 0.6953 - val_loss: 0.3956 - val_accuracy: 0.8816\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.699634\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.71143\n",
            "[2]\tvalidation_0-auc:0.717949\n",
            "[3]\tvalidation_0-auc:0.721755\n",
            "[4]\tvalidation_0-auc:0.733325\n",
            "[5]\tvalidation_0-auc:0.755276\n",
            "[6]\tvalidation_0-auc:0.761513\n",
            "[7]\tvalidation_0-auc:0.765413\n",
            "[8]\tvalidation_0-auc:0.786969\n",
            "[9]\tvalidation_0-auc:0.779771\n",
            "[10]\tvalidation_0-auc:0.781693\n",
            "[11]\tvalidation_0-auc:0.787477\n",
            "[12]\tvalidation_0-auc:0.793714\n",
            "[13]\tvalidation_0-auc:0.794035\n",
            "[14]\tvalidation_0-auc:0.791755\n",
            "[15]\tvalidation_0-auc:0.792\n",
            "[16]\tvalidation_0-auc:0.796729\n",
            "[17]\tvalidation_0-auc:0.800667\n",
            "[18]\tvalidation_0-auc:0.796823\n",
            "[19]\tvalidation_0-auc:0.800215\n",
            "[20]\tvalidation_0-auc:0.797219\n",
            "[21]\tvalidation_0-auc:0.798199\n",
            "[22]\tvalidation_0-auc:0.797709\n",
            "[23]\tvalidation_0-auc:0.794393\n",
            "[24]\tvalidation_0-auc:0.797257\n",
            "[25]\tvalidation_0-auc:0.803795\n",
            "[26]\tvalidation_0-auc:0.801609\n",
            "[27]\tvalidation_0-auc:0.801854\n",
            "[28]\tvalidation_0-auc:0.804247\n",
            "[29]\tvalidation_0-auc:0.805717\n",
            "[30]\tvalidation_0-auc:0.805415\n",
            "[31]\tvalidation_0-auc:0.806357\n",
            "[32]\tvalidation_0-auc:0.807526\n",
            "[33]\tvalidation_0-auc:0.809146\n",
            "[34]\tvalidation_0-auc:0.809937\n",
            "[35]\tvalidation_0-auc:0.807394\n",
            "[36]\tvalidation_0-auc:0.806263\n",
            "[37]\tvalidation_0-auc:0.807733\n",
            "[38]\tvalidation_0-auc:0.807413\n",
            "[39]\tvalidation_0-auc:0.806621\n",
            "[40]\tvalidation_0-auc:0.808618\n",
            "[41]\tvalidation_0-auc:0.811068\n",
            "[42]\tvalidation_0-auc:0.811671\n",
            "[43]\tvalidation_0-auc:0.811709\n",
            "[44]\tvalidation_0-auc:0.810993\n",
            "[45]\tvalidation_0-auc:0.809749\n",
            "[46]\tvalidation_0-auc:0.809259\n",
            "[47]\tvalidation_0-auc:0.809448\n",
            "[48]\tvalidation_0-auc:0.810804\n",
            "[49]\tvalidation_0-auc:0.811369\n",
            "[50]\tvalidation_0-auc:0.809975\n",
            "[51]\tvalidation_0-auc:0.812349\n",
            "[52]\tvalidation_0-auc:0.813348\n",
            "[53]\tvalidation_0-auc:0.814629\n",
            "[54]\tvalidation_0-auc:0.814893\n",
            "[55]\tvalidation_0-auc:0.813159\n",
            "[56]\tvalidation_0-auc:0.812896\n",
            "[57]\tvalidation_0-auc:0.814064\n",
            "[58]\tvalidation_0-auc:0.811916\n",
            "[59]\tvalidation_0-auc:0.813084\n",
            "[60]\tvalidation_0-auc:0.813009\n",
            "[61]\tvalidation_0-auc:0.814403\n",
            "[62]\tvalidation_0-auc:0.814177\n",
            "[63]\tvalidation_0-auc:0.814252\n",
            "[64]\tvalidation_0-auc:0.812783\n",
            "[65]\tvalidation_0-auc:0.81429\n",
            "[66]\tvalidation_0-auc:0.814516\n",
            "[67]\tvalidation_0-auc:0.814139\n",
            "[68]\tvalidation_0-auc:0.813009\n",
            "[69]\tvalidation_0-auc:0.810898\n",
            "[70]\tvalidation_0-auc:0.810974\n",
            "[71]\tvalidation_0-auc:0.811577\n",
            "[72]\tvalidation_0-auc:0.811614\n",
            "[73]\tvalidation_0-auc:0.811011\n",
            "[74]\tvalidation_0-auc:0.810823\n",
            "[75]\tvalidation_0-auc:0.811011\n",
            "[76]\tvalidation_0-auc:0.810069\n",
            "[77]\tvalidation_0-auc:0.811577\n",
            "[78]\tvalidation_0-auc:0.810408\n",
            "[79]\tvalidation_0-auc:0.810069\n",
            "[80]\tvalidation_0-auc:0.81022\n",
            "[81]\tvalidation_0-auc:0.810258\n",
            "[82]\tvalidation_0-auc:0.810446\n",
            "[83]\tvalidation_0-auc:0.812707\n",
            "[84]\tvalidation_0-auc:0.813649\n",
            "[85]\tvalidation_0-auc:0.812104\n",
            "[86]\tvalidation_0-auc:0.811841\n",
            "[87]\tvalidation_0-auc:0.811351\n",
            "[88]\tvalidation_0-auc:0.811539\n",
            "[89]\tvalidation_0-auc:0.813612\n",
            "[90]\tvalidation_0-auc:0.813951\n",
            "[91]\tvalidation_0-auc:0.810898\n",
            "[92]\tvalidation_0-auc:0.81169\n",
            "[93]\tvalidation_0-auc:0.81267\n",
            "[94]\tvalidation_0-auc:0.812933\n",
            "[95]\tvalidation_0-auc:0.814478\n",
            "[96]\tvalidation_0-auc:0.814931\n",
            "[97]\tvalidation_0-auc:0.816174\n",
            "[98]\tvalidation_0-auc:0.814177\n",
            "[99]\tvalidation_0-auc:0.813725\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6656 - accuracy: 0.6349 - val_loss: 0.5477 - val_accuracy: 0.8643\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6459 - accuracy: 0.6458 - val_loss: 0.6017 - val_accuracy: 0.8381\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6635 - accuracy: 0.6307 - val_loss: 0.5228 - val_accuracy: 0.8643\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6583 - accuracy: 0.6349 - val_loss: 0.5440 - val_accuracy: 0.8643\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6494 - accuracy: 0.6349 - val_loss: 0.4740 - val_accuracy: 0.8643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 2s 13ms/step - loss: 0.6572 - accuracy: 0.6301 - val_loss: 0.4928 - val_accuracy: 0.8643\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6071 - accuracy: 0.6815 - val_loss: 0.4807 - val_accuracy: 0.8206\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5698 - accuracy: 0.7213 - val_loss: 0.4363 - val_accuracy: 0.8600\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5488 - accuracy: 0.7097 - val_loss: 0.4022 - val_accuracy: 0.8731\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5354 - accuracy: 0.7385 - val_loss: 0.3420 - val_accuracy: 0.8840\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.756472\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.754247\n",
            "[2]\tvalidation_0-auc:0.755267\n",
            "[3]\tvalidation_0-auc:0.756513\n",
            "[4]\tvalidation_0-auc:0.761066\n",
            "[5]\tvalidation_0-auc:0.763883\n",
            "[6]\tvalidation_0-auc:0.762536\n",
            "[7]\tvalidation_0-auc:0.763352\n",
            "[8]\tvalidation_0-auc:0.763454\n",
            "[9]\tvalidation_0-auc:0.764108\n",
            "[10]\tvalidation_0-auc:0.768844\n",
            "[11]\tvalidation_0-auc:0.768987\n",
            "[12]\tvalidation_0-auc:0.769641\n",
            "[13]\tvalidation_0-auc:0.769498\n",
            "[14]\tvalidation_0-auc:0.772764\n",
            "[15]\tvalidation_0-auc:0.77207\n",
            "[16]\tvalidation_0-auc:0.772132\n",
            "[17]\tvalidation_0-auc:0.766354\n",
            "[18]\tvalidation_0-auc:0.76223\n",
            "[19]\tvalidation_0-auc:0.762454\n",
            "[20]\tvalidation_0-auc:0.764434\n",
            "[21]\tvalidation_0-auc:0.770825\n",
            "[22]\tvalidation_0-auc:0.773356\n",
            "[23]\tvalidation_0-auc:0.772948\n",
            "[24]\tvalidation_0-auc:0.776399\n",
            "[25]\tvalidation_0-auc:0.777889\n",
            "[26]\tvalidation_0-auc:0.778011\n",
            "[27]\tvalidation_0-auc:0.777276\n",
            "[28]\tvalidation_0-auc:0.777807\n",
            "[29]\tvalidation_0-auc:0.777195\n",
            "[30]\tvalidation_0-auc:0.777848\n",
            "[31]\tvalidation_0-auc:0.778195\n",
            "[32]\tvalidation_0-auc:0.777276\n",
            "[33]\tvalidation_0-auc:0.777521\n",
            "[34]\tvalidation_0-auc:0.777971\n",
            "[35]\tvalidation_0-auc:0.776133\n",
            "[36]\tvalidation_0-auc:0.780053\n",
            "[37]\tvalidation_0-auc:0.778052\n",
            "[38]\tvalidation_0-auc:0.775766\n",
            "[39]\tvalidation_0-auc:0.776051\n",
            "[40]\tvalidation_0-auc:0.778256\n",
            "[41]\tvalidation_0-auc:0.777726\n",
            "[42]\tvalidation_0-auc:0.778787\n",
            "[43]\tvalidation_0-auc:0.776439\n",
            "[44]\tvalidation_0-auc:0.775664\n",
            "[45]\tvalidation_0-auc:0.776684\n",
            "[46]\tvalidation_0-auc:0.776766\n",
            "[47]\tvalidation_0-auc:0.777501\n",
            "[48]\tvalidation_0-auc:0.776562\n",
            "[49]\tvalidation_0-auc:0.776684\n",
            "[50]\tvalidation_0-auc:0.776521\n",
            "[51]\tvalidation_0-auc:0.775255\n",
            "[52]\tvalidation_0-auc:0.774153\n",
            "[53]\tvalidation_0-auc:0.773826\n",
            "[54]\tvalidation_0-auc:0.774398\n",
            "[55]\tvalidation_0-auc:0.773132\n",
            "[56]\tvalidation_0-auc:0.771866\n",
            "[57]\tvalidation_0-auc:0.771131\n",
            "[58]\tvalidation_0-auc:0.769784\n",
            "[59]\tvalidation_0-auc:0.768354\n",
            "[60]\tvalidation_0-auc:0.76815\n",
            "[61]\tvalidation_0-auc:0.767007\n",
            "[62]\tvalidation_0-auc:0.767497\n",
            "[63]\tvalidation_0-auc:0.767456\n",
            "[64]\tvalidation_0-auc:0.766517\n",
            "[65]\tvalidation_0-auc:0.765374\n",
            "[66]\tvalidation_0-auc:0.76472\n",
            "[67]\tvalidation_0-auc:0.764843\n",
            "[68]\tvalidation_0-auc:0.763944\n",
            "[69]\tvalidation_0-auc:0.76423\n",
            "[70]\tvalidation_0-auc:0.764108\n",
            "[71]\tvalidation_0-auc:0.762025\n",
            "[72]\tvalidation_0-auc:0.763822\n",
            "[73]\tvalidation_0-auc:0.763904\n",
            "[74]\tvalidation_0-auc:0.762924\n",
            "[75]\tvalidation_0-auc:0.760923\n",
            "[76]\tvalidation_0-auc:0.758963\n",
            "[77]\tvalidation_0-auc:0.759045\n",
            "[78]\tvalidation_0-auc:0.758024\n",
            "[79]\tvalidation_0-auc:0.756227\n",
            "[80]\tvalidation_0-auc:0.757166\n",
            "[81]\tvalidation_0-auc:0.757574\n",
            "[82]\tvalidation_0-auc:0.756513\n",
            "[83]\tvalidation_0-auc:0.754124\n",
            "[84]\tvalidation_0-auc:0.754614\n",
            "[85]\tvalidation_0-auc:0.75392\n",
            "[86]\tvalidation_0-auc:0.753797\n",
            "Stopping. Best iteration:\n",
            "[36]\tvalidation_0-auc:0.780053\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.8734693877551021 |         0.0         |         0.0         |         0.0         |\n",
            "|     GRU 0.1      | 0.8816326530612245 |         1.0         | 0.06451612903225806 | 0.12121212121212122 |\n",
            "|   XGBoost 0.1    | 0.8346938775510204 | 0.39325842696629215 |  0.5645161290322581 | 0.46357615894039744 |\n",
            "|    Logreg 0.1    | 0.8877551020408163 |  0.5813953488372093 |  0.4032258064516129 |  0.4761904761904762 |\n",
            "|     SVM 0.1      | 0.8816326530612245 |  0.5357142857142857 |  0.4838709677419355 |  0.5084745762711865 |\n",
            "|  LSTM beta 0.1   | 0.8643326039387309 |         0.0         |         0.0         |         0.0         |\n",
            "|   GRU beta 0.1   | 0.8840262582056893 |  0.5616438356164384 |  0.6612903225806451 |  0.6074074074074075 |\n",
            "| XGBoost beta 0.1 | 0.7789934354485777 |  0.3333333333333333 |  0.6290322580645161 |  0.435754189944134  |\n",
            "| logreg beta 0.1  | 0.8271334792122538 |  0.4065934065934066 |  0.5967741935483871 | 0.48366013071895425 |\n",
            "|   svm beta 0.1   | 0.7614879649890591 | 0.27184466019417475 | 0.45161290322580644 |  0.3393939393939394 |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.5426 - accuracy: 0.7758 - val_loss: 0.4148 - val_accuracy: 0.8735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5354 - accuracy: 0.7805 - val_loss: 0.4161 - val_accuracy: 0.8735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5305 - accuracy: 0.7805 - val_loss: 0.4331 - val_accuracy: 0.8735\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5351 - accuracy: 0.7805 - val_loss: 0.3990 - val_accuracy: 0.8735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5293 - accuracy: 0.7805 - val_loss: 0.4254 - val_accuracy: 0.8735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.5523 - accuracy: 0.7765 - val_loss: 0.4283 - val_accuracy: 0.8735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5361 - accuracy: 0.7805 - val_loss: 0.4169 - val_accuracy: 0.8735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5316 - accuracy: 0.7805 - val_loss: 0.4034 - val_accuracy: 0.8735\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5284 - accuracy: 0.7805 - val_loss: 0.3814 - val_accuracy: 0.8735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5272 - accuracy: 0.7805 - val_loss: 0.3717 - val_accuracy: 0.8735\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.76383\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.790436\n",
            "[2]\tvalidation_0-auc:0.78955\n",
            "[3]\tvalidation_0-auc:0.806508\n",
            "[4]\tvalidation_0-auc:0.802438\n",
            "[5]\tvalidation_0-auc:0.802476\n",
            "[6]\tvalidation_0-auc:0.821789\n",
            "[7]\tvalidation_0-auc:0.828667\n",
            "[8]\tvalidation_0-auc:0.838691\n",
            "[9]\tvalidation_0-auc:0.83756\n",
            "[10]\tvalidation_0-auc:0.831568\n",
            "[11]\tvalidation_0-auc:0.835676\n",
            "[12]\tvalidation_0-auc:0.834809\n",
            "[13]\tvalidation_0-auc:0.837994\n",
            "[14]\tvalidation_0-auc:0.839539\n",
            "[15]\tvalidation_0-auc:0.838823\n",
            "[16]\tvalidation_0-auc:0.839237\n",
            "[17]\tvalidation_0-auc:0.840142\n",
            "[18]\tvalidation_0-auc:0.839689\n",
            "[19]\tvalidation_0-auc:0.839162\n",
            "[20]\tvalidation_0-auc:0.839124\n",
            "[21]\tvalidation_0-auc:0.835921\n",
            "[22]\tvalidation_0-auc:0.836882\n",
            "[23]\tvalidation_0-auc:0.834696\n",
            "[24]\tvalidation_0-auc:0.837183\n",
            "[25]\tvalidation_0-auc:0.836166\n",
            "[26]\tvalidation_0-auc:0.838239\n",
            "[27]\tvalidation_0-auc:0.837673\n",
            "[28]\tvalidation_0-auc:0.836844\n",
            "[29]\tvalidation_0-auc:0.836392\n",
            "[30]\tvalidation_0-auc:0.836185\n",
            "[31]\tvalidation_0-auc:0.835017\n",
            "[32]\tvalidation_0-auc:0.83415\n",
            "[33]\tvalidation_0-auc:0.832134\n",
            "[34]\tvalidation_0-auc:0.829533\n",
            "[35]\tvalidation_0-auc:0.829157\n",
            "[36]\tvalidation_0-auc:0.832228\n",
            "[37]\tvalidation_0-auc:0.833472\n",
            "[38]\tvalidation_0-auc:0.83366\n",
            "[39]\tvalidation_0-auc:0.833208\n",
            "[40]\tvalidation_0-auc:0.831889\n",
            "[41]\tvalidation_0-auc:0.834583\n",
            "[42]\tvalidation_0-auc:0.83594\n",
            "[43]\tvalidation_0-auc:0.835412\n",
            "[44]\tvalidation_0-auc:0.837296\n",
            "[45]\tvalidation_0-auc:0.837786\n",
            "[46]\tvalidation_0-auc:0.841084\n",
            "[47]\tvalidation_0-auc:0.839878\n",
            "[48]\tvalidation_0-auc:0.838031\n",
            "[49]\tvalidation_0-auc:0.836712\n",
            "[50]\tvalidation_0-auc:0.83773\n",
            "[51]\tvalidation_0-auc:0.839916\n",
            "[52]\tvalidation_0-auc:0.841235\n",
            "[53]\tvalidation_0-auc:0.840632\n",
            "[54]\tvalidation_0-auc:0.840933\n",
            "[55]\tvalidation_0-auc:0.841008\n",
            "[56]\tvalidation_0-auc:0.84033\n",
            "[57]\tvalidation_0-auc:0.840594\n",
            "[58]\tvalidation_0-auc:0.84082\n",
            "[59]\tvalidation_0-auc:0.840971\n",
            "[60]\tvalidation_0-auc:0.840858\n",
            "[61]\tvalidation_0-auc:0.842082\n",
            "[62]\tvalidation_0-auc:0.841253\n",
            "[63]\tvalidation_0-auc:0.841668\n",
            "[64]\tvalidation_0-auc:0.840688\n",
            "[65]\tvalidation_0-auc:0.839463\n",
            "[66]\tvalidation_0-auc:0.839652\n",
            "[67]\tvalidation_0-auc:0.839426\n",
            "[68]\tvalidation_0-auc:0.839614\n",
            "[69]\tvalidation_0-auc:0.841272\n",
            "[70]\tvalidation_0-auc:0.841121\n",
            "[71]\tvalidation_0-auc:0.841611\n",
            "[72]\tvalidation_0-auc:0.842365\n",
            "[73]\tvalidation_0-auc:0.841951\n",
            "[74]\tvalidation_0-auc:0.840556\n",
            "[75]\tvalidation_0-auc:0.837956\n",
            "[76]\tvalidation_0-auc:0.837202\n",
            "[77]\tvalidation_0-auc:0.836939\n",
            "[78]\tvalidation_0-auc:0.838446\n",
            "[79]\tvalidation_0-auc:0.839087\n",
            "[80]\tvalidation_0-auc:0.837504\n",
            "[81]\tvalidation_0-auc:0.835657\n",
            "[82]\tvalidation_0-auc:0.835318\n",
            "[83]\tvalidation_0-auc:0.838031\n",
            "[84]\tvalidation_0-auc:0.836825\n",
            "[85]\tvalidation_0-auc:0.836675\n",
            "[86]\tvalidation_0-auc:0.835996\n",
            "[87]\tvalidation_0-auc:0.835205\n",
            "[88]\tvalidation_0-auc:0.833509\n",
            "[89]\tvalidation_0-auc:0.834753\n",
            "[90]\tvalidation_0-auc:0.83479\n",
            "[91]\tvalidation_0-auc:0.834715\n",
            "[92]\tvalidation_0-auc:0.833924\n",
            "[93]\tvalidation_0-auc:0.833698\n",
            "[94]\tvalidation_0-auc:0.834715\n",
            "[95]\tvalidation_0-auc:0.832605\n",
            "[96]\tvalidation_0-auc:0.832982\n",
            "[97]\tvalidation_0-auc:0.832793\n",
            "[98]\tvalidation_0-auc:0.831813\n",
            "[99]\tvalidation_0-auc:0.830607\n",
            "end training. \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5588 - accuracy: 0.7694 - val_loss: 0.4212 - val_accuracy: 0.8643\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5405 - accuracy: 0.7756 - val_loss: 0.4267 - val_accuracy: 0.8643\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5451 - accuracy: 0.7756 - val_loss: 0.4060 - val_accuracy: 0.8643\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5317 - accuracy: 0.7756 - val_loss: 0.4279 - val_accuracy: 0.8643\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5407 - accuracy: 0.7756 - val_loss: 0.4132 - val_accuracy: 0.8643\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.5536 - accuracy: 0.7721 - val_loss: 0.4276 - val_accuracy: 0.8643\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5360 - accuracy: 0.7756 - val_loss: 0.4379 - val_accuracy: 0.8643\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5150 - accuracy: 0.7763 - val_loss: 0.3599 - val_accuracy: 0.8665\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5129 - accuracy: 0.7763 - val_loss: 0.4039 - val_accuracy: 0.8534\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5158 - accuracy: 0.7756 - val_loss: 0.3955 - val_accuracy: 0.8490\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.711515\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.722458\n",
            "[2]\tvalidation_0-auc:0.738505\n",
            "[3]\tvalidation_0-auc:0.752185\n",
            "[4]\tvalidation_0-auc:0.750715\n",
            "[5]\tvalidation_0-auc:0.752144\n",
            "[6]\tvalidation_0-auc:0.74116\n",
            "[7]\tvalidation_0-auc:0.744753\n",
            "[8]\tvalidation_0-auc:0.745427\n",
            "[9]\tvalidation_0-auc:0.751266\n",
            "[10]\tvalidation_0-auc:0.746448\n",
            "[11]\tvalidation_0-auc:0.739179\n",
            "[12]\tvalidation_0-auc:0.730523\n",
            "[13]\tvalidation_0-auc:0.732973\n",
            "[14]\tvalidation_0-auc:0.74312\n",
            "[15]\tvalidation_0-auc:0.745202\n",
            "[16]\tvalidation_0-auc:0.733687\n",
            "[17]\tvalidation_0-auc:0.732238\n",
            "[18]\tvalidation_0-auc:0.73236\n",
            "[19]\tvalidation_0-auc:0.724622\n",
            "[20]\tvalidation_0-auc:0.718028\n",
            "[21]\tvalidation_0-auc:0.711944\n",
            "[22]\tvalidation_0-auc:0.711678\n",
            "[23]\tvalidation_0-auc:0.709759\n",
            "[24]\tvalidation_0-auc:0.709575\n",
            "[25]\tvalidation_0-auc:0.706921\n",
            "[26]\tvalidation_0-auc:0.700551\n",
            "[27]\tvalidation_0-auc:0.697285\n",
            "[28]\tvalidation_0-auc:0.693242\n",
            "[29]\tvalidation_0-auc:0.693324\n",
            "[30]\tvalidation_0-auc:0.686505\n",
            "[31]\tvalidation_0-auc:0.682728\n",
            "[32]\tvalidation_0-auc:0.680278\n",
            "[33]\tvalidation_0-auc:0.673622\n",
            "[34]\tvalidation_0-auc:0.672826\n",
            "[35]\tvalidation_0-auc:0.670621\n",
            "[36]\tvalidation_0-auc:0.665067\n",
            "[37]\tvalidation_0-auc:0.660535\n",
            "[38]\tvalidation_0-auc:0.658534\n",
            "[39]\tvalidation_0-auc:0.652817\n",
            "[40]\tvalidation_0-auc:0.654512\n",
            "[41]\tvalidation_0-auc:0.653083\n",
            "[42]\tvalidation_0-auc:0.649857\n",
            "[43]\tvalidation_0-auc:0.64463\n",
            "[44]\tvalidation_0-auc:0.6402\n",
            "[45]\tvalidation_0-auc:0.63873\n",
            "[46]\tvalidation_0-auc:0.638975\n",
            "[47]\tvalidation_0-auc:0.638444\n",
            "[48]\tvalidation_0-auc:0.638567\n",
            "[49]\tvalidation_0-auc:0.635096\n",
            "[50]\tvalidation_0-auc:0.633136\n",
            "[51]\tvalidation_0-auc:0.631829\n",
            "[52]\tvalidation_0-auc:0.62844\n",
            "[53]\tvalidation_0-auc:0.62305\n",
            "Stopping. Best iteration:\n",
            "[3]\tvalidation_0-auc:0.752185\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+----------------------+----------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall        |       F1 score       |\n",
            "+------------------+--------------------+---------------------+----------------------+----------------------+\n",
            "|     LSTM 0.2     | 0.8734693877551021 |         0.0         |         0.0          |         0.0          |\n",
            "|     GRU 0.2      | 0.8734693877551021 |         0.0         |         0.0          |         0.0          |\n",
            "|   XGBoost 0.2    | 0.8836734693877552 |  0.8571428571428571 |  0.0967741935483871  | 0.17391304347826084  |\n",
            "|    Logreg 0.2    | 0.8734693877551021 |         0.0         |         0.0          |         0.0          |\n",
            "|     SVM 0.2      | 0.8734693877551021 |         0.0         |         0.0          |         0.0          |\n",
            "|  LSTM beta 0.2   | 0.8643326039387309 |         0.0         |         0.0          |         0.0          |\n",
            "|   GRU beta 0.2   | 0.849015317286652  |  0.1111111111111111 | 0.016129032258064516 | 0.028169014084507043 |\n",
            "| XGBoost beta 0.2 | 0.8708971553610503 |         0.8         | 0.06451612903225806  | 0.11940298507462686  |\n",
            "| logreg beta 0.2  | 0.8577680525164114 |         0.2         | 0.016129032258064516 | 0.029850746268656716 |\n",
            "|   svm beta 0.2   | 0.8293216630196937 | 0.05555555555555555 | 0.016129032258064516 | 0.024999999999999998 |\n",
            "+------------------+--------------------+---------------------+----------------------+----------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6222 - accuracy: 0.7013 - val_loss: 0.4577 - val_accuracy: 0.8735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6166 - accuracy: 0.7020 - val_loss: 0.4780 - val_accuracy: 0.8735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6126 - accuracy: 0.7020 - val_loss: 0.5231 - val_accuracy: 0.8735\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6082 - accuracy: 0.7020 - val_loss: 0.3992 - val_accuracy: 0.8735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5960 - accuracy: 0.7054 - val_loss: 0.4247 - val_accuracy: 0.8735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6246 - accuracy: 0.6966 - val_loss: 0.4584 - val_accuracy: 0.8735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6100 - accuracy: 0.7020 - val_loss: 0.4535 - val_accuracy: 0.8735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5920 - accuracy: 0.7107 - val_loss: 0.4242 - val_accuracy: 0.8755\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5826 - accuracy: 0.7195 - val_loss: 0.4422 - val_accuracy: 0.8857\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5827 - accuracy: 0.7195 - val_loss: 0.3829 - val_accuracy: 0.8816\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.689478\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.721228\n",
            "[2]\tvalidation_0-auc:0.722773\n",
            "[3]\tvalidation_0-auc:0.755653\n",
            "[4]\tvalidation_0-auc:0.765658\n",
            "[5]\tvalidation_0-auc:0.765338\n",
            "[6]\tvalidation_0-auc:0.765262\n",
            "[7]\tvalidation_0-auc:0.764094\n",
            "[8]\tvalidation_0-auc:0.773854\n",
            "[9]\tvalidation_0-auc:0.771932\n",
            "[10]\tvalidation_0-auc:0.780732\n",
            "[11]\tvalidation_0-auc:0.781372\n",
            "[12]\tvalidation_0-auc:0.783502\n",
            "[13]\tvalidation_0-auc:0.783577\n",
            "[14]\tvalidation_0-auc:0.79671\n",
            "[15]\tvalidation_0-auc:0.797049\n",
            "[16]\tvalidation_0-auc:0.798029\n",
            "[17]\tvalidation_0-auc:0.796465\n",
            "[18]\tvalidation_0-auc:0.798915\n",
            "[19]\tvalidation_0-auc:0.800874\n",
            "[20]\tvalidation_0-auc:0.800516\n",
            "[21]\tvalidation_0-auc:0.795467\n",
            "[22]\tvalidation_0-auc:0.796861\n",
            "[23]\tvalidation_0-auc:0.795014\n",
            "[24]\tvalidation_0-auc:0.792207\n",
            "[25]\tvalidation_0-auc:0.797275\n",
            "[26]\tvalidation_0-auc:0.797615\n",
            "[27]\tvalidation_0-auc:0.796729\n",
            "[28]\tvalidation_0-auc:0.794336\n",
            "[29]\tvalidation_0-auc:0.793733\n",
            "[30]\tvalidation_0-auc:0.794524\n",
            "[31]\tvalidation_0-auc:0.792565\n",
            "[32]\tvalidation_0-auc:0.796578\n",
            "[33]\tvalidation_0-auc:0.79541\n",
            "[34]\tvalidation_0-auc:0.797671\n",
            "[35]\tvalidation_0-auc:0.7985\n",
            "[36]\tvalidation_0-auc:0.798576\n",
            "[37]\tvalidation_0-auc:0.797633\n",
            "[38]\tvalidation_0-auc:0.796578\n",
            "[39]\tvalidation_0-auc:0.79541\n",
            "[40]\tvalidation_0-auc:0.796842\n",
            "[41]\tvalidation_0-auc:0.800309\n",
            "[42]\tvalidation_0-auc:0.799857\n",
            "[43]\tvalidation_0-auc:0.800196\n",
            "[44]\tvalidation_0-auc:0.798689\n",
            "[45]\tvalidation_0-auc:0.797407\n",
            "[46]\tvalidation_0-auc:0.799329\n",
            "[47]\tvalidation_0-auc:0.800158\n",
            "[48]\tvalidation_0-auc:0.800554\n",
            "[49]\tvalidation_0-auc:0.804134\n",
            "[50]\tvalidation_0-auc:0.804549\n",
            "[51]\tvalidation_0-auc:0.804285\n",
            "[52]\tvalidation_0-auc:0.803757\n",
            "[53]\tvalidation_0-auc:0.804624\n",
            "[54]\tvalidation_0-auc:0.80387\n",
            "[55]\tvalidation_0-auc:0.80323\n",
            "[56]\tvalidation_0-auc:0.798293\n",
            "[57]\tvalidation_0-auc:0.799423\n",
            "[58]\tvalidation_0-auc:0.799876\n",
            "[59]\tvalidation_0-auc:0.799235\n",
            "[60]\tvalidation_0-auc:0.804247\n",
            "[61]\tvalidation_0-auc:0.802514\n",
            "[62]\tvalidation_0-auc:0.803682\n",
            "[63]\tvalidation_0-auc:0.804511\n",
            "[64]\tvalidation_0-auc:0.805076\n",
            "[65]\tvalidation_0-auc:0.805038\n",
            "[66]\tvalidation_0-auc:0.805604\n",
            "[67]\tvalidation_0-auc:0.807469\n",
            "[68]\tvalidation_0-auc:0.810635\n",
            "[69]\tvalidation_0-auc:0.810974\n",
            "[70]\tvalidation_0-auc:0.810182\n",
            "[71]\tvalidation_0-auc:0.81022\n",
            "[72]\tvalidation_0-auc:0.810635\n",
            "[73]\tvalidation_0-auc:0.810823\n",
            "[74]\tvalidation_0-auc:0.808826\n",
            "[75]\tvalidation_0-auc:0.808788\n",
            "[76]\tvalidation_0-auc:0.807884\n",
            "[77]\tvalidation_0-auc:0.806602\n",
            "[78]\tvalidation_0-auc:0.807092\n",
            "[79]\tvalidation_0-auc:0.806414\n",
            "[80]\tvalidation_0-auc:0.806188\n",
            "[81]\tvalidation_0-auc:0.806527\n",
            "[82]\tvalidation_0-auc:0.806414\n",
            "[83]\tvalidation_0-auc:0.807582\n",
            "[84]\tvalidation_0-auc:0.8086\n",
            "[85]\tvalidation_0-auc:0.809617\n",
            "[86]\tvalidation_0-auc:0.808411\n",
            "[87]\tvalidation_0-auc:0.808336\n",
            "[88]\tvalidation_0-auc:0.809391\n",
            "[89]\tvalidation_0-auc:0.809391\n",
            "[90]\tvalidation_0-auc:0.808675\n",
            "[91]\tvalidation_0-auc:0.808298\n",
            "[92]\tvalidation_0-auc:0.806376\n",
            "[93]\tvalidation_0-auc:0.805999\n",
            "[94]\tvalidation_0-auc:0.80517\n",
            "[95]\tvalidation_0-auc:0.805396\n",
            "[96]\tvalidation_0-auc:0.805811\n",
            "[97]\tvalidation_0-auc:0.805849\n",
            "[98]\tvalidation_0-auc:0.80615\n",
            "[99]\tvalidation_0-auc:0.80468\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6328 - accuracy: 0.6891 - val_loss: 0.4843 - val_accuracy: 0.8643\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6212 - accuracy: 0.6953 - val_loss: 0.5053 - val_accuracy: 0.8643\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6090 - accuracy: 0.6953 - val_loss: 0.4925 - val_accuracy: 0.8643\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5772 - accuracy: 0.7097 - val_loss: 0.3659 - val_accuracy: 0.8534\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5739 - accuracy: 0.7035 - val_loss: 0.4209 - val_accuracy: 0.8512\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6311 - accuracy: 0.6905 - val_loss: 0.4548 - val_accuracy: 0.8643\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5854 - accuracy: 0.7042 - val_loss: 0.4457 - val_accuracy: 0.8818\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5408 - accuracy: 0.7261 - val_loss: 0.5750 - val_accuracy: 0.7593\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5520 - accuracy: 0.7255 - val_loss: 0.3654 - val_accuracy: 0.8840\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5220 - accuracy: 0.7234 - val_loss: 0.4268 - val_accuracy: 0.8359\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.760229\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.761086\n",
            "[2]\tvalidation_0-auc:0.761045\n",
            "[3]\tvalidation_0-auc:0.760514\n",
            "[4]\tvalidation_0-auc:0.759453\n",
            "[5]\tvalidation_0-auc:0.759596\n",
            "[6]\tvalidation_0-auc:0.760249\n",
            "[7]\tvalidation_0-auc:0.758657\n",
            "[8]\tvalidation_0-auc:0.754961\n",
            "[9]\tvalidation_0-auc:0.755329\n",
            "[10]\tvalidation_0-auc:0.754982\n",
            "[11]\tvalidation_0-auc:0.761576\n",
            "[12]\tvalidation_0-auc:0.762413\n",
            "[13]\tvalidation_0-auc:0.760596\n",
            "[14]\tvalidation_0-auc:0.762883\n",
            "[15]\tvalidation_0-auc:0.771478\n",
            "[16]\tvalidation_0-auc:0.770784\n",
            "[17]\tvalidation_0-auc:0.768824\n",
            "[18]\tvalidation_0-auc:0.773193\n",
            "[19]\tvalidation_0-auc:0.773479\n",
            "[20]\tvalidation_0-auc:0.773765\n",
            "[21]\tvalidation_0-auc:0.775112\n",
            "[22]\tvalidation_0-auc:0.771274\n",
            "[23]\tvalidation_0-auc:0.771151\n",
            "[24]\tvalidation_0-auc:0.77205\n",
            "[25]\tvalidation_0-auc:0.77158\n",
            "[26]\tvalidation_0-auc:0.771213\n",
            "[27]\tvalidation_0-auc:0.769171\n",
            "[28]\tvalidation_0-auc:0.769294\n",
            "[29]\tvalidation_0-auc:0.768599\n",
            "[30]\tvalidation_0-auc:0.76717\n",
            "[31]\tvalidation_0-auc:0.767497\n",
            "[32]\tvalidation_0-auc:0.764598\n",
            "[33]\tvalidation_0-auc:0.764884\n",
            "[34]\tvalidation_0-auc:0.765047\n",
            "[35]\tvalidation_0-auc:0.763883\n",
            "[36]\tvalidation_0-auc:0.76372\n",
            "[37]\tvalidation_0-auc:0.763965\n",
            "[38]\tvalidation_0-auc:0.761392\n",
            "[39]\tvalidation_0-auc:0.759147\n",
            "[40]\tvalidation_0-auc:0.758493\n",
            "[41]\tvalidation_0-auc:0.757309\n",
            "[42]\tvalidation_0-auc:0.754737\n",
            "[43]\tvalidation_0-auc:0.753389\n",
            "[44]\tvalidation_0-auc:0.753307\n",
            "[45]\tvalidation_0-auc:0.751266\n",
            "[46]\tvalidation_0-auc:0.751062\n",
            "[47]\tvalidation_0-auc:0.750041\n",
            "[48]\tvalidation_0-auc:0.749837\n",
            "[49]\tvalidation_0-auc:0.750429\n",
            "[50]\tvalidation_0-auc:0.74953\n",
            "[51]\tvalidation_0-auc:0.748673\n",
            "[52]\tvalidation_0-auc:0.74806\n",
            "[53]\tvalidation_0-auc:0.749204\n",
            "[54]\tvalidation_0-auc:0.748244\n",
            "[55]\tvalidation_0-auc:0.746284\n",
            "[56]\tvalidation_0-auc:0.745018\n",
            "[57]\tvalidation_0-auc:0.746529\n",
            "[58]\tvalidation_0-auc:0.746938\n",
            "[59]\tvalidation_0-auc:0.745345\n",
            "[60]\tvalidation_0-auc:0.7451\n",
            "[61]\tvalidation_0-auc:0.745304\n",
            "[62]\tvalidation_0-auc:0.744283\n",
            "[63]\tvalidation_0-auc:0.744202\n",
            "[64]\tvalidation_0-auc:0.744896\n",
            "[65]\tvalidation_0-auc:0.743957\n",
            "[66]\tvalidation_0-auc:0.743344\n",
            "[67]\tvalidation_0-auc:0.742323\n",
            "[68]\tvalidation_0-auc:0.741262\n",
            "[69]\tvalidation_0-auc:0.741017\n",
            "[70]\tvalidation_0-auc:0.740813\n",
            "[71]\tvalidation_0-auc:0.739588\n",
            "Stopping. Best iteration:\n",
            "[21]\tvalidation_0-auc:0.775112\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.8734693877551021 |         0.0         |         0.0         |         0.0         |\n",
            "|      GRU 0.15     | 0.8816326530612245 |  0.8333333333333334 | 0.08064516129032258 | 0.14705882352941174 |\n",
            "|    XGBoost 0.15   | 0.889795918367347  |         0.6         |  0.3870967741935484 | 0.47058823529411764 |\n",
            "|    Logreg 0.15    | 0.889795918367347  |  0.7222222222222222 | 0.20967741935483872 |        0.325        |\n",
            "|      SVM 0.15     | 0.889795918367347  |  0.7857142857142857 |  0.1774193548387097 |  0.2894736842105263 |\n",
            "|   LSTM beta 0.15  | 0.8512035010940919 |         0.4         |  0.1935483870967742 | 0.26086956521739135 |\n",
            "|   GRU beta 0.15   | 0.8358862144420132 | 0.43157894736842106 |  0.6612903225806451 |  0.5222929936305734 |\n",
            "| XGBoost beta 0.15 | 0.7943107221006565 |  0.3490566037735849 |  0.5967741935483871 |  0.4404761904761905 |\n",
            "|  logreg beta 0.15 | 0.862144420131291  | 0.49230769230769234 |  0.5161290322580645 |  0.5039370078740157 |\n",
            "|   svm beta 0.15   | 0.7964989059080962 |  0.3176470588235294 | 0.43548387096774194 |  0.3673469387755102 |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "fc-wOxfRFn7z",
        "outputId": "b5128fcc-036a-4f46-ba9d-8d7e4839cf80"
      },
      "source": [
        "Result_cross.to_csv('AMT_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.881633</td>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.064516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.393258</td>\n",
              "      <td>0.834694</td>\n",
              "      <td>0.463576</td>\n",
              "      <td>0.564516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.581395</td>\n",
              "      <td>0.887755</td>\n",
              "      <td>0.476190</td>\n",
              "      <td>0.403226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.535714</td>\n",
              "      <td>0.881633</td>\n",
              "      <td>0.508475</td>\n",
              "      <td>0.483871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.864333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.561644</td>\n",
              "      <td>0.884026</td>\n",
              "      <td>0.607407</td>\n",
              "      <td>0.661290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.778993</td>\n",
              "      <td>0.435754</td>\n",
              "      <td>0.629032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.406593</td>\n",
              "      <td>0.827133</td>\n",
              "      <td>0.483660</td>\n",
              "      <td>0.596774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.271845</td>\n",
              "      <td>0.761488</td>\n",
              "      <td>0.339394</td>\n",
              "      <td>0.451613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.883673</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>0.096774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.864333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.849015</td>\n",
              "      <td>0.028169</td>\n",
              "      <td>0.016129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.870897</td>\n",
              "      <td>0.119403</td>\n",
              "      <td>0.064516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.857768</td>\n",
              "      <td>0.029851</td>\n",
              "      <td>0.016129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.829322</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.016129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.881633</td>\n",
              "      <td>0.147059</td>\n",
              "      <td>0.080645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.889796</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.387097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>0.889796</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.209677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.889796</td>\n",
              "      <td>0.289474</td>\n",
              "      <td>0.177419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.851204</td>\n",
              "      <td>0.260870</td>\n",
              "      <td>0.193548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.431579</td>\n",
              "      <td>0.835886</td>\n",
              "      <td>0.522293</td>\n",
              "      <td>0.661290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.349057</td>\n",
              "      <td>0.794311</td>\n",
              "      <td>0.440476</td>\n",
              "      <td>0.596774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.492308</td>\n",
              "      <td>0.862144</td>\n",
              "      <td>0.503937</td>\n",
              "      <td>0.516129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.317647</td>\n",
              "      <td>0.796499</td>\n",
              "      <td>0.367347</td>\n",
              "      <td>0.435484</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "1            GRU 0.1  AMT  1.000000  0.881633  0.121212  0.064516\n",
              "2        XGBoost 0.1  AMT  0.393258  0.834694  0.463576  0.564516\n",
              "3         Logreg 0.1  AMT  0.581395  0.887755  0.476190  0.403226\n",
              "4            SVM 0.1  AMT  0.535714  0.881633  0.508475  0.483871\n",
              "5      LSTM beta 0.1  AMT  0.000000  0.864333  0.000000  0.000000\n",
              "6       GRU beta 0.1  AMT  0.561644  0.884026  0.607407  0.661290\n",
              "7   XGBoost beta 0.1  AMT  0.333333  0.778993  0.435754  0.629032\n",
              "8    logreg beta 0.1  AMT  0.406593  0.827133  0.483660  0.596774\n",
              "9       svm beta 0.1  AMT  0.271845  0.761488  0.339394  0.451613\n",
              "0           LSTM 0.2  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "1            GRU 0.2  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "2        XGBoost 0.2  AMT  0.857143  0.883673  0.173913  0.096774\n",
              "3         Logreg 0.2  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "4            SVM 0.2  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "5      LSTM beta 0.2  AMT  0.000000  0.864333  0.000000  0.000000\n",
              "6       GRU beta 0.2  AMT  0.111111  0.849015  0.028169  0.016129\n",
              "7   XGBoost beta 0.2  AMT  0.800000  0.870897  0.119403  0.064516\n",
              "8    logreg beta 0.2  AMT  0.200000  0.857768  0.029851  0.016129\n",
              "9       svm beta 0.2  AMT  0.055556  0.829322  0.025000  0.016129\n",
              "0          LSTM 0.15  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "1           GRU 0.15  AMT  0.833333  0.881633  0.147059  0.080645\n",
              "2       XGBoost 0.15  AMT  0.600000  0.889796  0.470588  0.387097\n",
              "3        Logreg 0.15  AMT  0.722222  0.889796  0.325000  0.209677\n",
              "4           SVM 0.15  AMT  0.785714  0.889796  0.289474  0.177419\n",
              "5     LSTM beta 0.15  AMT  0.400000  0.851204  0.260870  0.193548\n",
              "6      GRU beta 0.15  AMT  0.431579  0.835886  0.522293  0.661290\n",
              "7  XGBoost beta 0.15  AMT  0.349057  0.794311  0.440476  0.596774\n",
              "8   logreg beta 0.15  AMT  0.492308  0.862144  0.503937  0.516129\n",
              "9      svm beta 0.15  AMT  0.317647  0.796499  0.367347  0.435484"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0FFk1-MFn70"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x53P8CtyFn70"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez3sbdK-Fn70",
        "outputId": "0c4bc83f-09ce-4f60-aed7-8eff063ac711"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"AMT\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6624 - accuracy: 0.6409 - val_loss: 0.5089 - val_accuracy: 0.8735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6549 - accuracy: 0.6430 - val_loss: 0.5227 - val_accuracy: 0.8735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6500 - accuracy: 0.6423 - val_loss: 0.5266 - val_accuracy: 0.8735\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6328 - accuracy: 0.6644 - val_loss: 0.4690 - val_accuracy: 0.8735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6200 - accuracy: 0.6846 - val_loss: 0.4383 - val_accuracy: 0.8796\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 17ms/step - loss: 0.6606 - accuracy: 0.6430 - val_loss: 0.5285 - val_accuracy: 0.8735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6510 - accuracy: 0.6450 - val_loss: 0.5425 - val_accuracy: 0.8735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6348 - accuracy: 0.6544 - val_loss: 0.4651 - val_accuracy: 0.8918\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.6079 - accuracy: 0.6765 - val_loss: 0.4496 - val_accuracy: 0.8898\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5924 - accuracy: 0.7094 - val_loss: 0.4511 - val_accuracy: 0.8796\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.699634\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.71143\n",
            "[2]\tvalidation_0-auc:0.717949\n",
            "[3]\tvalidation_0-auc:0.721755\n",
            "[4]\tvalidation_0-auc:0.733325\n",
            "[5]\tvalidation_0-auc:0.755276\n",
            "[6]\tvalidation_0-auc:0.761513\n",
            "[7]\tvalidation_0-auc:0.765413\n",
            "[8]\tvalidation_0-auc:0.786969\n",
            "[9]\tvalidation_0-auc:0.779771\n",
            "[10]\tvalidation_0-auc:0.781693\n",
            "[11]\tvalidation_0-auc:0.787477\n",
            "[12]\tvalidation_0-auc:0.793714\n",
            "[13]\tvalidation_0-auc:0.794035\n",
            "[14]\tvalidation_0-auc:0.791755\n",
            "[15]\tvalidation_0-auc:0.792\n",
            "[16]\tvalidation_0-auc:0.796729\n",
            "[17]\tvalidation_0-auc:0.800667\n",
            "[18]\tvalidation_0-auc:0.796823\n",
            "[19]\tvalidation_0-auc:0.800215\n",
            "[20]\tvalidation_0-auc:0.797219\n",
            "[21]\tvalidation_0-auc:0.798199\n",
            "[22]\tvalidation_0-auc:0.797709\n",
            "[23]\tvalidation_0-auc:0.794393\n",
            "[24]\tvalidation_0-auc:0.797257\n",
            "[25]\tvalidation_0-auc:0.803795\n",
            "[26]\tvalidation_0-auc:0.801609\n",
            "[27]\tvalidation_0-auc:0.801854\n",
            "[28]\tvalidation_0-auc:0.804247\n",
            "[29]\tvalidation_0-auc:0.805717\n",
            "[30]\tvalidation_0-auc:0.805415\n",
            "[31]\tvalidation_0-auc:0.806357\n",
            "[32]\tvalidation_0-auc:0.807526\n",
            "[33]\tvalidation_0-auc:0.809146\n",
            "[34]\tvalidation_0-auc:0.809937\n",
            "[35]\tvalidation_0-auc:0.807394\n",
            "[36]\tvalidation_0-auc:0.806263\n",
            "[37]\tvalidation_0-auc:0.807733\n",
            "[38]\tvalidation_0-auc:0.807413\n",
            "[39]\tvalidation_0-auc:0.806621\n",
            "[40]\tvalidation_0-auc:0.808618\n",
            "[41]\tvalidation_0-auc:0.811068\n",
            "[42]\tvalidation_0-auc:0.811671\n",
            "[43]\tvalidation_0-auc:0.811709\n",
            "[44]\tvalidation_0-auc:0.810993\n",
            "[45]\tvalidation_0-auc:0.809749\n",
            "[46]\tvalidation_0-auc:0.809259\n",
            "[47]\tvalidation_0-auc:0.809448\n",
            "[48]\tvalidation_0-auc:0.810804\n",
            "[49]\tvalidation_0-auc:0.811369\n",
            "[50]\tvalidation_0-auc:0.809975\n",
            "[51]\tvalidation_0-auc:0.812349\n",
            "[52]\tvalidation_0-auc:0.813348\n",
            "[53]\tvalidation_0-auc:0.814629\n",
            "[54]\tvalidation_0-auc:0.814893\n",
            "[55]\tvalidation_0-auc:0.813159\n",
            "[56]\tvalidation_0-auc:0.812896\n",
            "[57]\tvalidation_0-auc:0.814064\n",
            "[58]\tvalidation_0-auc:0.811916\n",
            "[59]\tvalidation_0-auc:0.813084\n",
            "[60]\tvalidation_0-auc:0.813009\n",
            "[61]\tvalidation_0-auc:0.814403\n",
            "[62]\tvalidation_0-auc:0.814177\n",
            "[63]\tvalidation_0-auc:0.814252\n",
            "[64]\tvalidation_0-auc:0.812783\n",
            "[65]\tvalidation_0-auc:0.81429\n",
            "[66]\tvalidation_0-auc:0.814516\n",
            "[67]\tvalidation_0-auc:0.814139\n",
            "[68]\tvalidation_0-auc:0.813009\n",
            "[69]\tvalidation_0-auc:0.810898\n",
            "[70]\tvalidation_0-auc:0.810974\n",
            "[71]\tvalidation_0-auc:0.811577\n",
            "[72]\tvalidation_0-auc:0.811614\n",
            "[73]\tvalidation_0-auc:0.811011\n",
            "[74]\tvalidation_0-auc:0.810823\n",
            "[75]\tvalidation_0-auc:0.811011\n",
            "[76]\tvalidation_0-auc:0.810069\n",
            "[77]\tvalidation_0-auc:0.811577\n",
            "[78]\tvalidation_0-auc:0.810408\n",
            "[79]\tvalidation_0-auc:0.810069\n",
            "[80]\tvalidation_0-auc:0.81022\n",
            "[81]\tvalidation_0-auc:0.810258\n",
            "[82]\tvalidation_0-auc:0.810446\n",
            "[83]\tvalidation_0-auc:0.812707\n",
            "[84]\tvalidation_0-auc:0.813649\n",
            "[85]\tvalidation_0-auc:0.812104\n",
            "[86]\tvalidation_0-auc:0.811841\n",
            "[87]\tvalidation_0-auc:0.811351\n",
            "[88]\tvalidation_0-auc:0.811539\n",
            "[89]\tvalidation_0-auc:0.813612\n",
            "[90]\tvalidation_0-auc:0.813951\n",
            "[91]\tvalidation_0-auc:0.810898\n",
            "[92]\tvalidation_0-auc:0.81169\n",
            "[93]\tvalidation_0-auc:0.81267\n",
            "[94]\tvalidation_0-auc:0.812933\n",
            "[95]\tvalidation_0-auc:0.814478\n",
            "[96]\tvalidation_0-auc:0.814931\n",
            "[97]\tvalidation_0-auc:0.816174\n",
            "[98]\tvalidation_0-auc:0.814177\n",
            "[99]\tvalidation_0-auc:0.813725\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6633 - accuracy: 0.6280 - val_loss: 0.5723 - val_accuracy: 0.8643\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6607 - accuracy: 0.6342 - val_loss: 0.5521 - val_accuracy: 0.8643\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6432 - accuracy: 0.6342 - val_loss: 0.4725 - val_accuracy: 0.8381\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6169 - accuracy: 0.6699 - val_loss: 0.4996 - val_accuracy: 0.8621\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5992 - accuracy: 0.6754 - val_loss: 0.4668 - val_accuracy: 0.8556\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6515 - accuracy: 0.6314 - val_loss: 0.4655 - val_accuracy: 0.8490\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5990 - accuracy: 0.6863 - val_loss: 0.5268 - val_accuracy: 0.8140\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5745 - accuracy: 0.7042 - val_loss: 0.4313 - val_accuracy: 0.8578\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5623 - accuracy: 0.7138 - val_loss: 0.5451 - val_accuracy: 0.7856\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5499 - accuracy: 0.7207 - val_loss: 0.4289 - val_accuracy: 0.8425\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.756472\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.754247\n",
            "[2]\tvalidation_0-auc:0.755267\n",
            "[3]\tvalidation_0-auc:0.756513\n",
            "[4]\tvalidation_0-auc:0.761066\n",
            "[5]\tvalidation_0-auc:0.763883\n",
            "[6]\tvalidation_0-auc:0.762536\n",
            "[7]\tvalidation_0-auc:0.763352\n",
            "[8]\tvalidation_0-auc:0.763454\n",
            "[9]\tvalidation_0-auc:0.764108\n",
            "[10]\tvalidation_0-auc:0.768844\n",
            "[11]\tvalidation_0-auc:0.768987\n",
            "[12]\tvalidation_0-auc:0.769641\n",
            "[13]\tvalidation_0-auc:0.769498\n",
            "[14]\tvalidation_0-auc:0.772764\n",
            "[15]\tvalidation_0-auc:0.77207\n",
            "[16]\tvalidation_0-auc:0.772132\n",
            "[17]\tvalidation_0-auc:0.766354\n",
            "[18]\tvalidation_0-auc:0.76223\n",
            "[19]\tvalidation_0-auc:0.762454\n",
            "[20]\tvalidation_0-auc:0.764434\n",
            "[21]\tvalidation_0-auc:0.770825\n",
            "[22]\tvalidation_0-auc:0.773356\n",
            "[23]\tvalidation_0-auc:0.772948\n",
            "[24]\tvalidation_0-auc:0.776399\n",
            "[25]\tvalidation_0-auc:0.777889\n",
            "[26]\tvalidation_0-auc:0.778011\n",
            "[27]\tvalidation_0-auc:0.777276\n",
            "[28]\tvalidation_0-auc:0.777807\n",
            "[29]\tvalidation_0-auc:0.777195\n",
            "[30]\tvalidation_0-auc:0.777848\n",
            "[31]\tvalidation_0-auc:0.778195\n",
            "[32]\tvalidation_0-auc:0.777276\n",
            "[33]\tvalidation_0-auc:0.777521\n",
            "[34]\tvalidation_0-auc:0.777971\n",
            "[35]\tvalidation_0-auc:0.776133\n",
            "[36]\tvalidation_0-auc:0.780053\n",
            "[37]\tvalidation_0-auc:0.778052\n",
            "[38]\tvalidation_0-auc:0.775766\n",
            "[39]\tvalidation_0-auc:0.776051\n",
            "[40]\tvalidation_0-auc:0.778256\n",
            "[41]\tvalidation_0-auc:0.777726\n",
            "[42]\tvalidation_0-auc:0.778787\n",
            "[43]\tvalidation_0-auc:0.776439\n",
            "[44]\tvalidation_0-auc:0.775664\n",
            "[45]\tvalidation_0-auc:0.776684\n",
            "[46]\tvalidation_0-auc:0.776766\n",
            "[47]\tvalidation_0-auc:0.777501\n",
            "[48]\tvalidation_0-auc:0.776562\n",
            "[49]\tvalidation_0-auc:0.776684\n",
            "[50]\tvalidation_0-auc:0.776521\n",
            "[51]\tvalidation_0-auc:0.775255\n",
            "[52]\tvalidation_0-auc:0.774153\n",
            "[53]\tvalidation_0-auc:0.773826\n",
            "[54]\tvalidation_0-auc:0.774398\n",
            "[55]\tvalidation_0-auc:0.773132\n",
            "[56]\tvalidation_0-auc:0.771866\n",
            "[57]\tvalidation_0-auc:0.771131\n",
            "[58]\tvalidation_0-auc:0.769784\n",
            "[59]\tvalidation_0-auc:0.768354\n",
            "[60]\tvalidation_0-auc:0.76815\n",
            "[61]\tvalidation_0-auc:0.767007\n",
            "[62]\tvalidation_0-auc:0.767497\n",
            "[63]\tvalidation_0-auc:0.767456\n",
            "[64]\tvalidation_0-auc:0.766517\n",
            "[65]\tvalidation_0-auc:0.765374\n",
            "[66]\tvalidation_0-auc:0.76472\n",
            "[67]\tvalidation_0-auc:0.764843\n",
            "[68]\tvalidation_0-auc:0.763944\n",
            "[69]\tvalidation_0-auc:0.76423\n",
            "[70]\tvalidation_0-auc:0.764108\n",
            "[71]\tvalidation_0-auc:0.762025\n",
            "[72]\tvalidation_0-auc:0.763822\n",
            "[73]\tvalidation_0-auc:0.763904\n",
            "[74]\tvalidation_0-auc:0.762924\n",
            "[75]\tvalidation_0-auc:0.760923\n",
            "[76]\tvalidation_0-auc:0.758963\n",
            "[77]\tvalidation_0-auc:0.759045\n",
            "[78]\tvalidation_0-auc:0.758024\n",
            "[79]\tvalidation_0-auc:0.756227\n",
            "[80]\tvalidation_0-auc:0.757166\n",
            "[81]\tvalidation_0-auc:0.757574\n",
            "[82]\tvalidation_0-auc:0.756513\n",
            "[83]\tvalidation_0-auc:0.754124\n",
            "[84]\tvalidation_0-auc:0.754614\n",
            "[85]\tvalidation_0-auc:0.75392\n",
            "[86]\tvalidation_0-auc:0.753797\n",
            "Stopping. Best iteration:\n",
            "[36]\tvalidation_0-auc:0.780053\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.8795918367346939 |         1.0         | 0.04838709677419355 | 0.09230769230769231 |\n",
            "|     GRU 0.1      | 0.8795918367346939 |  0.5272727272727272 | 0.46774193548387094 |  0.4957264957264957 |\n",
            "|   XGBoost 0.1    | 0.8346938775510204 | 0.39325842696629215 |  0.5645161290322581 | 0.46357615894039744 |\n",
            "|    Logreg 0.1    | 0.8877551020408163 |  0.5813953488372093 |  0.4032258064516129 |  0.4761904761904762 |\n",
            "|     SVM 0.1      | 0.8816326530612245 |  0.5357142857142857 |  0.4838709677419355 |  0.5084745762711865 |\n",
            "|  LSTM beta 0.1   | 0.8555798687089715 |  0.4787234042553192 |  0.7258064516129032 |  0.576923076923077  |\n",
            "|   GRU beta 0.1   | 0.8424507658643327 |  0.4431818181818182 |  0.6290322580645161 |  0.5199999999999999 |\n",
            "| XGBoost beta 0.1 | 0.7789934354485777 |  0.3333333333333333 |  0.6290322580645161 |  0.435754189944134  |\n",
            "| logreg beta 0.1  | 0.8271334792122538 |  0.4065934065934066 |  0.5967741935483871 | 0.48366013071895425 |\n",
            "|   svm beta 0.1   | 0.7614879649890591 | 0.27184466019417475 | 0.45161290322580644 |  0.3393939393939394 |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.5472 - accuracy: 0.7799 - val_loss: 0.4126 - val_accuracy: 0.8735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5332 - accuracy: 0.7805 - val_loss: 0.3988 - val_accuracy: 0.8735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5402 - accuracy: 0.7805 - val_loss: 0.4227 - val_accuracy: 0.8735\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5339 - accuracy: 0.7805 - val_loss: 0.4059 - val_accuracy: 0.8735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5342 - accuracy: 0.7805 - val_loss: 0.4156 - val_accuracy: 0.8735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 2s 13ms/step - loss: 0.5514 - accuracy: 0.7765 - val_loss: 0.4112 - val_accuracy: 0.8735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5332 - accuracy: 0.7805 - val_loss: 0.4025 - val_accuracy: 0.8735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5306 - accuracy: 0.7805 - val_loss: 0.3941 - val_accuracy: 0.8735\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5232 - accuracy: 0.7819 - val_loss: 0.3749 - val_accuracy: 0.8735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5182 - accuracy: 0.7805 - val_loss: 0.3700 - val_accuracy: 0.8735\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.76383\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.790436\n",
            "[2]\tvalidation_0-auc:0.78955\n",
            "[3]\tvalidation_0-auc:0.806508\n",
            "[4]\tvalidation_0-auc:0.802438\n",
            "[5]\tvalidation_0-auc:0.802476\n",
            "[6]\tvalidation_0-auc:0.821789\n",
            "[7]\tvalidation_0-auc:0.828667\n",
            "[8]\tvalidation_0-auc:0.838691\n",
            "[9]\tvalidation_0-auc:0.83756\n",
            "[10]\tvalidation_0-auc:0.831568\n",
            "[11]\tvalidation_0-auc:0.835676\n",
            "[12]\tvalidation_0-auc:0.834809\n",
            "[13]\tvalidation_0-auc:0.837994\n",
            "[14]\tvalidation_0-auc:0.839539\n",
            "[15]\tvalidation_0-auc:0.838823\n",
            "[16]\tvalidation_0-auc:0.839237\n",
            "[17]\tvalidation_0-auc:0.840142\n",
            "[18]\tvalidation_0-auc:0.839689\n",
            "[19]\tvalidation_0-auc:0.839162\n",
            "[20]\tvalidation_0-auc:0.839124\n",
            "[21]\tvalidation_0-auc:0.835921\n",
            "[22]\tvalidation_0-auc:0.836882\n",
            "[23]\tvalidation_0-auc:0.834696\n",
            "[24]\tvalidation_0-auc:0.837183\n",
            "[25]\tvalidation_0-auc:0.836166\n",
            "[26]\tvalidation_0-auc:0.838239\n",
            "[27]\tvalidation_0-auc:0.837673\n",
            "[28]\tvalidation_0-auc:0.836844\n",
            "[29]\tvalidation_0-auc:0.836392\n",
            "[30]\tvalidation_0-auc:0.836185\n",
            "[31]\tvalidation_0-auc:0.835017\n",
            "[32]\tvalidation_0-auc:0.83415\n",
            "[33]\tvalidation_0-auc:0.832134\n",
            "[34]\tvalidation_0-auc:0.829533\n",
            "[35]\tvalidation_0-auc:0.829157\n",
            "[36]\tvalidation_0-auc:0.832228\n",
            "[37]\tvalidation_0-auc:0.833472\n",
            "[38]\tvalidation_0-auc:0.83366\n",
            "[39]\tvalidation_0-auc:0.833208\n",
            "[40]\tvalidation_0-auc:0.831889\n",
            "[41]\tvalidation_0-auc:0.834583\n",
            "[42]\tvalidation_0-auc:0.83594\n",
            "[43]\tvalidation_0-auc:0.835412\n",
            "[44]\tvalidation_0-auc:0.837296\n",
            "[45]\tvalidation_0-auc:0.837786\n",
            "[46]\tvalidation_0-auc:0.841084\n",
            "[47]\tvalidation_0-auc:0.839878\n",
            "[48]\tvalidation_0-auc:0.838031\n",
            "[49]\tvalidation_0-auc:0.836712\n",
            "[50]\tvalidation_0-auc:0.83773\n",
            "[51]\tvalidation_0-auc:0.839916\n",
            "[52]\tvalidation_0-auc:0.841235\n",
            "[53]\tvalidation_0-auc:0.840632\n",
            "[54]\tvalidation_0-auc:0.840933\n",
            "[55]\tvalidation_0-auc:0.841008\n",
            "[56]\tvalidation_0-auc:0.84033\n",
            "[57]\tvalidation_0-auc:0.840594\n",
            "[58]\tvalidation_0-auc:0.84082\n",
            "[59]\tvalidation_0-auc:0.840971\n",
            "[60]\tvalidation_0-auc:0.840858\n",
            "[61]\tvalidation_0-auc:0.842082\n",
            "[62]\tvalidation_0-auc:0.841253\n",
            "[63]\tvalidation_0-auc:0.841668\n",
            "[64]\tvalidation_0-auc:0.840688\n",
            "[65]\tvalidation_0-auc:0.839463\n",
            "[66]\tvalidation_0-auc:0.839652\n",
            "[67]\tvalidation_0-auc:0.839426\n",
            "[68]\tvalidation_0-auc:0.839614\n",
            "[69]\tvalidation_0-auc:0.841272\n",
            "[70]\tvalidation_0-auc:0.841121\n",
            "[71]\tvalidation_0-auc:0.841611\n",
            "[72]\tvalidation_0-auc:0.842365\n",
            "[73]\tvalidation_0-auc:0.841951\n",
            "[74]\tvalidation_0-auc:0.840556\n",
            "[75]\tvalidation_0-auc:0.837956\n",
            "[76]\tvalidation_0-auc:0.837202\n",
            "[77]\tvalidation_0-auc:0.836939\n",
            "[78]\tvalidation_0-auc:0.838446\n",
            "[79]\tvalidation_0-auc:0.839087\n",
            "[80]\tvalidation_0-auc:0.837504\n",
            "[81]\tvalidation_0-auc:0.835657\n",
            "[82]\tvalidation_0-auc:0.835318\n",
            "[83]\tvalidation_0-auc:0.838031\n",
            "[84]\tvalidation_0-auc:0.836825\n",
            "[85]\tvalidation_0-auc:0.836675\n",
            "[86]\tvalidation_0-auc:0.835996\n",
            "[87]\tvalidation_0-auc:0.835205\n",
            "[88]\tvalidation_0-auc:0.833509\n",
            "[89]\tvalidation_0-auc:0.834753\n",
            "[90]\tvalidation_0-auc:0.83479\n",
            "[91]\tvalidation_0-auc:0.834715\n",
            "[92]\tvalidation_0-auc:0.833924\n",
            "[93]\tvalidation_0-auc:0.833698\n",
            "[94]\tvalidation_0-auc:0.834715\n",
            "[95]\tvalidation_0-auc:0.832605\n",
            "[96]\tvalidation_0-auc:0.832982\n",
            "[97]\tvalidation_0-auc:0.832793\n",
            "[98]\tvalidation_0-auc:0.831813\n",
            "[99]\tvalidation_0-auc:0.830607\n",
            "end training. \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5474 - accuracy: 0.7749 - val_loss: 0.4342 - val_accuracy: 0.8643\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5453 - accuracy: 0.7756 - val_loss: 0.4287 - val_accuracy: 0.8643\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5402 - accuracy: 0.7756 - val_loss: 0.4154 - val_accuracy: 0.8643\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5397 - accuracy: 0.7756 - val_loss: 0.4224 - val_accuracy: 0.8643\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5371 - accuracy: 0.7756 - val_loss: 0.4116 - val_accuracy: 0.8643\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.5533 - accuracy: 0.7742 - val_loss: 0.4162 - val_accuracy: 0.8643\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5392 - accuracy: 0.7756 - val_loss: 0.3879 - val_accuracy: 0.8643\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5179 - accuracy: 0.7769 - val_loss: 0.3747 - val_accuracy: 0.8643\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5104 - accuracy: 0.7763 - val_loss: 0.3533 - val_accuracy: 0.8600\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5020 - accuracy: 0.7742 - val_loss: 0.3914 - val_accuracy: 0.8556\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.711515\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.722458\n",
            "[2]\tvalidation_0-auc:0.738505\n",
            "[3]\tvalidation_0-auc:0.752185\n",
            "[4]\tvalidation_0-auc:0.750715\n",
            "[5]\tvalidation_0-auc:0.752144\n",
            "[6]\tvalidation_0-auc:0.74116\n",
            "[7]\tvalidation_0-auc:0.744753\n",
            "[8]\tvalidation_0-auc:0.745427\n",
            "[9]\tvalidation_0-auc:0.751266\n",
            "[10]\tvalidation_0-auc:0.746448\n",
            "[11]\tvalidation_0-auc:0.739179\n",
            "[12]\tvalidation_0-auc:0.730523\n",
            "[13]\tvalidation_0-auc:0.732973\n",
            "[14]\tvalidation_0-auc:0.74312\n",
            "[15]\tvalidation_0-auc:0.745202\n",
            "[16]\tvalidation_0-auc:0.733687\n",
            "[17]\tvalidation_0-auc:0.732238\n",
            "[18]\tvalidation_0-auc:0.73236\n",
            "[19]\tvalidation_0-auc:0.724622\n",
            "[20]\tvalidation_0-auc:0.718028\n",
            "[21]\tvalidation_0-auc:0.711944\n",
            "[22]\tvalidation_0-auc:0.711678\n",
            "[23]\tvalidation_0-auc:0.709759\n",
            "[24]\tvalidation_0-auc:0.709575\n",
            "[25]\tvalidation_0-auc:0.706921\n",
            "[26]\tvalidation_0-auc:0.700551\n",
            "[27]\tvalidation_0-auc:0.697285\n",
            "[28]\tvalidation_0-auc:0.693242\n",
            "[29]\tvalidation_0-auc:0.693324\n",
            "[30]\tvalidation_0-auc:0.686505\n",
            "[31]\tvalidation_0-auc:0.682728\n",
            "[32]\tvalidation_0-auc:0.680278\n",
            "[33]\tvalidation_0-auc:0.673622\n",
            "[34]\tvalidation_0-auc:0.672826\n",
            "[35]\tvalidation_0-auc:0.670621\n",
            "[36]\tvalidation_0-auc:0.665067\n",
            "[37]\tvalidation_0-auc:0.660535\n",
            "[38]\tvalidation_0-auc:0.658534\n",
            "[39]\tvalidation_0-auc:0.652817\n",
            "[40]\tvalidation_0-auc:0.654512\n",
            "[41]\tvalidation_0-auc:0.653083\n",
            "[42]\tvalidation_0-auc:0.649857\n",
            "[43]\tvalidation_0-auc:0.64463\n",
            "[44]\tvalidation_0-auc:0.6402\n",
            "[45]\tvalidation_0-auc:0.63873\n",
            "[46]\tvalidation_0-auc:0.638975\n",
            "[47]\tvalidation_0-auc:0.638444\n",
            "[48]\tvalidation_0-auc:0.638567\n",
            "[49]\tvalidation_0-auc:0.635096\n",
            "[50]\tvalidation_0-auc:0.633136\n",
            "[51]\tvalidation_0-auc:0.631829\n",
            "[52]\tvalidation_0-auc:0.62844\n",
            "[53]\tvalidation_0-auc:0.62305\n",
            "Stopping. Best iteration:\n",
            "[3]\tvalidation_0-auc:0.752185\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+----------------------+----------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall        |       F1 score       |\n",
            "+------------------+--------------------+---------------------+----------------------+----------------------+\n",
            "|     LSTM 0.2     | 0.8734693877551021 |         0.0         |         0.0          |         0.0          |\n",
            "|     GRU 0.2      | 0.8734693877551021 |         0.0         |         0.0          |         0.0          |\n",
            "|   XGBoost 0.2    | 0.8836734693877552 |  0.8571428571428571 |  0.0967741935483871  | 0.17391304347826084  |\n",
            "|    Logreg 0.2    | 0.8734693877551021 |         0.0         |         0.0          |         0.0          |\n",
            "|     SVM 0.2      | 0.8734693877551021 |         0.0         |         0.0          |         0.0          |\n",
            "|  LSTM beta 0.2   | 0.8643326039387309 |         0.0         |         0.0          |         0.0          |\n",
            "|   GRU beta 0.2   | 0.8555798687089715 |         0.25        | 0.03225806451612903  | 0.05714285714285715  |\n",
            "| XGBoost beta 0.2 | 0.8708971553610503 |         0.8         | 0.06451612903225806  | 0.11940298507462686  |\n",
            "| logreg beta 0.2  | 0.8577680525164114 |         0.2         | 0.016129032258064516 | 0.029850746268656716 |\n",
            "|   svm beta 0.2   | 0.8293216630196937 | 0.05555555555555555 | 0.016129032258064516 | 0.024999999999999998 |\n",
            "+------------------+--------------------+---------------------+----------------------+----------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6254 - accuracy: 0.7013 - val_loss: 0.4487 - val_accuracy: 0.8735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6144 - accuracy: 0.7020 - val_loss: 0.4385 - val_accuracy: 0.8735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6119 - accuracy: 0.7020 - val_loss: 0.4578 - val_accuracy: 0.8735\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6116 - accuracy: 0.7020 - val_loss: 0.4463 - val_accuracy: 0.8735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6155 - accuracy: 0.7020 - val_loss: 0.4611 - val_accuracy: 0.8735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6176 - accuracy: 0.6933 - val_loss: 0.5102 - val_accuracy: 0.8735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6046 - accuracy: 0.7054 - val_loss: 0.3833 - val_accuracy: 0.8735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5913 - accuracy: 0.7174 - val_loss: 0.4165 - val_accuracy: 0.8816\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5762 - accuracy: 0.7248 - val_loss: 0.4081 - val_accuracy: 0.8857\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5754 - accuracy: 0.7255 - val_loss: 0.4071 - val_accuracy: 0.8878\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.689478\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.721228\n",
            "[2]\tvalidation_0-auc:0.722773\n",
            "[3]\tvalidation_0-auc:0.755653\n",
            "[4]\tvalidation_0-auc:0.765658\n",
            "[5]\tvalidation_0-auc:0.765338\n",
            "[6]\tvalidation_0-auc:0.765262\n",
            "[7]\tvalidation_0-auc:0.764094\n",
            "[8]\tvalidation_0-auc:0.773854\n",
            "[9]\tvalidation_0-auc:0.771932\n",
            "[10]\tvalidation_0-auc:0.780732\n",
            "[11]\tvalidation_0-auc:0.781372\n",
            "[12]\tvalidation_0-auc:0.783502\n",
            "[13]\tvalidation_0-auc:0.783577\n",
            "[14]\tvalidation_0-auc:0.79671\n",
            "[15]\tvalidation_0-auc:0.797049\n",
            "[16]\tvalidation_0-auc:0.798029\n",
            "[17]\tvalidation_0-auc:0.796465\n",
            "[18]\tvalidation_0-auc:0.798915\n",
            "[19]\tvalidation_0-auc:0.800874\n",
            "[20]\tvalidation_0-auc:0.800516\n",
            "[21]\tvalidation_0-auc:0.795467\n",
            "[22]\tvalidation_0-auc:0.796861\n",
            "[23]\tvalidation_0-auc:0.795014\n",
            "[24]\tvalidation_0-auc:0.792207\n",
            "[25]\tvalidation_0-auc:0.797275\n",
            "[26]\tvalidation_0-auc:0.797615\n",
            "[27]\tvalidation_0-auc:0.796729\n",
            "[28]\tvalidation_0-auc:0.794336\n",
            "[29]\tvalidation_0-auc:0.793733\n",
            "[30]\tvalidation_0-auc:0.794524\n",
            "[31]\tvalidation_0-auc:0.792565\n",
            "[32]\tvalidation_0-auc:0.796578\n",
            "[33]\tvalidation_0-auc:0.79541\n",
            "[34]\tvalidation_0-auc:0.797671\n",
            "[35]\tvalidation_0-auc:0.7985\n",
            "[36]\tvalidation_0-auc:0.798576\n",
            "[37]\tvalidation_0-auc:0.797633\n",
            "[38]\tvalidation_0-auc:0.796578\n",
            "[39]\tvalidation_0-auc:0.79541\n",
            "[40]\tvalidation_0-auc:0.796842\n",
            "[41]\tvalidation_0-auc:0.800309\n",
            "[42]\tvalidation_0-auc:0.799857\n",
            "[43]\tvalidation_0-auc:0.800196\n",
            "[44]\tvalidation_0-auc:0.798689\n",
            "[45]\tvalidation_0-auc:0.797407\n",
            "[46]\tvalidation_0-auc:0.799329\n",
            "[47]\tvalidation_0-auc:0.800158\n",
            "[48]\tvalidation_0-auc:0.800554\n",
            "[49]\tvalidation_0-auc:0.804134\n",
            "[50]\tvalidation_0-auc:0.804549\n",
            "[51]\tvalidation_0-auc:0.804285\n",
            "[52]\tvalidation_0-auc:0.803757\n",
            "[53]\tvalidation_0-auc:0.804624\n",
            "[54]\tvalidation_0-auc:0.80387\n",
            "[55]\tvalidation_0-auc:0.80323\n",
            "[56]\tvalidation_0-auc:0.798293\n",
            "[57]\tvalidation_0-auc:0.799423\n",
            "[58]\tvalidation_0-auc:0.799876\n",
            "[59]\tvalidation_0-auc:0.799235\n",
            "[60]\tvalidation_0-auc:0.804247\n",
            "[61]\tvalidation_0-auc:0.802514\n",
            "[62]\tvalidation_0-auc:0.803682\n",
            "[63]\tvalidation_0-auc:0.804511\n",
            "[64]\tvalidation_0-auc:0.805076\n",
            "[65]\tvalidation_0-auc:0.805038\n",
            "[66]\tvalidation_0-auc:0.805604\n",
            "[67]\tvalidation_0-auc:0.807469\n",
            "[68]\tvalidation_0-auc:0.810635\n",
            "[69]\tvalidation_0-auc:0.810974\n",
            "[70]\tvalidation_0-auc:0.810182\n",
            "[71]\tvalidation_0-auc:0.81022\n",
            "[72]\tvalidation_0-auc:0.810635\n",
            "[73]\tvalidation_0-auc:0.810823\n",
            "[74]\tvalidation_0-auc:0.808826\n",
            "[75]\tvalidation_0-auc:0.808788\n",
            "[76]\tvalidation_0-auc:0.807884\n",
            "[77]\tvalidation_0-auc:0.806602\n",
            "[78]\tvalidation_0-auc:0.807092\n",
            "[79]\tvalidation_0-auc:0.806414\n",
            "[80]\tvalidation_0-auc:0.806188\n",
            "[81]\tvalidation_0-auc:0.806527\n",
            "[82]\tvalidation_0-auc:0.806414\n",
            "[83]\tvalidation_0-auc:0.807582\n",
            "[84]\tvalidation_0-auc:0.8086\n",
            "[85]\tvalidation_0-auc:0.809617\n",
            "[86]\tvalidation_0-auc:0.808411\n",
            "[87]\tvalidation_0-auc:0.808336\n",
            "[88]\tvalidation_0-auc:0.809391\n",
            "[89]\tvalidation_0-auc:0.809391\n",
            "[90]\tvalidation_0-auc:0.808675\n",
            "[91]\tvalidation_0-auc:0.808298\n",
            "[92]\tvalidation_0-auc:0.806376\n",
            "[93]\tvalidation_0-auc:0.805999\n",
            "[94]\tvalidation_0-auc:0.80517\n",
            "[95]\tvalidation_0-auc:0.805396\n",
            "[96]\tvalidation_0-auc:0.805811\n",
            "[97]\tvalidation_0-auc:0.805849\n",
            "[98]\tvalidation_0-auc:0.80615\n",
            "[99]\tvalidation_0-auc:0.80468\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6244 - accuracy: 0.6939 - val_loss: 0.4480 - val_accuracy: 0.8643\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6094 - accuracy: 0.6953 - val_loss: 0.4273 - val_accuracy: 0.8381\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5755 - accuracy: 0.7014 - val_loss: 0.4069 - val_accuracy: 0.8578\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5684 - accuracy: 0.7062 - val_loss: 0.4307 - val_accuracy: 0.8249\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5529 - accuracy: 0.7028 - val_loss: 0.4247 - val_accuracy: 0.8468\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6216 - accuracy: 0.6911 - val_loss: 0.5054 - val_accuracy: 0.8643\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5773 - accuracy: 0.7117 - val_loss: 0.5078 - val_accuracy: 0.8709\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5356 - accuracy: 0.7220 - val_loss: 0.4708 - val_accuracy: 0.8359\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5262 - accuracy: 0.7344 - val_loss: 0.3958 - val_accuracy: 0.8775\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5281 - accuracy: 0.7399 - val_loss: 0.3990 - val_accuracy: 0.8709\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.760229\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.761086\n",
            "[2]\tvalidation_0-auc:0.761045\n",
            "[3]\tvalidation_0-auc:0.760514\n",
            "[4]\tvalidation_0-auc:0.759453\n",
            "[5]\tvalidation_0-auc:0.759596\n",
            "[6]\tvalidation_0-auc:0.760249\n",
            "[7]\tvalidation_0-auc:0.758657\n",
            "[8]\tvalidation_0-auc:0.754961\n",
            "[9]\tvalidation_0-auc:0.755329\n",
            "[10]\tvalidation_0-auc:0.754982\n",
            "[11]\tvalidation_0-auc:0.761576\n",
            "[12]\tvalidation_0-auc:0.762413\n",
            "[13]\tvalidation_0-auc:0.760596\n",
            "[14]\tvalidation_0-auc:0.762883\n",
            "[15]\tvalidation_0-auc:0.771478\n",
            "[16]\tvalidation_0-auc:0.770784\n",
            "[17]\tvalidation_0-auc:0.768824\n",
            "[18]\tvalidation_0-auc:0.773193\n",
            "[19]\tvalidation_0-auc:0.773479\n",
            "[20]\tvalidation_0-auc:0.773765\n",
            "[21]\tvalidation_0-auc:0.775112\n",
            "[22]\tvalidation_0-auc:0.771274\n",
            "[23]\tvalidation_0-auc:0.771151\n",
            "[24]\tvalidation_0-auc:0.77205\n",
            "[25]\tvalidation_0-auc:0.77158\n",
            "[26]\tvalidation_0-auc:0.771213\n",
            "[27]\tvalidation_0-auc:0.769171\n",
            "[28]\tvalidation_0-auc:0.769294\n",
            "[29]\tvalidation_0-auc:0.768599\n",
            "[30]\tvalidation_0-auc:0.76717\n",
            "[31]\tvalidation_0-auc:0.767497\n",
            "[32]\tvalidation_0-auc:0.764598\n",
            "[33]\tvalidation_0-auc:0.764884\n",
            "[34]\tvalidation_0-auc:0.765047\n",
            "[35]\tvalidation_0-auc:0.763883\n",
            "[36]\tvalidation_0-auc:0.76372\n",
            "[37]\tvalidation_0-auc:0.763965\n",
            "[38]\tvalidation_0-auc:0.761392\n",
            "[39]\tvalidation_0-auc:0.759147\n",
            "[40]\tvalidation_0-auc:0.758493\n",
            "[41]\tvalidation_0-auc:0.757309\n",
            "[42]\tvalidation_0-auc:0.754737\n",
            "[43]\tvalidation_0-auc:0.753389\n",
            "[44]\tvalidation_0-auc:0.753307\n",
            "[45]\tvalidation_0-auc:0.751266\n",
            "[46]\tvalidation_0-auc:0.751062\n",
            "[47]\tvalidation_0-auc:0.750041\n",
            "[48]\tvalidation_0-auc:0.749837\n",
            "[49]\tvalidation_0-auc:0.750429\n",
            "[50]\tvalidation_0-auc:0.74953\n",
            "[51]\tvalidation_0-auc:0.748673\n",
            "[52]\tvalidation_0-auc:0.74806\n",
            "[53]\tvalidation_0-auc:0.749204\n",
            "[54]\tvalidation_0-auc:0.748244\n",
            "[55]\tvalidation_0-auc:0.746284\n",
            "[56]\tvalidation_0-auc:0.745018\n",
            "[57]\tvalidation_0-auc:0.746529\n",
            "[58]\tvalidation_0-auc:0.746938\n",
            "[59]\tvalidation_0-auc:0.745345\n",
            "[60]\tvalidation_0-auc:0.7451\n",
            "[61]\tvalidation_0-auc:0.745304\n",
            "[62]\tvalidation_0-auc:0.744283\n",
            "[63]\tvalidation_0-auc:0.744202\n",
            "[64]\tvalidation_0-auc:0.744896\n",
            "[65]\tvalidation_0-auc:0.743957\n",
            "[66]\tvalidation_0-auc:0.743344\n",
            "[67]\tvalidation_0-auc:0.742323\n",
            "[68]\tvalidation_0-auc:0.741262\n",
            "[69]\tvalidation_0-auc:0.741017\n",
            "[70]\tvalidation_0-auc:0.740813\n",
            "[71]\tvalidation_0-auc:0.739588\n",
            "Stopping. Best iteration:\n",
            "[21]\tvalidation_0-auc:0.775112\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.8734693877551021 |         0.0         |         0.0         |         0.0         |\n",
            "|      GRU 0.15     | 0.8877551020408163 |  0.8181818181818182 | 0.14516129032258066 | 0.24657534246575347 |\n",
            "|    XGBoost 0.15   | 0.889795918367347  |         0.6         |  0.3870967741935484 | 0.47058823529411764 |\n",
            "|    Logreg 0.15    | 0.889795918367347  |  0.7222222222222222 | 0.20967741935483872 |        0.325        |\n",
            "|      SVM 0.15     | 0.889795918367347  |  0.7857142857142857 |  0.1774193548387097 |  0.2894736842105263 |\n",
            "|   LSTM beta 0.15  | 0.8468271334792122 | 0.45555555555555555 |  0.6612903225806451 |  0.5394736842105263 |\n",
            "|   GRU beta 0.15   | 0.8708971553610503 |  0.5238095238095238 |  0.532258064516129  |  0.5280000000000001 |\n",
            "| XGBoost beta 0.15 | 0.7943107221006565 |  0.3490566037735849 |  0.5967741935483871 |  0.4404761904761905 |\n",
            "|  logreg beta 0.15 | 0.862144420131291  | 0.49230769230769234 |  0.5161290322580645 |  0.5039370078740157 |\n",
            "|   svm beta 0.15   | 0.7964989059080962 |  0.3176470588235294 | 0.43548387096774194 |  0.3673469387755102 |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "fju2_PabFn71",
        "outputId": "e3ec2d66-758d-4ebc-f023-40b2f11c7695"
      },
      "source": [
        "Result_purging.to_csv('AMT_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.879592</td>\n",
              "      <td>0.092308</td>\n",
              "      <td>0.048387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.527273</td>\n",
              "      <td>0.879592</td>\n",
              "      <td>0.495726</td>\n",
              "      <td>0.467742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.393258</td>\n",
              "      <td>0.834694</td>\n",
              "      <td>0.463576</td>\n",
              "      <td>0.564516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.581395</td>\n",
              "      <td>0.887755</td>\n",
              "      <td>0.476190</td>\n",
              "      <td>0.403226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.535714</td>\n",
              "      <td>0.881633</td>\n",
              "      <td>0.508475</td>\n",
              "      <td>0.483871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.478723</td>\n",
              "      <td>0.855580</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.725806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.443182</td>\n",
              "      <td>0.842451</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.629032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.778993</td>\n",
              "      <td>0.435754</td>\n",
              "      <td>0.629032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.406593</td>\n",
              "      <td>0.827133</td>\n",
              "      <td>0.483660</td>\n",
              "      <td>0.596774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.271845</td>\n",
              "      <td>0.761488</td>\n",
              "      <td>0.339394</td>\n",
              "      <td>0.451613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.883673</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>0.096774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.864333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.855580</td>\n",
              "      <td>0.057143</td>\n",
              "      <td>0.032258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.870897</td>\n",
              "      <td>0.119403</td>\n",
              "      <td>0.064516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.857768</td>\n",
              "      <td>0.029851</td>\n",
              "      <td>0.016129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.829322</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.016129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.887755</td>\n",
              "      <td>0.246575</td>\n",
              "      <td>0.145161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.889796</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.387097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>0.889796</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.209677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.889796</td>\n",
              "      <td>0.289474</td>\n",
              "      <td>0.177419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.455556</td>\n",
              "      <td>0.846827</td>\n",
              "      <td>0.539474</td>\n",
              "      <td>0.661290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.523810</td>\n",
              "      <td>0.870897</td>\n",
              "      <td>0.528000</td>\n",
              "      <td>0.532258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.349057</td>\n",
              "      <td>0.794311</td>\n",
              "      <td>0.440476</td>\n",
              "      <td>0.596774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.492308</td>\n",
              "      <td>0.862144</td>\n",
              "      <td>0.503937</td>\n",
              "      <td>0.516129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>AMT</td>\n",
              "      <td>0.317647</td>\n",
              "      <td>0.796499</td>\n",
              "      <td>0.367347</td>\n",
              "      <td>0.435484</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  AMT  1.000000  0.879592  0.092308  0.048387\n",
              "1            GRU 0.1  AMT  0.527273  0.879592  0.495726  0.467742\n",
              "2        XGBoost 0.1  AMT  0.393258  0.834694  0.463576  0.564516\n",
              "3         Logreg 0.1  AMT  0.581395  0.887755  0.476190  0.403226\n",
              "4            SVM 0.1  AMT  0.535714  0.881633  0.508475  0.483871\n",
              "5      LSTM beta 0.1  AMT  0.478723  0.855580  0.576923  0.725806\n",
              "6       GRU beta 0.1  AMT  0.443182  0.842451  0.520000  0.629032\n",
              "7   XGBoost beta 0.1  AMT  0.333333  0.778993  0.435754  0.629032\n",
              "8    logreg beta 0.1  AMT  0.406593  0.827133  0.483660  0.596774\n",
              "9       svm beta 0.1  AMT  0.271845  0.761488  0.339394  0.451613\n",
              "0           LSTM 0.2  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "1            GRU 0.2  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "2        XGBoost 0.2  AMT  0.857143  0.883673  0.173913  0.096774\n",
              "3         Logreg 0.2  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "4            SVM 0.2  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "5      LSTM beta 0.2  AMT  0.000000  0.864333  0.000000  0.000000\n",
              "6       GRU beta 0.2  AMT  0.250000  0.855580  0.057143  0.032258\n",
              "7   XGBoost beta 0.2  AMT  0.800000  0.870897  0.119403  0.064516\n",
              "8    logreg beta 0.2  AMT  0.200000  0.857768  0.029851  0.016129\n",
              "9       svm beta 0.2  AMT  0.055556  0.829322  0.025000  0.016129\n",
              "0          LSTM 0.15  AMT  0.000000  0.873469  0.000000  0.000000\n",
              "1           GRU 0.15  AMT  0.818182  0.887755  0.246575  0.145161\n",
              "2       XGBoost 0.15  AMT  0.600000  0.889796  0.470588  0.387097\n",
              "3        Logreg 0.15  AMT  0.722222  0.889796  0.325000  0.209677\n",
              "4           SVM 0.15  AMT  0.785714  0.889796  0.289474  0.177419\n",
              "5     LSTM beta 0.15  AMT  0.455556  0.846827  0.539474  0.661290\n",
              "6      GRU beta 0.15  AMT  0.523810  0.870897  0.528000  0.532258\n",
              "7  XGBoost beta 0.15  AMT  0.349057  0.794311  0.440476  0.596774\n",
              "8   logreg beta 0.15  AMT  0.492308  0.862144  0.503937  0.516129\n",
              "9      svm beta 0.15  AMT  0.317647  0.796499  0.367347  0.435484"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdAbX2K3Fn72"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('AMT_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_pcL-boGa9k"
      },
      "source": [
        "## CL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "zKh5rMjiGa-A",
        "outputId": "2933af23-b7e1-4e19-81e8-e64a4905307a"
      },
      "source": [
        "dfs = pd.read_csv(\"CL.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2768</td>\n",
              "      <td>US1.CL</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>75.76</td>\n",
              "      <td>75.98</td>\n",
              "      <td>75.07</td>\n",
              "      <td>75.540</td>\n",
              "      <td>154087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2767</td>\n",
              "      <td>US1.CL</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>77.24</td>\n",
              "      <td>77.24</td>\n",
              "      <td>75.59</td>\n",
              "      <td>75.590</td>\n",
              "      <td>208601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.CL</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>75.89</td>\n",
              "      <td>77.35</td>\n",
              "      <td>75.72</td>\n",
              "      <td>77.075</td>\n",
              "      <td>244930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.CL</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>75.79</td>\n",
              "      <td>76.15</td>\n",
              "      <td>75.41</td>\n",
              "      <td>75.830</td>\n",
              "      <td>227270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.CL</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>76.58</td>\n",
              "      <td>76.65</td>\n",
              "      <td>75.92</td>\n",
              "      <td>76.010</td>\n",
              "      <td>144858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.CL</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>73.77</td>\n",
              "      <td>75.04</td>\n",
              "      <td>73.62</td>\n",
              "      <td>74.910</td>\n",
              "      <td>3042250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.CL</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>74.63</td>\n",
              "      <td>74.63</td>\n",
              "      <td>73.67</td>\n",
              "      <td>73.730</td>\n",
              "      <td>3563991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.CL</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>74.73</td>\n",
              "      <td>75.04</td>\n",
              "      <td>73.75</td>\n",
              "      <td>73.960</td>\n",
              "      <td>4674296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.CL</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>75.90</td>\n",
              "      <td>76.10</td>\n",
              "      <td>74.40</td>\n",
              "      <td>74.880</td>\n",
              "      <td>7170535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.CL</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>76.46</td>\n",
              "      <td>77.24</td>\n",
              "      <td>76.25</td>\n",
              "      <td>76.420</td>\n",
              "      <td>2872127</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2769 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...  <HIGH>  <LOW>  <CLOSE>    <VOL>\n",
              "0      2768   US1.CL     D  20211001  ...   75.98  75.07   75.540   154087\n",
              "1      2767   US1.CL     D  20210930  ...   77.24  75.59   75.590   208601\n",
              "2      2766   US1.CL     D  20210929  ...   77.35  75.72   77.075   244930\n",
              "3      2765   US1.CL     D  20210928  ...   76.15  75.41   75.830   227270\n",
              "4      2764   US1.CL     D  20210927  ...   76.65  75.92   76.010   144858\n",
              "...     ...      ...   ...       ...  ...     ...    ...      ...      ...\n",
              "2764      4   US1.CL     D  20101008  ...   75.04  73.62   74.910  3042250\n",
              "2765      3   US1.CL     D  20101007  ...   74.63  73.67   73.730  3563991\n",
              "2766      2   US1.CL     D  20101006  ...   75.04  73.75   73.960  4674296\n",
              "2767      1   US1.CL     D  20101005  ...   76.10  74.40   74.880  7170535\n",
              "2768      0   US1.CL     D  20101004  ...   77.24  76.25   76.420  2872127\n",
              "\n",
              "[2769 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G6biKoROGa-B",
        "outputId": "77a7e62b-325f-4c1a-b8ac-1e3fc79c06c9"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"2b224fb7-232e-4a2f-bba1-e169db3b59c3\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"2b224fb7-232e-4a2f-bba1-e169db3b59c3\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '2b224fb7-232e-4a2f-bba1-e169db3b59c3',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [70.7, 71.58, 72.07, 70.88, 70.53, 71.71, 73.5, 73.27, 72.73, 72.25, 72.49, 71.5, 70.98, 70.94, 70.77, 69.86, 69.42, 70.68, 72.29, 72.84, 72.09, 73.17, 74.7, 74.29, 74.87, 74.31, 74.12, 74.2, 74.02, 73.43, 72.8, 72.28, 73.04, 72.6017, 72.56, 73.07, 72.29, 71.36, 70.52, 71.68, 70.75, 71.51, 71.73, 70.44, 69.86, 68.805, 71.21, 71.26, 71.69, 74.74, 75.57, 73.69, 72.01, 71.29, 72.27, 72.34, 72.72, 73.87, 73.76, 74.5, 74.0, 74.37, 73.88, 73.49, 73.115, 73.39, 73.15, 74.0, 72.741, 72.08, 71.63, 72.14, 72.335, 73.91, 73.98, 72.965, 73.6, 73.52, 72.9, 73.41, 73.35, 72.755, 73.05, 73.32, 73.02, 73.99, 73.11, 71.91, 70.84, 70.66, 69.65, 70.85, 69.95, 70.1, 71.22, 71.75, 71.56, 71.52, 71.69, 71.96, 72.55, 71.34, 71.16, 71.55, 71.51, 70.86, 71.01, 70.51, 71.69, 71.97, 71.83, 71.29, 72.78, 71.39, 71.28, 68.66, 68.87, 69.01, 69.62, 69.34, 68.68, 69.04, 68.68, 68.77, 68.07, 68.12, 68.12, 68.03, 67.9, 67.55, 67.81, 68.38, 68.29, 68.52, 67.39, 67.0, 66.92, 66.27, 66.0, 65.82, 65.1, 65.76, 66.28, 67.19, 66.25, 66.51, 65.75, 65.85, 65.34, 65.34, 65.6, 65.72, 66.04, 66.0, 65.88, 65.85, 66.49, 66.52, 66.89, 66.72, 66.51, 66.73, 66.48, 65.44, 65.93, 66.01, 65.78, 65.69, 65.05, 65.34, 64.75, 65.2, 65.03, 64.68, 63.95, 63.71, 62.75, 61.84, 62.16, 62.37, 61.76, 62.62, 62.0, 62.12, 62.22, 61.6, 62.08, 61.82, 61.29, 60.5, 59.9, 59.52, 58.97, 59.25, 59.53, 59.63, 59.67, 59.39, 58.04, 59.91, 60.97, 61.88, 62.45, 62.93, 65.2, 65.48, 65.28, 64.57, 63.53, 62.71, 63.22, 63.09, 63.75, 63.52, 63.04, 62.82, 63.23, 62.73, 61.78, 61.91, 62.32, 62.77, 63.24, 62.43, 62.48, 61.88, 61.36, 63.79, 60.9, 61.33, 60.5, 60.22, 60.28, 60.06, 59.55, 60.18, 57.89, 59.55, 63.84, 64.66, 63.41, 63.63, 64.51, 62.65, 63.55, 62.61, 62.55, 62.545, 62.34, 63.87, 65.07, 65.35, 64.71, 64.92, 65.92, 67.39, 67.07, 66.95, 66.73, 67.11, 67.14, 67.94, 69.09, 68.795, 67.94, 68.27, 68.81, 67.99, 68.16, 67.99, 67.19, 67.17, 67.01, 67.0, 66.6, 65.23, 66.41, 66.24, 66.97, 66.79, 67.08, 66.88, 66.58, 67.0, 67.4, 67.65, 67.5, 66.73, 65.84, 65.53, 65.55, 65.42, 65.92, 66.61, 67.23, 67.37, 67.5, 67.26, 66.08, 67.0, 66.45, 66.68, 66.95, 65.95, 65.29, 65.85, 65.94, 65.75, 64.805, 65.565, 65.61, 65.75, 65.16, 65.24, 65.72, 65.03, 65.77, 65.96, 64.67, 64.57, 64.82, 64.92, 64.71, 65.14, 65.11, 64.64, 63.48, 63.91, 63.98, 63.31, 64.73, 63.31, 63.05, 63.72, 63.74, 63.35, 62.76, 62.57, 62.3, 62.7, 62.7, 63.07, 64.4, 63.56, 63.76, 62.49, 62.7, 62.39, 62.26, 62.21, 62.28, 62.09, 61.65, 62.03, 62.7, 62.49, 61.88, 62.28, 62.82, 63.72, 63.3, 63.15, 64.85, 65.22, 66.59, 66.64, 66.42, 66.64, 66.38, 67.53, 69.77, 72.09, 72.26, 72.435, 71.58, 70.91, 71.37, 71.72, 71.44, 71.62, 71.81, 71.67, 70.96, 70.18, 71.69, 70.83, 69.27, 68.19, 68.37, 68.8, 68.71, 69.68, 70.07, 69.7, 69.55, 70.01, 70.86, 71.08, 71.24, 71.01, 69.72, 69.65, 70.06, 69.27, 68.84, 68.97, 69.93, 71.07, 70.29, 69.39, 69.15, 70.02, 71.47, 71.22, 69.63, 70.29, 70.19, 69.76, 68.95, 70.17, 70.43, 69.82, 72.38, 73.33, 74.25, 74.69, 73.77, 73.45, 77.32, 76.81, 76.91, 77.38, 77.51, 76.64, 76.69, 75.42, 74.61, 74.55, 74.82, 75.655, 75.5, 75.4, 75.331, 74.84, 75.17, 75.51, 75.13, 75.36, 75.48, 75.15, 74.67, 74.525, 74.845, 74.0, 74.04, 73.23, 73.37, 72.48, 73.17, 73.329, 73.2, 74.26, 74.58, 74.5, 73.03, 72.45, 72.01, 72.78, 72.01, 71.83, 71.76, 72.21, 72.07, 71.9, 72.63, 72.48, 73.45, 73.7, 73.21, 72.92, 73.27, 70.89, 70.26, 70.15, 70.94, 70.56, 70.47, 69.21, 70.41, 71.22, 71.4167, 71.82, 72.19, 72.32, 73.15, 74.4333, 74.96, 75.751, 75.99, 75.14, 74.71, 72.89, 72.34, 72.7, 73.19, 72.89, 72.44, 72.96, 72.82, 73.14, 73.08, 73.4, 72.99, 71.17, 71.38, 72.54, 72.81, 72.41, 72.26, 72.18, 71.65, 71.97, 72.03, 71.84, 71.56, 71.47, 71.1, 71.28, 71.625, 70.93, 71.29, 70.86, 71.19, 70.77, 71.23, 71.6, 71.62, 71.03, 71.32, 71.93, 71.79, 71.43, 71.26, 71.55, 71.56, 71.695, 71.73, 71.485, 71.62, 71.62, 71.7, 72.21, 72.55, 72.97, 73.02, 72.05, 71.91, 73.33, 72.13, 72.46, 72.51, 72.59, 72.495, 72.25, 72.29, 72.46, 73.11, 73.34, 73.44, 73.77, 73.94, 74.135, 74.12, 75.03, 75.38, 75.88, 75.92, 75.53, 75.7, 76.35, 76.87, 76.3, 76.36, 75.87, 75.7, 75.54, 75.76, 75.81, 77.16, 77.0, 77.23, 77.08, 76.68, 76.35, 76.08, 75.45, 75.36, 74.6, 73.85, 74.26, 74.82, 73.94, 75.67, 71.58, 71.635, 71.25, 71.65, 71.76, 71.515, 71.8, 71.795, 71.94, 71.055, 70.75, 71.81, 72.01, 73.31, 73.5, 74.2, 73.65, 73.04, 73.18, 73.39, 74.37, 74.07, 73.67, 73.99, 73.705, 73.735, 73.465, 73.11, 73.45, 73.625, 73.39, 73.19, 73.2, 73.56, 73.77, 73.44, 73.69, 73.91, 73.685, 73.71, 73.789, 73.93, 73.9, 73.93, 73.64, 73.9, 74.26, 73.38, 73.16, 73.71, 73.78, 73.595, 73.73, 73.45, 72.98, 72.64, 73.34, 74.46, 73.49, 73.17, 71.965, 68.99, 68.625, 67.74, 67.7, 67.73, 66.59, 66.58, 66.565, 65.5, 65.93, 65.68, 64.72, 64.58, 64.53, 64.67, 68.24, 68.43, 68.26, 67.565, 67.95, 67.06, 67.85, 67.0948, 65.875, 65.84, 65.32, 65.4, 66.43, 67.34, 66.86, 66.26, 65.54, 65.45, 65.89, 65.61, 66.01, 66.27, 66.38, 66.06, 66.39, 66.06, 66.23, 66.23, 65.92, 66.89, 66.76, 66.29, 65.7, 65.92, 65.03, 64.63, 64.72, 65.08, 65.26, 66.54, 66.43, 66.53, 65.96, 66.33, 66.2108, 65.86, 65.975, 66.72, 66.6, 66.59, 67.44, 67.2, 70.34, 71.59, 70.54, 69.57, 70.55, 71.18, 70.9825, 71.36, 71.18, 70.57, 71.26, 71.32, 71.0, 70.93, 70.905, 71.71, 71.93, 72.09, 72.32, 72.16, 72.44, 72.2, 72.61, 73.05, 73.27, 72.97, 72.8, 73.62, 74.12, 73.29, 73.6, 73.44, 73.14, 73.26, 73.52, 72.98, 72.49, 72.195, 71.94, 72.09, 71.84, 71.85, 72.76, 70.89, 72.995, 73.76, 75.28, 74.89, 74.265, 74.34, 74.51, 74.8, 74.48, 74.65, 74.37, 74.545, 74.5, 74.48, 74.58, 74.56, 74.33, 74.69, 74.98, 74.895, 74.85, 74.785, 74.41, 74.57, 74.78, 74.05, 74.88, 75.09, 74.425, 73.23, 73.3, 73.98, 74.24, 74.61, 73.88, 74.1, 74.56, 74.5, 74.72, 74.68, 74.47, 73.94, 74.48, 74.23, 73.3899, 73.27, 73.27, 72.91, 73.19, 71.845, 71.14, 69.985, 70.56, 72.65, 72.23, 71.8, 71.63, 71.41, 71.99, 71.72, 72.1, 71.68, 72.1, 72.35, 72.01, 71.62, 71.83, 71.6, 71.4, 71.18, 70.41, 70.58, 70.505, 70.64, 70.865, 70.1, 70.03, 70.43, 70.13, 70.84, 72.05, 71.66, 72.64, 72.0, 72.2, 71.75, 71.82, 71.485, 71.885, 71.84, 72.36, 70.94, 71.16, 69.29, 68.95, 69.42, 69.06, 68.98, 70.32, 71.65, 71.32, 71.27, 70.79, 70.82, 71.245, 70.93, 71.07, 70.63, 71.41, 71.14, 71.17, 71.205, 70.66, 70.71, 70.52, 69.9, 69.94, 69.79, 69.56, 70.43, 70.45, 69.88, 68.7, 68.585, 68.23, 68.37, 68.24, 68.0, 67.53, 67.55, 68.15, 67.59, 67.07, 67.31, 65.63, 66.28, 67.43, 66.33, 67.17, 67.6, 67.45, 67.21, 67.05, 65.81, 65.7, 64.43, 65.81, 65.91, 66.08, 66.19, 65.98, 66.02, 65.36, 66.15, 67.51, 65.02, 64.3, 64.33, 63.94, 64.61, 63.66, 62.45, 64.3, 63.35, 63.51, 62.9, 63.39, 62.81, 62.54, 63.0, 64.16, 65.0, 64.71, 66.61, 67.51, 67.64, 66.9, 66.99, 67.16, 66.61, 65.99, 65.59, 67.33, 68.31, 66.75, 65.85, 65.2, 66.1, 66.06, 66.2, 66.35, 66.33, 65.6, 65.69, 65.83, 65.68, 67.81, 66.44, 66.83, 66.895, 66.1, 66.37, 66.06, 65.33, 65.59, 64.565, 65.25, 66.45, 66.4, 65.76, 65.85, 67.63, 67.49, 68.02, 67.4733, 66.35, 69.23, 68.82, 68.94, 68.71, 68.94, 68.6399, 66.83, 67.08, 67.26, 67.11, 66.575, 65.72, 65.97, 66.24, 66.34, 65.88, 65.33, 64.8, 65.33, 64.25, 63.7, 63.48, 63.07, 62.17, 63.16, 62.255, 62.08, 62.15, 63.12, 62.72, 63.3, 63.06, 62.43, 62.1, 61.93, 61.92, 62.4, 63.44, 61.46, 62.93, 62.2, 61.28, 62.82, 63.33, 63.09, 62.23, 60.37, 61.95, 64.98, 66.0, 66.74, 67.7, 67.68, 67.8, 67.65, 68.03, 68.28, 68.75, 68.5, 68.65, 69.11, 68.45, 68.45, 68.01, 68.02, 68.92, 68.39, 67.29, 67.01, 67.07, 67.0, 66.93, 67.81, 67.5, 67.61, 66.93, 67.265, 67.45, 66.63, 66.18, 66.47, 67.33, 65.95, 66.15, 66.19, 65.39, 65.4, 66.5, 66.48, 66.5, 67.02, 67.24, 66.89, 67.15, 66.41, 65.97, 65.345, 66.29, 66.55, 67.06, 66.01, 65.36, 65.48, 65.94, 66.55, 66.71, 66.92, 66.82, 67.33, 67.55, 67.15, 68.5, 68.82, 68.9, 69.14, 68.62, 68.62, 68.62, 67.61, 67.89, 67.83, 68.19, 67.88, 67.8, 67.37, 67.86, 67.66, 67.26, 68.49, 68.77, 68.97, 69.17, 69.22, 69.27, 69.02, 68.44, 69.29, 70.09, 69.88, 69.63, 69.32, 69.61, 69.94, 69.89, 69.7, 69.97, 69.32, 69.1, 69.33, 69.29, 68.73, 68.74, 68.9, 69.32, 70.18, 69.56, 68.31, 68.33, 68.42, 69.01, 68.46, 68.52, 67.9, 68.42, 69.59, 69.255, 71.47, 70.69, 70.69, 71.01, 70.825, 71.12, 71.0, 71.03, 70.7, 70.25, 70.22, 70.56, 70.18, 70.095, 69.83, 69.46, 69.44, 69.05, 69.3, 69.79, 69.19, 69.17, 69.0, 67.54, 69.04, 65.14, 65.67, 66.28, 67.15, 69.36, 68.37, 68.94, 69.16, 68.33, 68.46, 68.75, 68.27, 68.59, 69.26, 68.32, 67.77, 68.55, 69.05, 69.17, 70.03, 70.49, 70.87, 70.47, 71.01, 70.26, 70.01, 69.48, 68.66, 67.51, 67.93, 68.58, 69.27, 68.88, 69.32, 69.44, 69.36, 69.44, 69.05, 69.85, 69.4099, 69.59, 68.68, 68.33, 67.96, 68.34, 68.04, 68.28, 67.98, 67.93, 67.57, 68.3, 68.71, 68.48, 68.54, 68.04, 68.12, 68.14, 68.04, 67.4, 66.88, 66.4, 65.84, 65.78, 66.1, 65.35, 65.05, 65.33, 65.42, 64.52, 63.72, 63.105, 63.55, 64.41, 64.185, 65.28, 65.22, 65.82, 64.89, 65.26, 65.45, 64.78, 64.89, 65.23, 65.62, 65.69, 65.48, 66.24, 65.41, 65.69, 65.3, 65.3, 65.245, 65.15, 64.51, 64.475, 64.4, 63.99, 63.41, 63.64, 64.85, 64.31, 64.43, 64.419, 64.72, 64.5168, 64.82, 64.8, 64.88, 64.58, 64.82, 64.66, 64.26, 64.62, 64.14, 64.21, 64.37, 64.4, 65.05, 65.18, 64.45, 64.79, 63.77, 63.78, 63.96, 63.41, 66.11, 66.38, 67.315, 67.59, 68.1, 68.17, 68.6, 68.77, 68.92, 68.61, 69.45, 69.68, 69.79, 69.54, 69.435, 69.645, 69.51, 68.86, 68.82, 68.46, 68.47, 68.16, 68.38, 67.91, 67.96, 67.96, 67.82, 68.39, 69.42, 68.73, 67.89, 68.08, 67.85, 67.83, 68.605, 68.69, 68.43, 68.01, 67.94, 67.72, 67.25, 68.21, 68.42, 66.92, 66.12, 66.7, 66.72, 66.61, 66.8, 66.57, 66.72, 66.93, 66.42, 66.95, 67.255, 67.19, 67.66, 67.11, 67.23, 66.72, 66.895, 67.11, 67.01, 67.32, 67.05, 67.86, 66.24, 66.41, 65.88, 66.37, 66.85, 66.92, 67.19, 66.38, 65.86, 65.31, 65.94, 65.73, 65.62, 64.9, 64.23, 64.29, 64.3, 64.21, 64.87, 64.11, 64.05, 64.26, 63.78, 63.31, 63.4, 63.58, 63.39, 64.33, 64.4, 63.83, 63.24, 63.22, 63.29, 63.06, 63.12, 63.02, 62.63, 62.99, 62.22, 62.83, 62.12, 61.95, 62.08, 61.95, 61.56, 61.85, 61.53, 62.39, 62.68, 62.31, 61.9, 62.39, 61.52, 60.84, 60.61, 60.42, 60.36, 60.19, 61.21, 61.48, 61.6, 62.88, 62.1, 62.39, 63.44, 64.02, 64.74, 64.7, 64.99, 64.77, 64.83, 64.66, 65.09, 65.01, 63.53, 64.2, 64.14, 64.19, 64.29, 65.2, 65.42, 65.38, 64.84, 64.71, 64.62, 64.77, 64.25, 64.63, 63.58, 64.03, 63.97, 63.98, 64.94, 64.69, 65.69, 65.66, 64.53, 65.03, 65.38, 65.29, 65.79, 65.84, 66.02, 65.97, 66.26, 65.99, 65.26, 65.3, 65.18, 65.79, 65.6, 65.08, 64.71, 64.61, 64.84, 64.81, 66.01, 65.36, 65.73, 65.4, 64.73, 65.23, 65.96, 64.75, 63.62, 63.05, 63.49]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2b224fb7-232e-4a2f-bba1-e169db3b59c3');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"74a77bae-c0b7-482c-897c-2fcc43ad00f3\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"74a77bae-c0b7-482c-897c-2fcc43ad00f3\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '74a77bae-c0b7-482c-897c-2fcc43ad00f3',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('74a77bae-c0b7-482c-897c-2fcc43ad00f3');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp4WLtN5Ga-B"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO6ZOjONGa-B",
        "outputId": "1cf553af-94b7-4aa2-9e90-eaaaf8f90e7d"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .2, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"CL\", step_sizes=4, th= th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6860 - accuracy: 0.5738 - val_loss: 0.6342 - val_accuracy: 0.8327\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6841 - accuracy: 0.5745 - val_loss: 0.6326 - val_accuracy: 0.8327\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6845 - accuracy: 0.5725 - val_loss: 0.6040 - val_accuracy: 0.8327\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6775 - accuracy: 0.5765 - val_loss: 0.5790 - val_accuracy: 0.8224\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6670 - accuracy: 0.5987 - val_loss: 0.6031 - val_accuracy: 0.8367\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6894 - accuracy: 0.5570 - val_loss: 0.5776 - val_accuracy: 0.8265\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.6771 - accuracy: 0.5886 - val_loss: 0.5975 - val_accuracy: 0.8429\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6450 - accuracy: 0.6242 - val_loss: 0.6406 - val_accuracy: 0.7449\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.6231 - accuracy: 0.6490 - val_loss: 0.5581 - val_accuracy: 0.8224\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6392 - accuracy: 0.6477 - val_loss: 0.5150 - val_accuracy: 0.8469\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.791577\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.80437\n",
            "[2]\tvalidation_0-auc:0.801814\n",
            "[3]\tvalidation_0-auc:0.810273\n",
            "[4]\tvalidation_0-auc:0.807493\n",
            "[5]\tvalidation_0-auc:0.810766\n",
            "[6]\tvalidation_0-auc:0.813247\n",
            "[7]\tvalidation_0-auc:0.81782\n",
            "[8]\tvalidation_0-auc:0.817372\n",
            "[9]\tvalidation_0-auc:0.816221\n",
            "[10]\tvalidation_0-auc:0.815399\n",
            "[11]\tvalidation_0-auc:0.816057\n",
            "[12]\tvalidation_0-auc:0.816162\n",
            "[13]\tvalidation_0-auc:0.818194\n",
            "[14]\tvalidation_0-auc:0.819076\n",
            "[15]\tvalidation_0-auc:0.820511\n",
            "[16]\tvalidation_0-auc:0.822065\n",
            "[17]\tvalidation_0-auc:0.822125\n",
            "[18]\tvalidation_0-auc:0.822065\n",
            "[19]\tvalidation_0-auc:0.82063\n",
            "[20]\tvalidation_0-auc:0.821034\n",
            "[21]\tvalidation_0-auc:0.821721\n",
            "[22]\tvalidation_0-auc:0.821063\n",
            "[23]\tvalidation_0-auc:0.821811\n",
            "[24]\tvalidation_0-auc:0.821228\n",
            "[25]\tvalidation_0-auc:0.820511\n",
            "[26]\tvalidation_0-auc:0.820989\n",
            "[27]\tvalidation_0-auc:0.820002\n",
            "[28]\tvalidation_0-auc:0.820271\n",
            "[29]\tvalidation_0-auc:0.821347\n",
            "[30]\tvalidation_0-auc:0.821063\n",
            "[31]\tvalidation_0-auc:0.820017\n",
            "[32]\tvalidation_0-auc:0.819539\n",
            "[33]\tvalidation_0-auc:0.820182\n",
            "[34]\tvalidation_0-auc:0.821721\n",
            "[35]\tvalidation_0-auc:0.820256\n",
            "[36]\tvalidation_0-auc:0.820451\n",
            "[37]\tvalidation_0-auc:0.822095\n",
            "[38]\tvalidation_0-auc:0.821258\n",
            "[39]\tvalidation_0-auc:0.818583\n",
            "[40]\tvalidation_0-auc:0.817686\n",
            "[41]\tvalidation_0-auc:0.818747\n",
            "[42]\tvalidation_0-auc:0.818956\n",
            "[43]\tvalidation_0-auc:0.818478\n",
            "[44]\tvalidation_0-auc:0.818523\n",
            "[45]\tvalidation_0-auc:0.818074\n",
            "[46]\tvalidation_0-auc:0.816984\n",
            "[47]\tvalidation_0-auc:0.816774\n",
            "[48]\tvalidation_0-auc:0.818119\n",
            "[49]\tvalidation_0-auc:0.818777\n",
            "[50]\tvalidation_0-auc:0.81788\n",
            "[51]\tvalidation_0-auc:0.817432\n",
            "[52]\tvalidation_0-auc:0.816984\n",
            "[53]\tvalidation_0-auc:0.817163\n",
            "[54]\tvalidation_0-auc:0.81794\n",
            "[55]\tvalidation_0-auc:0.817462\n",
            "[56]\tvalidation_0-auc:0.81785\n",
            "[57]\tvalidation_0-auc:0.818926\n",
            "[58]\tvalidation_0-auc:0.81782\n",
            "[59]\tvalidation_0-auc:0.81803\n",
            "[60]\tvalidation_0-auc:0.817133\n",
            "[61]\tvalidation_0-auc:0.816057\n",
            "[62]\tvalidation_0-auc:0.816057\n",
            "[63]\tvalidation_0-auc:0.814562\n",
            "[64]\tvalidation_0-auc:0.814084\n",
            "[65]\tvalidation_0-auc:0.813965\n",
            "[66]\tvalidation_0-auc:0.812874\n",
            "[67]\tvalidation_0-auc:0.811917\n",
            "Stopping. Best iteration:\n",
            "[17]\tvalidation_0-auc:0.822125\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6859 - accuracy: 0.5566 - val_loss: 0.6339 - val_accuracy: 0.8425\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6848 - accuracy: 0.5546 - val_loss: 0.6403 - val_accuracy: 0.7987\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6748 - accuracy: 0.5745 - val_loss: 0.5496 - val_accuracy: 0.7637\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6547 - accuracy: 0.6170 - val_loss: 0.5580 - val_accuracy: 0.7681\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6552 - accuracy: 0.6156 - val_loss: 0.5866 - val_accuracy: 0.7637\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6827 - accuracy: 0.5539 - val_loss: 0.6873 - val_accuracy: 0.8162\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6311 - accuracy: 0.6493 - val_loss: 0.7249 - val_accuracy: 0.8096\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6145 - accuracy: 0.6692 - val_loss: 0.7961 - val_accuracy: 0.7724\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6070 - accuracy: 0.6712 - val_loss: 0.7886 - val_accuracy: 0.7899\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6038 - accuracy: 0.6685 - val_loss: 0.7805 - val_accuracy: 0.7812\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.780519\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.809019\n",
            "[2]\tvalidation_0-auc:0.817785\n",
            "[3]\tvalidation_0-auc:0.83768\n",
            "[4]\tvalidation_0-auc:0.844895\n",
            "[5]\tvalidation_0-auc:0.850216\n",
            "[6]\tvalidation_0-auc:0.853662\n",
            "[7]\tvalidation_0-auc:0.852327\n",
            "[8]\tvalidation_0-auc:0.870112\n",
            "[9]\tvalidation_0-auc:0.864015\n",
            "[10]\tvalidation_0-auc:0.863167\n",
            "[11]\tvalidation_0-auc:0.856926\n",
            "[12]\tvalidation_0-auc:0.857973\n",
            "[13]\tvalidation_0-auc:0.85561\n",
            "[14]\tvalidation_0-auc:0.858947\n",
            "[15]\tvalidation_0-auc:0.853734\n",
            "[16]\tvalidation_0-auc:0.848034\n",
            "[17]\tvalidation_0-auc:0.844318\n",
            "[18]\tvalidation_0-auc:0.846609\n",
            "[19]\tvalidation_0-auc:0.842731\n",
            "[20]\tvalidation_0-auc:0.843074\n",
            "[21]\tvalidation_0-auc:0.840566\n",
            "[22]\tvalidation_0-auc:0.836851\n",
            "[23]\tvalidation_0-auc:0.834343\n",
            "[24]\tvalidation_0-auc:0.834235\n",
            "[25]\tvalidation_0-auc:0.835823\n",
            "[26]\tvalidation_0-auc:0.83483\n",
            "[27]\tvalidation_0-auc:0.831097\n",
            "[28]\tvalidation_0-auc:0.82987\n",
            "[29]\tvalidation_0-auc:0.829473\n",
            "[30]\tvalidation_0-auc:0.827417\n",
            "[31]\tvalidation_0-auc:0.828716\n",
            "[32]\tvalidation_0-auc:0.831782\n",
            "[33]\tvalidation_0-auc:0.833622\n",
            "[34]\tvalidation_0-auc:0.831782\n",
            "[35]\tvalidation_0-auc:0.831205\n",
            "[36]\tvalidation_0-auc:0.830556\n",
            "[37]\tvalidation_0-auc:0.826046\n",
            "[38]\tvalidation_0-auc:0.824459\n",
            "[39]\tvalidation_0-auc:0.823088\n",
            "[40]\tvalidation_0-auc:0.822727\n",
            "[41]\tvalidation_0-auc:0.819156\n",
            "[42]\tvalidation_0-auc:0.818831\n",
            "[43]\tvalidation_0-auc:0.816234\n",
            "[44]\tvalidation_0-auc:0.819841\n",
            "[45]\tvalidation_0-auc:0.817605\n",
            "[46]\tvalidation_0-auc:0.814646\n",
            "[47]\tvalidation_0-auc:0.813907\n",
            "[48]\tvalidation_0-auc:0.810949\n",
            "[49]\tvalidation_0-auc:0.813889\n",
            "[50]\tvalidation_0-auc:0.816378\n",
            "[51]\tvalidation_0-auc:0.815729\n",
            "[52]\tvalidation_0-auc:0.815584\n",
            "[53]\tvalidation_0-auc:0.816414\n",
            "[54]\tvalidation_0-auc:0.821032\n",
            "[55]\tvalidation_0-auc:0.822565\n",
            "[56]\tvalidation_0-auc:0.821573\n",
            "[57]\tvalidation_0-auc:0.822511\n",
            "[58]\tvalidation_0-auc:0.817893\n",
            "Stopping. Best iteration:\n",
            "[8]\tvalidation_0-auc:0.870112\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.8367346938775511 |  0.5121951219512195 | 0.5121951219512195 | 0.5121951219512195 |\n",
            "|     GRU 0.1      | 0.8469387755102041 |  0.5555555555555556 | 0.4268292682926829 | 0.4827586206896552 |\n",
            "|   XGBoost 0.1    |        0.8         | 0.42857142857142855 | 0.5853658536585366 | 0.4948453608247423 |\n",
            "|    Logreg 0.1    | 0.810204081632653  |  0.4485981308411215 | 0.5853658536585366 | 0.507936507936508  |\n",
            "|     SVM 0.1      | 0.8244897959183674 | 0.47619047619047616 | 0.4878048780487805 | 0.4819277108433735 |\n",
            "|  LSTM beta 0.1   | 0.7636761487964989 |  0.3448275862068966 | 0.5555555555555556 | 0.4255319148936171 |\n",
            "|   GRU beta 0.1   | 0.7811816192560175 |  0.4014084507042254 | 0.7916666666666666 | 0.5327102803738317 |\n",
            "| XGBoost beta 0.1 | 0.8533916849015317 |  0.5257731958762887 | 0.7083333333333334 | 0.6035502958579881 |\n",
            "| logreg beta 0.1  | 0.7964989059080962 | 0.41025641025641024 | 0.6666666666666666 | 0.5079365079365079 |\n",
            "|   svm beta 0.1   | 0.8096280087527352 | 0.43119266055045874 | 0.6527777777777778 | 0.5193370165745858 |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.4536 - accuracy: 0.8470 - val_loss: 0.2351 - val_accuracy: 0.9490\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4351 - accuracy: 0.8490 - val_loss: 0.2498 - val_accuracy: 0.9490\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4373 - accuracy: 0.8490 - val_loss: 0.2401 - val_accuracy: 0.9490\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4328 - accuracy: 0.8490 - val_loss: 0.2695 - val_accuracy: 0.9490\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4303 - accuracy: 0.8490 - val_loss: 0.2484 - val_accuracy: 0.9490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 2s 13ms/step - loss: 0.4538 - accuracy: 0.8463 - val_loss: 0.2953 - val_accuracy: 0.9490\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.4366 - accuracy: 0.8490 - val_loss: 0.2641 - val_accuracy: 0.9490\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.4269 - accuracy: 0.8490 - val_loss: 0.2257 - val_accuracy: 0.9429\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4301 - accuracy: 0.8490 - val_loss: 0.3445 - val_accuracy: 0.9367\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4214 - accuracy: 0.8490 - val_loss: 0.2469 - val_accuracy: 0.9388\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.631355\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.631398\n",
            "[2]\tvalidation_0-auc:0.629763\n",
            "[3]\tvalidation_0-auc:0.667613\n",
            "[4]\tvalidation_0-auc:0.668645\n",
            "[5]\tvalidation_0-auc:0.718495\n",
            "[6]\tvalidation_0-auc:0.719011\n",
            "[7]\tvalidation_0-auc:0.718925\n",
            "[8]\tvalidation_0-auc:0.718538\n",
            "[9]\tvalidation_0-auc:0.749806\n",
            "[10]\tvalidation_0-auc:0.770796\n",
            "[11]\tvalidation_0-auc:0.776043\n",
            "[12]\tvalidation_0-auc:0.784473\n",
            "[13]\tvalidation_0-auc:0.78757\n",
            "[14]\tvalidation_0-auc:0.777936\n",
            "[15]\tvalidation_0-auc:0.782925\n",
            "[16]\tvalidation_0-auc:0.790151\n",
            "[17]\tvalidation_0-auc:0.80228\n",
            "[18]\tvalidation_0-auc:0.799742\n",
            "[19]\tvalidation_0-auc:0.814624\n",
            "[20]\tvalidation_0-auc:0.810409\n",
            "[21]\tvalidation_0-auc:0.805462\n",
            "[22]\tvalidation_0-auc:0.802624\n",
            "[23]\tvalidation_0-auc:0.813806\n",
            "[24]\tvalidation_0-auc:0.811312\n",
            "[25]\tvalidation_0-auc:0.815398\n",
            "[26]\tvalidation_0-auc:0.811871\n",
            "[27]\tvalidation_0-auc:0.832473\n",
            "[28]\tvalidation_0-auc:0.830237\n",
            "[29]\tvalidation_0-auc:0.839656\n",
            "[30]\tvalidation_0-auc:0.831656\n",
            "[31]\tvalidation_0-auc:0.843914\n",
            "[32]\tvalidation_0-auc:0.846022\n",
            "[33]\tvalidation_0-auc:0.847054\n",
            "[34]\tvalidation_0-auc:0.84757\n",
            "[35]\tvalidation_0-auc:0.847312\n",
            "[36]\tvalidation_0-auc:0.856344\n",
            "[37]\tvalidation_0-auc:0.851699\n",
            "[38]\tvalidation_0-auc:0.851699\n",
            "[39]\tvalidation_0-auc:0.851183\n",
            "[40]\tvalidation_0-auc:0.852645\n",
            "[41]\tvalidation_0-auc:0.845591\n",
            "[42]\tvalidation_0-auc:0.844989\n",
            "[43]\tvalidation_0-auc:0.846968\n",
            "[44]\tvalidation_0-auc:0.843355\n",
            "[45]\tvalidation_0-auc:0.843613\n",
            "[46]\tvalidation_0-auc:0.842839\n",
            "[47]\tvalidation_0-auc:0.842323\n",
            "[48]\tvalidation_0-auc:0.844645\n",
            "[49]\tvalidation_0-auc:0.837763\n",
            "[50]\tvalidation_0-auc:0.836301\n",
            "[51]\tvalidation_0-auc:0.837247\n",
            "[52]\tvalidation_0-auc:0.832774\n",
            "[53]\tvalidation_0-auc:0.832946\n",
            "[54]\tvalidation_0-auc:0.833376\n",
            "[55]\tvalidation_0-auc:0.833548\n",
            "[56]\tvalidation_0-auc:0.827699\n",
            "[57]\tvalidation_0-auc:0.821075\n",
            "[58]\tvalidation_0-auc:0.821677\n",
            "[59]\tvalidation_0-auc:0.821763\n",
            "[60]\tvalidation_0-auc:0.82228\n",
            "[61]\tvalidation_0-auc:0.820731\n",
            "[62]\tvalidation_0-auc:0.823398\n",
            "[63]\tvalidation_0-auc:0.822882\n",
            "[64]\tvalidation_0-auc:0.818065\n",
            "[65]\tvalidation_0-auc:0.81428\n",
            "[66]\tvalidation_0-auc:0.813763\n",
            "[67]\tvalidation_0-auc:0.818495\n",
            "[68]\tvalidation_0-auc:0.817634\n",
            "[69]\tvalidation_0-auc:0.813763\n",
            "[70]\tvalidation_0-auc:0.815914\n",
            "[71]\tvalidation_0-auc:0.816688\n",
            "[72]\tvalidation_0-auc:0.820387\n",
            "[73]\tvalidation_0-auc:0.821505\n",
            "[74]\tvalidation_0-auc:0.822452\n",
            "[75]\tvalidation_0-auc:0.824516\n",
            "[76]\tvalidation_0-auc:0.819613\n",
            "[77]\tvalidation_0-auc:0.821505\n",
            "[78]\tvalidation_0-auc:0.819699\n",
            "[79]\tvalidation_0-auc:0.819785\n",
            "[80]\tvalidation_0-auc:0.817548\n",
            "[81]\tvalidation_0-auc:0.817634\n",
            "[82]\tvalidation_0-auc:0.814882\n",
            "[83]\tvalidation_0-auc:0.818409\n",
            "[84]\tvalidation_0-auc:0.817548\n",
            "[85]\tvalidation_0-auc:0.818495\n",
            "[86]\tvalidation_0-auc:0.816344\n",
            "Stopping. Best iteration:\n",
            "[36]\tvalidation_0-auc:0.856344\n",
            "\n",
            "end training. \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.4568 - accuracy: 0.8428 - val_loss: 0.2789 - val_accuracy: 0.9453\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4439 - accuracy: 0.8456 - val_loss: 0.2507 - val_accuracy: 0.9453\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4375 - accuracy: 0.8456 - val_loss: 0.2647 - val_accuracy: 0.9453\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4398 - accuracy: 0.8456 - val_loss: 0.2634 - val_accuracy: 0.9453\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4381 - accuracy: 0.8456 - val_loss: 0.2546 - val_accuracy: 0.9453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 2s 13ms/step - loss: 0.4662 - accuracy: 0.8442 - val_loss: 0.2623 - val_accuracy: 0.9453\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4405 - accuracy: 0.8456 - val_loss: 0.2478 - val_accuracy: 0.9453\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4309 - accuracy: 0.8456 - val_loss: 0.2983 - val_accuracy: 0.8972\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4163 - accuracy: 0.8463 - val_loss: 0.3448 - val_accuracy: 0.8796\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4131 - accuracy: 0.8497 - val_loss: 0.3515 - val_accuracy: 0.8796\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.474259\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.474259\n",
            "[2]\tvalidation_0-auc:0.474074\n",
            "[3]\tvalidation_0-auc:0.474074\n",
            "[4]\tvalidation_0-auc:0.473704\n",
            "[5]\tvalidation_0-auc:0.545463\n",
            "[6]\tvalidation_0-auc:0.545556\n",
            "[7]\tvalidation_0-auc:0.545556\n",
            "[8]\tvalidation_0-auc:0.618704\n",
            "[9]\tvalidation_0-auc:0.618796\n",
            "[10]\tvalidation_0-auc:0.667917\n",
            "[11]\tvalidation_0-auc:0.724583\n",
            "[12]\tvalidation_0-auc:0.71662\n",
            "[13]\tvalidation_0-auc:0.711991\n",
            "[14]\tvalidation_0-auc:0.711991\n",
            "[15]\tvalidation_0-auc:0.713009\n",
            "[16]\tvalidation_0-auc:0.711157\n",
            "[17]\tvalidation_0-auc:0.686713\n",
            "[18]\tvalidation_0-auc:0.712639\n",
            "[19]\tvalidation_0-auc:0.709028\n",
            "[20]\tvalidation_0-auc:0.701065\n",
            "[21]\tvalidation_0-auc:0.716389\n",
            "[22]\tvalidation_0-auc:0.701759\n",
            "[23]\tvalidation_0-auc:0.701574\n",
            "[24]\tvalidation_0-auc:0.718148\n",
            "[25]\tvalidation_0-auc:0.712593\n",
            "[26]\tvalidation_0-auc:0.706667\n",
            "[27]\tvalidation_0-auc:0.699537\n",
            "[28]\tvalidation_0-auc:0.70713\n",
            "[29]\tvalidation_0-auc:0.702454\n",
            "[30]\tvalidation_0-auc:0.695324\n",
            "[31]\tvalidation_0-auc:0.691898\n",
            "[32]\tvalidation_0-auc:0.70162\n",
            "[33]\tvalidation_0-auc:0.693519\n",
            "[34]\tvalidation_0-auc:0.696667\n",
            "[35]\tvalidation_0-auc:0.698333\n",
            "[36]\tvalidation_0-auc:0.694444\n",
            "[37]\tvalidation_0-auc:0.694537\n",
            "[38]\tvalidation_0-auc:0.697454\n",
            "[39]\tvalidation_0-auc:0.695185\n",
            "[40]\tvalidation_0-auc:0.685972\n",
            "[41]\tvalidation_0-auc:0.678102\n",
            "[42]\tvalidation_0-auc:0.683981\n",
            "[43]\tvalidation_0-auc:0.67875\n",
            "[44]\tvalidation_0-auc:0.672731\n",
            "[45]\tvalidation_0-auc:0.671528\n",
            "[46]\tvalidation_0-auc:0.67625\n",
            "[47]\tvalidation_0-auc:0.676991\n",
            "[48]\tvalidation_0-auc:0.682176\n",
            "[49]\tvalidation_0-auc:0.680417\n",
            "[50]\tvalidation_0-auc:0.682917\n",
            "[51]\tvalidation_0-auc:0.690787\n",
            "[52]\tvalidation_0-auc:0.692731\n",
            "[53]\tvalidation_0-auc:0.690833\n",
            "[54]\tvalidation_0-auc:0.690463\n",
            "[55]\tvalidation_0-auc:0.692315\n",
            "[56]\tvalidation_0-auc:0.69213\n",
            "[57]\tvalidation_0-auc:0.690926\n",
            "[58]\tvalidation_0-auc:0.691944\n",
            "[59]\tvalidation_0-auc:0.696111\n",
            "[60]\tvalidation_0-auc:0.693889\n",
            "[61]\tvalidation_0-auc:0.696204\n",
            "Stopping. Best iteration:\n",
            "[11]\tvalidation_0-auc:0.724583\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------+----------------------+\n",
            "|      Model       |      Accuracy      |      Precision      | Recall |       F1 score       |\n",
            "+------------------+--------------------+---------------------+--------+----------------------+\n",
            "|     LSTM 0.2     | 0.9489795918367347 |         0.0         |  0.0   |         0.0          |\n",
            "|     GRU 0.2      | 0.9387755102040817 | 0.14285714285714285 |  0.04  |        0.0625        |\n",
            "|   XGBoost 0.2    | 0.926530612244898  | 0.13333333333333333 |  0.08  |         0.1          |\n",
            "|    Logreg 0.2    | 0.9326530612244898 |         0.1         |  0.04  | 0.05714285714285714  |\n",
            "|     SVM 0.2      | 0.9489795918367347 |         0.0         |  0.0   |         0.0          |\n",
            "|  LSTM beta 0.2   | 0.9452954048140044 |         0.0         |  0.0   |         0.0          |\n",
            "|   GRU beta 0.2   | 0.8796498905908097 |       0.03125       |  0.04  | 0.03508771929824561  |\n",
            "| XGBoost beta 0.2 | 0.8687089715536105 | 0.02702702702702703 |  0.04  | 0.03225806451612903  |\n",
            "| logreg beta 0.2  | 0.862144420131291  |        0.025        |  0.04  | 0.030769230769230767 |\n",
            "|   svm beta 0.2   | 0.9452954048140044 |         0.0         |  0.0   |         0.0          |\n",
            "+------------------+--------------------+---------------------+--------+----------------------+\n",
            "Threshhold =  0.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6506 - accuracy: 0.6490 - val_loss: 0.4890 - val_accuracy: 0.9490\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6505 - accuracy: 0.6537 - val_loss: 0.4716 - val_accuracy: 0.9490\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6484 - accuracy: 0.6537 - val_loss: 0.4499 - val_accuracy: 0.9490\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6463 - accuracy: 0.6537 - val_loss: 0.4410 - val_accuracy: 0.9490\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6468 - accuracy: 0.6537 - val_loss: 0.4417 - val_accuracy: 0.9490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 2s 13ms/step - loss: 0.6550 - accuracy: 0.6503 - val_loss: 0.4339 - val_accuracy: 0.9490\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6462 - accuracy: 0.6537 - val_loss: 0.4675 - val_accuracy: 0.9388\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6401 - accuracy: 0.6537 - val_loss: 0.4431 - val_accuracy: 0.9347\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6204 - accuracy: 0.6698 - val_loss: 0.3973 - val_accuracy: 0.9265\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6124 - accuracy: 0.6846 - val_loss: 0.4005 - val_accuracy: 0.9143\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.791785\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.812774\n",
            "[2]\tvalidation_0-auc:0.837505\n",
            "[3]\tvalidation_0-auc:0.840903\n",
            "[4]\tvalidation_0-auc:0.843097\n",
            "[5]\tvalidation_0-auc:0.847398\n",
            "[6]\tvalidation_0-auc:0.851312\n",
            "[7]\tvalidation_0-auc:0.852774\n",
            "[8]\tvalidation_0-auc:0.854538\n",
            "[9]\tvalidation_0-auc:0.854237\n",
            "[10]\tvalidation_0-auc:0.854968\n",
            "[11]\tvalidation_0-auc:0.850237\n",
            "[12]\tvalidation_0-auc:0.851527\n",
            "[13]\tvalidation_0-auc:0.854065\n",
            "[14]\tvalidation_0-auc:0.85372\n",
            "[15]\tvalidation_0-auc:0.854065\n",
            "[16]\tvalidation_0-auc:0.851484\n",
            "[17]\tvalidation_0-auc:0.851054\n",
            "[18]\tvalidation_0-auc:0.860774\n",
            "[19]\tvalidation_0-auc:0.860387\n",
            "[20]\tvalidation_0-auc:0.860731\n",
            "[21]\tvalidation_0-auc:0.860473\n",
            "[22]\tvalidation_0-auc:0.862366\n",
            "[23]\tvalidation_0-auc:0.860559\n",
            "[24]\tvalidation_0-auc:0.857118\n",
            "[25]\tvalidation_0-auc:0.855914\n",
            "[26]\tvalidation_0-auc:0.857118\n",
            "[27]\tvalidation_0-auc:0.856516\n",
            "[28]\tvalidation_0-auc:0.858323\n",
            "[29]\tvalidation_0-auc:0.863828\n",
            "[30]\tvalidation_0-auc:0.863656\n",
            "[31]\tvalidation_0-auc:0.865548\n",
            "[32]\tvalidation_0-auc:0.867011\n",
            "[33]\tvalidation_0-auc:0.868129\n",
            "[34]\tvalidation_0-auc:0.873806\n",
            "[35]\tvalidation_0-auc:0.873634\n",
            "[36]\tvalidation_0-auc:0.874323\n",
            "[37]\tvalidation_0-auc:0.869936\n",
            "[38]\tvalidation_0-auc:0.862108\n",
            "[39]\tvalidation_0-auc:0.861247\n",
            "[40]\tvalidation_0-auc:0.865118\n",
            "[41]\tvalidation_0-auc:0.863312\n",
            "[42]\tvalidation_0-auc:0.860129\n",
            "[43]\tvalidation_0-auc:0.861935\n",
            "[44]\tvalidation_0-auc:0.861935\n",
            "[45]\tvalidation_0-auc:0.857548\n",
            "[46]\tvalidation_0-auc:0.856172\n",
            "[47]\tvalidation_0-auc:0.857118\n",
            "[48]\tvalidation_0-auc:0.859269\n",
            "[49]\tvalidation_0-auc:0.857634\n",
            "[50]\tvalidation_0-auc:0.856602\n",
            "[51]\tvalidation_0-auc:0.852559\n",
            "[52]\tvalidation_0-auc:0.85471\n",
            "[53]\tvalidation_0-auc:0.850581\n",
            "[54]\tvalidation_0-auc:0.850237\n",
            "[55]\tvalidation_0-auc:0.850495\n",
            "[56]\tvalidation_0-auc:0.851011\n",
            "[57]\tvalidation_0-auc:0.850753\n",
            "[58]\tvalidation_0-auc:0.856258\n",
            "[59]\tvalidation_0-auc:0.854538\n",
            "[60]\tvalidation_0-auc:0.853419\n",
            "[61]\tvalidation_0-auc:0.850839\n",
            "[62]\tvalidation_0-auc:0.850667\n",
            "[63]\tvalidation_0-auc:0.849806\n",
            "[64]\tvalidation_0-auc:0.847398\n",
            "[65]\tvalidation_0-auc:0.84671\n",
            "[66]\tvalidation_0-auc:0.847484\n",
            "[67]\tvalidation_0-auc:0.848344\n",
            "[68]\tvalidation_0-auc:0.845935\n",
            "[69]\tvalidation_0-auc:0.843441\n",
            "[70]\tvalidation_0-auc:0.842409\n",
            "[71]\tvalidation_0-auc:0.843613\n",
            "[72]\tvalidation_0-auc:0.848172\n",
            "[73]\tvalidation_0-auc:0.847914\n",
            "[74]\tvalidation_0-auc:0.84972\n",
            "[75]\tvalidation_0-auc:0.84929\n",
            "[76]\tvalidation_0-auc:0.846882\n",
            "[77]\tvalidation_0-auc:0.848172\n",
            "[78]\tvalidation_0-auc:0.846366\n",
            "[79]\tvalidation_0-auc:0.848344\n",
            "[80]\tvalidation_0-auc:0.848172\n",
            "[81]\tvalidation_0-auc:0.847054\n",
            "[82]\tvalidation_0-auc:0.844731\n",
            "[83]\tvalidation_0-auc:0.842409\n",
            "[84]\tvalidation_0-auc:0.849032\n",
            "[85]\tvalidation_0-auc:0.846366\n",
            "[86]\tvalidation_0-auc:0.84757\n",
            "Stopping. Best iteration:\n",
            "[36]\tvalidation_0-auc:0.874323\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6605 - accuracy: 0.6438 - val_loss: 0.5073 - val_accuracy: 0.9453\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6533 - accuracy: 0.6458 - val_loss: 0.5334 - val_accuracy: 0.9453\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6442 - accuracy: 0.6465 - val_loss: 0.4779 - val_accuracy: 0.8709\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6403 - accuracy: 0.6472 - val_loss: 0.4813 - val_accuracy: 0.8753\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6189 - accuracy: 0.6802 - val_loss: 0.6087 - val_accuracy: 0.8053\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6570 - accuracy: 0.6431 - val_loss: 0.5092 - val_accuracy: 0.8972\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6266 - accuracy: 0.6603 - val_loss: 0.5861 - val_accuracy: 0.8621\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5989 - accuracy: 0.7062 - val_loss: 0.5549 - val_accuracy: 0.8796\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5872 - accuracy: 0.7131 - val_loss: 0.6105 - val_accuracy: 0.8228\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5844 - accuracy: 0.7021 - val_loss: 0.5786 - val_accuracy: 0.8600\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.753611\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.704352\n",
            "[2]\tvalidation_0-auc:0.704537\n",
            "[3]\tvalidation_0-auc:0.7\n",
            "[4]\tvalidation_0-auc:0.703519\n",
            "[5]\tvalidation_0-auc:0.723565\n",
            "[6]\tvalidation_0-auc:0.708102\n",
            "[7]\tvalidation_0-auc:0.707083\n",
            "[8]\tvalidation_0-auc:0.705278\n",
            "[9]\tvalidation_0-auc:0.705463\n",
            "[10]\tvalidation_0-auc:0.693889\n",
            "[11]\tvalidation_0-auc:0.695278\n",
            "[12]\tvalidation_0-auc:0.695648\n",
            "[13]\tvalidation_0-auc:0.695926\n",
            "[14]\tvalidation_0-auc:0.693843\n",
            "[15]\tvalidation_0-auc:0.695046\n",
            "[16]\tvalidation_0-auc:0.694861\n",
            "[17]\tvalidation_0-auc:0.661343\n",
            "[18]\tvalidation_0-auc:0.624861\n",
            "[19]\tvalidation_0-auc:0.636806\n",
            "[20]\tvalidation_0-auc:0.624259\n",
            "[21]\tvalidation_0-auc:0.624537\n",
            "[22]\tvalidation_0-auc:0.607963\n",
            "[23]\tvalidation_0-auc:0.608241\n",
            "[24]\tvalidation_0-auc:0.593796\n",
            "[25]\tvalidation_0-auc:0.592454\n",
            "[26]\tvalidation_0-auc:0.577037\n",
            "[27]\tvalidation_0-auc:0.589444\n",
            "[28]\tvalidation_0-auc:0.588056\n",
            "[29]\tvalidation_0-auc:0.579444\n",
            "[30]\tvalidation_0-auc:0.578519\n",
            "[31]\tvalidation_0-auc:0.580463\n",
            "[32]\tvalidation_0-auc:0.58037\n",
            "[33]\tvalidation_0-auc:0.578056\n",
            "[34]\tvalidation_0-auc:0.576574\n",
            "[35]\tvalidation_0-auc:0.578611\n",
            "[36]\tvalidation_0-auc:0.568981\n",
            "[37]\tvalidation_0-auc:0.568426\n",
            "[38]\tvalidation_0-auc:0.572407\n",
            "[39]\tvalidation_0-auc:0.571389\n",
            "[40]\tvalidation_0-auc:0.568056\n",
            "[41]\tvalidation_0-auc:0.564722\n",
            "[42]\tvalidation_0-auc:0.565648\n",
            "[43]\tvalidation_0-auc:0.566389\n",
            "[44]\tvalidation_0-auc:0.563241\n",
            "[45]\tvalidation_0-auc:0.563981\n",
            "[46]\tvalidation_0-auc:0.563194\n",
            "[47]\tvalidation_0-auc:0.561435\n",
            "[48]\tvalidation_0-auc:0.565139\n",
            "[49]\tvalidation_0-auc:0.564213\n",
            "[50]\tvalidation_0-auc:0.565833\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.753611\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+--------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      | Recall |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+--------+---------------------+\n",
            "|     LSTM 0.15     | 0.9489795918367347 |         0.0         |  0.0   |         0.0         |\n",
            "|      GRU 0.15     | 0.9142857142857143 |  0.2571428571428571 |  0.36  |         0.3         |\n",
            "|    XGBoost 0.15   | 0.9040816326530612 |         0.25        |  0.44  | 0.31884057971014496 |\n",
            "|    Logreg 0.15    | 0.9102040816326531 |  0.2564102564102564 |  0.4   |        0.3125       |\n",
            "|      SVM 0.15     | 0.9224489795918367 |  0.3333333333333333 |  0.52  | 0.40625000000000006 |\n",
            "|   LSTM beta 0.15  | 0.8052516411378556 | 0.07894736842105263 |  0.24  | 0.11881188118811879 |\n",
            "|   GRU beta 0.15   | 0.8599562363238512 | 0.15789473684210525 |  0.36  | 0.21951219512195122 |\n",
            "| XGBoost beta 0.15 | 0.9168490153172867 | 0.25925925925925924 |  0.28  |  0.2692307692307692 |\n",
            "|  logreg beta 0.15 | 0.8358862144420132 |  0.1323529411764706 |  0.36  | 0.19354838709677422 |\n",
            "|   svm beta 0.15   | 0.862144420131291  | 0.16071428571428573 |  0.36  | 0.22222222222222224 |\n",
            "+-------------------+--------------------+---------------------+--------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "81QNjEISGa-C",
        "outputId": "8e7720ff-32ce-48ad-b12f-4954ab9d70cb"
      },
      "source": [
        "Result_cross.to_csv('CL_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.512195</td>\n",
              "      <td>0.836735</td>\n",
              "      <td>0.512195</td>\n",
              "      <td>0.512195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.846939</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.426829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.494845</td>\n",
              "      <td>0.585366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.448598</td>\n",
              "      <td>0.810204</td>\n",
              "      <td>0.507937</td>\n",
              "      <td>0.585366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.476190</td>\n",
              "      <td>0.824490</td>\n",
              "      <td>0.481928</td>\n",
              "      <td>0.487805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.344828</td>\n",
              "      <td>0.763676</td>\n",
              "      <td>0.425532</td>\n",
              "      <td>0.555556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.401408</td>\n",
              "      <td>0.781182</td>\n",
              "      <td>0.532710</td>\n",
              "      <td>0.791667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.525773</td>\n",
              "      <td>0.853392</td>\n",
              "      <td>0.603550</td>\n",
              "      <td>0.708333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.410256</td>\n",
              "      <td>0.796499</td>\n",
              "      <td>0.507937</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.431193</td>\n",
              "      <td>0.809628</td>\n",
              "      <td>0.519337</td>\n",
              "      <td>0.652778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.948980</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.938776</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.926531</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.080000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.932653</td>\n",
              "      <td>0.057143</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.948980</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.945295</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.879650</td>\n",
              "      <td>0.035088</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.868709</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.862144</td>\n",
              "      <td>0.030769</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.945295</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.948980</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>0.914286</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.360000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.904082</td>\n",
              "      <td>0.318841</td>\n",
              "      <td>0.440000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.256410</td>\n",
              "      <td>0.910204</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.922449</td>\n",
              "      <td>0.406250</td>\n",
              "      <td>0.520000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.078947</td>\n",
              "      <td>0.805252</td>\n",
              "      <td>0.118812</td>\n",
              "      <td>0.240000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.157895</td>\n",
              "      <td>0.859956</td>\n",
              "      <td>0.219512</td>\n",
              "      <td>0.360000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.259259</td>\n",
              "      <td>0.916849</td>\n",
              "      <td>0.269231</td>\n",
              "      <td>0.280000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.132353</td>\n",
              "      <td>0.835886</td>\n",
              "      <td>0.193548</td>\n",
              "      <td>0.360000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.160714</td>\n",
              "      <td>0.862144</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.360000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1   CL  0.512195  0.836735  0.512195  0.512195\n",
              "1            GRU 0.1   CL  0.555556  0.846939  0.482759  0.426829\n",
              "2        XGBoost 0.1   CL  0.428571  0.800000  0.494845  0.585366\n",
              "3         Logreg 0.1   CL  0.448598  0.810204  0.507937  0.585366\n",
              "4            SVM 0.1   CL  0.476190  0.824490  0.481928  0.487805\n",
              "5      LSTM beta 0.1   CL  0.344828  0.763676  0.425532  0.555556\n",
              "6       GRU beta 0.1   CL  0.401408  0.781182  0.532710  0.791667\n",
              "7   XGBoost beta 0.1   CL  0.525773  0.853392  0.603550  0.708333\n",
              "8    logreg beta 0.1   CL  0.410256  0.796499  0.507937  0.666667\n",
              "9       svm beta 0.1   CL  0.431193  0.809628  0.519337  0.652778\n",
              "0           LSTM 0.2   CL  0.000000  0.948980  0.000000  0.000000\n",
              "1            GRU 0.2   CL  0.142857  0.938776  0.062500  0.040000\n",
              "2        XGBoost 0.2   CL  0.133333  0.926531  0.100000  0.080000\n",
              "3         Logreg 0.2   CL  0.100000  0.932653  0.057143  0.040000\n",
              "4            SVM 0.2   CL  0.000000  0.948980  0.000000  0.000000\n",
              "5      LSTM beta 0.2   CL  0.000000  0.945295  0.000000  0.000000\n",
              "6       GRU beta 0.2   CL  0.031250  0.879650  0.035088  0.040000\n",
              "7   XGBoost beta 0.2   CL  0.027027  0.868709  0.032258  0.040000\n",
              "8    logreg beta 0.2   CL  0.025000  0.862144  0.030769  0.040000\n",
              "9       svm beta 0.2   CL  0.000000  0.945295  0.000000  0.000000\n",
              "0          LSTM 0.15   CL  0.000000  0.948980  0.000000  0.000000\n",
              "1           GRU 0.15   CL  0.257143  0.914286  0.300000  0.360000\n",
              "2       XGBoost 0.15   CL  0.250000  0.904082  0.318841  0.440000\n",
              "3        Logreg 0.15   CL  0.256410  0.910204  0.312500  0.400000\n",
              "4           SVM 0.15   CL  0.333333  0.922449  0.406250  0.520000\n",
              "5     LSTM beta 0.15   CL  0.078947  0.805252  0.118812  0.240000\n",
              "6      GRU beta 0.15   CL  0.157895  0.859956  0.219512  0.360000\n",
              "7  XGBoost beta 0.15   CL  0.259259  0.916849  0.269231  0.280000\n",
              "8   logreg beta 0.15   CL  0.132353  0.835886  0.193548  0.360000\n",
              "9      svm beta 0.15   CL  0.160714  0.862144  0.222222  0.360000"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz6pbErBGa-D"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV6gJ7eKGa-D"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3j3CQsqGa-D",
        "outputId": "5d2a1bba-7ce3-4e47-d1cd-173ebdf63892"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"CL\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6883 - accuracy: 0.5544 - val_loss: 0.6046 - val_accuracy: 0.8327\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6823 - accuracy: 0.5745 - val_loss: 0.6078 - val_accuracy: 0.8265\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6754 - accuracy: 0.5906 - val_loss: 0.5504 - val_accuracy: 0.8184\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6761 - accuracy: 0.5752 - val_loss: 0.5590 - val_accuracy: 0.8449\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6552 - accuracy: 0.6060 - val_loss: 0.6935 - val_accuracy: 0.5102\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6864 - accuracy: 0.5631 - val_loss: 0.6243 - val_accuracy: 0.8204\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6735 - accuracy: 0.5940 - val_loss: 0.6361 - val_accuracy: 0.8265\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6505 - accuracy: 0.6342 - val_loss: 0.5175 - val_accuracy: 0.8469\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6349 - accuracy: 0.6403 - val_loss: 0.5314 - val_accuracy: 0.8388\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6344 - accuracy: 0.6443 - val_loss: 0.6193 - val_accuracy: 0.7408\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.791577\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.80437\n",
            "[2]\tvalidation_0-auc:0.801814\n",
            "[3]\tvalidation_0-auc:0.810273\n",
            "[4]\tvalidation_0-auc:0.807493\n",
            "[5]\tvalidation_0-auc:0.810766\n",
            "[6]\tvalidation_0-auc:0.813247\n",
            "[7]\tvalidation_0-auc:0.81782\n",
            "[8]\tvalidation_0-auc:0.817372\n",
            "[9]\tvalidation_0-auc:0.816221\n",
            "[10]\tvalidation_0-auc:0.815399\n",
            "[11]\tvalidation_0-auc:0.816057\n",
            "[12]\tvalidation_0-auc:0.816162\n",
            "[13]\tvalidation_0-auc:0.818194\n",
            "[14]\tvalidation_0-auc:0.819076\n",
            "[15]\tvalidation_0-auc:0.820511\n",
            "[16]\tvalidation_0-auc:0.822065\n",
            "[17]\tvalidation_0-auc:0.822125\n",
            "[18]\tvalidation_0-auc:0.822065\n",
            "[19]\tvalidation_0-auc:0.82063\n",
            "[20]\tvalidation_0-auc:0.821034\n",
            "[21]\tvalidation_0-auc:0.821721\n",
            "[22]\tvalidation_0-auc:0.821063\n",
            "[23]\tvalidation_0-auc:0.821811\n",
            "[24]\tvalidation_0-auc:0.821228\n",
            "[25]\tvalidation_0-auc:0.820511\n",
            "[26]\tvalidation_0-auc:0.820989\n",
            "[27]\tvalidation_0-auc:0.820002\n",
            "[28]\tvalidation_0-auc:0.820271\n",
            "[29]\tvalidation_0-auc:0.821347\n",
            "[30]\tvalidation_0-auc:0.821063\n",
            "[31]\tvalidation_0-auc:0.820017\n",
            "[32]\tvalidation_0-auc:0.819539\n",
            "[33]\tvalidation_0-auc:0.820182\n",
            "[34]\tvalidation_0-auc:0.821721\n",
            "[35]\tvalidation_0-auc:0.820256\n",
            "[36]\tvalidation_0-auc:0.820451\n",
            "[37]\tvalidation_0-auc:0.822095\n",
            "[38]\tvalidation_0-auc:0.821258\n",
            "[39]\tvalidation_0-auc:0.818583\n",
            "[40]\tvalidation_0-auc:0.817686\n",
            "[41]\tvalidation_0-auc:0.818747\n",
            "[42]\tvalidation_0-auc:0.818956\n",
            "[43]\tvalidation_0-auc:0.818478\n",
            "[44]\tvalidation_0-auc:0.818523\n",
            "[45]\tvalidation_0-auc:0.818074\n",
            "[46]\tvalidation_0-auc:0.816984\n",
            "[47]\tvalidation_0-auc:0.816774\n",
            "[48]\tvalidation_0-auc:0.818119\n",
            "[49]\tvalidation_0-auc:0.818777\n",
            "[50]\tvalidation_0-auc:0.81788\n",
            "[51]\tvalidation_0-auc:0.817432\n",
            "[52]\tvalidation_0-auc:0.816984\n",
            "[53]\tvalidation_0-auc:0.817163\n",
            "[54]\tvalidation_0-auc:0.81794\n",
            "[55]\tvalidation_0-auc:0.817462\n",
            "[56]\tvalidation_0-auc:0.81785\n",
            "[57]\tvalidation_0-auc:0.818926\n",
            "[58]\tvalidation_0-auc:0.81782\n",
            "[59]\tvalidation_0-auc:0.81803\n",
            "[60]\tvalidation_0-auc:0.817133\n",
            "[61]\tvalidation_0-auc:0.816057\n",
            "[62]\tvalidation_0-auc:0.816057\n",
            "[63]\tvalidation_0-auc:0.814562\n",
            "[64]\tvalidation_0-auc:0.814084\n",
            "[65]\tvalidation_0-auc:0.813965\n",
            "[66]\tvalidation_0-auc:0.812874\n",
            "[67]\tvalidation_0-auc:0.811917\n",
            "Stopping. Best iteration:\n",
            "[17]\tvalidation_0-auc:0.822125\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 20ms/step - loss: 0.6886 - accuracy: 0.5601 - val_loss: 0.6505 - val_accuracy: 0.8425\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6887 - accuracy: 0.5621 - val_loss: 0.6278 - val_accuracy: 0.8425\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6819 - accuracy: 0.5655 - val_loss: 0.6181 - val_accuracy: 0.7768\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6696 - accuracy: 0.5923 - val_loss: 0.6169 - val_accuracy: 0.7505\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6476 - accuracy: 0.6273 - val_loss: 0.5740 - val_accuracy: 0.7943\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6851 - accuracy: 0.5635 - val_loss: 0.6208 - val_accuracy: 0.7768\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6475 - accuracy: 0.6294 - val_loss: 0.6991 - val_accuracy: 0.8096\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6145 - accuracy: 0.6767 - val_loss: 0.7123 - val_accuracy: 0.8009\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6175 - accuracy: 0.6637 - val_loss: 0.6824 - val_accuracy: 0.8206\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6038 - accuracy: 0.6898 - val_loss: 0.6688 - val_accuracy: 0.8184\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.780519\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.809019\n",
            "[2]\tvalidation_0-auc:0.817785\n",
            "[3]\tvalidation_0-auc:0.83768\n",
            "[4]\tvalidation_0-auc:0.844895\n",
            "[5]\tvalidation_0-auc:0.850216\n",
            "[6]\tvalidation_0-auc:0.853662\n",
            "[7]\tvalidation_0-auc:0.852327\n",
            "[8]\tvalidation_0-auc:0.870112\n",
            "[9]\tvalidation_0-auc:0.864015\n",
            "[10]\tvalidation_0-auc:0.863167\n",
            "[11]\tvalidation_0-auc:0.856926\n",
            "[12]\tvalidation_0-auc:0.857973\n",
            "[13]\tvalidation_0-auc:0.85561\n",
            "[14]\tvalidation_0-auc:0.858947\n",
            "[15]\tvalidation_0-auc:0.853734\n",
            "[16]\tvalidation_0-auc:0.848034\n",
            "[17]\tvalidation_0-auc:0.844318\n",
            "[18]\tvalidation_0-auc:0.846609\n",
            "[19]\tvalidation_0-auc:0.842731\n",
            "[20]\tvalidation_0-auc:0.843074\n",
            "[21]\tvalidation_0-auc:0.840566\n",
            "[22]\tvalidation_0-auc:0.836851\n",
            "[23]\tvalidation_0-auc:0.834343\n",
            "[24]\tvalidation_0-auc:0.834235\n",
            "[25]\tvalidation_0-auc:0.835823\n",
            "[26]\tvalidation_0-auc:0.83483\n",
            "[27]\tvalidation_0-auc:0.831097\n",
            "[28]\tvalidation_0-auc:0.82987\n",
            "[29]\tvalidation_0-auc:0.829473\n",
            "[30]\tvalidation_0-auc:0.827417\n",
            "[31]\tvalidation_0-auc:0.828716\n",
            "[32]\tvalidation_0-auc:0.831782\n",
            "[33]\tvalidation_0-auc:0.833622\n",
            "[34]\tvalidation_0-auc:0.831782\n",
            "[35]\tvalidation_0-auc:0.831205\n",
            "[36]\tvalidation_0-auc:0.830556\n",
            "[37]\tvalidation_0-auc:0.826046\n",
            "[38]\tvalidation_0-auc:0.824459\n",
            "[39]\tvalidation_0-auc:0.823088\n",
            "[40]\tvalidation_0-auc:0.822727\n",
            "[41]\tvalidation_0-auc:0.819156\n",
            "[42]\tvalidation_0-auc:0.818831\n",
            "[43]\tvalidation_0-auc:0.816234\n",
            "[44]\tvalidation_0-auc:0.819841\n",
            "[45]\tvalidation_0-auc:0.817605\n",
            "[46]\tvalidation_0-auc:0.814646\n",
            "[47]\tvalidation_0-auc:0.813907\n",
            "[48]\tvalidation_0-auc:0.810949\n",
            "[49]\tvalidation_0-auc:0.813889\n",
            "[50]\tvalidation_0-auc:0.816378\n",
            "[51]\tvalidation_0-auc:0.815729\n",
            "[52]\tvalidation_0-auc:0.815584\n",
            "[53]\tvalidation_0-auc:0.816414\n",
            "[54]\tvalidation_0-auc:0.821032\n",
            "[55]\tvalidation_0-auc:0.822565\n",
            "[56]\tvalidation_0-auc:0.821573\n",
            "[57]\tvalidation_0-auc:0.822511\n",
            "[58]\tvalidation_0-auc:0.817893\n",
            "Stopping. Best iteration:\n",
            "[8]\tvalidation_0-auc:0.870112\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.5102040816326531 | 0.23841059602649006 |  0.8780487804878049 |        0.375        |\n",
            "|     GRU 0.1      | 0.7408163265306122 |  0.3668639053254438 |  0.7560975609756098 |  0.4940239043824701 |\n",
            "|   XGBoost 0.1    |        0.8         | 0.42857142857142855 |  0.5853658536585366 |  0.4948453608247423 |\n",
            "|    Logreg 0.1    | 0.810204081632653  |  0.4485981308411215 |  0.5853658536585366 |  0.507936507936508  |\n",
            "|     SVM 0.1      | 0.8244897959183674 | 0.47619047619047616 |  0.4878048780487805 |  0.4819277108433735 |\n",
            "|  LSTM beta 0.1   | 0.7943107221006565 |  0.2708333333333333 | 0.18055555555555555 | 0.21666666666666667 |\n",
            "|   GRU beta 0.1   | 0.8183807439824945 | 0.44660194174757284 |  0.6388888888888888 |  0.5257142857142858 |\n",
            "| XGBoost beta 0.1 | 0.8533916849015317 |  0.5257731958762887 |  0.7083333333333334 |  0.6035502958579881 |\n",
            "| logreg beta 0.1  | 0.7964989059080962 | 0.41025641025641024 |  0.6666666666666666 |  0.5079365079365079 |\n",
            "|   svm beta 0.1   | 0.8096280087527352 | 0.43119266055045874 |  0.6527777777777778 |  0.5193370165745858 |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.4564 - accuracy: 0.8456 - val_loss: 0.2541 - val_accuracy: 0.9490\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4322 - accuracy: 0.8490 - val_loss: 0.2854 - val_accuracy: 0.9490\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4383 - accuracy: 0.8490 - val_loss: 0.2735 - val_accuracy: 0.9490\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4388 - accuracy: 0.8490 - val_loss: 0.2482 - val_accuracy: 0.9490\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4328 - accuracy: 0.8490 - val_loss: 0.2787 - val_accuracy: 0.9490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.4610 - accuracy: 0.8409 - val_loss: 0.2818 - val_accuracy: 0.9490\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4419 - accuracy: 0.8490 - val_loss: 0.3098 - val_accuracy: 0.9490\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4254 - accuracy: 0.8490 - val_loss: 0.2663 - val_accuracy: 0.9408\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4221 - accuracy: 0.8490 - val_loss: 0.2528 - val_accuracy: 0.9367\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4152 - accuracy: 0.8503 - val_loss: 0.3438 - val_accuracy: 0.9347\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.631355\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.631398\n",
            "[2]\tvalidation_0-auc:0.629763\n",
            "[3]\tvalidation_0-auc:0.667613\n",
            "[4]\tvalidation_0-auc:0.668645\n",
            "[5]\tvalidation_0-auc:0.718495\n",
            "[6]\tvalidation_0-auc:0.719011\n",
            "[7]\tvalidation_0-auc:0.718925\n",
            "[8]\tvalidation_0-auc:0.718538\n",
            "[9]\tvalidation_0-auc:0.749806\n",
            "[10]\tvalidation_0-auc:0.770796\n",
            "[11]\tvalidation_0-auc:0.776043\n",
            "[12]\tvalidation_0-auc:0.784473\n",
            "[13]\tvalidation_0-auc:0.78757\n",
            "[14]\tvalidation_0-auc:0.777936\n",
            "[15]\tvalidation_0-auc:0.782925\n",
            "[16]\tvalidation_0-auc:0.790151\n",
            "[17]\tvalidation_0-auc:0.80228\n",
            "[18]\tvalidation_0-auc:0.799742\n",
            "[19]\tvalidation_0-auc:0.814624\n",
            "[20]\tvalidation_0-auc:0.810409\n",
            "[21]\tvalidation_0-auc:0.805462\n",
            "[22]\tvalidation_0-auc:0.802624\n",
            "[23]\tvalidation_0-auc:0.813806\n",
            "[24]\tvalidation_0-auc:0.811312\n",
            "[25]\tvalidation_0-auc:0.815398\n",
            "[26]\tvalidation_0-auc:0.811871\n",
            "[27]\tvalidation_0-auc:0.832473\n",
            "[28]\tvalidation_0-auc:0.830237\n",
            "[29]\tvalidation_0-auc:0.839656\n",
            "[30]\tvalidation_0-auc:0.831656\n",
            "[31]\tvalidation_0-auc:0.843914\n",
            "[32]\tvalidation_0-auc:0.846022\n",
            "[33]\tvalidation_0-auc:0.847054\n",
            "[34]\tvalidation_0-auc:0.84757\n",
            "[35]\tvalidation_0-auc:0.847312\n",
            "[36]\tvalidation_0-auc:0.856344\n",
            "[37]\tvalidation_0-auc:0.851699\n",
            "[38]\tvalidation_0-auc:0.851699\n",
            "[39]\tvalidation_0-auc:0.851183\n",
            "[40]\tvalidation_0-auc:0.852645\n",
            "[41]\tvalidation_0-auc:0.845591\n",
            "[42]\tvalidation_0-auc:0.844989\n",
            "[43]\tvalidation_0-auc:0.846968\n",
            "[44]\tvalidation_0-auc:0.843355\n",
            "[45]\tvalidation_0-auc:0.843613\n",
            "[46]\tvalidation_0-auc:0.842839\n",
            "[47]\tvalidation_0-auc:0.842323\n",
            "[48]\tvalidation_0-auc:0.844645\n",
            "[49]\tvalidation_0-auc:0.837763\n",
            "[50]\tvalidation_0-auc:0.836301\n",
            "[51]\tvalidation_0-auc:0.837247\n",
            "[52]\tvalidation_0-auc:0.832774\n",
            "[53]\tvalidation_0-auc:0.832946\n",
            "[54]\tvalidation_0-auc:0.833376\n",
            "[55]\tvalidation_0-auc:0.833548\n",
            "[56]\tvalidation_0-auc:0.827699\n",
            "[57]\tvalidation_0-auc:0.821075\n",
            "[58]\tvalidation_0-auc:0.821677\n",
            "[59]\tvalidation_0-auc:0.821763\n",
            "[60]\tvalidation_0-auc:0.82228\n",
            "[61]\tvalidation_0-auc:0.820731\n",
            "[62]\tvalidation_0-auc:0.823398\n",
            "[63]\tvalidation_0-auc:0.822882\n",
            "[64]\tvalidation_0-auc:0.818065\n",
            "[65]\tvalidation_0-auc:0.81428\n",
            "[66]\tvalidation_0-auc:0.813763\n",
            "[67]\tvalidation_0-auc:0.818495\n",
            "[68]\tvalidation_0-auc:0.817634\n",
            "[69]\tvalidation_0-auc:0.813763\n",
            "[70]\tvalidation_0-auc:0.815914\n",
            "[71]\tvalidation_0-auc:0.816688\n",
            "[72]\tvalidation_0-auc:0.820387\n",
            "[73]\tvalidation_0-auc:0.821505\n",
            "[74]\tvalidation_0-auc:0.822452\n",
            "[75]\tvalidation_0-auc:0.824516\n",
            "[76]\tvalidation_0-auc:0.819613\n",
            "[77]\tvalidation_0-auc:0.821505\n",
            "[78]\tvalidation_0-auc:0.819699\n",
            "[79]\tvalidation_0-auc:0.819785\n",
            "[80]\tvalidation_0-auc:0.817548\n",
            "[81]\tvalidation_0-auc:0.817634\n",
            "[82]\tvalidation_0-auc:0.814882\n",
            "[83]\tvalidation_0-auc:0.818409\n",
            "[84]\tvalidation_0-auc:0.817548\n",
            "[85]\tvalidation_0-auc:0.818495\n",
            "[86]\tvalidation_0-auc:0.816344\n",
            "Stopping. Best iteration:\n",
            "[36]\tvalidation_0-auc:0.856344\n",
            "\n",
            "end training. \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.4676 - accuracy: 0.8428 - val_loss: 0.2832 - val_accuracy: 0.9453\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4419 - accuracy: 0.8456 - val_loss: 0.2936 - val_accuracy: 0.9453\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4514 - accuracy: 0.8456 - val_loss: 0.2437 - val_accuracy: 0.9453\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4473 - accuracy: 0.8456 - val_loss: 0.2702 - val_accuracy: 0.9453\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4284 - accuracy: 0.8456 - val_loss: 0.2894 - val_accuracy: 0.9453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.4662 - accuracy: 0.8428 - val_loss: 0.2343 - val_accuracy: 0.9453\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4384 - accuracy: 0.8456 - val_loss: 0.2766 - val_accuracy: 0.9059\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4264 - accuracy: 0.8456 - val_loss: 0.3505 - val_accuracy: 0.8862\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4145 - accuracy: 0.8524 - val_loss: 0.3957 - val_accuracy: 0.8796\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4125 - accuracy: 0.8476 - val_loss: 0.4261 - val_accuracy: 0.8796\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.474259\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.474259\n",
            "[2]\tvalidation_0-auc:0.474074\n",
            "[3]\tvalidation_0-auc:0.474074\n",
            "[4]\tvalidation_0-auc:0.473704\n",
            "[5]\tvalidation_0-auc:0.545463\n",
            "[6]\tvalidation_0-auc:0.545556\n",
            "[7]\tvalidation_0-auc:0.545556\n",
            "[8]\tvalidation_0-auc:0.618704\n",
            "[9]\tvalidation_0-auc:0.618796\n",
            "[10]\tvalidation_0-auc:0.667917\n",
            "[11]\tvalidation_0-auc:0.724583\n",
            "[12]\tvalidation_0-auc:0.71662\n",
            "[13]\tvalidation_0-auc:0.711991\n",
            "[14]\tvalidation_0-auc:0.711991\n",
            "[15]\tvalidation_0-auc:0.713009\n",
            "[16]\tvalidation_0-auc:0.711157\n",
            "[17]\tvalidation_0-auc:0.686713\n",
            "[18]\tvalidation_0-auc:0.712639\n",
            "[19]\tvalidation_0-auc:0.709028\n",
            "[20]\tvalidation_0-auc:0.701065\n",
            "[21]\tvalidation_0-auc:0.716389\n",
            "[22]\tvalidation_0-auc:0.701759\n",
            "[23]\tvalidation_0-auc:0.701574\n",
            "[24]\tvalidation_0-auc:0.718148\n",
            "[25]\tvalidation_0-auc:0.712593\n",
            "[26]\tvalidation_0-auc:0.706667\n",
            "[27]\tvalidation_0-auc:0.699537\n",
            "[28]\tvalidation_0-auc:0.70713\n",
            "[29]\tvalidation_0-auc:0.702454\n",
            "[30]\tvalidation_0-auc:0.695324\n",
            "[31]\tvalidation_0-auc:0.691898\n",
            "[32]\tvalidation_0-auc:0.70162\n",
            "[33]\tvalidation_0-auc:0.693519\n",
            "[34]\tvalidation_0-auc:0.696667\n",
            "[35]\tvalidation_0-auc:0.698333\n",
            "[36]\tvalidation_0-auc:0.694444\n",
            "[37]\tvalidation_0-auc:0.694537\n",
            "[38]\tvalidation_0-auc:0.697454\n",
            "[39]\tvalidation_0-auc:0.695185\n",
            "[40]\tvalidation_0-auc:0.685972\n",
            "[41]\tvalidation_0-auc:0.678102\n",
            "[42]\tvalidation_0-auc:0.683981\n",
            "[43]\tvalidation_0-auc:0.67875\n",
            "[44]\tvalidation_0-auc:0.672731\n",
            "[45]\tvalidation_0-auc:0.671528\n",
            "[46]\tvalidation_0-auc:0.67625\n",
            "[47]\tvalidation_0-auc:0.676991\n",
            "[48]\tvalidation_0-auc:0.682176\n",
            "[49]\tvalidation_0-auc:0.680417\n",
            "[50]\tvalidation_0-auc:0.682917\n",
            "[51]\tvalidation_0-auc:0.690787\n",
            "[52]\tvalidation_0-auc:0.692731\n",
            "[53]\tvalidation_0-auc:0.690833\n",
            "[54]\tvalidation_0-auc:0.690463\n",
            "[55]\tvalidation_0-auc:0.692315\n",
            "[56]\tvalidation_0-auc:0.69213\n",
            "[57]\tvalidation_0-auc:0.690926\n",
            "[58]\tvalidation_0-auc:0.691944\n",
            "[59]\tvalidation_0-auc:0.696111\n",
            "[60]\tvalidation_0-auc:0.693889\n",
            "[61]\tvalidation_0-auc:0.696204\n",
            "Stopping. Best iteration:\n",
            "[11]\tvalidation_0-auc:0.724583\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------+----------------------+\n",
            "|      Model       |      Accuracy      |      Precision      | Recall |       F1 score       |\n",
            "+------------------+--------------------+---------------------+--------+----------------------+\n",
            "|     LSTM 0.2     | 0.9489795918367347 |         0.0         |  0.0   |         0.0          |\n",
            "|     GRU 0.2      | 0.9346938775510204 |  0.1111111111111111 |  0.04  | 0.058823529411764705 |\n",
            "|   XGBoost 0.2    | 0.926530612244898  | 0.13333333333333333 |  0.08  |         0.1          |\n",
            "|    Logreg 0.2    | 0.9326530612244898 |         0.1         |  0.04  | 0.05714285714285714  |\n",
            "|     SVM 0.2      | 0.9489795918367347 |         0.0         |  0.0   |         0.0          |\n",
            "|  LSTM beta 0.2   | 0.9452954048140044 |         0.0         |  0.0   |         0.0          |\n",
            "|   GRU beta 0.2   | 0.8796498905908097 |       0.03125       |  0.04  | 0.03508771929824561  |\n",
            "| XGBoost beta 0.2 | 0.8687089715536105 | 0.02702702702702703 |  0.04  | 0.03225806451612903  |\n",
            "| logreg beta 0.2  | 0.862144420131291  |        0.025        |  0.04  | 0.030769230769230767 |\n",
            "|   svm beta 0.2   | 0.9452954048140044 |         0.0         |  0.0   |         0.0          |\n",
            "+------------------+--------------------+---------------------+--------+----------------------+\n",
            "Threshhold =  0.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6513 - accuracy: 0.6477 - val_loss: 0.5052 - val_accuracy: 0.9490\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6505 - accuracy: 0.6537 - val_loss: 0.4315 - val_accuracy: 0.9490\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6466 - accuracy: 0.6537 - val_loss: 0.5670 - val_accuracy: 0.9490\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6485 - accuracy: 0.6517 - val_loss: 0.4457 - val_accuracy: 0.9490\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6415 - accuracy: 0.6550 - val_loss: 0.4451 - val_accuracy: 0.9306\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6501 - accuracy: 0.6537 - val_loss: 0.5251 - val_accuracy: 0.9388\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6463 - accuracy: 0.6564 - val_loss: 0.5168 - val_accuracy: 0.9367\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6285 - accuracy: 0.6638 - val_loss: 0.4406 - val_accuracy: 0.9327\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6172 - accuracy: 0.6852 - val_loss: 0.5238 - val_accuracy: 0.8939\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6137 - accuracy: 0.6933 - val_loss: 0.4512 - val_accuracy: 0.9143\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.791785\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.812774\n",
            "[2]\tvalidation_0-auc:0.837505\n",
            "[3]\tvalidation_0-auc:0.840903\n",
            "[4]\tvalidation_0-auc:0.843097\n",
            "[5]\tvalidation_0-auc:0.847398\n",
            "[6]\tvalidation_0-auc:0.851312\n",
            "[7]\tvalidation_0-auc:0.852774\n",
            "[8]\tvalidation_0-auc:0.854538\n",
            "[9]\tvalidation_0-auc:0.854237\n",
            "[10]\tvalidation_0-auc:0.854968\n",
            "[11]\tvalidation_0-auc:0.850237\n",
            "[12]\tvalidation_0-auc:0.851527\n",
            "[13]\tvalidation_0-auc:0.854065\n",
            "[14]\tvalidation_0-auc:0.85372\n",
            "[15]\tvalidation_0-auc:0.854065\n",
            "[16]\tvalidation_0-auc:0.851484\n",
            "[17]\tvalidation_0-auc:0.851054\n",
            "[18]\tvalidation_0-auc:0.860774\n",
            "[19]\tvalidation_0-auc:0.860387\n",
            "[20]\tvalidation_0-auc:0.860731\n",
            "[21]\tvalidation_0-auc:0.860473\n",
            "[22]\tvalidation_0-auc:0.862366\n",
            "[23]\tvalidation_0-auc:0.860559\n",
            "[24]\tvalidation_0-auc:0.857118\n",
            "[25]\tvalidation_0-auc:0.855914\n",
            "[26]\tvalidation_0-auc:0.857118\n",
            "[27]\tvalidation_0-auc:0.856516\n",
            "[28]\tvalidation_0-auc:0.858323\n",
            "[29]\tvalidation_0-auc:0.863828\n",
            "[30]\tvalidation_0-auc:0.863656\n",
            "[31]\tvalidation_0-auc:0.865548\n",
            "[32]\tvalidation_0-auc:0.867011\n",
            "[33]\tvalidation_0-auc:0.868129\n",
            "[34]\tvalidation_0-auc:0.873806\n",
            "[35]\tvalidation_0-auc:0.873634\n",
            "[36]\tvalidation_0-auc:0.874323\n",
            "[37]\tvalidation_0-auc:0.869936\n",
            "[38]\tvalidation_0-auc:0.862108\n",
            "[39]\tvalidation_0-auc:0.861247\n",
            "[40]\tvalidation_0-auc:0.865118\n",
            "[41]\tvalidation_0-auc:0.863312\n",
            "[42]\tvalidation_0-auc:0.860129\n",
            "[43]\tvalidation_0-auc:0.861935\n",
            "[44]\tvalidation_0-auc:0.861935\n",
            "[45]\tvalidation_0-auc:0.857548\n",
            "[46]\tvalidation_0-auc:0.856172\n",
            "[47]\tvalidation_0-auc:0.857118\n",
            "[48]\tvalidation_0-auc:0.859269\n",
            "[49]\tvalidation_0-auc:0.857634\n",
            "[50]\tvalidation_0-auc:0.856602\n",
            "[51]\tvalidation_0-auc:0.852559\n",
            "[52]\tvalidation_0-auc:0.85471\n",
            "[53]\tvalidation_0-auc:0.850581\n",
            "[54]\tvalidation_0-auc:0.850237\n",
            "[55]\tvalidation_0-auc:0.850495\n",
            "[56]\tvalidation_0-auc:0.851011\n",
            "[57]\tvalidation_0-auc:0.850753\n",
            "[58]\tvalidation_0-auc:0.856258\n",
            "[59]\tvalidation_0-auc:0.854538\n",
            "[60]\tvalidation_0-auc:0.853419\n",
            "[61]\tvalidation_0-auc:0.850839\n",
            "[62]\tvalidation_0-auc:0.850667\n",
            "[63]\tvalidation_0-auc:0.849806\n",
            "[64]\tvalidation_0-auc:0.847398\n",
            "[65]\tvalidation_0-auc:0.84671\n",
            "[66]\tvalidation_0-auc:0.847484\n",
            "[67]\tvalidation_0-auc:0.848344\n",
            "[68]\tvalidation_0-auc:0.845935\n",
            "[69]\tvalidation_0-auc:0.843441\n",
            "[70]\tvalidation_0-auc:0.842409\n",
            "[71]\tvalidation_0-auc:0.843613\n",
            "[72]\tvalidation_0-auc:0.848172\n",
            "[73]\tvalidation_0-auc:0.847914\n",
            "[74]\tvalidation_0-auc:0.84972\n",
            "[75]\tvalidation_0-auc:0.84929\n",
            "[76]\tvalidation_0-auc:0.846882\n",
            "[77]\tvalidation_0-auc:0.848172\n",
            "[78]\tvalidation_0-auc:0.846366\n",
            "[79]\tvalidation_0-auc:0.848344\n",
            "[80]\tvalidation_0-auc:0.848172\n",
            "[81]\tvalidation_0-auc:0.847054\n",
            "[82]\tvalidation_0-auc:0.844731\n",
            "[83]\tvalidation_0-auc:0.842409\n",
            "[84]\tvalidation_0-auc:0.849032\n",
            "[85]\tvalidation_0-auc:0.846366\n",
            "[86]\tvalidation_0-auc:0.84757\n",
            "Stopping. Best iteration:\n",
            "[36]\tvalidation_0-auc:0.874323\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6619 - accuracy: 0.6431 - val_loss: 0.4564 - val_accuracy: 0.9453\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6535 - accuracy: 0.6458 - val_loss: 0.4751 - val_accuracy: 0.9453\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6473 - accuracy: 0.6410 - val_loss: 0.4112 - val_accuracy: 0.9453\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6391 - accuracy: 0.6507 - val_loss: 0.4466 - val_accuracy: 0.8753\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6173 - accuracy: 0.6733 - val_loss: 0.4732 - val_accuracy: 0.8709\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6490 - accuracy: 0.6376 - val_loss: 0.4651 - val_accuracy: 0.8796\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6119 - accuracy: 0.6870 - val_loss: 0.6200 - val_accuracy: 0.8753\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5858 - accuracy: 0.7159 - val_loss: 0.6990 - val_accuracy: 0.8118\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5861 - accuracy: 0.7076 - val_loss: 0.6514 - val_accuracy: 0.8490\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5795 - accuracy: 0.7261 - val_loss: 0.7624 - val_accuracy: 0.7462\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.753611\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.704352\n",
            "[2]\tvalidation_0-auc:0.704537\n",
            "[3]\tvalidation_0-auc:0.7\n",
            "[4]\tvalidation_0-auc:0.703519\n",
            "[5]\tvalidation_0-auc:0.723565\n",
            "[6]\tvalidation_0-auc:0.708102\n",
            "[7]\tvalidation_0-auc:0.707083\n",
            "[8]\tvalidation_0-auc:0.705278\n",
            "[9]\tvalidation_0-auc:0.705463\n",
            "[10]\tvalidation_0-auc:0.693889\n",
            "[11]\tvalidation_0-auc:0.695278\n",
            "[12]\tvalidation_0-auc:0.695648\n",
            "[13]\tvalidation_0-auc:0.695926\n",
            "[14]\tvalidation_0-auc:0.693843\n",
            "[15]\tvalidation_0-auc:0.695046\n",
            "[16]\tvalidation_0-auc:0.694861\n",
            "[17]\tvalidation_0-auc:0.661343\n",
            "[18]\tvalidation_0-auc:0.624861\n",
            "[19]\tvalidation_0-auc:0.636806\n",
            "[20]\tvalidation_0-auc:0.624259\n",
            "[21]\tvalidation_0-auc:0.624537\n",
            "[22]\tvalidation_0-auc:0.607963\n",
            "[23]\tvalidation_0-auc:0.608241\n",
            "[24]\tvalidation_0-auc:0.593796\n",
            "[25]\tvalidation_0-auc:0.592454\n",
            "[26]\tvalidation_0-auc:0.577037\n",
            "[27]\tvalidation_0-auc:0.589444\n",
            "[28]\tvalidation_0-auc:0.588056\n",
            "[29]\tvalidation_0-auc:0.579444\n",
            "[30]\tvalidation_0-auc:0.578519\n",
            "[31]\tvalidation_0-auc:0.580463\n",
            "[32]\tvalidation_0-auc:0.58037\n",
            "[33]\tvalidation_0-auc:0.578056\n",
            "[34]\tvalidation_0-auc:0.576574\n",
            "[35]\tvalidation_0-auc:0.578611\n",
            "[36]\tvalidation_0-auc:0.568981\n",
            "[37]\tvalidation_0-auc:0.568426\n",
            "[38]\tvalidation_0-auc:0.572407\n",
            "[39]\tvalidation_0-auc:0.571389\n",
            "[40]\tvalidation_0-auc:0.568056\n",
            "[41]\tvalidation_0-auc:0.564722\n",
            "[42]\tvalidation_0-auc:0.565648\n",
            "[43]\tvalidation_0-auc:0.566389\n",
            "[44]\tvalidation_0-auc:0.563241\n",
            "[45]\tvalidation_0-auc:0.563981\n",
            "[46]\tvalidation_0-auc:0.563194\n",
            "[47]\tvalidation_0-auc:0.561435\n",
            "[48]\tvalidation_0-auc:0.565139\n",
            "[49]\tvalidation_0-auc:0.564213\n",
            "[50]\tvalidation_0-auc:0.565833\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.753611\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+----------------------+--------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision       | Recall |       F1 score      |\n",
            "+-------------------+--------------------+----------------------+--------+---------------------+\n",
            "|     LSTM 0.15     | 0.9306122448979591 |         0.0          |  0.0   |         0.0         |\n",
            "|      GRU 0.15     | 0.9142857142857143 | 0.32653061224489793  |  0.64  | 0.43243243243243235 |\n",
            "|    XGBoost 0.15   | 0.9040816326530612 |         0.25         |  0.44  | 0.31884057971014496 |\n",
            "|    Logreg 0.15    | 0.9102040816326531 |  0.2564102564102564  |  0.4   |        0.3125       |\n",
            "|      SVM 0.15     | 0.9224489795918367 |  0.3333333333333333  |  0.52  | 0.40625000000000006 |\n",
            "|   LSTM beta 0.15  | 0.8708971553610503 | 0.027777777777777776 |  0.04  | 0.03278688524590164 |\n",
            "|   GRU beta 0.15   | 0.7461706783369803 | 0.10434782608695652  |  0.48  | 0.17142857142857143 |\n",
            "| XGBoost beta 0.15 | 0.9168490153172867 | 0.25925925925925924  |  0.28  |  0.2692307692307692 |\n",
            "|  logreg beta 0.15 | 0.8358862144420132 |  0.1323529411764706  |  0.36  | 0.19354838709677422 |\n",
            "|   svm beta 0.15   | 0.862144420131291  | 0.16071428571428573  |  0.36  | 0.22222222222222224 |\n",
            "+-------------------+--------------------+----------------------+--------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "wiYyeVi7Ga-E",
        "outputId": "228a1b35-cb0d-409b-aa45-515fd45c0b04"
      },
      "source": [
        "Result_purging.to_csv('CL_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.238411</td>\n",
              "      <td>0.510204</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.878049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.366864</td>\n",
              "      <td>0.740816</td>\n",
              "      <td>0.494024</td>\n",
              "      <td>0.756098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.494845</td>\n",
              "      <td>0.585366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.448598</td>\n",
              "      <td>0.810204</td>\n",
              "      <td>0.507937</td>\n",
              "      <td>0.585366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.476190</td>\n",
              "      <td>0.824490</td>\n",
              "      <td>0.481928</td>\n",
              "      <td>0.487805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.270833</td>\n",
              "      <td>0.794311</td>\n",
              "      <td>0.216667</td>\n",
              "      <td>0.180556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.446602</td>\n",
              "      <td>0.818381</td>\n",
              "      <td>0.525714</td>\n",
              "      <td>0.638889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.525773</td>\n",
              "      <td>0.853392</td>\n",
              "      <td>0.603550</td>\n",
              "      <td>0.708333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.410256</td>\n",
              "      <td>0.796499</td>\n",
              "      <td>0.507937</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.431193</td>\n",
              "      <td>0.809628</td>\n",
              "      <td>0.519337</td>\n",
              "      <td>0.652778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.948980</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.934694</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.926531</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.080000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.932653</td>\n",
              "      <td>0.057143</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.948980</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.945295</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.879650</td>\n",
              "      <td>0.035088</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.868709</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.862144</td>\n",
              "      <td>0.030769</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.945295</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.930612</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.326531</td>\n",
              "      <td>0.914286</td>\n",
              "      <td>0.432432</td>\n",
              "      <td>0.640000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.904082</td>\n",
              "      <td>0.318841</td>\n",
              "      <td>0.440000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.256410</td>\n",
              "      <td>0.910204</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.922449</td>\n",
              "      <td>0.406250</td>\n",
              "      <td>0.520000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.027778</td>\n",
              "      <td>0.870897</td>\n",
              "      <td>0.032787</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.104348</td>\n",
              "      <td>0.746171</td>\n",
              "      <td>0.171429</td>\n",
              "      <td>0.480000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.259259</td>\n",
              "      <td>0.916849</td>\n",
              "      <td>0.269231</td>\n",
              "      <td>0.280000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.132353</td>\n",
              "      <td>0.835886</td>\n",
              "      <td>0.193548</td>\n",
              "      <td>0.360000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>CL</td>\n",
              "      <td>0.160714</td>\n",
              "      <td>0.862144</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.360000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1   CL  0.238411  0.510204  0.375000  0.878049\n",
              "1            GRU 0.1   CL  0.366864  0.740816  0.494024  0.756098\n",
              "2        XGBoost 0.1   CL  0.428571  0.800000  0.494845  0.585366\n",
              "3         Logreg 0.1   CL  0.448598  0.810204  0.507937  0.585366\n",
              "4            SVM 0.1   CL  0.476190  0.824490  0.481928  0.487805\n",
              "5      LSTM beta 0.1   CL  0.270833  0.794311  0.216667  0.180556\n",
              "6       GRU beta 0.1   CL  0.446602  0.818381  0.525714  0.638889\n",
              "7   XGBoost beta 0.1   CL  0.525773  0.853392  0.603550  0.708333\n",
              "8    logreg beta 0.1   CL  0.410256  0.796499  0.507937  0.666667\n",
              "9       svm beta 0.1   CL  0.431193  0.809628  0.519337  0.652778\n",
              "0           LSTM 0.2   CL  0.000000  0.948980  0.000000  0.000000\n",
              "1            GRU 0.2   CL  0.111111  0.934694  0.058824  0.040000\n",
              "2        XGBoost 0.2   CL  0.133333  0.926531  0.100000  0.080000\n",
              "3         Logreg 0.2   CL  0.100000  0.932653  0.057143  0.040000\n",
              "4            SVM 0.2   CL  0.000000  0.948980  0.000000  0.000000\n",
              "5      LSTM beta 0.2   CL  0.000000  0.945295  0.000000  0.000000\n",
              "6       GRU beta 0.2   CL  0.031250  0.879650  0.035088  0.040000\n",
              "7   XGBoost beta 0.2   CL  0.027027  0.868709  0.032258  0.040000\n",
              "8    logreg beta 0.2   CL  0.025000  0.862144  0.030769  0.040000\n",
              "9       svm beta 0.2   CL  0.000000  0.945295  0.000000  0.000000\n",
              "0          LSTM 0.15   CL  0.000000  0.930612  0.000000  0.000000\n",
              "1           GRU 0.15   CL  0.326531  0.914286  0.432432  0.640000\n",
              "2       XGBoost 0.15   CL  0.250000  0.904082  0.318841  0.440000\n",
              "3        Logreg 0.15   CL  0.256410  0.910204  0.312500  0.400000\n",
              "4           SVM 0.15   CL  0.333333  0.922449  0.406250  0.520000\n",
              "5     LSTM beta 0.15   CL  0.027778  0.870897  0.032787  0.040000\n",
              "6      GRU beta 0.15   CL  0.104348  0.746171  0.171429  0.480000\n",
              "7  XGBoost beta 0.15   CL  0.259259  0.916849  0.269231  0.280000\n",
              "8   logreg beta 0.15   CL  0.132353  0.835886  0.193548  0.360000\n",
              "9      svm beta 0.15   CL  0.160714  0.862144  0.222222  0.360000"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKg3AipZGa-E"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CL_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEjp-1zWHUKQ"
      },
      "source": [
        "## CPRI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "aEooKxjUHUKp",
        "outputId": "de675405-f0b5-4169-c1d5-ebe280dd67c6"
      },
      "source": [
        "dfs = pd.read_csv(\"CPRI.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2464</td>\n",
              "      <td>US1.CPRI</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>48.61</td>\n",
              "      <td>50.18</td>\n",
              "      <td>47.93</td>\n",
              "      <td>49.78</td>\n",
              "      <td>37955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2463</td>\n",
              "      <td>US1.CPRI</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>50.23</td>\n",
              "      <td>50.73</td>\n",
              "      <td>48.33</td>\n",
              "      <td>48.40</td>\n",
              "      <td>51156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2462</td>\n",
              "      <td>US1.CPRI</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>52.41</td>\n",
              "      <td>52.96</td>\n",
              "      <td>51.17</td>\n",
              "      <td>51.19</td>\n",
              "      <td>16510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2461</td>\n",
              "      <td>US1.CPRI</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>53.13</td>\n",
              "      <td>53.13</td>\n",
              "      <td>52.07</td>\n",
              "      <td>52.50</td>\n",
              "      <td>15965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2460</td>\n",
              "      <td>US1.CPRI</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>52.69</td>\n",
              "      <td>53.40</td>\n",
              "      <td>52.69</td>\n",
              "      <td>52.99</td>\n",
              "      <td>43844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2460</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.CPRI</td>\n",
              "      <td>D</td>\n",
              "      <td>20111221</td>\n",
              "      <td>0</td>\n",
              "      <td>25.64</td>\n",
              "      <td>26.78</td>\n",
              "      <td>25.46</td>\n",
              "      <td>26.50</td>\n",
              "      <td>2485638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2461</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.CPRI</td>\n",
              "      <td>D</td>\n",
              "      <td>20111220</td>\n",
              "      <td>0</td>\n",
              "      <td>25.02</td>\n",
              "      <td>25.76</td>\n",
              "      <td>25.02</td>\n",
              "      <td>25.40</td>\n",
              "      <td>1697064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2462</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.CPRI</td>\n",
              "      <td>D</td>\n",
              "      <td>20111219</td>\n",
              "      <td>0</td>\n",
              "      <td>24.50</td>\n",
              "      <td>25.09</td>\n",
              "      <td>24.31</td>\n",
              "      <td>24.83</td>\n",
              "      <td>3133441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2463</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.CPRI</td>\n",
              "      <td>D</td>\n",
              "      <td>20111216</td>\n",
              "      <td>0</td>\n",
              "      <td>24.45</td>\n",
              "      <td>24.80</td>\n",
              "      <td>23.51</td>\n",
              "      <td>24.21</td>\n",
              "      <td>3921157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2464</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.CPRI</td>\n",
              "      <td>D</td>\n",
              "      <td>20111215</td>\n",
              "      <td>0</td>\n",
              "      <td>24.82</td>\n",
              "      <td>25.19</td>\n",
              "      <td>23.51</td>\n",
              "      <td>24.39</td>\n",
              "      <td>31936188</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2465 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  <TICKER> <PER>    <DATE>  ...  <HIGH>  <LOW>  <CLOSE>     <VOL>\n",
              "0      2464  US1.CPRI     D  20211001  ...   50.18  47.93    49.78     37955\n",
              "1      2463  US1.CPRI     D  20210930  ...   50.73  48.33    48.40     51156\n",
              "2      2462  US1.CPRI     D  20210929  ...   52.96  51.17    51.19     16510\n",
              "3      2461  US1.CPRI     D  20210928  ...   53.13  52.07    52.50     15965\n",
              "4      2460  US1.CPRI     D  20210927  ...   53.40  52.69    52.99     43844\n",
              "...     ...       ...   ...       ...  ...     ...    ...      ...       ...\n",
              "2460      4  US1.CPRI     D  20111221  ...   26.78  25.46    26.50   2485638\n",
              "2461      3  US1.CPRI     D  20111220  ...   25.76  25.02    25.40   1697064\n",
              "2462      2  US1.CPRI     D  20111219  ...   25.09  24.31    24.83   3133441\n",
              "2463      1  US1.CPRI     D  20111216  ...   24.80  23.51    24.21   3921157\n",
              "2464      0  US1.CPRI     D  20111215  ...   25.19  23.51    24.39  31936188\n",
              "\n",
              "[2465 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9sbOQsCGHUKr",
        "outputId": "61e8aa55-263a-45d3-d88d-c74ec4a18deb"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"d32df22c-eec2-4cf7-980a-8245a7878a89\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"d32df22c-eec2-4cf7-980a-8245a7878a89\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'd32df22c-eec2-4cf7-980a-8245a7878a89',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [27.92, 28.83, 29.33, 29.48, 29.61, 31.45, 33.16, 32.62, 32.74, 32.86, 32.72, 32.8, 32.19, 32.56, 32.31, 31.66, 32.26, 32.01, 32.21, 32.04, 31.41, 29.76, 28.07, 28.3, 26.49, 25.56, 26.37, 26.54, 26.21, 25.875, 26.75, 26.76, 28.01, 27.91, 27.68, 28.58, 28.25, 26.98, 28.68, 30.35, 29.86, 30.98, 32.69, 32.58, 31.6, 31.71, 33.19, 33.96, 35.59, 35.83, 36.315, 36.07, 36.53, 36.72, 36.34, 35.65, 36.33, 35.95, 36.61, 37.05, 36.95, 36.47, 35.11, 34.63, 34.3, 34.5, 34.75, 34.76, 34.25, 34.85, 34.66, 33.99, 34.25, 33.66, 34.14, 34.71, 34.94, 34.25, 34.68, 33.89, 33.5, 33.68, 33.12, 33.93, 34.5, 34.21, 33.95, 35.05, 35.386, 34.02, 32.47, 33.78, 35.05, 38.88, 39.01, 39.5, 39.74, 40.62, 39.95, 39.99, 40.28, 40.19, 40.74, 40.48, 42.8, 43.09, 42.29, 41.97, 43.73, 44.08, 43.68, 43.19, 44.08, 44.87, 44.67, 44.52, 46.12, 46.09, 46.64, 49.21, 48.66, 47.845, 48.09, 48.45, 48.04, 49.83, 48.57, 48.85, 48.1, 49.31, 46.94, 46.62, 47.15, 45.75, 46.65, 45.38, 44.37, 43.86, 43.69, 45.36, 45.4, 46.57, 47.09, 45.92, 46.23, 46.41, 45.55, 44.69, 43.55, 44.69, 46.01, 45.21, 45.09, 46.81, 45.63, 45.44, 45.6, 45.17, 43.83, 43.6, 43.7, 44.04, 44.5, 44.89, 45.12, 45.48, 45.51, 46.04, 47.25, 48.53, 43.54, 42.33, 42.43, 42.48, 42.9, 42.84, 42.56, 42.405, 41.05, 40.82, 41.26, 42.42, 41.1, 40.4, 40.33, 40.23, 40.21, 40.29, 41.86, 40.9, 40.0, 38.98, 37.11, 39.44, 37.93, 37.18, 37.88, 37.76, 35.54, 36.41, 37.07, 37.92, 38.0, 38.11, 37.94, 37.48, 38.82, 38.45, 39.65, 40.5, 42.92, 42.43, 45.05, 43.76, 43.79, 44.96, 44.21, 45.34, 44.88, 45.3, 44.23, 44.6, 45.88, 47.24, 48.05, 48.22, 47.75, 47.78, 50.09, 49.0501, 57.51, 58.31, 58.29, 57.37, 55.41, 54.65, 55.04, 55.66, 56.92, 56.3, 57.88, 58.24, 58.05, 60.24, 62.37, 64.06, 62.46, 63.59, 61.98, 62.28, 67.06, 68.02, 66.88, 66.59, 68.09, 66.1, 68.37, 68.56, 69.45, 68.02, 68.01, 66.7, 72.67, 72.71, 73.05, 73.15, 73.36, 73.05, 72.61, 73.05, 73.94, 73.37, 72.36, 72.81, 73.25, 72.08, 72.63, 72.12, 73.73, 73.84, 74.42, 74.82, 74.25, 74.21, 75.41, 74.96, 72.97, 71.94, 71.32, 73.31, 71.41, 72.67, 72.34, 70.01, 65.59, 64.63, 64.26, 64.66, 65.77, 66.72, 66.17, 66.72, 68.34, 68.32, 67.27, 67.63, 67.61, 67.96, 67.08, 67.67, 67.17, 66.55, 66.43, 66.49, 67.3, 67.4, 66.98, 65.97, 65.88, 66.47, 66.6, 66.97, 66.8, 67.99, 66.7, 66.64, 67.26, 68.0, 67.01, 68.62, 67.52, 65.87, 66.84, 68.175, 66.35, 64.65, 63.42, 63.46, 63.52, 60.55, 59.84, 57.39, 60.4, 68.21, 68.62, 68.3635, 67.31, 66.16, 67.09, 66.17, 66.71, 65.8, 64.45, 63.2, 63.09, 62.92, 62.39, 61.39, 61.55, 61.54, 61.42, 65.44, 66.38, 68.44, 69.06, 68.49, 66.91, 67.7, 67.86, 65.02, 65.72, 65.53, 66.83, 65.51, 65.38, 66.08, 65.05, 65.22, 64.27, 64.85, 66.03, 64.31, 62.78, 61.94, 62.08, 61.21, 61.51, 62.73, 61.16, 62.11, 63.24, 62.65, 61.75, 61.49, 60.85, 61.11, 62.27, 61.41, 61.39, 60.71, 61.66, 62.08, 61.93, 61.84, 63.23, 62.92, 63.6, 64.91, 64.76, 63.79, 63.62, 63.98, 63.9, 64.73, 63.38, 62.38, 60.88, 59.79, 60.01, 66.09, 65.4, 63.94, 65.26, 65.93, 66.0, 66.92, 67.2, 68.14, 67.045, 67.27, 66.95, 65.69, 65.87, 65.02, 64.62, 63.83, 65.23, 64.44, 63.93, 63.83, 64.8, 63.91, 64.07, 63.79, 64.03, 62.96, 63.55, 63.585, 63.99, 63.41, 64.04, 63.71, 63.345, 63.19, 61.65, 61.43, 61.1, 61.11, 61.04, 61.81, 59.95, 59.85, 59.81, 58.78, 57.97, 58.45, 58.63, 57.87, 56.61, 56.61, 56.59, 56.31, 56.15, 56.28, 55.77, 55.17, 54.88, 54.1, 54.72, 54.54, 53.96, 54.6202, 54.6, 47.65, 47.93, 48.47, 48.81, 48.79, 49.01, 49.75, 49.73, 49.82, 49.45, 49.94, 48.86, 49.13, 48.99, 48.615, 47.75, 47.64, 47.01, 47.55, 47.29, 47.5, 47.61, 47.53, 47.6, 48.18, 47.83, 47.72, 48.55, 47.02, 45.88, 46.15, 45.93, 46.1203, 46.18, 44.67, 44.91, 44.42, 42.92, 42.59, 42.69, 42.17, 42.39, 42.63, 42.45, 42.8, 42.22, 42.07, 42.3, 42.45, 42.291, 41.79, 41.68, 41.79, 41.94, 41.96, 43.34, 44.02, 44.059, 45.02, 44.7099, 45.16, 45.77, 45.34, 37.24, 36.83, 36.469, 36.34, 36.81, 36.4408, 36.55, 36.23, 35.51, 34.93, 34.94, 35.26, 35.025, 34.72, 34.18, 34.52, 34.04, 33.45, 33.3, 33.26, 35.86, 36.04, 35.51, 36.41, 36.25, 36.25, 35.7, 35.97, 35.75, 35.67, 35.36, 34.87, 34.54, 34.56, 34.94, 34.6, 34.7, 34.55, 34.77, 35.41, 35.715, 35.091, 34.21, 34.63, 34.49, 33.05, 33.08, 33.18, 36.28, 36.75, 36.62, 36.74, 36.98, 36.89, 36.39, 35.45, 35.83, 35.93, 36.47, 36.75, 37.18, 38.23, 38.66, 38.2, 38.1, 37.5, 37.19, 37.9, 36.72, 37.32, 38.03, 38.04, 37.46, 36.64, 36.76, 37.59, 36.86, 36.91, 37.32, 37.57, 37.78, 37.77, 37.3, 37.555, 37.0801, 36.84, 36.81, 38.03, 38.12, 38.13, 38.295, 38.35, 37.8, 37.56, 37.22, 36.57, 36.74, 37.38, 37.82, 37.73, 37.61, 37.2, 36.92, 36.85, 36.46, 36.41, 36.03, 36.51, 36.41, 37.02, 36.89, 36.52, 37.59, 37.35, 37.3, 37.94, 38.3, 38.311, 38.21, 38.07, 38.09, 37.6, 38.39, 38.27, 37.865, 36.81, 41.28, 41.0299, 40.89, 41.34, 42.81, 41.79, 41.535, 42.06, 42.81, 43.4, 42.57, 42.45, 42.03, 43.21, 43.54, 42.39, 42.65, 42.87, 43.38, 42.05, 42.48, 42.11, 43.5, 43.06, 42.98, 42.85, 43.14, 42.92, 42.835, 42.74, 44.21, 44.27, 44.84, 45.21, 46.01, 47.29, 47.9, 47.88, 49.365, 50.61, 48.91, 48.37, 48.47, 46.21, 46.23, 46.49, 47.71, 48.03, 48.88, 48.595, 48.27, 48.14, 47.58, 48.57, 47.52, 46.9, 47.16, 49.67, 51.75, 50.15, 50.32, 50.3, 49.1, 48.24, 49.73, 50.37, 50.78, 50.13, 48.175, 48.57, 48.56, 48.84, 48.96, 49.6, 50.1, 47.52, 46.56, 46.67, 46.64, 46.78, 45.65, 46.41, 46.96, 47.52, 47.01, 47.36, 46.44, 46.8, 46.87, 48.25, 48.17, 47.99, 50.51, 50.11, 49.83, 49.11, 49.61, 47.85, 47.68, 47.49, 47.5, 48.32, 47.74, 49.16, 49.93, 48.75, 49.21, 49.62, 48.945, 49.26, 50.19, 49.59, 50.26, 50.34, 50.77, 50.12, 50.06, 49.31, 48.79, 49.91, 49.96, 48.785, 49.13, 48.71, 50.12, 50.27, 50.4, 49.67, 49.02, 50.32, 52.89, 51.71, 52.06, 52.35, 52.16, 51.69, 51.44, 51.34, 51.58, 51.21, 52.01, 51.64, 51.54, 51.71, 51.9, 51.1, 50.22, 49.06, 48.56, 49.16, 50.0805, 49.46, 48.41, 47.44, 46.18, 47.61, 50.79, 50.79, 50.66, 50.05, 49.96, 49.1, 49.88, 49.425, 49.67, 49.84, 50.73, 50.66, 50.845, 49.59, 48.45, 47.53, 45.52, 42.72, 41.79, 41.81, 41.8, 41.56, 41.52, 41.56, 40.7, 40.75, 41.63, 41.48, 41.915, 43.79, 44.005, 49.9, 50.68, 50.87, 50.18, 51.31, 52.43, 52.92, 51.65, 52.83, 55.12, 53.41, 51.84, 51.6, 51.65, 51.98, 51.98, 52.64, 52.28, 51.79, 53.14, 51.61, 51.38, 51.2, 53.74, 56.31, 55.9, 56.36, 56.98, 56.97, 56.88, 56.37, 56.27, 55.54, 55.93, 56.05, 56.91, 56.955, 56.79, 56.07, 57.29, 58.54, 58.49, 57.37, 58.12, 56.76, 57.4192, 57.13, 57.19, 57.84, 57.28, 56.64, 56.61, 56.46, 55.45, 54.78, 54.81, 52.73, 53.24, 52.4801, 50.745, 49.57, 49.13, 50.16, 50.25, 50.21, 51.85, 51.72, 51.73, 50.12, 40.36, 39.88, 38.72, 38.03, 37.79, 36.72, 38.4, 37.51, 36.5, 35.76, 35.57, 36.11, 36.15, 37.54, 37.115, 37.81, 38.97, 39.77, 40.71, 40.38, 40.06, 40.23, 41.03, 40.64, 40.86, 41.64, 40.83, 40.33, 39.72, 39.42, 40.94, 40.48, 40.16, 40.39, 42.22, 41.95, 41.99, 42.87, 42.34, 42.695, 43.03, 42.88, 43.0, 43.45, 42.89, 42.79, 42.23, 41.11, 40.88, 40.85, 39.31, 39.67, 38.99, 41.63, 41.3, 43.1, 42.83, 43.48, 43.48, 42.58, 39.32, 38.96, 38.65, 38.69, 38.54, 38.69, 38.74, 38.54, 39.46, 39.6, 39.91, 39.77, 40.27, 41.74, 41.74, 41.54, 41.9, 42.6, 43.49, 42.71, 43.37, 43.88, 43.515, 41.66, 42.24, 40.71, 40.565, 42.4, 42.72, 42.21, 43.05, 43.57, 43.18, 44.34, 45.1, 43.87, 43.16, 44.13, 43.68, 43.91, 44.5, 43.94, 44.91, 44.51, 42.8, 43.46, 43.1799, 43.29, 41.09, 39.39, 38.06, 40.08, 42.17, 43.18, 42.87, 43.525, 43.37, 43.62, 44.44, 45.38, 45.27, 43.85, 43.77, 39.47, 38.54, 38.71, 41.99, 42.11, 40.12, 39.42, 39.07, 39.33, 39.8, 40.12, 40.58, 39.91, 40.52, 41.25, 42.96, 43.475, 44.06, 42.48, 41.99, 41.69, 42.42, 41.83, 43.07, 43.12, 42.08, 43.36, 44.92, 45.36, 46.2, 46.8, 47.34, 46.61, 46.15, 46.41, 46.51, 47.08, 48.21, 47.84, 48.2, 48.58, 48.14, 48.89, 49.28, 48.573, 48.61, 47.75, 46.49, 45.72, 45.93, 60.62, 61.655, 61.34, 60.44, 60.71, 61.15, 61.02, 61.06, 61.1, 62.16, 62.83, 62.51, 62.43, 62.36, 62.57, 62.65, 62.55, 61.85, 62.5, 62.68, 63.015, 63.47, 63.32, 62.31, 62.28, 62.2, 63.26, 63.73, 63.7472, 62.94, 64.1448, 64.6, 65.25, 64.89, 64.38, 63.49, 63.38, 64.09, 65.75, 66.25, 66.97, 66.69, 68.02, 67.74, 67.9, 66.13, 66.81, 66.37, 64.81, 64.54, 64.71, 65.4225, 65.21, 64.7, 64.336, 65.96, 67.13, 67.14, 68.34, 67.95, 67.42, 67.88, 68.56, 68.35, 69.7, 68.98, 69.56, 69.78, 72.76, 73.27, 71.54, 71.02, 71.27, 71.39, 71.99, 69.78, 71.38, 72.22, 71.56, 70.8, 72.54, 70.02, 70.79, 69.8601, 69.36, 69.1, 68.51, 67.02, 67.02, 66.01, 66.49, 68.41, 69.36, 69.26, 70.0, 68.7, 66.86, 73.02, 74.79, 75.0999, 75.37, 74.95, 75.22, 75.3, 75.31, 74.36, 74.61, 74.3, 75.65, 75.88, 77.7, 77.02, 76.71, 75.58, 76.3, 76.16, 77.6636, 76.52, 75.73, 74.185, 73.3399, 76.71, 75.82, 76.8, 75.54, 73.97, 73.45, 72.45, 72.94, 72.32, 72.04, 70.75, 71.01, 68.89, 69.13, 69.942, 72.31, 72.39, 71.34, 77.9, 78.58, 77.91, 77.06, 78.16, 76.7, 75.72, 75.75, 75.07, 74.85, 73.14, 71.51, 72.06, 71.41, 72.15, 72.25, 73.67, 74.91, 77.08, 75.48, 75.67, 75.42, 74.08, 71.66, 71.4, 71.17, 71.49, 73.145, 74.6, 74.45, 75.09, 76.94, 77.22, 76.1, 76.38, 76.6, 76.81, 75.67, 75.63, 75.8, 76.57, 76.23, 79.97, 78.73, 80.56, 80.12, 81.0, 82.33, 82.94, 82.02, 82.62, 81.73, 82.85, 79.959, 79.34, 78.06, 78.36, 78.72, 77.43, 79.98, 79.26, 78.74, 79.7, 79.11, 76.99, 81.81, 81.47, 82.26, 83.66, 84.25, 81.12, 82.04, 81.71, 82.22, 82.22, 81.05, 81.87, 81.11, 79.4399, 85.72, 88.79, 89.18, 91.38, 89.91, 91.16, 90.27, 89.71, 89.92, 88.64, 90.0, 90.11, 90.13, 88.37, 88.52, 88.6, 89.33, 90.82, 91.06, 90.29, 93.89, 94.64, 95.37, 94.33, 93.89, 94.31, 93.33, 93.85, 94.05, 94.46, 94.38, 92.27, 97.01, 95.75, 96.39, 93.67, 93.3, 91.85, 93.03, 92.85, 91.26, 93.37, 93.71, 94.45, 91.82, 91.79, 91.37, 93.16, 94.73, 93.2, 92.4, 91.2, 90.3, 88.16, 89.92, 91.14, 91.43, 93.22, 90.67, 89.94, 90.12, 89.36, 87.12, 86.89, 89.65, 90.52, 89.0, 86.58, 89.54, 92.62, 95.03, 95.25, 93.27, 92.74, 91.55, 92.05, 93.45, 94.1, 97.47, 97.67, 98.7, 96.77, 98.53, 98.12, 97.02, 97.99, 98.07, 98.26, 98.275, 98.61, 98.99, 98.87, 97.96, 98.02, 99.57, 99.33, 99.84, 98.58, 97.57, 97.99, 97.79, 98.82, 98.44, 98.37, 96.0, 96.22, 95.56, 94.21, 91.5, 91.34, 89.92, 76.79, 79.94, 80.38, 78.73, 81.87, 81.08, 80.07, 80.29, 79.74, 77.18, 77.03, 75.69, 77.39, 77.04, 76.66, 79.81, 78.6, 77.91, 78.91, 82.05, 82.52, 82.48, 81.2, 80.62, 80.58, 81.03, 81.4, 80.8, 83.68, 83.92, 84.19, 82.92, 82.21, 82.27, 82.67, 80.59, 81.81, 80.61, 79.67, 80.77, 80.74, 80.4, 81.72, 81.35, 81.84, 80.6, 80.1, 80.63, 80.43, 79.08, 79.92, 80.95, 82.72, 81.86, 82.17, 82.0, 81.08, 79.4, 77.91, 79.47, 79.13, 74.78, 75.44, 76.93, 77.97, 77.41, 76.39, 76.21, 76.92, 77.59]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d32df22c-eec2-4cf7-980a-8245a7878a89');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"297c4966-74fb-45d8-9b82-f85b5705cea6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"297c4966-74fb-45d8-9b82-f85b5705cea6\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '297c4966-74fb-45d8-9b82-f85b5705cea6',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('297c4966-74fb-45d8-9b82-f85b5705cea6');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8bXzt1tHUKs"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaRzK0XhHUKs",
        "outputId": "624c6af6-e1f7-4a2e-f940-e7ed9a8616c7"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .2, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2100)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"CPRI\", step_sizes=4, th= th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6860 - accuracy: 0.5685 - val_loss: 0.7166 - val_accuracy: 0.3621\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6553 - accuracy: 0.5953 - val_loss: 0.6748 - val_accuracy: 0.4724\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6304 - accuracy: 0.6564 - val_loss: 0.6339 - val_accuracy: 0.7207\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6199 - accuracy: 0.6651 - val_loss: 0.5717 - val_accuracy: 0.7552\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6036 - accuracy: 0.6738 - val_loss: 0.7500 - val_accuracy: 0.5138\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6854 - accuracy: 0.5671 - val_loss: 0.7362 - val_accuracy: 0.3621\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.6409 - accuracy: 0.6074 - val_loss: 0.6334 - val_accuracy: 0.6552\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5675 - accuracy: 0.7054 - val_loss: 0.5721 - val_accuracy: 0.7034\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5699 - accuracy: 0.7054 - val_loss: 0.5691 - val_accuracy: 0.7103\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5720 - accuracy: 0.6966 - val_loss: 0.5669 - val_accuracy: 0.7207\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.794131\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.809292\n",
            "[2]\tvalidation_0-auc:0.807207\n",
            "[3]\tvalidation_0-auc:0.813024\n",
            "[4]\tvalidation_0-auc:0.809575\n",
            "[5]\tvalidation_0-auc:0.809833\n",
            "[6]\tvalidation_0-auc:0.807362\n",
            "[7]\tvalidation_0-auc:0.80749\n",
            "[8]\tvalidation_0-auc:0.809447\n",
            "[9]\tvalidation_0-auc:0.810167\n",
            "[10]\tvalidation_0-auc:0.808391\n",
            "[11]\tvalidation_0-auc:0.80888\n",
            "[12]\tvalidation_0-auc:0.816628\n",
            "[13]\tvalidation_0-auc:0.818044\n",
            "[14]\tvalidation_0-auc:0.819485\n",
            "[15]\tvalidation_0-auc:0.818867\n",
            "[16]\tvalidation_0-auc:0.817864\n",
            "[17]\tvalidation_0-auc:0.817555\n",
            "[18]\tvalidation_0-auc:0.818867\n",
            "[19]\tvalidation_0-auc:0.820952\n",
            "[20]\tvalidation_0-auc:0.821184\n",
            "[21]\tvalidation_0-auc:0.822523\n",
            "[22]\tvalidation_0-auc:0.823089\n",
            "[23]\tvalidation_0-auc:0.825174\n",
            "[24]\tvalidation_0-auc:0.824865\n",
            "[25]\tvalidation_0-auc:0.824891\n",
            "[26]\tvalidation_0-auc:0.824221\n",
            "[27]\tvalidation_0-auc:0.824427\n",
            "[28]\tvalidation_0-auc:0.824041\n",
            "[29]\tvalidation_0-auc:0.824685\n",
            "[30]\tvalidation_0-auc:0.823758\n",
            "[31]\tvalidation_0-auc:0.823964\n",
            "[32]\tvalidation_0-auc:0.824067\n",
            "[33]\tvalidation_0-auc:0.822728\n",
            "[34]\tvalidation_0-auc:0.822728\n",
            "[35]\tvalidation_0-auc:0.823295\n",
            "[36]\tvalidation_0-auc:0.821828\n",
            "[37]\tvalidation_0-auc:0.820489\n",
            "[38]\tvalidation_0-auc:0.82018\n",
            "[39]\tvalidation_0-auc:0.818147\n",
            "[40]\tvalidation_0-auc:0.818095\n",
            "[41]\tvalidation_0-auc:0.81686\n",
            "[42]\tvalidation_0-auc:0.815624\n",
            "[43]\tvalidation_0-auc:0.813205\n",
            "[44]\tvalidation_0-auc:0.81305\n",
            "[45]\tvalidation_0-auc:0.813462\n",
            "[46]\tvalidation_0-auc:0.812741\n",
            "[47]\tvalidation_0-auc:0.811532\n",
            "[48]\tvalidation_0-auc:0.810862\n",
            "[49]\tvalidation_0-auc:0.810811\n",
            "[50]\tvalidation_0-auc:0.810347\n",
            "[51]\tvalidation_0-auc:0.810656\n",
            "[52]\tvalidation_0-auc:0.810553\n",
            "[53]\tvalidation_0-auc:0.809575\n",
            "[54]\tvalidation_0-auc:0.80834\n",
            "[55]\tvalidation_0-auc:0.807156\n",
            "[56]\tvalidation_0-auc:0.806795\n",
            "[57]\tvalidation_0-auc:0.805714\n",
            "[58]\tvalidation_0-auc:0.806075\n",
            "[59]\tvalidation_0-auc:0.806435\n",
            "[60]\tvalidation_0-auc:0.805457\n",
            "[61]\tvalidation_0-auc:0.80556\n",
            "[62]\tvalidation_0-auc:0.805869\n",
            "[63]\tvalidation_0-auc:0.805457\n",
            "[64]\tvalidation_0-auc:0.805714\n",
            "[65]\tvalidation_0-auc:0.80556\n",
            "[66]\tvalidation_0-auc:0.80556\n",
            "[67]\tvalidation_0-auc:0.804788\n",
            "[68]\tvalidation_0-auc:0.804427\n",
            "[69]\tvalidation_0-auc:0.804427\n",
            "[70]\tvalidation_0-auc:0.804118\n",
            "[71]\tvalidation_0-auc:0.804118\n",
            "[72]\tvalidation_0-auc:0.803398\n",
            "[73]\tvalidation_0-auc:0.803192\n",
            "Stopping. Best iteration:\n",
            "[23]\tvalidation_0-auc:0.825174\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6834 - accuracy: 0.5697 - val_loss: 0.7748 - val_accuracy: 0.2802\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6300 - accuracy: 0.6493 - val_loss: 0.7130 - val_accuracy: 0.4942\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6045 - accuracy: 0.6548 - val_loss: 0.6968 - val_accuracy: 0.4514\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5864 - accuracy: 0.6760 - val_loss: 0.5824 - val_accuracy: 0.7588\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5598 - accuracy: 0.7076 - val_loss: 0.5579 - val_accuracy: 0.7626\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6503 - accuracy: 0.6253 - val_loss: 0.5954 - val_accuracy: 0.7782\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5680 - accuracy: 0.6939 - val_loss: 0.5850 - val_accuracy: 0.6848\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5608 - accuracy: 0.7111 - val_loss: 0.5401 - val_accuracy: 0.7860\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5564 - accuracy: 0.7241 - val_loss: 0.6232 - val_accuracy: 0.6226\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5297 - accuracy: 0.7220 - val_loss: 0.5007 - val_accuracy: 0.7938\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.632658\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.720571\n",
            "[2]\tvalidation_0-auc:0.718018\n",
            "[3]\tvalidation_0-auc:0.702665\n",
            "[4]\tvalidation_0-auc:0.699812\n",
            "[5]\tvalidation_0-auc:0.697222\n",
            "[6]\tvalidation_0-auc:0.666141\n",
            "[7]\tvalidation_0-auc:0.639189\n",
            "[8]\tvalidation_0-auc:0.63964\n",
            "[9]\tvalidation_0-auc:0.658108\n",
            "[10]\tvalidation_0-auc:0.652628\n",
            "[11]\tvalidation_0-auc:0.655368\n",
            "[12]\tvalidation_0-auc:0.659647\n",
            "[13]\tvalidation_0-auc:0.653829\n",
            "[14]\tvalidation_0-auc:0.65488\n",
            "[15]\tvalidation_0-auc:0.643619\n",
            "[16]\tvalidation_0-auc:0.644707\n",
            "[17]\tvalidation_0-auc:0.636562\n",
            "[18]\tvalidation_0-auc:0.636974\n",
            "[19]\tvalidation_0-auc:0.638063\n",
            "[20]\tvalidation_0-auc:0.634272\n",
            "[21]\tvalidation_0-auc:0.633596\n",
            "[22]\tvalidation_0-auc:0.631532\n",
            "[23]\tvalidation_0-auc:0.631757\n",
            "[24]\tvalidation_0-auc:0.635135\n",
            "[25]\tvalidation_0-auc:0.635098\n",
            "[26]\tvalidation_0-auc:0.629317\n",
            "[27]\tvalidation_0-auc:0.63247\n",
            "[28]\tvalidation_0-auc:0.632132\n",
            "[29]\tvalidation_0-auc:0.636824\n",
            "[30]\tvalidation_0-auc:0.632695\n",
            "[31]\tvalidation_0-auc:0.635473\n",
            "[32]\tvalidation_0-auc:0.628116\n",
            "[33]\tvalidation_0-auc:0.63262\n",
            "[34]\tvalidation_0-auc:0.639152\n",
            "[35]\tvalidation_0-auc:0.637425\n",
            "[36]\tvalidation_0-auc:0.634722\n",
            "[37]\tvalidation_0-auc:0.636824\n",
            "[38]\tvalidation_0-auc:0.633408\n",
            "[39]\tvalidation_0-auc:0.638026\n",
            "[40]\tvalidation_0-auc:0.638476\n",
            "[41]\tvalidation_0-auc:0.633146\n",
            "[42]\tvalidation_0-auc:0.633596\n",
            "[43]\tvalidation_0-auc:0.636899\n",
            "[44]\tvalidation_0-auc:0.638551\n",
            "[45]\tvalidation_0-auc:0.638701\n",
            "[46]\tvalidation_0-auc:0.638701\n",
            "[47]\tvalidation_0-auc:0.639827\n",
            "[48]\tvalidation_0-auc:0.64039\n",
            "[49]\tvalidation_0-auc:0.641742\n",
            "[50]\tvalidation_0-auc:0.641216\n",
            "[51]\tvalidation_0-auc:0.641216\n",
            "Stopping. Best iteration:\n",
            "[1]\tvalidation_0-auc:0.720571\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.5137931034482759 |  0.4230769230769231 | 0.9428571428571428 |  0.5840707964601769 |\n",
            "|     GRU 0.1      | 0.7206896551724138 |  0.5769230769230769 | 0.8571428571428571 |  0.689655172413793  |\n",
            "|   XGBoost 0.1    | 0.7103448275862069 |  0.567741935483871  | 0.8380952380952381 |  0.676923076923077  |\n",
            "|    Logreg 0.1    | 0.6551724137931034 |  0.5135135135135135 | 0.9047619047619048 |  0.6551724137931034 |\n",
            "|     SVM 0.1      | 0.696551724137931  |  0.5502958579881657 | 0.8857142857142857 |  0.6788321167883212 |\n",
            "|  LSTM beta 0.1   | 0.7626459143968871 |  0.5901639344262295 |        0.5         |  0.5413533834586466 |\n",
            "|   GRU beta 0.1   | 0.7937743190661478 |  0.6610169491525424 | 0.5416666666666666 |  0.5954198473282444 |\n",
            "| XGBoost beta 0.1 | 0.603112840466926  | 0.36607142857142855 | 0.5694444444444444 | 0.44565217391304346 |\n",
            "| logreg beta 0.1  | 0.6731517509727627 | 0.45161290322580644 | 0.7777777777777778 |  0.5714285714285714 |\n",
            "|   svm beta 0.1   | 0.6809338521400778 | 0.43902439024390244 |        0.5         |  0.4675324675324676 |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6657 - accuracy: 0.6275 - val_loss: 0.8009 - val_accuracy: 0.3655\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6483 - accuracy: 0.6416 - val_loss: 0.9306 - val_accuracy: 0.3655\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6291 - accuracy: 0.6477 - val_loss: 0.7786 - val_accuracy: 0.4414\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6036 - accuracy: 0.6698 - val_loss: 0.6833 - val_accuracy: 0.6034\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6045 - accuracy: 0.6685 - val_loss: 0.7364 - val_accuracy: 0.5448\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6503 - accuracy: 0.6403 - val_loss: 0.8259 - val_accuracy: 0.3759\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.6087 - accuracy: 0.6691 - val_loss: 0.6781 - val_accuracy: 0.5690\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5907 - accuracy: 0.6779 - val_loss: 0.6796 - val_accuracy: 0.5897\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5948 - accuracy: 0.6886 - val_loss: 0.6942 - val_accuracy: 0.5655\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5887 - accuracy: 0.7020 - val_loss: 0.6734 - val_accuracy: 0.6000\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.716238\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.70888\n",
            "[2]\tvalidation_0-auc:0.693832\n",
            "[3]\tvalidation_0-auc:0.688577\n",
            "[4]\tvalidation_0-auc:0.697626\n",
            "[5]\tvalidation_0-auc:0.695703\n",
            "[6]\tvalidation_0-auc:0.693781\n",
            "[7]\tvalidation_0-auc:0.693576\n",
            "[8]\tvalidation_0-auc:0.696011\n",
            "[9]\tvalidation_0-auc:0.689525\n",
            "[10]\tvalidation_0-auc:0.68668\n",
            "[11]\tvalidation_0-auc:0.686782\n",
            "[12]\tvalidation_0-auc:0.685449\n",
            "[13]\tvalidation_0-auc:0.687987\n",
            "[14]\tvalidation_0-auc:0.687782\n",
            "[15]\tvalidation_0-auc:0.687961\n",
            "[16]\tvalidation_0-auc:0.686526\n",
            "[17]\tvalidation_0-auc:0.683962\n",
            "[18]\tvalidation_0-auc:0.681911\n",
            "[19]\tvalidation_0-auc:0.683296\n",
            "[20]\tvalidation_0-auc:0.682219\n",
            "[21]\tvalidation_0-auc:0.679732\n",
            "[22]\tvalidation_0-auc:0.68045\n",
            "[23]\tvalidation_0-auc:0.67963\n",
            "[24]\tvalidation_0-auc:0.679579\n",
            "[25]\tvalidation_0-auc:0.679373\n",
            "[26]\tvalidation_0-auc:0.678707\n",
            "[27]\tvalidation_0-auc:0.678245\n",
            "[28]\tvalidation_0-auc:0.674964\n",
            "[29]\tvalidation_0-auc:0.67481\n",
            "[30]\tvalidation_0-auc:0.672785\n",
            "[31]\tvalidation_0-auc:0.672067\n",
            "[32]\tvalidation_0-auc:0.672324\n",
            "[33]\tvalidation_0-auc:0.671401\n",
            "[34]\tvalidation_0-auc:0.670273\n",
            "[35]\tvalidation_0-auc:0.669401\n",
            "[36]\tvalidation_0-auc:0.669093\n",
            "[37]\tvalidation_0-auc:0.670145\n",
            "[38]\tvalidation_0-auc:0.667581\n",
            "[39]\tvalidation_0-auc:0.667325\n",
            "[40]\tvalidation_0-auc:0.665171\n",
            "[41]\tvalidation_0-auc:0.664761\n",
            "[42]\tvalidation_0-auc:0.660762\n",
            "[43]\tvalidation_0-auc:0.65907\n",
            "[44]\tvalidation_0-auc:0.659326\n",
            "[45]\tvalidation_0-auc:0.659736\n",
            "[46]\tvalidation_0-auc:0.659942\n",
            "[47]\tvalidation_0-auc:0.658045\n",
            "[48]\tvalidation_0-auc:0.657019\n",
            "[49]\tvalidation_0-auc:0.656045\n",
            "[50]\tvalidation_0-auc:0.655789\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.716238\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6402 - accuracy: 0.6424 - val_loss: 0.6713 - val_accuracy: 0.5837\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5624 - accuracy: 0.7172 - val_loss: 0.7053 - val_accuracy: 0.6109\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5628 - accuracy: 0.7083 - val_loss: 0.6195 - val_accuracy: 0.7082\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5431 - accuracy: 0.7090 - val_loss: 0.5703 - val_accuracy: 0.7471\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5146 - accuracy: 0.7474 - val_loss: 0.4958 - val_accuracy: 0.7938\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 2s 13ms/step - loss: 0.5943 - accuracy: 0.6644 - val_loss: 0.5291 - val_accuracy: 0.8366\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5220 - accuracy: 0.7268 - val_loss: 0.6562 - val_accuracy: 0.5603\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4908 - accuracy: 0.7584 - val_loss: 0.5112 - val_accuracy: 0.7510\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.4834 - accuracy: 0.7577 - val_loss: 0.5606 - val_accuracy: 0.6848\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4879 - accuracy: 0.7598 - val_loss: 0.4917 - val_accuracy: 0.7510\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.86588\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.896404\n",
            "[2]\tvalidation_0-auc:0.895399\n",
            "[3]\tvalidation_0-auc:0.902062\n",
            "[4]\tvalidation_0-auc:0.903551\n",
            "[5]\tvalidation_0-auc:0.902509\n",
            "[6]\tvalidation_0-auc:0.903923\n",
            "[7]\tvalidation_0-auc:0.904854\n",
            "[8]\tvalidation_0-auc:0.904891\n",
            "[9]\tvalidation_0-auc:0.906083\n",
            "[10]\tvalidation_0-auc:0.905896\n",
            "[11]\tvalidation_0-auc:0.906715\n",
            "[12]\tvalidation_0-auc:0.908502\n",
            "[13]\tvalidation_0-auc:0.909544\n",
            "[14]\tvalidation_0-auc:0.909917\n",
            "[15]\tvalidation_0-auc:0.910661\n",
            "[16]\tvalidation_0-auc:0.910661\n",
            "[17]\tvalidation_0-auc:0.910289\n",
            "[18]\tvalidation_0-auc:0.91014\n",
            "[19]\tvalidation_0-auc:0.912001\n",
            "[20]\tvalidation_0-auc:0.912746\n",
            "[21]\tvalidation_0-auc:0.912671\n",
            "[22]\tvalidation_0-auc:0.912299\n",
            "[23]\tvalidation_0-auc:0.912895\n",
            "[24]\tvalidation_0-auc:0.912597\n",
            "[25]\tvalidation_0-auc:0.912076\n",
            "[26]\tvalidation_0-auc:0.911741\n",
            "[27]\tvalidation_0-auc:0.913118\n",
            "[28]\tvalidation_0-auc:0.912895\n",
            "[29]\tvalidation_0-auc:0.91282\n",
            "[30]\tvalidation_0-auc:0.912522\n",
            "[31]\tvalidation_0-auc:0.913118\n",
            "[32]\tvalidation_0-auc:0.913714\n",
            "[33]\tvalidation_0-auc:0.913267\n",
            "[34]\tvalidation_0-auc:0.913416\n",
            "[35]\tvalidation_0-auc:0.912225\n",
            "[36]\tvalidation_0-auc:0.911406\n",
            "[37]\tvalidation_0-auc:0.910363\n",
            "[38]\tvalidation_0-auc:0.910512\n",
            "[39]\tvalidation_0-auc:0.909991\n",
            "[40]\tvalidation_0-auc:0.910587\n",
            "[41]\tvalidation_0-auc:0.910363\n",
            "[42]\tvalidation_0-auc:0.910363\n",
            "[43]\tvalidation_0-auc:0.909321\n",
            "[44]\tvalidation_0-auc:0.908465\n",
            "[45]\tvalidation_0-auc:0.908539\n",
            "[46]\tvalidation_0-auc:0.908986\n",
            "[47]\tvalidation_0-auc:0.908725\n",
            "[48]\tvalidation_0-auc:0.909917\n",
            "[49]\tvalidation_0-auc:0.910587\n",
            "[50]\tvalidation_0-auc:0.910587\n",
            "[51]\tvalidation_0-auc:0.910289\n",
            "[52]\tvalidation_0-auc:0.910363\n",
            "[53]\tvalidation_0-auc:0.910214\n",
            "[54]\tvalidation_0-auc:0.909842\n",
            "[55]\tvalidation_0-auc:0.909917\n",
            "[56]\tvalidation_0-auc:0.909768\n",
            "[57]\tvalidation_0-auc:0.911033\n",
            "[58]\tvalidation_0-auc:0.911927\n",
            "[59]\tvalidation_0-auc:0.912225\n",
            "[60]\tvalidation_0-auc:0.911406\n",
            "[61]\tvalidation_0-auc:0.912001\n",
            "[62]\tvalidation_0-auc:0.91215\n",
            "[63]\tvalidation_0-auc:0.912225\n",
            "[64]\tvalidation_0-auc:0.911555\n",
            "[65]\tvalidation_0-auc:0.911182\n",
            "[66]\tvalidation_0-auc:0.91215\n",
            "[67]\tvalidation_0-auc:0.912969\n",
            "[68]\tvalidation_0-auc:0.912597\n",
            "[69]\tvalidation_0-auc:0.912597\n",
            "[70]\tvalidation_0-auc:0.913043\n",
            "[71]\tvalidation_0-auc:0.912225\n",
            "[72]\tvalidation_0-auc:0.912373\n",
            "[73]\tvalidation_0-auc:0.911331\n",
            "[74]\tvalidation_0-auc:0.91148\n",
            "[75]\tvalidation_0-auc:0.909917\n",
            "[76]\tvalidation_0-auc:0.90947\n",
            "[77]\tvalidation_0-auc:0.909247\n",
            "[78]\tvalidation_0-auc:0.909247\n",
            "[79]\tvalidation_0-auc:0.909247\n",
            "[80]\tvalidation_0-auc:0.909917\n",
            "[81]\tvalidation_0-auc:0.909768\n",
            "[82]\tvalidation_0-auc:0.909619\n",
            "Stopping. Best iteration:\n",
            "[32]\tvalidation_0-auc:0.913714\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|     LSTM 0.2     | 0.5448275862068965 | 0.44537815126050423 |        1.0         | 0.616279069767442  |\n",
            "|     GRU 0.2      |        0.6         | 0.47474747474747475 | 0.8867924528301887 | 0.618421052631579  |\n",
            "|   XGBoost 0.2    | 0.5620689655172414 |  0.4461538461538462 | 0.8207547169811321 | 0.5780730897009967 |\n",
            "|    Logreg 0.2    |        0.5         | 0.41975308641975306 | 0.9622641509433962 | 0.5845272206303724 |\n",
            "|     SVM 0.2      | 0.5689655172413793 | 0.45701357466063347 | 0.9528301886792453 | 0.617737003058104  |\n",
            "|  LSTM beta 0.2   | 0.7937743190661478 |        0.625        | 0.684931506849315  |  0.65359477124183  |\n",
            "|   GRU beta 0.2   | 0.7509727626459144 |  0.5436893203883495 | 0.7671232876712328 | 0.6363636363636364 |\n",
            "| XGBoost beta 0.2 | 0.8093385214007782 |         0.62        | 0.8493150684931506 | 0.7167630057803469 |\n",
            "| logreg beta 0.2  | 0.6770428015564203 |  0.4652777777777778 | 0.9178082191780822 | 0.6175115207373272 |\n",
            "|   svm beta 0.2   | 0.7937743190661478 |  0.5961538461538461 | 0.8493150684931506 | 0.7005649717514123 |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6718 - accuracy: 0.6154 - val_loss: 0.7778 - val_accuracy: 0.3655\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6453 - accuracy: 0.6295 - val_loss: 0.8345 - val_accuracy: 0.4172\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6312 - accuracy: 0.6322 - val_loss: 0.6975 - val_accuracy: 0.5138\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6260 - accuracy: 0.6443 - val_loss: 0.7299 - val_accuracy: 0.5517\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6209 - accuracy: 0.6550 - val_loss: 0.7153 - val_accuracy: 0.5793\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6589 - accuracy: 0.6215 - val_loss: 0.7534 - val_accuracy: 0.3793\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.6173 - accuracy: 0.6463 - val_loss: 0.7204 - val_accuracy: 0.4759\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5935 - accuracy: 0.6772 - val_loss: 0.7112 - val_accuracy: 0.5172\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5904 - accuracy: 0.6846 - val_loss: 0.6913 - val_accuracy: 0.5483\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5870 - accuracy: 0.6819 - val_loss: 0.6755 - val_accuracy: 0.5897\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.665889\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.679502\n",
            "[2]\tvalidation_0-auc:0.685859\n",
            "[3]\tvalidation_0-auc:0.687885\n",
            "[4]\tvalidation_0-auc:0.686039\n",
            "[5]\tvalidation_0-auc:0.682091\n",
            "[6]\tvalidation_0-auc:0.681732\n",
            "[7]\tvalidation_0-auc:0.681732\n",
            "[8]\tvalidation_0-auc:0.682886\n",
            "[9]\tvalidation_0-auc:0.68186\n",
            "[10]\tvalidation_0-auc:0.671965\n",
            "[11]\tvalidation_0-auc:0.668478\n",
            "[12]\tvalidation_0-auc:0.671785\n",
            "[13]\tvalidation_0-auc:0.67199\n",
            "[14]\tvalidation_0-auc:0.66917\n",
            "[15]\tvalidation_0-auc:0.670914\n",
            "[16]\tvalidation_0-auc:0.666094\n",
            "[17]\tvalidation_0-auc:0.66653\n",
            "[18]\tvalidation_0-auc:0.668478\n",
            "[19]\tvalidation_0-auc:0.670016\n",
            "[20]\tvalidation_0-auc:0.674092\n",
            "[21]\tvalidation_0-auc:0.672375\n",
            "[22]\tvalidation_0-auc:0.672477\n",
            "[23]\tvalidation_0-auc:0.671067\n",
            "[24]\tvalidation_0-auc:0.669581\n",
            "[25]\tvalidation_0-auc:0.670555\n",
            "[26]\tvalidation_0-auc:0.666325\n",
            "[27]\tvalidation_0-auc:0.665094\n",
            "[28]\tvalidation_0-auc:0.667196\n",
            "[29]\tvalidation_0-auc:0.66571\n",
            "[30]\tvalidation_0-auc:0.667812\n",
            "[31]\tvalidation_0-auc:0.667453\n",
            "[32]\tvalidation_0-auc:0.668837\n",
            "[33]\tvalidation_0-auc:0.669145\n",
            "[34]\tvalidation_0-auc:0.669196\n",
            "[35]\tvalidation_0-auc:0.67058\n",
            "[36]\tvalidation_0-auc:0.668042\n",
            "[37]\tvalidation_0-auc:0.666299\n",
            "[38]\tvalidation_0-auc:0.665376\n",
            "[39]\tvalidation_0-auc:0.664351\n",
            "[40]\tvalidation_0-auc:0.663428\n",
            "[41]\tvalidation_0-auc:0.662146\n",
            "[42]\tvalidation_0-auc:0.662044\n",
            "[43]\tvalidation_0-auc:0.660326\n",
            "[44]\tvalidation_0-auc:0.660326\n",
            "[45]\tvalidation_0-auc:0.659762\n",
            "[46]\tvalidation_0-auc:0.659249\n",
            "[47]\tvalidation_0-auc:0.65848\n",
            "[48]\tvalidation_0-auc:0.658224\n",
            "[49]\tvalidation_0-auc:0.658224\n",
            "[50]\tvalidation_0-auc:0.658429\n",
            "[51]\tvalidation_0-auc:0.658121\n",
            "[52]\tvalidation_0-auc:0.65725\n",
            "[53]\tvalidation_0-auc:0.656686\n",
            "Stopping. Best iteration:\n",
            "[3]\tvalidation_0-auc:0.687885\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6659 - accuracy: 0.6115 - val_loss: 0.7240 - val_accuracy: 0.3268\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6033 - accuracy: 0.6740 - val_loss: 0.7639 - val_accuracy: 0.3113\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5732 - accuracy: 0.6932 - val_loss: 0.5717 - val_accuracy: 0.7588\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5618 - accuracy: 0.7028 - val_loss: 0.6250 - val_accuracy: 0.6809\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5338 - accuracy: 0.7330 - val_loss: 0.5306 - val_accuracy: 0.7393\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6132 - accuracy: 0.6582 - val_loss: 0.6571 - val_accuracy: 0.5486\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5426 - accuracy: 0.7227 - val_loss: 0.5649 - val_accuracy: 0.7237\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5096 - accuracy: 0.7358 - val_loss: 0.6291 - val_accuracy: 0.6109\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5083 - accuracy: 0.7426 - val_loss: 0.5328 - val_accuracy: 0.7549\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5005 - accuracy: 0.7591 - val_loss: 0.4720 - val_accuracy: 0.8210\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.860706\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.85985\n",
            "[2]\tvalidation_0-auc:0.886689\n",
            "[3]\tvalidation_0-auc:0.894468\n",
            "[4]\tvalidation_0-auc:0.896069\n",
            "[5]\tvalidation_0-auc:0.898786\n",
            "[6]\tvalidation_0-auc:0.900238\n",
            "[7]\tvalidation_0-auc:0.89994\n",
            "[8]\tvalidation_0-auc:0.900313\n",
            "[9]\tvalidation_0-auc:0.898563\n",
            "[10]\tvalidation_0-auc:0.902918\n",
            "[11]\tvalidation_0-auc:0.904147\n",
            "[12]\tvalidation_0-auc:0.905487\n",
            "[13]\tvalidation_0-auc:0.90679\n",
            "[14]\tvalidation_0-auc:0.908986\n",
            "[15]\tvalidation_0-auc:0.90973\n",
            "[16]\tvalidation_0-auc:0.910624\n",
            "[17]\tvalidation_0-auc:0.909507\n",
            "[18]\tvalidation_0-auc:0.909433\n",
            "[19]\tvalidation_0-auc:0.909656\n",
            "[20]\tvalidation_0-auc:0.910326\n",
            "[21]\tvalidation_0-auc:0.908614\n",
            "[22]\tvalidation_0-auc:0.909098\n",
            "[23]\tvalidation_0-auc:0.909023\n",
            "[24]\tvalidation_0-auc:0.908725\n",
            "[25]\tvalidation_0-auc:0.908018\n",
            "[26]\tvalidation_0-auc:0.908242\n",
            "[27]\tvalidation_0-auc:0.908018\n",
            "[28]\tvalidation_0-auc:0.907199\n",
            "[29]\tvalidation_0-auc:0.906231\n",
            "[30]\tvalidation_0-auc:0.905934\n",
            "[31]\tvalidation_0-auc:0.904817\n",
            "[32]\tvalidation_0-auc:0.90504\n",
            "[33]\tvalidation_0-auc:0.90504\n",
            "[34]\tvalidation_0-auc:0.904966\n",
            "[35]\tvalidation_0-auc:0.903477\n",
            "[36]\tvalidation_0-auc:0.901951\n",
            "[37]\tvalidation_0-auc:0.901951\n",
            "[38]\tvalidation_0-auc:0.901802\n",
            "[39]\tvalidation_0-auc:0.901951\n",
            "[40]\tvalidation_0-auc:0.902621\n",
            "[41]\tvalidation_0-auc:0.901206\n",
            "[42]\tvalidation_0-auc:0.900908\n",
            "[43]\tvalidation_0-auc:0.900089\n",
            "[44]\tvalidation_0-auc:0.89901\n",
            "[45]\tvalidation_0-auc:0.89901\n",
            "[46]\tvalidation_0-auc:0.898712\n",
            "[47]\tvalidation_0-auc:0.897521\n",
            "[48]\tvalidation_0-auc:0.897484\n",
            "[49]\tvalidation_0-auc:0.897856\n",
            "[50]\tvalidation_0-auc:0.898005\n",
            "[51]\tvalidation_0-auc:0.897298\n",
            "[52]\tvalidation_0-auc:0.897298\n",
            "[53]\tvalidation_0-auc:0.897595\n",
            "[54]\tvalidation_0-auc:0.897223\n",
            "[55]\tvalidation_0-auc:0.899196\n",
            "[56]\tvalidation_0-auc:0.897781\n",
            "[57]\tvalidation_0-auc:0.896888\n",
            "[58]\tvalidation_0-auc:0.896888\n",
            "[59]\tvalidation_0-auc:0.897484\n",
            "[60]\tvalidation_0-auc:0.898005\n",
            "[61]\tvalidation_0-auc:0.899457\n",
            "[62]\tvalidation_0-auc:0.899084\n",
            "[63]\tvalidation_0-auc:0.897074\n",
            "[64]\tvalidation_0-auc:0.896553\n",
            "[65]\tvalidation_0-auc:0.896181\n",
            "[66]\tvalidation_0-auc:0.89566\n",
            "Stopping. Best iteration:\n",
            "[16]\tvalidation_0-auc:0.910624\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.5793103448275863 |  0.4603960396039604 | 0.8773584905660378 | 0.6038961038961039 |\n",
            "|      GRU 0.15     | 0.5896551724137931 | 0.46766169154228854 | 0.8867924528301887 | 0.6123778501628665 |\n",
            "|    XGBoost 0.15   | 0.5413793103448276 | 0.42780748663101603 | 0.7547169811320755 | 0.5460750853242321 |\n",
            "|    Logreg 0.15    | 0.5241379310344828 | 0.43103448275862066 | 0.9433962264150944 | 0.5917159763313609 |\n",
            "|      SVM 0.15     | 0.5620689655172414 |  0.4507042253521127 | 0.9056603773584906 | 0.6018808777429466 |\n",
            "|   LSTM beta 0.15  | 0.7392996108949417 |  0.5272727272727272 | 0.7945205479452054 | 0.6338797814207651 |\n",
            "|   GRU beta 0.15   | 0.8210116731517509 |  0.7288135593220338 | 0.589041095890411  | 0.6515151515151515 |\n",
            "| XGBoost beta 0.15 | 0.7898832684824902 |  0.5887850467289719 | 0.863013698630137  |        0.7         |\n",
            "|  logreg beta 0.15 | 0.6731517509727627 | 0.46153846153846156 | 0.9041095890410958 | 0.6111111111111112 |\n",
            "|   svm beta 0.15   | 0.8132295719844358 |  0.6288659793814433 | 0.8356164383561644 | 0.7176470588235294 |\n",
            "+-------------------+--------------------+---------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "25tTnnbIHUKt",
        "outputId": "fea73f70-8c58-4191-d7b1-520ce56d9628"
      },
      "source": [
        "Result_cross.to_csv('CPRI_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.423077</td>\n",
              "      <td>0.513793</td>\n",
              "      <td>0.584071</td>\n",
              "      <td>0.942857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.720690</td>\n",
              "      <td>0.689655</td>\n",
              "      <td>0.857143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.567742</td>\n",
              "      <td>0.710345</td>\n",
              "      <td>0.676923</td>\n",
              "      <td>0.838095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.513514</td>\n",
              "      <td>0.655172</td>\n",
              "      <td>0.655172</td>\n",
              "      <td>0.904762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.550296</td>\n",
              "      <td>0.696552</td>\n",
              "      <td>0.678832</td>\n",
              "      <td>0.885714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.590164</td>\n",
              "      <td>0.762646</td>\n",
              "      <td>0.541353</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.661017</td>\n",
              "      <td>0.793774</td>\n",
              "      <td>0.595420</td>\n",
              "      <td>0.541667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.366071</td>\n",
              "      <td>0.603113</td>\n",
              "      <td>0.445652</td>\n",
              "      <td>0.569444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.451613</td>\n",
              "      <td>0.673152</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.777778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.439024</td>\n",
              "      <td>0.680934</td>\n",
              "      <td>0.467532</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.445378</td>\n",
              "      <td>0.544828</td>\n",
              "      <td>0.616279</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.474747</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.618421</td>\n",
              "      <td>0.886792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.446154</td>\n",
              "      <td>0.562069</td>\n",
              "      <td>0.578073</td>\n",
              "      <td>0.820755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.419753</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.584527</td>\n",
              "      <td>0.962264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.457014</td>\n",
              "      <td>0.568966</td>\n",
              "      <td>0.617737</td>\n",
              "      <td>0.952830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.793774</td>\n",
              "      <td>0.653595</td>\n",
              "      <td>0.684932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.543689</td>\n",
              "      <td>0.750973</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.767123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.620000</td>\n",
              "      <td>0.809339</td>\n",
              "      <td>0.716763</td>\n",
              "      <td>0.849315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.465278</td>\n",
              "      <td>0.677043</td>\n",
              "      <td>0.617512</td>\n",
              "      <td>0.917808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.596154</td>\n",
              "      <td>0.793774</td>\n",
              "      <td>0.700565</td>\n",
              "      <td>0.849315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.460396</td>\n",
              "      <td>0.579310</td>\n",
              "      <td>0.603896</td>\n",
              "      <td>0.877358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.467662</td>\n",
              "      <td>0.589655</td>\n",
              "      <td>0.612378</td>\n",
              "      <td>0.886792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.427807</td>\n",
              "      <td>0.541379</td>\n",
              "      <td>0.546075</td>\n",
              "      <td>0.754717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.431034</td>\n",
              "      <td>0.524138</td>\n",
              "      <td>0.591716</td>\n",
              "      <td>0.943396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.450704</td>\n",
              "      <td>0.562069</td>\n",
              "      <td>0.601881</td>\n",
              "      <td>0.905660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.527273</td>\n",
              "      <td>0.739300</td>\n",
              "      <td>0.633880</td>\n",
              "      <td>0.794521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.728814</td>\n",
              "      <td>0.821012</td>\n",
              "      <td>0.651515</td>\n",
              "      <td>0.589041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.588785</td>\n",
              "      <td>0.789883</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.863014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.673152</td>\n",
              "      <td>0.611111</td>\n",
              "      <td>0.904110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.628866</td>\n",
              "      <td>0.813230</td>\n",
              "      <td>0.717647</td>\n",
              "      <td>0.835616</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  CPRI  0.423077  0.513793  0.584071  0.942857\n",
              "1            GRU 0.1  CPRI  0.576923  0.720690  0.689655  0.857143\n",
              "2        XGBoost 0.1  CPRI  0.567742  0.710345  0.676923  0.838095\n",
              "3         Logreg 0.1  CPRI  0.513514  0.655172  0.655172  0.904762\n",
              "4            SVM 0.1  CPRI  0.550296  0.696552  0.678832  0.885714\n",
              "5      LSTM beta 0.1  CPRI  0.590164  0.762646  0.541353  0.500000\n",
              "6       GRU beta 0.1  CPRI  0.661017  0.793774  0.595420  0.541667\n",
              "7   XGBoost beta 0.1  CPRI  0.366071  0.603113  0.445652  0.569444\n",
              "8    logreg beta 0.1  CPRI  0.451613  0.673152  0.571429  0.777778\n",
              "9       svm beta 0.1  CPRI  0.439024  0.680934  0.467532  0.500000\n",
              "0           LSTM 0.2  CPRI  0.445378  0.544828  0.616279  1.000000\n",
              "1            GRU 0.2  CPRI  0.474747  0.600000  0.618421  0.886792\n",
              "2        XGBoost 0.2  CPRI  0.446154  0.562069  0.578073  0.820755\n",
              "3         Logreg 0.2  CPRI  0.419753  0.500000  0.584527  0.962264\n",
              "4            SVM 0.2  CPRI  0.457014  0.568966  0.617737  0.952830\n",
              "5      LSTM beta 0.2  CPRI  0.625000  0.793774  0.653595  0.684932\n",
              "6       GRU beta 0.2  CPRI  0.543689  0.750973  0.636364  0.767123\n",
              "7   XGBoost beta 0.2  CPRI  0.620000  0.809339  0.716763  0.849315\n",
              "8    logreg beta 0.2  CPRI  0.465278  0.677043  0.617512  0.917808\n",
              "9       svm beta 0.2  CPRI  0.596154  0.793774  0.700565  0.849315\n",
              "0          LSTM 0.15  CPRI  0.460396  0.579310  0.603896  0.877358\n",
              "1           GRU 0.15  CPRI  0.467662  0.589655  0.612378  0.886792\n",
              "2       XGBoost 0.15  CPRI  0.427807  0.541379  0.546075  0.754717\n",
              "3        Logreg 0.15  CPRI  0.431034  0.524138  0.591716  0.943396\n",
              "4           SVM 0.15  CPRI  0.450704  0.562069  0.601881  0.905660\n",
              "5     LSTM beta 0.15  CPRI  0.527273  0.739300  0.633880  0.794521\n",
              "6      GRU beta 0.15  CPRI  0.728814  0.821012  0.651515  0.589041\n",
              "7  XGBoost beta 0.15  CPRI  0.588785  0.789883  0.700000  0.863014\n",
              "8   logreg beta 0.15  CPRI  0.461538  0.673152  0.611111  0.904110\n",
              "9      svm beta 0.15  CPRI  0.628866  0.813230  0.717647  0.835616"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irmrld7pHUKu"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iehVFbMWHUKv"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sp9DHgTNHUKv",
        "outputId": "8b823134-2adf-4dc7-b3d8-87ca7bc9e6d2"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2100)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"CPRI\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6806 - accuracy: 0.5678 - val_loss: 0.6912 - val_accuracy: 0.4448\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6543 - accuracy: 0.6107 - val_loss: 0.6448 - val_accuracy: 0.6241\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6132 - accuracy: 0.6685 - val_loss: 0.6168 - val_accuracy: 0.6690\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5873 - accuracy: 0.6933 - val_loss: 0.6643 - val_accuracy: 0.5345\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5932 - accuracy: 0.6718 - val_loss: 0.5984 - val_accuracy: 0.6897\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6597 - accuracy: 0.6148 - val_loss: 0.6102 - val_accuracy: 0.7310\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5914 - accuracy: 0.7007 - val_loss: 0.5793 - val_accuracy: 0.7379\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5733 - accuracy: 0.7094 - val_loss: 0.6129 - val_accuracy: 0.6621\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5722 - accuracy: 0.7121 - val_loss: 0.5849 - val_accuracy: 0.6862\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5685 - accuracy: 0.7154 - val_loss: 0.5910 - val_accuracy: 0.6828\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.794131\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.809292\n",
            "[2]\tvalidation_0-auc:0.807207\n",
            "[3]\tvalidation_0-auc:0.813024\n",
            "[4]\tvalidation_0-auc:0.809575\n",
            "[5]\tvalidation_0-auc:0.809833\n",
            "[6]\tvalidation_0-auc:0.807362\n",
            "[7]\tvalidation_0-auc:0.80749\n",
            "[8]\tvalidation_0-auc:0.809447\n",
            "[9]\tvalidation_0-auc:0.810167\n",
            "[10]\tvalidation_0-auc:0.808391\n",
            "[11]\tvalidation_0-auc:0.80888\n",
            "[12]\tvalidation_0-auc:0.816628\n",
            "[13]\tvalidation_0-auc:0.818044\n",
            "[14]\tvalidation_0-auc:0.819485\n",
            "[15]\tvalidation_0-auc:0.818867\n",
            "[16]\tvalidation_0-auc:0.817864\n",
            "[17]\tvalidation_0-auc:0.817555\n",
            "[18]\tvalidation_0-auc:0.818867\n",
            "[19]\tvalidation_0-auc:0.820952\n",
            "[20]\tvalidation_0-auc:0.821184\n",
            "[21]\tvalidation_0-auc:0.822523\n",
            "[22]\tvalidation_0-auc:0.823089\n",
            "[23]\tvalidation_0-auc:0.825174\n",
            "[24]\tvalidation_0-auc:0.824865\n",
            "[25]\tvalidation_0-auc:0.824891\n",
            "[26]\tvalidation_0-auc:0.824221\n",
            "[27]\tvalidation_0-auc:0.824427\n",
            "[28]\tvalidation_0-auc:0.824041\n",
            "[29]\tvalidation_0-auc:0.824685\n",
            "[30]\tvalidation_0-auc:0.823758\n",
            "[31]\tvalidation_0-auc:0.823964\n",
            "[32]\tvalidation_0-auc:0.824067\n",
            "[33]\tvalidation_0-auc:0.822728\n",
            "[34]\tvalidation_0-auc:0.822728\n",
            "[35]\tvalidation_0-auc:0.823295\n",
            "[36]\tvalidation_0-auc:0.821828\n",
            "[37]\tvalidation_0-auc:0.820489\n",
            "[38]\tvalidation_0-auc:0.82018\n",
            "[39]\tvalidation_0-auc:0.818147\n",
            "[40]\tvalidation_0-auc:0.818095\n",
            "[41]\tvalidation_0-auc:0.81686\n",
            "[42]\tvalidation_0-auc:0.815624\n",
            "[43]\tvalidation_0-auc:0.813205\n",
            "[44]\tvalidation_0-auc:0.81305\n",
            "[45]\tvalidation_0-auc:0.813462\n",
            "[46]\tvalidation_0-auc:0.812741\n",
            "[47]\tvalidation_0-auc:0.811532\n",
            "[48]\tvalidation_0-auc:0.810862\n",
            "[49]\tvalidation_0-auc:0.810811\n",
            "[50]\tvalidation_0-auc:0.810347\n",
            "[51]\tvalidation_0-auc:0.810656\n",
            "[52]\tvalidation_0-auc:0.810553\n",
            "[53]\tvalidation_0-auc:0.809575\n",
            "[54]\tvalidation_0-auc:0.80834\n",
            "[55]\tvalidation_0-auc:0.807156\n",
            "[56]\tvalidation_0-auc:0.806795\n",
            "[57]\tvalidation_0-auc:0.805714\n",
            "[58]\tvalidation_0-auc:0.806075\n",
            "[59]\tvalidation_0-auc:0.806435\n",
            "[60]\tvalidation_0-auc:0.805457\n",
            "[61]\tvalidation_0-auc:0.80556\n",
            "[62]\tvalidation_0-auc:0.805869\n",
            "[63]\tvalidation_0-auc:0.805457\n",
            "[64]\tvalidation_0-auc:0.805714\n",
            "[65]\tvalidation_0-auc:0.80556\n",
            "[66]\tvalidation_0-auc:0.80556\n",
            "[67]\tvalidation_0-auc:0.804788\n",
            "[68]\tvalidation_0-auc:0.804427\n",
            "[69]\tvalidation_0-auc:0.804427\n",
            "[70]\tvalidation_0-auc:0.804118\n",
            "[71]\tvalidation_0-auc:0.804118\n",
            "[72]\tvalidation_0-auc:0.803398\n",
            "[73]\tvalidation_0-auc:0.803192\n",
            "Stopping. Best iteration:\n",
            "[23]\tvalidation_0-auc:0.825174\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6824 - accuracy: 0.5745 - val_loss: 0.7182 - val_accuracy: 0.3152\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6214 - accuracy: 0.6452 - val_loss: 0.6028 - val_accuracy: 0.7626\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6030 - accuracy: 0.6568 - val_loss: 0.6755 - val_accuracy: 0.5486\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5961 - accuracy: 0.6857 - val_loss: 0.6443 - val_accuracy: 0.6265\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5743 - accuracy: 0.6918 - val_loss: 0.7276 - val_accuracy: 0.4630\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 2s 13ms/step - loss: 0.6529 - accuracy: 0.6026 - val_loss: 0.7093 - val_accuracy: 0.4125\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5628 - accuracy: 0.7076 - val_loss: 0.6156 - val_accuracy: 0.6615\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5484 - accuracy: 0.7049 - val_loss: 0.5871 - val_accuracy: 0.6848\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5347 - accuracy: 0.7323 - val_loss: 0.5135 - val_accuracy: 0.8210\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5453 - accuracy: 0.7097 - val_loss: 0.5206 - val_accuracy: 0.8093\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.632658\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.720571\n",
            "[2]\tvalidation_0-auc:0.718018\n",
            "[3]\tvalidation_0-auc:0.702665\n",
            "[4]\tvalidation_0-auc:0.699812\n",
            "[5]\tvalidation_0-auc:0.697222\n",
            "[6]\tvalidation_0-auc:0.666141\n",
            "[7]\tvalidation_0-auc:0.639189\n",
            "[8]\tvalidation_0-auc:0.63964\n",
            "[9]\tvalidation_0-auc:0.658108\n",
            "[10]\tvalidation_0-auc:0.652628\n",
            "[11]\tvalidation_0-auc:0.655368\n",
            "[12]\tvalidation_0-auc:0.659647\n",
            "[13]\tvalidation_0-auc:0.653829\n",
            "[14]\tvalidation_0-auc:0.65488\n",
            "[15]\tvalidation_0-auc:0.643619\n",
            "[16]\tvalidation_0-auc:0.644707\n",
            "[17]\tvalidation_0-auc:0.636562\n",
            "[18]\tvalidation_0-auc:0.636974\n",
            "[19]\tvalidation_0-auc:0.638063\n",
            "[20]\tvalidation_0-auc:0.634272\n",
            "[21]\tvalidation_0-auc:0.633596\n",
            "[22]\tvalidation_0-auc:0.631532\n",
            "[23]\tvalidation_0-auc:0.631757\n",
            "[24]\tvalidation_0-auc:0.635135\n",
            "[25]\tvalidation_0-auc:0.635098\n",
            "[26]\tvalidation_0-auc:0.629317\n",
            "[27]\tvalidation_0-auc:0.63247\n",
            "[28]\tvalidation_0-auc:0.632132\n",
            "[29]\tvalidation_0-auc:0.636824\n",
            "[30]\tvalidation_0-auc:0.632695\n",
            "[31]\tvalidation_0-auc:0.635473\n",
            "[32]\tvalidation_0-auc:0.628116\n",
            "[33]\tvalidation_0-auc:0.63262\n",
            "[34]\tvalidation_0-auc:0.639152\n",
            "[35]\tvalidation_0-auc:0.637425\n",
            "[36]\tvalidation_0-auc:0.634722\n",
            "[37]\tvalidation_0-auc:0.636824\n",
            "[38]\tvalidation_0-auc:0.633408\n",
            "[39]\tvalidation_0-auc:0.638026\n",
            "[40]\tvalidation_0-auc:0.638476\n",
            "[41]\tvalidation_0-auc:0.633146\n",
            "[42]\tvalidation_0-auc:0.633596\n",
            "[43]\tvalidation_0-auc:0.636899\n",
            "[44]\tvalidation_0-auc:0.638551\n",
            "[45]\tvalidation_0-auc:0.638701\n",
            "[46]\tvalidation_0-auc:0.638701\n",
            "[47]\tvalidation_0-auc:0.639827\n",
            "[48]\tvalidation_0-auc:0.64039\n",
            "[49]\tvalidation_0-auc:0.641742\n",
            "[50]\tvalidation_0-auc:0.641216\n",
            "[51]\tvalidation_0-auc:0.641216\n",
            "Stopping. Best iteration:\n",
            "[1]\tvalidation_0-auc:0.720571\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+---------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |       Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+---------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.1     |  0.6896551724137931 |  0.543859649122807  | 0.8857142857142857 |  0.673913043478261  |\n",
            "|     GRU 0.1      |  0.6827586206896552 |  0.5371428571428571 | 0.8952380952380953 |  0.6714285714285714 |\n",
            "|   XGBoost 0.1    |  0.7103448275862069 |  0.567741935483871  | 0.8380952380952381 |  0.676923076923077  |\n",
            "|    Logreg 0.1    |  0.6551724137931034 |  0.5135135135135135 | 0.9047619047619048 |  0.6551724137931034 |\n",
            "|     SVM 0.1      |  0.696551724137931  |  0.5502958579881657 | 0.8857142857142857 |  0.6788321167883212 |\n",
            "|  LSTM beta 0.1   | 0.46303501945525294 |  0.3058823529411765 | 0.7222222222222222 | 0.42975206611570255 |\n",
            "|   GRU beta 0.1   |  0.8093385214007782 |  0.6825396825396826 | 0.5972222222222222 |  0.6370370370370372 |\n",
            "| XGBoost beta 0.1 |  0.603112840466926  | 0.36607142857142855 | 0.5694444444444444 | 0.44565217391304346 |\n",
            "| logreg beta 0.1  |  0.6731517509727627 | 0.45161290322580644 | 0.7777777777777778 |  0.5714285714285714 |\n",
            "|   svm beta 0.1   |  0.6809338521400778 | 0.43902439024390244 |        0.5         |  0.4675324675324676 |\n",
            "+------------------+---------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6666 - accuracy: 0.6295 - val_loss: 0.7489 - val_accuracy: 0.3655\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6616 - accuracy: 0.6309 - val_loss: 0.7737 - val_accuracy: 0.3655\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6450 - accuracy: 0.6309 - val_loss: 0.7459 - val_accuracy: 0.4345\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6094 - accuracy: 0.6698 - val_loss: 0.6920 - val_accuracy: 0.5448\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6018 - accuracy: 0.6658 - val_loss: 0.7212 - val_accuracy: 0.6000\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6516 - accuracy: 0.6289 - val_loss: 0.8412 - val_accuracy: 0.3793\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.6093 - accuracy: 0.6658 - val_loss: 0.6867 - val_accuracy: 0.5345\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5925 - accuracy: 0.6738 - val_loss: 0.6947 - val_accuracy: 0.5241\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.5853 - accuracy: 0.6893 - val_loss: 0.7621 - val_accuracy: 0.5034\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5829 - accuracy: 0.6926 - val_loss: 0.6231 - val_accuracy: 0.6172\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.716238\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.70888\n",
            "[2]\tvalidation_0-auc:0.693832\n",
            "[3]\tvalidation_0-auc:0.688577\n",
            "[4]\tvalidation_0-auc:0.697626\n",
            "[5]\tvalidation_0-auc:0.695703\n",
            "[6]\tvalidation_0-auc:0.693781\n",
            "[7]\tvalidation_0-auc:0.693576\n",
            "[8]\tvalidation_0-auc:0.696011\n",
            "[9]\tvalidation_0-auc:0.689525\n",
            "[10]\tvalidation_0-auc:0.68668\n",
            "[11]\tvalidation_0-auc:0.686782\n",
            "[12]\tvalidation_0-auc:0.685449\n",
            "[13]\tvalidation_0-auc:0.687987\n",
            "[14]\tvalidation_0-auc:0.687782\n",
            "[15]\tvalidation_0-auc:0.687961\n",
            "[16]\tvalidation_0-auc:0.686526\n",
            "[17]\tvalidation_0-auc:0.683962\n",
            "[18]\tvalidation_0-auc:0.681911\n",
            "[19]\tvalidation_0-auc:0.683296\n",
            "[20]\tvalidation_0-auc:0.682219\n",
            "[21]\tvalidation_0-auc:0.679732\n",
            "[22]\tvalidation_0-auc:0.68045\n",
            "[23]\tvalidation_0-auc:0.67963\n",
            "[24]\tvalidation_0-auc:0.679579\n",
            "[25]\tvalidation_0-auc:0.679373\n",
            "[26]\tvalidation_0-auc:0.678707\n",
            "[27]\tvalidation_0-auc:0.678245\n",
            "[28]\tvalidation_0-auc:0.674964\n",
            "[29]\tvalidation_0-auc:0.67481\n",
            "[30]\tvalidation_0-auc:0.672785\n",
            "[31]\tvalidation_0-auc:0.672067\n",
            "[32]\tvalidation_0-auc:0.672324\n",
            "[33]\tvalidation_0-auc:0.671401\n",
            "[34]\tvalidation_0-auc:0.670273\n",
            "[35]\tvalidation_0-auc:0.669401\n",
            "[36]\tvalidation_0-auc:0.669093\n",
            "[37]\tvalidation_0-auc:0.670145\n",
            "[38]\tvalidation_0-auc:0.667581\n",
            "[39]\tvalidation_0-auc:0.667325\n",
            "[40]\tvalidation_0-auc:0.665171\n",
            "[41]\tvalidation_0-auc:0.664761\n",
            "[42]\tvalidation_0-auc:0.660762\n",
            "[43]\tvalidation_0-auc:0.65907\n",
            "[44]\tvalidation_0-auc:0.659326\n",
            "[45]\tvalidation_0-auc:0.659736\n",
            "[46]\tvalidation_0-auc:0.659942\n",
            "[47]\tvalidation_0-auc:0.658045\n",
            "[48]\tvalidation_0-auc:0.657019\n",
            "[49]\tvalidation_0-auc:0.656045\n",
            "[50]\tvalidation_0-auc:0.655789\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.716238\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6238 - accuracy: 0.6637 - val_loss: 0.8361 - val_accuracy: 0.3424\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5832 - accuracy: 0.6740 - val_loss: 0.5449 - val_accuracy: 0.7860\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5512 - accuracy: 0.7111 - val_loss: 0.7070 - val_accuracy: 0.4864\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5448 - accuracy: 0.7193 - val_loss: 0.6138 - val_accuracy: 0.6693\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5478 - accuracy: 0.7159 - val_loss: 0.5316 - val_accuracy: 0.7549\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 18ms/step - loss: 0.5932 - accuracy: 0.6754 - val_loss: 0.7000 - val_accuracy: 0.4591\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5193 - accuracy: 0.7310 - val_loss: 0.6229 - val_accuracy: 0.6226\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.4986 - accuracy: 0.7440 - val_loss: 0.5957 - val_accuracy: 0.6459\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5004 - accuracy: 0.7605 - val_loss: 0.5661 - val_accuracy: 0.6848\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4942 - accuracy: 0.7543 - val_loss: 0.5486 - val_accuracy: 0.7393\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.86588\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.896404\n",
            "[2]\tvalidation_0-auc:0.895399\n",
            "[3]\tvalidation_0-auc:0.902062\n",
            "[4]\tvalidation_0-auc:0.903551\n",
            "[5]\tvalidation_0-auc:0.902509\n",
            "[6]\tvalidation_0-auc:0.903923\n",
            "[7]\tvalidation_0-auc:0.904854\n",
            "[8]\tvalidation_0-auc:0.904891\n",
            "[9]\tvalidation_0-auc:0.906083\n",
            "[10]\tvalidation_0-auc:0.905896\n",
            "[11]\tvalidation_0-auc:0.906715\n",
            "[12]\tvalidation_0-auc:0.908502\n",
            "[13]\tvalidation_0-auc:0.909544\n",
            "[14]\tvalidation_0-auc:0.909917\n",
            "[15]\tvalidation_0-auc:0.910661\n",
            "[16]\tvalidation_0-auc:0.910661\n",
            "[17]\tvalidation_0-auc:0.910289\n",
            "[18]\tvalidation_0-auc:0.91014\n",
            "[19]\tvalidation_0-auc:0.912001\n",
            "[20]\tvalidation_0-auc:0.912746\n",
            "[21]\tvalidation_0-auc:0.912671\n",
            "[22]\tvalidation_0-auc:0.912299\n",
            "[23]\tvalidation_0-auc:0.912895\n",
            "[24]\tvalidation_0-auc:0.912597\n",
            "[25]\tvalidation_0-auc:0.912076\n",
            "[26]\tvalidation_0-auc:0.911741\n",
            "[27]\tvalidation_0-auc:0.913118\n",
            "[28]\tvalidation_0-auc:0.912895\n",
            "[29]\tvalidation_0-auc:0.91282\n",
            "[30]\tvalidation_0-auc:0.912522\n",
            "[31]\tvalidation_0-auc:0.913118\n",
            "[32]\tvalidation_0-auc:0.913714\n",
            "[33]\tvalidation_0-auc:0.913267\n",
            "[34]\tvalidation_0-auc:0.913416\n",
            "[35]\tvalidation_0-auc:0.912225\n",
            "[36]\tvalidation_0-auc:0.911406\n",
            "[37]\tvalidation_0-auc:0.910363\n",
            "[38]\tvalidation_0-auc:0.910512\n",
            "[39]\tvalidation_0-auc:0.909991\n",
            "[40]\tvalidation_0-auc:0.910587\n",
            "[41]\tvalidation_0-auc:0.910363\n",
            "[42]\tvalidation_0-auc:0.910363\n",
            "[43]\tvalidation_0-auc:0.909321\n",
            "[44]\tvalidation_0-auc:0.908465\n",
            "[45]\tvalidation_0-auc:0.908539\n",
            "[46]\tvalidation_0-auc:0.908986\n",
            "[47]\tvalidation_0-auc:0.908725\n",
            "[48]\tvalidation_0-auc:0.909917\n",
            "[49]\tvalidation_0-auc:0.910587\n",
            "[50]\tvalidation_0-auc:0.910587\n",
            "[51]\tvalidation_0-auc:0.910289\n",
            "[52]\tvalidation_0-auc:0.910363\n",
            "[53]\tvalidation_0-auc:0.910214\n",
            "[54]\tvalidation_0-auc:0.909842\n",
            "[55]\tvalidation_0-auc:0.909917\n",
            "[56]\tvalidation_0-auc:0.909768\n",
            "[57]\tvalidation_0-auc:0.911033\n",
            "[58]\tvalidation_0-auc:0.911927\n",
            "[59]\tvalidation_0-auc:0.912225\n",
            "[60]\tvalidation_0-auc:0.911406\n",
            "[61]\tvalidation_0-auc:0.912001\n",
            "[62]\tvalidation_0-auc:0.91215\n",
            "[63]\tvalidation_0-auc:0.912225\n",
            "[64]\tvalidation_0-auc:0.911555\n",
            "[65]\tvalidation_0-auc:0.911182\n",
            "[66]\tvalidation_0-auc:0.91215\n",
            "[67]\tvalidation_0-auc:0.912969\n",
            "[68]\tvalidation_0-auc:0.912597\n",
            "[69]\tvalidation_0-auc:0.912597\n",
            "[70]\tvalidation_0-auc:0.913043\n",
            "[71]\tvalidation_0-auc:0.912225\n",
            "[72]\tvalidation_0-auc:0.912373\n",
            "[73]\tvalidation_0-auc:0.911331\n",
            "[74]\tvalidation_0-auc:0.91148\n",
            "[75]\tvalidation_0-auc:0.909917\n",
            "[76]\tvalidation_0-auc:0.90947\n",
            "[77]\tvalidation_0-auc:0.909247\n",
            "[78]\tvalidation_0-auc:0.909247\n",
            "[79]\tvalidation_0-auc:0.909247\n",
            "[80]\tvalidation_0-auc:0.909917\n",
            "[81]\tvalidation_0-auc:0.909768\n",
            "[82]\tvalidation_0-auc:0.909619\n",
            "Stopping. Best iteration:\n",
            "[32]\tvalidation_0-auc:0.913714\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|     LSTM 0.2     |        0.6         |  0.4739583333333333 | 0.8584905660377359 | 0.6107382550335569 |\n",
            "|     GRU 0.2      | 0.6172413793103448 |  0.4825174825174825 | 0.6509433962264151 | 0.5542168674698795 |\n",
            "|   XGBoost 0.2    | 0.5620689655172414 |  0.4461538461538462 | 0.8207547169811321 | 0.5780730897009967 |\n",
            "|    Logreg 0.2    |        0.5         | 0.41975308641975306 | 0.9622641509433962 | 0.5845272206303724 |\n",
            "|     SVM 0.2      | 0.5689655172413793 | 0.45701357466063347 | 0.9528301886792453 | 0.617737003058104  |\n",
            "|  LSTM beta 0.2   | 0.754863813229572  |         0.55        | 0.7534246575342466 | 0.6358381502890174 |\n",
            "|   GRU beta 0.2   | 0.7392996108949417 |  0.5263157894736842 | 0.821917808219178  | 0.641711229946524  |\n",
            "| XGBoost beta 0.2 | 0.8093385214007782 |         0.62        | 0.8493150684931506 | 0.7167630057803469 |\n",
            "| logreg beta 0.2  | 0.6770428015564203 |  0.4652777777777778 | 0.9178082191780822 | 0.6175115207373272 |\n",
            "|   svm beta 0.2   | 0.7937743190661478 |  0.5961538461538461 | 0.8493150684931506 | 0.7005649717514123 |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6691 - accuracy: 0.6174 - val_loss: 0.7901 - val_accuracy: 0.3655\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6388 - accuracy: 0.6235 - val_loss: 0.7150 - val_accuracy: 0.5552\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6146 - accuracy: 0.6591 - val_loss: 0.6937 - val_accuracy: 0.6034\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6314 - accuracy: 0.6651 - val_loss: 0.8388 - val_accuracy: 0.3793\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6079 - accuracy: 0.6678 - val_loss: 0.8110 - val_accuracy: 0.5241\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6536 - accuracy: 0.6188 - val_loss: 0.7325 - val_accuracy: 0.4241\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6149 - accuracy: 0.6651 - val_loss: 0.7185 - val_accuracy: 0.5069\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5949 - accuracy: 0.6705 - val_loss: 0.7216 - val_accuracy: 0.4828\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5900 - accuracy: 0.6772 - val_loss: 0.7034 - val_accuracy: 0.5207\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5860 - accuracy: 0.6752 - val_loss: 0.6847 - val_accuracy: 0.5655\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.665889\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.679502\n",
            "[2]\tvalidation_0-auc:0.685859\n",
            "[3]\tvalidation_0-auc:0.687885\n",
            "[4]\tvalidation_0-auc:0.686039\n",
            "[5]\tvalidation_0-auc:0.682091\n",
            "[6]\tvalidation_0-auc:0.681732\n",
            "[7]\tvalidation_0-auc:0.681732\n",
            "[8]\tvalidation_0-auc:0.682886\n",
            "[9]\tvalidation_0-auc:0.68186\n",
            "[10]\tvalidation_0-auc:0.671965\n",
            "[11]\tvalidation_0-auc:0.668478\n",
            "[12]\tvalidation_0-auc:0.671785\n",
            "[13]\tvalidation_0-auc:0.67199\n",
            "[14]\tvalidation_0-auc:0.66917\n",
            "[15]\tvalidation_0-auc:0.670914\n",
            "[16]\tvalidation_0-auc:0.666094\n",
            "[17]\tvalidation_0-auc:0.66653\n",
            "[18]\tvalidation_0-auc:0.668478\n",
            "[19]\tvalidation_0-auc:0.670016\n",
            "[20]\tvalidation_0-auc:0.674092\n",
            "[21]\tvalidation_0-auc:0.672375\n",
            "[22]\tvalidation_0-auc:0.672477\n",
            "[23]\tvalidation_0-auc:0.671067\n",
            "[24]\tvalidation_0-auc:0.669581\n",
            "[25]\tvalidation_0-auc:0.670555\n",
            "[26]\tvalidation_0-auc:0.666325\n",
            "[27]\tvalidation_0-auc:0.665094\n",
            "[28]\tvalidation_0-auc:0.667196\n",
            "[29]\tvalidation_0-auc:0.66571\n",
            "[30]\tvalidation_0-auc:0.667812\n",
            "[31]\tvalidation_0-auc:0.667453\n",
            "[32]\tvalidation_0-auc:0.668837\n",
            "[33]\tvalidation_0-auc:0.669145\n",
            "[34]\tvalidation_0-auc:0.669196\n",
            "[35]\tvalidation_0-auc:0.67058\n",
            "[36]\tvalidation_0-auc:0.668042\n",
            "[37]\tvalidation_0-auc:0.666299\n",
            "[38]\tvalidation_0-auc:0.665376\n",
            "[39]\tvalidation_0-auc:0.664351\n",
            "[40]\tvalidation_0-auc:0.663428\n",
            "[41]\tvalidation_0-auc:0.662146\n",
            "[42]\tvalidation_0-auc:0.662044\n",
            "[43]\tvalidation_0-auc:0.660326\n",
            "[44]\tvalidation_0-auc:0.660326\n",
            "[45]\tvalidation_0-auc:0.659762\n",
            "[46]\tvalidation_0-auc:0.659249\n",
            "[47]\tvalidation_0-auc:0.65848\n",
            "[48]\tvalidation_0-auc:0.658224\n",
            "[49]\tvalidation_0-auc:0.658224\n",
            "[50]\tvalidation_0-auc:0.658429\n",
            "[51]\tvalidation_0-auc:0.658121\n",
            "[52]\tvalidation_0-auc:0.65725\n",
            "[53]\tvalidation_0-auc:0.656686\n",
            "Stopping. Best iteration:\n",
            "[3]\tvalidation_0-auc:0.687885\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6330 - accuracy: 0.6431 - val_loss: 0.6743 - val_accuracy: 0.5992\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5674 - accuracy: 0.6857 - val_loss: 0.6138 - val_accuracy: 0.7043\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5557 - accuracy: 0.6939 - val_loss: 0.5885 - val_accuracy: 0.7043\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5457 - accuracy: 0.7069 - val_loss: 0.5427 - val_accuracy: 0.7665\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5275 - accuracy: 0.7213 - val_loss: 0.5818 - val_accuracy: 0.7043\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 2s 13ms/step - loss: 0.6173 - accuracy: 0.6493 - val_loss: 0.5249 - val_accuracy: 0.7704\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5460 - accuracy: 0.7241 - val_loss: 0.5549 - val_accuracy: 0.7004\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5185 - accuracy: 0.7220 - val_loss: 0.5621 - val_accuracy: 0.6926\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5100 - accuracy: 0.7461 - val_loss: 0.5175 - val_accuracy: 0.7588\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4911 - accuracy: 0.7584 - val_loss: 0.7199 - val_accuracy: 0.5409\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.860706\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.85985\n",
            "[2]\tvalidation_0-auc:0.886689\n",
            "[3]\tvalidation_0-auc:0.894468\n",
            "[4]\tvalidation_0-auc:0.896069\n",
            "[5]\tvalidation_0-auc:0.898786\n",
            "[6]\tvalidation_0-auc:0.900238\n",
            "[7]\tvalidation_0-auc:0.89994\n",
            "[8]\tvalidation_0-auc:0.900313\n",
            "[9]\tvalidation_0-auc:0.898563\n",
            "[10]\tvalidation_0-auc:0.902918\n",
            "[11]\tvalidation_0-auc:0.904147\n",
            "[12]\tvalidation_0-auc:0.905487\n",
            "[13]\tvalidation_0-auc:0.90679\n",
            "[14]\tvalidation_0-auc:0.908986\n",
            "[15]\tvalidation_0-auc:0.90973\n",
            "[16]\tvalidation_0-auc:0.910624\n",
            "[17]\tvalidation_0-auc:0.909507\n",
            "[18]\tvalidation_0-auc:0.909433\n",
            "[19]\tvalidation_0-auc:0.909656\n",
            "[20]\tvalidation_0-auc:0.910326\n",
            "[21]\tvalidation_0-auc:0.908614\n",
            "[22]\tvalidation_0-auc:0.909098\n",
            "[23]\tvalidation_0-auc:0.909023\n",
            "[24]\tvalidation_0-auc:0.908725\n",
            "[25]\tvalidation_0-auc:0.908018\n",
            "[26]\tvalidation_0-auc:0.908242\n",
            "[27]\tvalidation_0-auc:0.908018\n",
            "[28]\tvalidation_0-auc:0.907199\n",
            "[29]\tvalidation_0-auc:0.906231\n",
            "[30]\tvalidation_0-auc:0.905934\n",
            "[31]\tvalidation_0-auc:0.904817\n",
            "[32]\tvalidation_0-auc:0.90504\n",
            "[33]\tvalidation_0-auc:0.90504\n",
            "[34]\tvalidation_0-auc:0.904966\n",
            "[35]\tvalidation_0-auc:0.903477\n",
            "[36]\tvalidation_0-auc:0.901951\n",
            "[37]\tvalidation_0-auc:0.901951\n",
            "[38]\tvalidation_0-auc:0.901802\n",
            "[39]\tvalidation_0-auc:0.901951\n",
            "[40]\tvalidation_0-auc:0.902621\n",
            "[41]\tvalidation_0-auc:0.901206\n",
            "[42]\tvalidation_0-auc:0.900908\n",
            "[43]\tvalidation_0-auc:0.900089\n",
            "[44]\tvalidation_0-auc:0.89901\n",
            "[45]\tvalidation_0-auc:0.89901\n",
            "[46]\tvalidation_0-auc:0.898712\n",
            "[47]\tvalidation_0-auc:0.897521\n",
            "[48]\tvalidation_0-auc:0.897484\n",
            "[49]\tvalidation_0-auc:0.897856\n",
            "[50]\tvalidation_0-auc:0.898005\n",
            "[51]\tvalidation_0-auc:0.897298\n",
            "[52]\tvalidation_0-auc:0.897298\n",
            "[53]\tvalidation_0-auc:0.897595\n",
            "[54]\tvalidation_0-auc:0.897223\n",
            "[55]\tvalidation_0-auc:0.899196\n",
            "[56]\tvalidation_0-auc:0.897781\n",
            "[57]\tvalidation_0-auc:0.896888\n",
            "[58]\tvalidation_0-auc:0.896888\n",
            "[59]\tvalidation_0-auc:0.897484\n",
            "[60]\tvalidation_0-auc:0.898005\n",
            "[61]\tvalidation_0-auc:0.899457\n",
            "[62]\tvalidation_0-auc:0.899084\n",
            "[63]\tvalidation_0-auc:0.897074\n",
            "[64]\tvalidation_0-auc:0.896553\n",
            "[65]\tvalidation_0-auc:0.896181\n",
            "[66]\tvalidation_0-auc:0.89566\n",
            "Stopping. Best iteration:\n",
            "[16]\tvalidation_0-auc:0.910624\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.5241379310344828 |  0.4344262295081967 |        1.0         | 0.6057142857142858 |\n",
            "|      GRU 0.15     | 0.5655172413793104 |  0.4523809523809524 | 0.8962264150943396 | 0.6012658227848101 |\n",
            "|    XGBoost 0.15   | 0.5413793103448276 | 0.42780748663101603 | 0.7547169811320755 | 0.5460750853242321 |\n",
            "|    Logreg 0.15    | 0.5241379310344828 | 0.43103448275862066 | 0.9433962264150944 | 0.5917159763313609 |\n",
            "|      SVM 0.15     | 0.5620689655172414 |  0.4507042253521127 | 0.9056603773584906 | 0.6018808777429466 |\n",
            "|   LSTM beta 0.15  | 0.7042801556420234 | 0.48760330578512395 | 0.8082191780821918 | 0.6082474226804123 |\n",
            "|   GRU beta 0.15   | 0.5408560311284046 |  0.3783783783783784 | 0.958904109589041  | 0.5426356589147286 |\n",
            "| XGBoost beta 0.15 | 0.7898832684824902 |  0.5887850467289719 | 0.863013698630137  |        0.7         |\n",
            "|  logreg beta 0.15 | 0.6731517509727627 | 0.46153846153846156 | 0.9041095890410958 | 0.6111111111111112 |\n",
            "|   svm beta 0.15   | 0.8132295719844358 |  0.6288659793814433 | 0.8356164383561644 | 0.7176470588235294 |\n",
            "+-------------------+--------------------+---------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "axbc8LrwHUKw",
        "outputId": "444101d6-95ed-43a0-d4dc-bf4681119577"
      },
      "source": [
        "Result_purging.to_csv('CPRI_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.553648</td>\n",
              "      <td>0.716327</td>\n",
              "      <td>0.649874</td>\n",
              "      <td>0.786585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.493103</td>\n",
              "      <td>0.657143</td>\n",
              "      <td>0.629956</td>\n",
              "      <td>0.871951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.513410</td>\n",
              "      <td>0.679592</td>\n",
              "      <td>0.630588</td>\n",
              "      <td>0.817073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.491694</td>\n",
              "      <td>0.655102</td>\n",
              "      <td>0.636559</td>\n",
              "      <td>0.902439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.512545</td>\n",
              "      <td>0.679592</td>\n",
              "      <td>0.645598</td>\n",
              "      <td>0.871951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.368132</td>\n",
              "      <td>0.608315</td>\n",
              "      <td>0.428115</td>\n",
              "      <td>0.511450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.724289</td>\n",
              "      <td>0.543478</td>\n",
              "      <td>0.572519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.348416</td>\n",
              "      <td>0.566740</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.587786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>0.654267</td>\n",
              "      <td>0.556180</td>\n",
              "      <td>0.755725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.398810</td>\n",
              "      <td>0.638950</td>\n",
              "      <td>0.448161</td>\n",
              "      <td>0.511450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.418803</td>\n",
              "      <td>0.618367</td>\n",
              "      <td>0.511749</td>\n",
              "      <td>0.657718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.441860</td>\n",
              "      <td>0.634694</td>\n",
              "      <td>0.560197</td>\n",
              "      <td>0.765101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.376176</td>\n",
              "      <td>0.534694</td>\n",
              "      <td>0.512821</td>\n",
              "      <td>0.805369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.362025</td>\n",
              "      <td>0.473469</td>\n",
              "      <td>0.525735</td>\n",
              "      <td>0.959732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.387187</td>\n",
              "      <td>0.530612</td>\n",
              "      <td>0.547244</td>\n",
              "      <td>0.932886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.365297</td>\n",
              "      <td>0.617068</td>\n",
              "      <td>0.477612</td>\n",
              "      <td>0.689655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.467626</td>\n",
              "      <td>0.726477</td>\n",
              "      <td>0.509804</td>\n",
              "      <td>0.560345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.421053</td>\n",
              "      <td>0.680525</td>\n",
              "      <td>0.522876</td>\n",
              "      <td>0.689655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.349091</td>\n",
              "      <td>0.564551</td>\n",
              "      <td>0.491049</td>\n",
              "      <td>0.827586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.400990</td>\n",
              "      <td>0.658643</td>\n",
              "      <td>0.509434</td>\n",
              "      <td>0.698276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.385965</td>\n",
              "      <td>0.491837</td>\n",
              "      <td>0.552962</td>\n",
              "      <td>0.974684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.486842</td>\n",
              "      <td>0.669388</td>\n",
              "      <td>0.477419</td>\n",
              "      <td>0.468354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.403175</td>\n",
              "      <td>0.553061</td>\n",
              "      <td>0.536998</td>\n",
              "      <td>0.803797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.396277</td>\n",
              "      <td>0.518367</td>\n",
              "      <td>0.558052</td>\n",
              "      <td>0.943038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.410405</td>\n",
              "      <td>0.551020</td>\n",
              "      <td>0.563492</td>\n",
              "      <td>0.898734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.458824</td>\n",
              "      <td>0.695842</td>\n",
              "      <td>0.528814</td>\n",
              "      <td>0.624000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.385593</td>\n",
              "      <td>0.608315</td>\n",
              "      <td>0.504155</td>\n",
              "      <td>0.728000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.754923</td>\n",
              "      <td>0.582090</td>\n",
              "      <td>0.624000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.390977</td>\n",
              "      <td>0.599562</td>\n",
              "      <td>0.531969</td>\n",
              "      <td>0.832000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.458763</td>\n",
              "      <td>0.691466</td>\n",
              "      <td>0.557994</td>\n",
              "      <td>0.712000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.543860</td>\n",
              "      <td>0.689655</td>\n",
              "      <td>0.673913</td>\n",
              "      <td>0.885714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.537143</td>\n",
              "      <td>0.682759</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.895238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.567742</td>\n",
              "      <td>0.710345</td>\n",
              "      <td>0.676923</td>\n",
              "      <td>0.838095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.513514</td>\n",
              "      <td>0.655172</td>\n",
              "      <td>0.655172</td>\n",
              "      <td>0.904762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.550296</td>\n",
              "      <td>0.696552</td>\n",
              "      <td>0.678832</td>\n",
              "      <td>0.885714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.305882</td>\n",
              "      <td>0.463035</td>\n",
              "      <td>0.429752</td>\n",
              "      <td>0.722222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.682540</td>\n",
              "      <td>0.809339</td>\n",
              "      <td>0.637037</td>\n",
              "      <td>0.597222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.366071</td>\n",
              "      <td>0.603113</td>\n",
              "      <td>0.445652</td>\n",
              "      <td>0.569444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.451613</td>\n",
              "      <td>0.673152</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.777778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.439024</td>\n",
              "      <td>0.680934</td>\n",
              "      <td>0.467532</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.473958</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.610738</td>\n",
              "      <td>0.858491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.482517</td>\n",
              "      <td>0.617241</td>\n",
              "      <td>0.554217</td>\n",
              "      <td>0.650943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.446154</td>\n",
              "      <td>0.562069</td>\n",
              "      <td>0.578073</td>\n",
              "      <td>0.820755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.419753</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.584527</td>\n",
              "      <td>0.962264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.457014</td>\n",
              "      <td>0.568966</td>\n",
              "      <td>0.617737</td>\n",
              "      <td>0.952830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.754864</td>\n",
              "      <td>0.635838</td>\n",
              "      <td>0.753425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.526316</td>\n",
              "      <td>0.739300</td>\n",
              "      <td>0.641711</td>\n",
              "      <td>0.821918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.620000</td>\n",
              "      <td>0.809339</td>\n",
              "      <td>0.716763</td>\n",
              "      <td>0.849315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.465278</td>\n",
              "      <td>0.677043</td>\n",
              "      <td>0.617512</td>\n",
              "      <td>0.917808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.596154</td>\n",
              "      <td>0.793774</td>\n",
              "      <td>0.700565</td>\n",
              "      <td>0.849315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.434426</td>\n",
              "      <td>0.524138</td>\n",
              "      <td>0.605714</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.452381</td>\n",
              "      <td>0.565517</td>\n",
              "      <td>0.601266</td>\n",
              "      <td>0.896226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.427807</td>\n",
              "      <td>0.541379</td>\n",
              "      <td>0.546075</td>\n",
              "      <td>0.754717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.431034</td>\n",
              "      <td>0.524138</td>\n",
              "      <td>0.591716</td>\n",
              "      <td>0.943396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.450704</td>\n",
              "      <td>0.562069</td>\n",
              "      <td>0.601881</td>\n",
              "      <td>0.905660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.487603</td>\n",
              "      <td>0.704280</td>\n",
              "      <td>0.608247</td>\n",
              "      <td>0.808219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.378378</td>\n",
              "      <td>0.540856</td>\n",
              "      <td>0.542636</td>\n",
              "      <td>0.958904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.588785</td>\n",
              "      <td>0.789883</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.863014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.673152</td>\n",
              "      <td>0.611111</td>\n",
              "      <td>0.904110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>CPRI</td>\n",
              "      <td>0.628866</td>\n",
              "      <td>0.813230</td>\n",
              "      <td>0.717647</td>\n",
              "      <td>0.835616</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  CPRI  0.553648  0.716327  0.649874  0.786585\n",
              "1            GRU 0.1  CPRI  0.493103  0.657143  0.629956  0.871951\n",
              "2        XGBoost 0.1  CPRI  0.513410  0.679592  0.630588  0.817073\n",
              "3         Logreg 0.1  CPRI  0.491694  0.655102  0.636559  0.902439\n",
              "4            SVM 0.1  CPRI  0.512545  0.679592  0.645598  0.871951\n",
              "5      LSTM beta 0.1  CPRI  0.368132  0.608315  0.428115  0.511450\n",
              "6       GRU beta 0.1  CPRI  0.517241  0.724289  0.543478  0.572519\n",
              "7   XGBoost beta 0.1  CPRI  0.348416  0.566740  0.437500  0.587786\n",
              "8    logreg beta 0.1  CPRI  0.440000  0.654267  0.556180  0.755725\n",
              "9       svm beta 0.1  CPRI  0.398810  0.638950  0.448161  0.511450\n",
              "0           LSTM 0.2  CPRI  0.418803  0.618367  0.511749  0.657718\n",
              "1            GRU 0.2  CPRI  0.441860  0.634694  0.560197  0.765101\n",
              "2        XGBoost 0.2  CPRI  0.376176  0.534694  0.512821  0.805369\n",
              "3         Logreg 0.2  CPRI  0.362025  0.473469  0.525735  0.959732\n",
              "4            SVM 0.2  CPRI  0.387187  0.530612  0.547244  0.932886\n",
              "5      LSTM beta 0.2  CPRI  0.365297  0.617068  0.477612  0.689655\n",
              "6       GRU beta 0.2  CPRI  0.467626  0.726477  0.509804  0.560345\n",
              "7   XGBoost beta 0.2  CPRI  0.421053  0.680525  0.522876  0.689655\n",
              "8    logreg beta 0.2  CPRI  0.349091  0.564551  0.491049  0.827586\n",
              "9       svm beta 0.2  CPRI  0.400990  0.658643  0.509434  0.698276\n",
              "0          LSTM 0.15  CPRI  0.385965  0.491837  0.552962  0.974684\n",
              "1           GRU 0.15  CPRI  0.486842  0.669388  0.477419  0.468354\n",
              "2       XGBoost 0.15  CPRI  0.403175  0.553061  0.536998  0.803797\n",
              "3        Logreg 0.15  CPRI  0.396277  0.518367  0.558052  0.943038\n",
              "4           SVM 0.15  CPRI  0.410405  0.551020  0.563492  0.898734\n",
              "5     LSTM beta 0.15  CPRI  0.458824  0.695842  0.528814  0.624000\n",
              "6      GRU beta 0.15  CPRI  0.385593  0.608315  0.504155  0.728000\n",
              "7  XGBoost beta 0.15  CPRI  0.545455  0.754923  0.582090  0.624000\n",
              "8   logreg beta 0.15  CPRI  0.390977  0.599562  0.531969  0.832000\n",
              "9      svm beta 0.15  CPRI  0.458763  0.691466  0.557994  0.712000\n",
              "0           LSTM 0.1  CPRI  0.543860  0.689655  0.673913  0.885714\n",
              "1            GRU 0.1  CPRI  0.537143  0.682759  0.671429  0.895238\n",
              "2        XGBoost 0.1  CPRI  0.567742  0.710345  0.676923  0.838095\n",
              "3         Logreg 0.1  CPRI  0.513514  0.655172  0.655172  0.904762\n",
              "4            SVM 0.1  CPRI  0.550296  0.696552  0.678832  0.885714\n",
              "5      LSTM beta 0.1  CPRI  0.305882  0.463035  0.429752  0.722222\n",
              "6       GRU beta 0.1  CPRI  0.682540  0.809339  0.637037  0.597222\n",
              "7   XGBoost beta 0.1  CPRI  0.366071  0.603113  0.445652  0.569444\n",
              "8    logreg beta 0.1  CPRI  0.451613  0.673152  0.571429  0.777778\n",
              "9       svm beta 0.1  CPRI  0.439024  0.680934  0.467532  0.500000\n",
              "0           LSTM 0.2  CPRI  0.473958  0.600000  0.610738  0.858491\n",
              "1            GRU 0.2  CPRI  0.482517  0.617241  0.554217  0.650943\n",
              "2        XGBoost 0.2  CPRI  0.446154  0.562069  0.578073  0.820755\n",
              "3         Logreg 0.2  CPRI  0.419753  0.500000  0.584527  0.962264\n",
              "4            SVM 0.2  CPRI  0.457014  0.568966  0.617737  0.952830\n",
              "5      LSTM beta 0.2  CPRI  0.550000  0.754864  0.635838  0.753425\n",
              "6       GRU beta 0.2  CPRI  0.526316  0.739300  0.641711  0.821918\n",
              "7   XGBoost beta 0.2  CPRI  0.620000  0.809339  0.716763  0.849315\n",
              "8    logreg beta 0.2  CPRI  0.465278  0.677043  0.617512  0.917808\n",
              "9       svm beta 0.2  CPRI  0.596154  0.793774  0.700565  0.849315\n",
              "0          LSTM 0.15  CPRI  0.434426  0.524138  0.605714  1.000000\n",
              "1           GRU 0.15  CPRI  0.452381  0.565517  0.601266  0.896226\n",
              "2       XGBoost 0.15  CPRI  0.427807  0.541379  0.546075  0.754717\n",
              "3        Logreg 0.15  CPRI  0.431034  0.524138  0.591716  0.943396\n",
              "4           SVM 0.15  CPRI  0.450704  0.562069  0.601881  0.905660\n",
              "5     LSTM beta 0.15  CPRI  0.487603  0.704280  0.608247  0.808219\n",
              "6      GRU beta 0.15  CPRI  0.378378  0.540856  0.542636  0.958904\n",
              "7  XGBoost beta 0.15  CPRI  0.588785  0.789883  0.700000  0.863014\n",
              "8   logreg beta 0.15  CPRI  0.461538  0.673152  0.611111  0.904110\n",
              "9      svm beta 0.15  CPRI  0.628866  0.813230  0.717647  0.835616"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPLkChLoHUKx"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2100].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('CPRI_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BFQRf6LSQxc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vqBsjlEUbos"
      },
      "source": [
        "## DVN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScbH1woyUbot",
        "outputId": "762fd494-9e09-469f-b53a-9148b4c10396"
      },
      "source": [
        "dfs = pd.read_csv(\"DVN.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2768</td>\n",
              "      <td>US1.DVN</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>35.97</td>\n",
              "      <td>37.165</td>\n",
              "      <td>35.87</td>\n",
              "      <td>36.80</td>\n",
              "      <td>661181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2767</td>\n",
              "      <td>US1.DVN</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>35.38</td>\n",
              "      <td>35.905</td>\n",
              "      <td>34.91</td>\n",
              "      <td>35.50</td>\n",
              "      <td>633147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.DVN</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>35.15</td>\n",
              "      <td>36.040</td>\n",
              "      <td>34.55</td>\n",
              "      <td>35.75</td>\n",
              "      <td>483024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.DVN</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>35.58</td>\n",
              "      <td>35.970</td>\n",
              "      <td>34.87</td>\n",
              "      <td>35.12</td>\n",
              "      <td>723106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.DVN</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>34.05</td>\n",
              "      <td>35.090</td>\n",
              "      <td>33.92</td>\n",
              "      <td>35.01</td>\n",
              "      <td>826165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.DVN</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>65.84</td>\n",
              "      <td>66.980</td>\n",
              "      <td>65.74</td>\n",
              "      <td>66.76</td>\n",
              "      <td>2343172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.DVN</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>66.67</td>\n",
              "      <td>66.940</td>\n",
              "      <td>65.22</td>\n",
              "      <td>65.81</td>\n",
              "      <td>2097798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.DVN</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>65.37</td>\n",
              "      <td>66.500</td>\n",
              "      <td>64.99</td>\n",
              "      <td>66.29</td>\n",
              "      <td>3011060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.DVN</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>65.14</td>\n",
              "      <td>65.550</td>\n",
              "      <td>64.68</td>\n",
              "      <td>65.31</td>\n",
              "      <td>2226445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.DVN</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>65.00</td>\n",
              "      <td>65.520</td>\n",
              "      <td>63.85</td>\n",
              "      <td>64.32</td>\n",
              "      <td>2465941</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2769 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...  <HIGH>  <LOW>  <CLOSE>    <VOL>\n",
              "0      2768  US1.DVN     D  20211001  ...  37.165  35.87    36.80   661181\n",
              "1      2767  US1.DVN     D  20210930  ...  35.905  34.91    35.50   633147\n",
              "2      2766  US1.DVN     D  20210929  ...  36.040  34.55    35.75   483024\n",
              "3      2765  US1.DVN     D  20210928  ...  35.970  34.87    35.12   723106\n",
              "4      2764  US1.DVN     D  20210927  ...  35.090  33.92    35.01   826165\n",
              "...     ...      ...   ...       ...  ...     ...    ...      ...      ...\n",
              "2764      4  US1.DVN     D  20101008  ...  66.980  65.74    66.76  2343172\n",
              "2765      3  US1.DVN     D  20101007  ...  66.940  65.22    65.81  2097798\n",
              "2766      2  US1.DVN     D  20101006  ...  66.500  64.99    66.29  3011060\n",
              "2767      1  US1.DVN     D  20101005  ...  65.550  64.68    65.31  2226445\n",
              "2768      0  US1.DVN     D  20101004  ...  65.520  63.85    64.32  2465941\n",
              "\n",
              "[2769 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uG9VxqmUbou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f42cb898-1b33-4008-d0ee-569f90a81ca8"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"de3cebeb-9104-4b34-82cc-fdb083098499\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"de3cebeb-9104-4b34-82cc-fdb083098499\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'de3cebeb-9104-4b34-82cc-fdb083098499',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [20.92, 21.88, 22.59, 22.68, 22.04, 23.14, 24.07, 24.24, 24.41, 24.71, 24.73, 25.84, 26.105, 25.96, 26.52, 26.69, 28.12, 25.06, 24.57, 24.97, 24.55, 24.63, 23.35, 23.5, 22.84, 22.08, 21.98, 22.54, 21.91, 21.33, 21.57, 21.68, 23.03, 23.38, 23.38, 23.27, 22.55, 21.63, 22.04, 23.62, 23.52, 24.05, 24.47, 24.14, 24.2, 24.06, 25.04, 25.17, 27.005, 26.89, 25.51, 25.54, 25.57, 26.49, 26.225, 26.41, 26.33, 25.7075, 25.925, 26.805, 27.65, 28.65, 28.66, 28.84, 28.08, 27.94, 27.9, 27.492, 27.54, 28.9, 28.51, 28.29, 28.73, 27.91, 27.93, 28.24, 28.07, 26.9, 26.9, 26.41, 25.77, 26.12, 25.48, 26.11, 26.115, 25.69, 25.635, 25.485, 25.985, 25.5, 25.16, 25.78, 26.85, 26.605, 27.0, 27.15, 29.31, 31.1, 30.03, 30.61, 31.31, 30.93, 30.76, 29.91, 31.3, 31.25, 31.54, 31.05, 31.49, 31.22, 30.93, 31.85, 32.14, 32.72, 32.68, 33.55, 34.48, 34.59, 35.19, 33.58, 33.21, 32.84, 32.57, 33.483, 31.18, 31.39, 31.06, 32.04, 32.19, 31.02, 30.71, 31.21, 31.54, 31.55, 31.87, 31.34, 31.59, 30.83, 30.34, 31.66, 31.25, 30.24, 30.51, 29.64, 29.5, 29.33, 28.77, 27.8, 26.7, 28.06, 28.72, 29.27, 29.44, 30.14, 29.52, 29.88, 29.58, 29.49, 29.65, 29.85, 30.31, 28.31, 28.43, 27.33, 26.91, 25.97, 25.43, 24.93, 25.3, 26.78, 26.87, 27.17, 27.01, 26.65, 27.68, 26.84, 26.51, 27.25, 26.59, 26.32, 26.62, 27.13, 26.38, 25.88, 26.39, 26.15, 26.36, 26.28, 25.83, 25.55, 25.36, 24.57, 23.52, 23.2, 22.55, 22.44, 22.9, 22.7, 20.96, 21.92, 22.47, 23.86, 24.4, 25.51, 25.84, 26.87, 27.28, 26.76, 26.26, 27.21, 27.75, 28.18, 28.78, 27.03, 26.9, 26.84, 26.61, 27.12, 26.836, 28.54, 27.44, 29.61, 29.56, 30.63, 30.55, 30.9, 31.55, 33.01, 32.72, 33.84, 32.91, 33.09, 32.29, 32.86, 32.4, 32.325, 30.97, 32.37, 32.85, 32.12, 33.74, 34.87, 34.95, 35.105, 36.0, 37.07, 36.1, 36.64, 36.05, 37.31, 39.53, 38.78, 40.04, 40.14, 40.65, 39.85, 40.2, 39.94, 39.38, 39.69, 41.4, 41.55, 40.53, 40.12, 39.87, 39.32, 38.8, 39.68, 40.88, 41.95, 41.49, 40.51, 40.48, 40.63, 41.72, 42.36, 42.93, 43.12, 42.95, 42.52, 42.87, 42.66, 42.53, 42.72, 41.84, 41.03, 40.59, 40.69, 40.49, 42.42, 42.06, 42.84, 42.07, 42.675, 43.44, 43.66, 43.05, 43.95, 43.47, 45.0, 45.07, 44.56, 45.15, 45.27, 44.56, 44.2, 44.13, 44.13, 43.76, 44.08, 44.22, 44.71, 43.93, 44.12, 45.62, 45.3, 44.51, 43.99, 44.0, 43.535, 43.97, 43.58, 43.47, 42.59, 41.88, 43.28, 41.16, 42.62, 41.62, 41.04, 40.25, 41.7, 42.6, 41.94, 42.27, 42.1575, 41.95, 41.51, 39.33, 40.09, 41.38, 41.57, 41.93, 40.88, 40.585, 41.69, 42.18, 41.82, 42.33, 41.97, 42.36, 41.42, 41.27, 41.32, 40.95, 40.885, 40.22, 39.03, 37.89, 37.88, 37.2, 38.23, 36.25, 36.33, 35.74, 35.78, 35.21, 34.48, 35.42, 35.4, 35.67, 36.1, 33.69, 33.61, 33.59, 32.84, 33.73, 33.6, 31.47, 31.45, 32.3, 31.33, 31.37, 30.36, 31.8, 31.2, 31.71, 32.48, 32.25, 32.28, 32.92, 31.55, 31.16, 31.96, 31.72, 32.62, 32.79, 32.83, 32.71, 32.39, 30.95, 31.34, 32.01, 31.36, 30.5467, 30.665, 31.86, 31.62, 31.43, 30.41, 30.545, 34.65, 34.12, 34.74, 35.57, 34.58, 35.78, 34.68, 34.93, 36.75, 38.38, 38.66, 40.03, 42.22, 41.37, 41.53, 43.32, 43.87, 43.92, 44.53, 44.52, 44.82, 42.84, 43.2, 43.43, 43.26, 44.03, 43.07, 42.18, 42.53, 43.24, 43.1, 43.235, 42.71, 42.24, 41.41, 41.76, 41.84, 42.59, 41.58, 40.92, 39.84, 38.69, 38.08, 37.67, 38.01, 38.13, 38.42, 38.49, 38.06, 37.15, 36.88, 38.39, 38.64, 38.83, 38.53, 37.085, 37.17, 36.8, 38.1218, 37.98, 37.555, 37.41, 38.68, 38.15, 37.91, 38.225, 39.66, 40.41, 40.82, 40.26, 41.27, 40.859, 39.26, 38.83, 39.085, 36.89, 36.01, 35.88, 34.91, 34.35, 34.84, 34.66, 34.93, 34.95, 35.23, 35.73, 35.86, 35.87, 35.61, 36.0603, 36.0, 35.885, 35.77, 36.33, 35.87, 36.52, 36.59, 36.71, 36.58, 37.18, 36.92, 36.66, 35.775, 35.35, 34.875, 33.76, 33.97, 33.64, 33.9, 33.51, 32.165, 31.815, 31.46, 32.39, 32.68, 32.13, 32.11, 31.38, 30.97, 31.0, 30.93, 31.155, 31.11, 30.99, 30.4, 30.33, 30.4, 30.18, 30.67, 30.92, 31.055, 31.6, 31.77, 32.07, 32.13, 32.4, 33.655, 32.27, 33.4, 32.97, 33.31, 33.205, 33.46, 33.0, 32.98, 31.77, 31.95, 32.0, 32.49, 31.37, 31.33, 31.65, 31.21, 30.79, 30.52, 29.75, 29.53, 30.18, 31.1, 32.8, 31.97, 31.81, 30.55, 30.35, 30.71, 30.48, 30.01, 30.15, 31.16, 31.31, 31.76, 31.5, 32.19, 33.48, 32.72, 32.57, 31.47, 31.91, 33.88, 33.03, 33.13, 34.28, 33.99, 34.48, 35.81, 35.73, 37.1248, 37.83, 38.09, 38.38, 36.89, 37.37, 37.92, 38.15, 37.855, 37.96, 38.53, 37.89, 38.16, 38.02, 36.99, 38.17, 38.85, 39.3101, 39.49, 39.36, 40.59, 40.59, 39.82, 39.745, 39.7, 39.24, 40.54, 41.02, 40.66, 41.87, 42.53, 43.07, 42.545, 42.88, 41.52, 42.47, 42.03, 41.73, 41.46, 41.22, 39.62, 38.88, 38.8, 39.09, 39.34, 39.8, 40.535, 40.53, 40.71, 41.11, 40.02, 40.96, 40.25, 40.78, 40.73, 43.54, 43.94, 43.36, 43.47, 44.63, 43.35, 43.06, 42.79, 43.99, 43.76, 45.06, 44.15, 44.1, 44.23, 45.93, 45.26, 45.4, 44.75, 44.06, 44.14, 45.28, 46.78, 45.64, 45.55, 45.55, 45.07, 46.63, 47.15, 46.46, 46.57, 45.63, 46.25, 46.07, 46.36, 46.47, 46.42, 46.46, 47.39, 46.7, 46.5801, 48.67, 49.01, 47.51, 47.07, 45.68, 45.96, 46.23, 46.84, 46.54, 46.89, 46.5792, 46.65, 47.03, 47.37, 47.53, 47.03, 48.18, 48.11, 48.07, 47.74, 47.54, 47.3, 47.72, 47.85, 47.85, 48.32, 42.16, 42.66, 44.23, 45.02, 44.47, 44.89, 43.83, 42.87, 44.0, 44.99, 42.83, 41.84, 43.14, 43.62, 41.99, 41.83, 40.22, 40.535, 39.57, 37.68, 37.89, 39.18, 40.45, 40.19, 40.68, 41.7, 42.69, 42.93, 42.95, 41.9025, 41.765, 43.1, 43.9, 44.55, 44.13, 44.39, 42.8, 43.48, 44.18, 43.29, 44.19, 44.1, 43.03, 41.589, 38.39, 39.64, 39.541, 41.88, 42.17, 40.65, 40.85, 41.0, 40.89, 40.19, 41.14, 43.19, 43.36, 45.28, 44.22, 44.21, 44.265, 43.69, 43.33, 44.439, 45.02, 44.44, 44.37, 44.52, 45.23, 44.1, 44.33, 44.76, 43.305, 42.48, 41.97, 41.66, 41.32, 39.59, 39.9, 40.2, 39.5, 38.62, 37.99, 36.14, 35.92, 38.3, 36.13, 36.14, 37.455, 36.1, 37.74, 37.34, 37.97, 37.75, 38.1, 38.28, 38.39, 39.0, 40.0, 37.66, 37.56, 36.82, 36.98, 36.16, 37.45, 36.23, 36.68, 35.99, 33.75, 35.85, 38.79, 37.94, 38.185, 37.33, 35.98, 34.8, 35.09, 35.55, 35.8, 35.11, 37.44, 37.5801, 38.77, 37.56, 35.91, 36.8, 36.15, 36.095, 35.89, 35.89, 36.42, 34.49, 34.54, 34.24, 33.9, 34.13, 34.64, 33.79, 32.52, 32.72, 32.54, 31.54, 30.0599, 31.4, 31.19, 31.32, 32.74, 34.56, 34.7, 36.11, 36.91, 34.61, 33.97, 35.2, 33.94, 34.77, 33.61, 31.99, 31.4, 31.35, 30.96, 30.95, 29.05, 28.7699, 27.6, 27.57, 26.395, 26.05, 26.61, 27.45, 26.21, 26.09, 25.83, 26.46, 25.76, 28.01, 28.02, 27.55, 27.39, 26.21, 24.09, 24.78, 24.78, 22.48, 22.92, 21.46, 24.185, 23.31, 21.99, 21.52, 19.21, 19.68, 20.28, 19.55, 19.41, 18.91, 20.28, 18.65, 19.72, 20.32, 21.24, 21.69, 21.25, 21.64, 22.67, 23.67, 24.83, 26.57, 26.58, 24.69, 26.05, 27.91, 26.78, 24.47, 24.55, 22.77, 25.63, 24.18, 21.58, 23.47, 24.49, 25.96, 24.81, 25.72, 26.97, 28.49, 28.06, 30.23, 32.83, 32.32, 31.995, 31.01, 32.24, 31.69, 32.7, 33.2, 29.84, 28.84, 28.67, 29.0, 30.56, 31.87, 31.55, 33.56, 35.5, 35.34, 35.19, 36.44, 40.51, 41.13, 43.53, 46.48, 46.01, 44.6545, 45.92, 46.54, 44.92, 45.25, 45.98, 47.07, 45.91, 46.51, 45.06, 45.07, 45.64, 47.76, 46.98, 46.87, 47.5, 46.49, 45.209, 43.14, 41.93, 41.01, 41.55, 41.22, 42.8, 44.96, 44.75, 43.9, 45.68, 45.28, 46.43, 45.68, 44.65, 43.7, 43.99, 45.24, 44.66, 43.85, 43.74, 42.91, 40.73, 37.75, 37.09, 36.42, 36.77, 38.47, 38.61, 38.15, 39.48, 40.33, 39.77, 40.8, 41.05, 39.17, 38.89, 38.85, 40.01, 39.66, 40.5, 40.15, 40.82, 40.72, 40.33, 42.63, 42.22, 40.77, 38.45, 36.64, 38.05, 41.12, 42.81, 44.66, 46.22, 46.41, 46.08, 46.0, 47.88, 47.31, 47.88, 46.4, 48.3, 46.98, 48.73, 48.36, 49.43, 50.6, 51.17, 50.0, 48.83, 49.4, 51.16, 51.15, 52.07, 51.848, 53.62, 54.37, 54.84, 56.11, 55.4, 55.85, 55.94, 55.57, 57.34, 56.67, 57.85, 57.74, 59.49, 58.83, 60.46, 60.39, 61.09, 61.44, 61.37, 60.95, 61.44, 61.62, 62.48, 61.64, 61.88, 62.95, 63.8, 62.13, 62.49, 63.79, 63.09, 63.92, 64.82, 64.5, 65.22, 65.39, 66.06, 66.29, 67.48, 67.53, 65.82, 64.79, 66.25, 65.86, 65.69, 65.93, 67.57, 66.43, 67.89, 66.5, 67.5, 66.66, 69.03, 68.6, 68.19, 68.93, 67.5, 66.56, 66.5, 66.31, 65.83, 65.28, 66.69, 65.93, 66.58, 66.92, 66.16, 64.59, 64.85, 64.95, 63.04, 63.31, 63.43, 62.54, 61.57, 60.3, 60.71, 59.15, 60.06, 59.64, 57.85, 57.95, 58.66, 58.26, 60.22, 58.0447, 58.74, 57.7, 57.17, 57.84, 56.99, 58.05, 59.18, 60.82, 61.85, 61.26, 60.85, 61.6, 62.57, 65.46, 64.83, 63.94, 63.8, 64.4, 63.96, 66.5898, 66.4599, 65.76, 64.6, 64.14, 64.72, 65.27, 64.97, 63.92, 64.05, 62.84, 60.27, 58.74, 58.42, 62.1, 61.72, 60.16, 61.25, 61.61, 59.65, 60.69, 57.83, 58.5, 58.89, 59.17, 60.43, 59.83, 57.77, 57.49, 58.72, 60.95, 61.21, 61.08, 61.14, 60.84, 60.21, 61.49, 60.2, 61.31, 60.28, 58.47, 54.14, 52.64, 53.03, 53.83, 54.49, 57.225, 56.6, 59.9, 59.52, 60.92, 59.9, 59.94, 58.97, 64.01, 64.78, 66.3, 67.21, 66.75, 64.84, 64.5, 63.84, 64.17, 63.26, 64.07, 64.55, 64.1901, 65.03, 62.54, 61.58, 56.01, 58.542, 59.98, 58.32, 59.72, 59.58, 57.33, 59.89, 60.0, 58.65, 60.03, 57.99, 58.15, 56.54, 55.15, 55.26, 56.56, 59.42, 61.31, 63.95, 63.6, 64.81, 65.43, 66.56, 67.06, 68.195, 69.58, 69.28, 68.68, 70.08, 69.9199, 69.98, 70.97, 71.49, 71.595, 71.85, 70.27, 69.96, 70.635, 70.68, 70.75, 71.21, 72.72, 72.31, 74.18, 74.09, 75.41, 74.65, 74.26, 74.56, 74.7, 74.15, 74.9, 73.86, 74.27, 73.54, 73.36, 72.65, 72.65, 73.01, 74.16, 74.37, 73.44, 74.37, 74.56, 76.77, 75.1, 75.49, 77.3, 77.38, 77.83, 78.33, 79.02, 78.52, 78.15, 77.11, 77.03, 75.74, 77.9, 76.51, 77.82, 77.13, 78.09, 79.4, 78.13, 78.28, 79.21, 78.58, 79.08, 79.4, 79.5, 78.62, 79.46, 77.57, 78.88, 78.94, 78.47, 78.13, 78.11, 78.23, 78.01, 77.0, 76.42, 74.95, 74.44, 74.67, 74.78, 74.86, 74.7, 73.7, 73.89, 73.885, 73.43, 73.0, 72.65, 72.72, 73.54, 72.54, 71.28, 70.67, 70.74, 71.75, 72.43, 71.03, 70.74, 70.99, 73.06, 70.18, 70.36, 70.28, 69.9, 69.99, 70.47, 70.27, 70.41, 71.9, 71.31, 70.64, 70.49, 70.71, 69.86, 68.67, 67.79, 67.54, 68.57, 68.37, 67.84, 67.56, 68.02, 68.49, 67.88, 67.68, 66.92, 65.77, 64.69, 63.99, 63.95, 63.91, 64.09, 63.28, 63.44, 63.5, 62.8, 62.74, 62.6, 63.08, 63.55, 64.53, 65.13, 64.52, 64.48, 64.63, 63.79, 64.41, 64.33, 63.74, 63.92, 64.37, 63.69, 64.57, 64.25, 62.9, 62.04, 62.28, 61.18, 61.52, 60.79, 61.0, 60.45, 59.09, 59.33, 57.85, 59.23, 59.66, 59.02, 59.28, 58.68, 59.52, 60.13, 60.43, 59.68, 59.02, 59.38, 59.2, 59.78, 59.17, 60.52, 60.765, 61.31, 61.47, 60.25, 61.03, 61.17, 61.88, 61.79, 61.7, 61.14, 61.26, 60.81, 60.955, 60.76, 59.36, 59.65, 60.4, 59.24, 59.42, 59.57, 60.82, 60.7, 60.47, 61.35, 60.84, 61.45, 60.54, 60.62, 60.56, 61.28, 61.6, 62.58, 62.61, 62.74, 62.77, 59.801, 60.4899, 60.83, 60.59, 59.82, 60.7, 60.15, 59.77, 61.39, 63.79, 64.46, 63.67, 63.22, 63.39, 64.52, 64.04, 64.54, 64.49, 64.09]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('de3cebeb-9104-4b34-82cc-fdb083098499');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"e9c6e1f3-c3e5-4441-8c98-5a843494194f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"e9c6e1f3-c3e5-4441-8c98-5a843494194f\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'e9c6e1f3-c3e5-4441-8c98-5a843494194f',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e9c6e1f3-c3e5-4441-8c98-5a843494194f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSTadIOfUbou"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH5xT-hWUbou",
        "outputId": "79dc06f5-a00e-4fb0-ff55-db3b7e1b972e"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .2, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"DVN\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 8s 16ms/step - loss: 0.6782 - accuracy: 0.5604 - val_loss: 0.6606 - val_accuracy: 0.6673\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6029 - accuracy: 0.6826 - val_loss: 0.6381 - val_accuracy: 0.6408\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5916 - accuracy: 0.6993 - val_loss: 0.6172 - val_accuracy: 0.6796\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5624 - accuracy: 0.7154 - val_loss: 0.6271 - val_accuracy: 0.6510\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5541 - accuracy: 0.7215 - val_loss: 0.6338 - val_accuracy: 0.6408\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6336 - accuracy: 0.6329 - val_loss: 0.5965 - val_accuracy: 0.6980\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5288 - accuracy: 0.7295 - val_loss: 0.5846 - val_accuracy: 0.7041\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4944 - accuracy: 0.7570 - val_loss: 0.5721 - val_accuracy: 0.6918\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4891 - accuracy: 0.7597 - val_loss: 0.5769 - val_accuracy: 0.6878\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4761 - accuracy: 0.7779 - val_loss: 0.5817 - val_accuracy: 0.6898\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.74117\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.757335\n",
            "[2]\tvalidation_0-auc:0.766712\n",
            "[3]\tvalidation_0-auc:0.76809\n",
            "[4]\tvalidation_0-auc:0.76809\n",
            "[5]\tvalidation_0-auc:0.770261\n",
            "[6]\tvalidation_0-auc:0.772382\n",
            "[7]\tvalidation_0-auc:0.773017\n",
            "[8]\tvalidation_0-auc:0.772749\n",
            "[9]\tvalidation_0-auc:0.772215\n",
            "[10]\tvalidation_0-auc:0.772983\n",
            "[11]\tvalidation_0-auc:0.773175\n",
            "[12]\tvalidation_0-auc:0.773685\n",
            "[13]\tvalidation_0-auc:0.775805\n",
            "[14]\tvalidation_0-auc:0.776181\n",
            "[15]\tvalidation_0-auc:0.776148\n",
            "[16]\tvalidation_0-auc:0.776874\n",
            "[17]\tvalidation_0-auc:0.776774\n",
            "[18]\tvalidation_0-auc:0.777066\n",
            "[19]\tvalidation_0-auc:0.777083\n",
            "[20]\tvalidation_0-auc:0.77796\n",
            "[21]\tvalidation_0-auc:0.777926\n",
            "[22]\tvalidation_0-auc:0.778494\n",
            "[23]\tvalidation_0-auc:0.779337\n",
            "[24]\tvalidation_0-auc:0.779521\n",
            "[25]\tvalidation_0-auc:0.778653\n",
            "[26]\tvalidation_0-auc:0.778628\n",
            "[27]\tvalidation_0-auc:0.778519\n",
            "[28]\tvalidation_0-auc:0.777091\n",
            "[29]\tvalidation_0-auc:0.777125\n",
            "[30]\tvalidation_0-auc:0.77624\n",
            "[31]\tvalidation_0-auc:0.77624\n",
            "[32]\tvalidation_0-auc:0.776507\n",
            "[33]\tvalidation_0-auc:0.774486\n",
            "[34]\tvalidation_0-auc:0.773968\n",
            "[35]\tvalidation_0-auc:0.773576\n",
            "[36]\tvalidation_0-auc:0.773526\n",
            "[37]\tvalidation_0-auc:0.773593\n",
            "[38]\tvalidation_0-auc:0.774077\n",
            "[39]\tvalidation_0-auc:0.774327\n",
            "[40]\tvalidation_0-auc:0.774361\n",
            "[41]\tvalidation_0-auc:0.773943\n",
            "[42]\tvalidation_0-auc:0.773943\n",
            "[43]\tvalidation_0-auc:0.774762\n",
            "[44]\tvalidation_0-auc:0.774553\n",
            "[45]\tvalidation_0-auc:0.774352\n",
            "[46]\tvalidation_0-auc:0.774052\n",
            "[47]\tvalidation_0-auc:0.775304\n",
            "[48]\tvalidation_0-auc:0.775213\n",
            "[49]\tvalidation_0-auc:0.775029\n",
            "[50]\tvalidation_0-auc:0.774962\n",
            "[51]\tvalidation_0-auc:0.774962\n",
            "[52]\tvalidation_0-auc:0.774979\n",
            "[53]\tvalidation_0-auc:0.775112\n",
            "[54]\tvalidation_0-auc:0.775238\n",
            "[55]\tvalidation_0-auc:0.775187\n",
            "[56]\tvalidation_0-auc:0.774678\n",
            "[57]\tvalidation_0-auc:0.774611\n",
            "[58]\tvalidation_0-auc:0.774578\n",
            "[59]\tvalidation_0-auc:0.768975\n",
            "[60]\tvalidation_0-auc:0.769125\n",
            "[61]\tvalidation_0-auc:0.770528\n",
            "[62]\tvalidation_0-auc:0.770679\n",
            "[63]\tvalidation_0-auc:0.770695\n",
            "[64]\tvalidation_0-auc:0.771012\n",
            "[65]\tvalidation_0-auc:0.77148\n",
            "[66]\tvalidation_0-auc:0.771447\n",
            "[67]\tvalidation_0-auc:0.767639\n",
            "[68]\tvalidation_0-auc:0.767589\n",
            "[69]\tvalidation_0-auc:0.767848\n",
            "[70]\tvalidation_0-auc:0.768374\n",
            "[71]\tvalidation_0-auc:0.768407\n",
            "[72]\tvalidation_0-auc:0.768524\n",
            "[73]\tvalidation_0-auc:0.768708\n",
            "[74]\tvalidation_0-auc:0.768391\n",
            "Stopping. Best iteration:\n",
            "[24]\tvalidation_0-auc:0.779521\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6761 - accuracy: 0.5738 - val_loss: 0.6828 - val_accuracy: 0.4902\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6582 - accuracy: 0.6246 - val_loss: 0.6847 - val_accuracy: 0.5733\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6427 - accuracy: 0.6397 - val_loss: 0.6453 - val_accuracy: 0.6302\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6048 - accuracy: 0.6815 - val_loss: 0.6305 - val_accuracy: 0.6871\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5478 - accuracy: 0.7255 - val_loss: 0.6186 - val_accuracy: 0.6411\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6359 - accuracy: 0.6239 - val_loss: 0.6347 - val_accuracy: 0.6893\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5258 - accuracy: 0.7392 - val_loss: 0.6405 - val_accuracy: 0.6083\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5295 - accuracy: 0.7419 - val_loss: 0.5858 - val_accuracy: 0.6980\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5108 - accuracy: 0.7502 - val_loss: 0.5872 - val_accuracy: 0.6849\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5116 - accuracy: 0.7687 - val_loss: 0.5593 - val_accuracy: 0.7221\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.570932\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.584488\n",
            "[2]\tvalidation_0-auc:0.723976\n",
            "[3]\tvalidation_0-auc:0.716097\n",
            "[4]\tvalidation_0-auc:0.726458\n",
            "[5]\tvalidation_0-auc:0.777342\n",
            "[6]\tvalidation_0-auc:0.77257\n",
            "[7]\tvalidation_0-auc:0.754483\n",
            "[8]\tvalidation_0-auc:0.739013\n",
            "[9]\tvalidation_0-auc:0.73656\n",
            "[10]\tvalidation_0-auc:0.747508\n",
            "[11]\tvalidation_0-auc:0.75101\n",
            "[12]\tvalidation_0-auc:0.752963\n",
            "[13]\tvalidation_0-auc:0.74238\n",
            "[14]\tvalidation_0-auc:0.741005\n",
            "[15]\tvalidation_0-auc:0.731615\n",
            "[16]\tvalidation_0-auc:0.732596\n",
            "[17]\tvalidation_0-auc:0.728652\n",
            "[18]\tvalidation_0-auc:0.729248\n",
            "[19]\tvalidation_0-auc:0.72971\n",
            "[20]\tvalidation_0-auc:0.725343\n",
            "[21]\tvalidation_0-auc:0.725314\n",
            "[22]\tvalidation_0-auc:0.72541\n",
            "[23]\tvalidation_0-auc:0.720869\n",
            "[24]\tvalidation_0-auc:0.720869\n",
            "[25]\tvalidation_0-auc:0.720638\n",
            "[26]\tvalidation_0-auc:0.723697\n",
            "[27]\tvalidation_0-auc:0.717598\n",
            "[28]\tvalidation_0-auc:0.717598\n",
            "[29]\tvalidation_0-auc:0.717598\n",
            "[30]\tvalidation_0-auc:0.717069\n",
            "[31]\tvalidation_0-auc:0.717069\n",
            "[32]\tvalidation_0-auc:0.717069\n",
            "[33]\tvalidation_0-auc:0.715683\n",
            "[34]\tvalidation_0-auc:0.714298\n",
            "[35]\tvalidation_0-auc:0.714298\n",
            "[36]\tvalidation_0-auc:0.71805\n",
            "[37]\tvalidation_0-auc:0.711268\n",
            "[38]\tvalidation_0-auc:0.705899\n",
            "[39]\tvalidation_0-auc:0.706034\n",
            "[40]\tvalidation_0-auc:0.706034\n",
            "[41]\tvalidation_0-auc:0.707862\n",
            "[42]\tvalidation_0-auc:0.707391\n",
            "[43]\tvalidation_0-auc:0.707391\n",
            "[44]\tvalidation_0-auc:0.707275\n",
            "[45]\tvalidation_0-auc:0.707304\n",
            "[46]\tvalidation_0-auc:0.707304\n",
            "[47]\tvalidation_0-auc:0.705361\n",
            "[48]\tvalidation_0-auc:0.705514\n",
            "[49]\tvalidation_0-auc:0.701243\n",
            "[50]\tvalidation_0-auc:0.693142\n",
            "[51]\tvalidation_0-auc:0.692796\n",
            "[52]\tvalidation_0-auc:0.692912\n",
            "[53]\tvalidation_0-auc:0.689525\n",
            "[54]\tvalidation_0-auc:0.688659\n",
            "[55]\tvalidation_0-auc:0.691719\n",
            "Stopping. Best iteration:\n",
            "[5]\tvalidation_0-auc:0.777342\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.6408163265306123 | 0.5776566757493188 |  0.9098712446351931 |  0.7066666666666667 |\n",
            "|     GRU 0.1      | 0.689795918367347  | 0.7515527950310559 |   0.51931330472103  |  0.6142131979695431 |\n",
            "|   XGBoost 0.1    |        0.7         | 0.6822033898305084 |  0.6909871244635193 |  0.6865671641791045 |\n",
            "|    Logreg 0.1    | 0.6979591836734694 | 0.6653696498054474 |  0.7339055793991416 |  0.6979591836734694 |\n",
            "|     SVM 0.1      | 0.6979591836734694 | 0.7093596059113301 |  0.6180257510729614 |  0.6605504587155964 |\n",
            "|  LSTM beta 0.1   | 0.6411378555798687 | 0.855072463768116  | 0.27699530516431925 | 0.41843971631205673 |\n",
            "|   GRU beta 0.1   | 0.7221006564551422 |        0.75        |  0.6056338028169014 |   0.67012987012987  |\n",
            "| XGBoost beta 0.1 | 0.6367614879649891 | 0.6942148760330579 | 0.39436619718309857 |  0.5029940119760479 |\n",
            "| logreg beta 0.1  | 0.7133479212253829 |       0.705        |  0.6619718309859155 |  0.6828087167070217 |\n",
            "|   svm beta 0.1   | 0.6761487964989059 | 0.782608695652174  |  0.4225352112676056 |  0.548780487804878  |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6921 - accuracy: 0.5195 - val_loss: 0.6974 - val_accuracy: 0.4551\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6262 - accuracy: 0.6597 - val_loss: 0.7090 - val_accuracy: 0.5592\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6085 - accuracy: 0.6812 - val_loss: 0.6477 - val_accuracy: 0.6429\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.5918 - accuracy: 0.7054 - val_loss: 0.7762 - val_accuracy: 0.4388\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.5838 - accuracy: 0.6926 - val_loss: 0.7146 - val_accuracy: 0.5776\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6666 - accuracy: 0.5685 - val_loss: 0.7907 - val_accuracy: 0.3388\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5718 - accuracy: 0.6946 - val_loss: 0.7207 - val_accuracy: 0.5510\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5646 - accuracy: 0.7114 - val_loss: 0.6342 - val_accuracy: 0.6469\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5496 - accuracy: 0.7268 - val_loss: 0.6532 - val_accuracy: 0.6265\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5568 - accuracy: 0.7087 - val_loss: 0.6247 - val_accuracy: 0.6510\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.640782\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.640782\n",
            "[2]\tvalidation_0-auc:0.640782\n",
            "[3]\tvalidation_0-auc:0.638974\n",
            "[4]\tvalidation_0-auc:0.64854\n",
            "[5]\tvalidation_0-auc:0.658863\n",
            "[6]\tvalidation_0-auc:0.66424\n",
            "[7]\tvalidation_0-auc:0.66459\n",
            "[8]\tvalidation_0-auc:0.662479\n",
            "[9]\tvalidation_0-auc:0.661581\n",
            "[10]\tvalidation_0-auc:0.661674\n",
            "[11]\tvalidation_0-auc:0.65506\n",
            "[12]\tvalidation_0-auc:0.655317\n",
            "[13]\tvalidation_0-auc:0.648026\n",
            "[14]\tvalidation_0-auc:0.65373\n",
            "[15]\tvalidation_0-auc:0.653835\n",
            "[16]\tvalidation_0-auc:0.651152\n",
            "[17]\tvalidation_0-auc:0.655154\n",
            "[18]\tvalidation_0-auc:0.655119\n",
            "[19]\tvalidation_0-auc:0.651887\n",
            "[20]\tvalidation_0-auc:0.65317\n",
            "[21]\tvalidation_0-auc:0.653485\n",
            "[22]\tvalidation_0-auc:0.653462\n",
            "[23]\tvalidation_0-auc:0.652926\n",
            "[24]\tvalidation_0-auc:0.652984\n",
            "[25]\tvalidation_0-auc:0.653299\n",
            "[26]\tvalidation_0-auc:0.653089\n",
            "[27]\tvalidation_0-auc:0.653345\n",
            "[28]\tvalidation_0-auc:0.653369\n",
            "[29]\tvalidation_0-auc:0.653275\n",
            "[30]\tvalidation_0-auc:0.653182\n",
            "[31]\tvalidation_0-auc:0.652412\n",
            "[32]\tvalidation_0-auc:0.652237\n",
            "[33]\tvalidation_0-auc:0.652062\n",
            "[34]\tvalidation_0-auc:0.651992\n",
            "[35]\tvalidation_0-auc:0.650884\n",
            "[36]\tvalidation_0-auc:0.651607\n",
            "[37]\tvalidation_0-auc:0.651444\n",
            "[38]\tvalidation_0-auc:0.651257\n",
            "[39]\tvalidation_0-auc:0.649799\n",
            "[40]\tvalidation_0-auc:0.649683\n",
            "[41]\tvalidation_0-auc:0.648936\n",
            "[42]\tvalidation_0-auc:0.649099\n",
            "[43]\tvalidation_0-auc:0.649053\n",
            "[44]\tvalidation_0-auc:0.649006\n",
            "[45]\tvalidation_0-auc:0.649006\n",
            "[46]\tvalidation_0-auc:0.649006\n",
            "[47]\tvalidation_0-auc:0.649006\n",
            "[48]\tvalidation_0-auc:0.649006\n",
            "[49]\tvalidation_0-auc:0.648854\n",
            "[50]\tvalidation_0-auc:0.646592\n",
            "[51]\tvalidation_0-auc:0.646452\n",
            "[52]\tvalidation_0-auc:0.646662\n",
            "[53]\tvalidation_0-auc:0.646662\n",
            "[54]\tvalidation_0-auc:0.646662\n",
            "[55]\tvalidation_0-auc:0.646662\n",
            "[56]\tvalidation_0-auc:0.645052\n",
            "[57]\tvalidation_0-auc:0.644818\n",
            "Stopping. Best iteration:\n",
            "[7]\tvalidation_0-auc:0.66459\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6697 - accuracy: 0.5882 - val_loss: 0.7494 - val_accuracy: 0.3282\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6128 - accuracy: 0.6857 - val_loss: 0.6350 - val_accuracy: 0.6193\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5722 - accuracy: 0.7248 - val_loss: 0.7119 - val_accuracy: 0.5908\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5403 - accuracy: 0.7385 - val_loss: 0.5944 - val_accuracy: 0.6958\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5115 - accuracy: 0.7570 - val_loss: 0.4501 - val_accuracy: 0.8228\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6201 - accuracy: 0.6568 - val_loss: 0.5813 - val_accuracy: 0.7155\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5323 - accuracy: 0.7447 - val_loss: 0.6341 - val_accuracy: 0.6783\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5212 - accuracy: 0.7529 - val_loss: 0.5644 - val_accuracy: 0.7265\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5068 - accuracy: 0.7714 - val_loss: 0.5760 - val_accuracy: 0.7221\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5041 - accuracy: 0.7639 - val_loss: 0.5214 - val_accuracy: 0.7374\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.667297\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.675034\n",
            "[2]\tvalidation_0-auc:0.675195\n",
            "[3]\tvalidation_0-auc:0.675869\n",
            "[4]\tvalidation_0-auc:0.674521\n",
            "[5]\tvalidation_0-auc:0.666769\n",
            "[6]\tvalidation_0-auc:0.671634\n",
            "[7]\tvalidation_0-auc:0.679459\n",
            "[8]\tvalidation_0-auc:0.677525\n",
            "[9]\tvalidation_0-auc:0.675092\n",
            "[10]\tvalidation_0-auc:0.675092\n",
            "[11]\tvalidation_0-auc:0.676382\n",
            "[12]\tvalidation_0-auc:0.67159\n",
            "[13]\tvalidation_0-auc:0.67159\n",
            "[14]\tvalidation_0-auc:0.669436\n",
            "[15]\tvalidation_0-auc:0.670491\n",
            "[16]\tvalidation_0-auc:0.679825\n",
            "[17]\tvalidation_0-auc:0.6794\n",
            "[18]\tvalidation_0-auc:0.681891\n",
            "[19]\tvalidation_0-auc:0.673187\n",
            "[20]\tvalidation_0-auc:0.672748\n",
            "[21]\tvalidation_0-auc:0.673217\n",
            "[22]\tvalidation_0-auc:0.674125\n",
            "[23]\tvalidation_0-auc:0.674711\n",
            "[24]\tvalidation_0-auc:0.674711\n",
            "[25]\tvalidation_0-auc:0.672689\n",
            "[26]\tvalidation_0-auc:0.67351\n",
            "[27]\tvalidation_0-auc:0.674067\n",
            "[28]\tvalidation_0-auc:0.674096\n",
            "[29]\tvalidation_0-auc:0.674096\n",
            "[30]\tvalidation_0-auc:0.674917\n",
            "[31]\tvalidation_0-auc:0.679767\n",
            "[32]\tvalidation_0-auc:0.679767\n",
            "[33]\tvalidation_0-auc:0.680661\n",
            "[34]\tvalidation_0-auc:0.68217\n",
            "[35]\tvalidation_0-auc:0.677686\n",
            "[36]\tvalidation_0-auc:0.678213\n",
            "[37]\tvalidation_0-auc:0.678213\n",
            "[38]\tvalidation_0-auc:0.6788\n",
            "[39]\tvalidation_0-auc:0.679444\n",
            "[40]\tvalidation_0-auc:0.671209\n",
            "[41]\tvalidation_0-auc:0.674462\n",
            "[42]\tvalidation_0-auc:0.674521\n",
            "[43]\tvalidation_0-auc:0.674521\n",
            "[44]\tvalidation_0-auc:0.674682\n",
            "[45]\tvalidation_0-auc:0.674213\n",
            "[46]\tvalidation_0-auc:0.672806\n",
            "[47]\tvalidation_0-auc:0.673246\n",
            "[48]\tvalidation_0-auc:0.673246\n",
            "[49]\tvalidation_0-auc:0.673246\n",
            "[50]\tvalidation_0-auc:0.670315\n",
            "[51]\tvalidation_0-auc:0.670315\n",
            "[52]\tvalidation_0-auc:0.670315\n",
            "[53]\tvalidation_0-auc:0.671488\n",
            "[54]\tvalidation_0-auc:0.673363\n",
            "[55]\tvalidation_0-auc:0.672675\n",
            "[56]\tvalidation_0-auc:0.672616\n",
            "[57]\tvalidation_0-auc:0.672381\n",
            "[58]\tvalidation_0-auc:0.672059\n",
            "[59]\tvalidation_0-auc:0.670403\n",
            "[60]\tvalidation_0-auc:0.668703\n",
            "[61]\tvalidation_0-auc:0.669026\n",
            "[62]\tvalidation_0-auc:0.667487\n",
            "[63]\tvalidation_0-auc:0.667487\n",
            "[64]\tvalidation_0-auc:0.667106\n",
            "[65]\tvalidation_0-auc:0.667106\n",
            "[66]\tvalidation_0-auc:0.666989\n",
            "[67]\tvalidation_0-auc:0.666989\n",
            "[68]\tvalidation_0-auc:0.663384\n",
            "[69]\tvalidation_0-auc:0.66356\n",
            "[70]\tvalidation_0-auc:0.66441\n",
            "[71]\tvalidation_0-auc:0.66441\n",
            "[72]\tvalidation_0-auc:0.664586\n",
            "[73]\tvalidation_0-auc:0.664586\n",
            "[74]\tvalidation_0-auc:0.664014\n",
            "[75]\tvalidation_0-auc:0.664014\n",
            "[76]\tvalidation_0-auc:0.663751\n",
            "[77]\tvalidation_0-auc:0.663135\n",
            "[78]\tvalidation_0-auc:0.663135\n",
            "[79]\tvalidation_0-auc:0.661992\n",
            "[80]\tvalidation_0-auc:0.661699\n",
            "[81]\tvalidation_0-auc:0.662022\n",
            "[82]\tvalidation_0-auc:0.662315\n",
            "[83]\tvalidation_0-auc:0.662315\n",
            "[84]\tvalidation_0-auc:0.661494\n",
            "Stopping. Best iteration:\n",
            "[34]\tvalidation_0-auc:0.68217\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.2     | 0.5775510204081633 |  0.3147410358565737 |  0.6929824561403509 |  0.4328767123287671 |\n",
            "|     GRU 0.2      | 0.6510204081632653 | 0.32075471698113206 |  0.4473684210526316 | 0.37362637362637363 |\n",
            "|   XGBoost 0.2    | 0.5408163265306123 | 0.30662020905923343 |  0.7719298245614035 |  0.4389027431421446 |\n",
            "|    Logreg 0.2    | 0.6020408163265306 |  0.3276595744680851 |  0.6754385964912281 |  0.4412607449856733 |\n",
            "|     SVM 0.2      | 0.6265306122448979 |  0.3333333333333333 |  0.6052631578947368 |  0.4299065420560747 |\n",
            "|  LSTM beta 0.2   | 0.8227571115973742 |  0.6226415094339622 | 0.35106382978723405 |  0.4489795918367347 |\n",
            "|   GRU beta 0.2   | 0.737417943107221  |  0.3673469387755102 |  0.3829787234042553 | 0.37499999999999994 |\n",
            "| XGBoost beta 0.2 | 0.787746170678337  |  0.4810126582278481 | 0.40425531914893614 |  0.4393063583815029 |\n",
            "| logreg beta 0.2  | 0.6958424507658644 |  0.3448275862068966 |  0.5319148936170213 | 0.41841004184100417 |\n",
            "|   svm beta 0.2   |  0.75054704595186  |  0.3979591836734694 |  0.4148936170212766 |       0.40625       |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6744 - accuracy: 0.5564 - val_loss: 0.6687 - val_accuracy: 0.5878\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6082 - accuracy: 0.6819 - val_loss: 0.6658 - val_accuracy: 0.6388\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5932 - accuracy: 0.6980 - val_loss: 0.6377 - val_accuracy: 0.6612\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5695 - accuracy: 0.7141 - val_loss: 0.6497 - val_accuracy: 0.6286\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5601 - accuracy: 0.7195 - val_loss: 0.6363 - val_accuracy: 0.6449\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6508 - accuracy: 0.6181 - val_loss: 0.6613 - val_accuracy: 0.6163\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5566 - accuracy: 0.7107 - val_loss: 0.6359 - val_accuracy: 0.6633\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5521 - accuracy: 0.7081 - val_loss: 0.6394 - val_accuracy: 0.6408\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5401 - accuracy: 0.7195 - val_loss: 0.6433 - val_accuracy: 0.6327\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5323 - accuracy: 0.7349 - val_loss: 0.6342 - val_accuracy: 0.6429\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.677697\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.677697\n",
            "[2]\tvalidation_0-auc:0.676435\n",
            "[3]\tvalidation_0-auc:0.679513\n",
            "[4]\tvalidation_0-auc:0.686895\n",
            "[5]\tvalidation_0-auc:0.685454\n",
            "[6]\tvalidation_0-auc:0.689034\n",
            "[7]\tvalidation_0-auc:0.689486\n",
            "[8]\tvalidation_0-auc:0.690066\n",
            "[9]\tvalidation_0-auc:0.689767\n",
            "[10]\tvalidation_0-auc:0.688813\n",
            "[11]\tvalidation_0-auc:0.688685\n",
            "[12]\tvalidation_0-auc:0.689836\n",
            "[13]\tvalidation_0-auc:0.684772\n",
            "[14]\tvalidation_0-auc:0.684397\n",
            "[15]\tvalidation_0-auc:0.685079\n",
            "[16]\tvalidation_0-auc:0.681738\n",
            "[17]\tvalidation_0-auc:0.682607\n",
            "[18]\tvalidation_0-auc:0.681942\n",
            "[19]\tvalidation_0-auc:0.682684\n",
            "[20]\tvalidation_0-auc:0.685735\n",
            "[21]\tvalidation_0-auc:0.685718\n",
            "[22]\tvalidation_0-auc:0.685429\n",
            "[23]\tvalidation_0-auc:0.683724\n",
            "[24]\tvalidation_0-auc:0.681917\n",
            "[25]\tvalidation_0-auc:0.679283\n",
            "[26]\tvalidation_0-auc:0.679973\n",
            "[27]\tvalidation_0-auc:0.679973\n",
            "[28]\tvalidation_0-auc:0.678967\n",
            "[29]\tvalidation_0-auc:0.680834\n",
            "[30]\tvalidation_0-auc:0.680834\n",
            "[31]\tvalidation_0-auc:0.682104\n",
            "[32]\tvalidation_0-auc:0.68161\n",
            "[33]\tvalidation_0-auc:0.681243\n",
            "[34]\tvalidation_0-auc:0.681286\n",
            "[35]\tvalidation_0-auc:0.680382\n",
            "[36]\tvalidation_0-auc:0.680263\n",
            "[37]\tvalidation_0-auc:0.679939\n",
            "[38]\tvalidation_0-auc:0.680041\n",
            "[39]\tvalidation_0-auc:0.680041\n",
            "[40]\tvalidation_0-auc:0.679982\n",
            "[41]\tvalidation_0-auc:0.679982\n",
            "[42]\tvalidation_0-auc:0.678959\n",
            "[43]\tvalidation_0-auc:0.679257\n",
            "[44]\tvalidation_0-auc:0.680621\n",
            "[45]\tvalidation_0-auc:0.681286\n",
            "[46]\tvalidation_0-auc:0.681294\n",
            "[47]\tvalidation_0-auc:0.680203\n",
            "[48]\tvalidation_0-auc:0.679291\n",
            "[49]\tvalidation_0-auc:0.679189\n",
            "[50]\tvalidation_0-auc:0.680561\n",
            "[51]\tvalidation_0-auc:0.680561\n",
            "[52]\tvalidation_0-auc:0.682044\n",
            "[53]\tvalidation_0-auc:0.682232\n",
            "[54]\tvalidation_0-auc:0.682616\n",
            "[55]\tvalidation_0-auc:0.682616\n",
            "[56]\tvalidation_0-auc:0.682581\n",
            "[57]\tvalidation_0-auc:0.682283\n",
            "[58]\tvalidation_0-auc:0.682283\n",
            "Stopping. Best iteration:\n",
            "[8]\tvalidation_0-auc:0.690066\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6794 - accuracy: 0.5621 - val_loss: 0.6804 - val_accuracy: 0.6018\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6053 - accuracy: 0.6712 - val_loss: 0.6289 - val_accuracy: 0.6608\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5537 - accuracy: 0.7303 - val_loss: 0.6052 - val_accuracy: 0.6980\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5016 - accuracy: 0.7708 - val_loss: 0.5939 - val_accuracy: 0.7046\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.5109 - accuracy: 0.7577 - val_loss: 0.5860 - val_accuracy: 0.7112\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6123 - accuracy: 0.6706 - val_loss: 0.6062 - val_accuracy: 0.6980\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5218 - accuracy: 0.7481 - val_loss: 0.6100 - val_accuracy: 0.7024\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5038 - accuracy: 0.7639 - val_loss: 0.5876 - val_accuracy: 0.7155\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5084 - accuracy: 0.7577 - val_loss: 0.5987 - val_accuracy: 0.6958\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5109 - accuracy: 0.7721 - val_loss: 0.5957 - val_accuracy: 0.7068\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.686081\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.717966\n",
            "[2]\tvalidation_0-auc:0.725441\n",
            "[3]\tvalidation_0-auc:0.701366\n",
            "[4]\tvalidation_0-auc:0.705351\n",
            "[5]\tvalidation_0-auc:0.718026\n",
            "[6]\tvalidation_0-auc:0.707595\n",
            "[7]\tvalidation_0-auc:0.703759\n",
            "[8]\tvalidation_0-auc:0.712548\n",
            "[9]\tvalidation_0-auc:0.713557\n",
            "[10]\tvalidation_0-auc:0.701851\n",
            "[11]\tvalidation_0-auc:0.707585\n",
            "[12]\tvalidation_0-auc:0.71154\n",
            "[13]\tvalidation_0-auc:0.711441\n",
            "[14]\tvalidation_0-auc:0.711421\n",
            "[15]\tvalidation_0-auc:0.711362\n",
            "[16]\tvalidation_0-auc:0.712262\n",
            "[17]\tvalidation_0-auc:0.715633\n",
            "[18]\tvalidation_0-auc:0.715564\n",
            "[19]\tvalidation_0-auc:0.717136\n",
            "[20]\tvalidation_0-auc:0.718065\n",
            "[21]\tvalidation_0-auc:0.722247\n",
            "[22]\tvalidation_0-auc:0.722495\n",
            "[23]\tvalidation_0-auc:0.723721\n",
            "[24]\tvalidation_0-auc:0.724571\n",
            "[25]\tvalidation_0-auc:0.724571\n",
            "[26]\tvalidation_0-auc:0.727932\n",
            "[27]\tvalidation_0-auc:0.727379\n",
            "[28]\tvalidation_0-auc:0.727596\n",
            "[29]\tvalidation_0-auc:0.727932\n",
            "[30]\tvalidation_0-auc:0.728367\n",
            "[31]\tvalidation_0-auc:0.728367\n",
            "[32]\tvalidation_0-auc:0.723493\n",
            "[33]\tvalidation_0-auc:0.723493\n",
            "[34]\tvalidation_0-auc:0.723493\n",
            "[35]\tvalidation_0-auc:0.723493\n",
            "[36]\tvalidation_0-auc:0.726944\n",
            "[37]\tvalidation_0-auc:0.724739\n",
            "[38]\tvalidation_0-auc:0.724996\n",
            "[39]\tvalidation_0-auc:0.726786\n",
            "[40]\tvalidation_0-auc:0.728427\n",
            "[41]\tvalidation_0-auc:0.728427\n",
            "[42]\tvalidation_0-auc:0.728427\n",
            "[43]\tvalidation_0-auc:0.728269\n",
            "[44]\tvalidation_0-auc:0.730187\n",
            "[45]\tvalidation_0-auc:0.730187\n",
            "[46]\tvalidation_0-auc:0.732223\n",
            "[47]\tvalidation_0-auc:0.732223\n",
            "[48]\tvalidation_0-auc:0.731788\n",
            "[49]\tvalidation_0-auc:0.731848\n",
            "[50]\tvalidation_0-auc:0.72552\n",
            "[51]\tvalidation_0-auc:0.72552\n",
            "[52]\tvalidation_0-auc:0.72552\n",
            "[53]\tvalidation_0-auc:0.722327\n",
            "[54]\tvalidation_0-auc:0.722327\n",
            "[55]\tvalidation_0-auc:0.724126\n",
            "[56]\tvalidation_0-auc:0.724126\n",
            "[57]\tvalidation_0-auc:0.723582\n",
            "[58]\tvalidation_0-auc:0.724096\n",
            "[59]\tvalidation_0-auc:0.724225\n",
            "[60]\tvalidation_0-auc:0.724225\n",
            "[61]\tvalidation_0-auc:0.724225\n",
            "[62]\tvalidation_0-auc:0.722277\n",
            "[63]\tvalidation_0-auc:0.721625\n",
            "[64]\tvalidation_0-auc:0.724729\n",
            "[65]\tvalidation_0-auc:0.724156\n",
            "[66]\tvalidation_0-auc:0.725115\n",
            "[67]\tvalidation_0-auc:0.726707\n",
            "[68]\tvalidation_0-auc:0.723928\n",
            "[69]\tvalidation_0-auc:0.723958\n",
            "[70]\tvalidation_0-auc:0.724255\n",
            "[71]\tvalidation_0-auc:0.724255\n",
            "[72]\tvalidation_0-auc:0.724255\n",
            "[73]\tvalidation_0-auc:0.728041\n",
            "[74]\tvalidation_0-auc:0.728041\n",
            "[75]\tvalidation_0-auc:0.726815\n",
            "[76]\tvalidation_0-auc:0.722168\n",
            "[77]\tvalidation_0-auc:0.722168\n",
            "[78]\tvalidation_0-auc:0.721852\n",
            "[79]\tvalidation_0-auc:0.721832\n",
            "[80]\tvalidation_0-auc:0.72205\n",
            "[81]\tvalidation_0-auc:0.724136\n",
            "[82]\tvalidation_0-auc:0.724077\n",
            "[83]\tvalidation_0-auc:0.723819\n",
            "[84]\tvalidation_0-auc:0.725036\n",
            "[85]\tvalidation_0-auc:0.725036\n",
            "[86]\tvalidation_0-auc:0.725055\n",
            "[87]\tvalidation_0-auc:0.725134\n",
            "[88]\tvalidation_0-auc:0.725253\n",
            "[89]\tvalidation_0-auc:0.725471\n",
            "[90]\tvalidation_0-auc:0.725026\n",
            "[91]\tvalidation_0-auc:0.725836\n",
            "[92]\tvalidation_0-auc:0.725678\n",
            "[93]\tvalidation_0-auc:0.723187\n",
            "[94]\tvalidation_0-auc:0.724096\n",
            "[95]\tvalidation_0-auc:0.724591\n",
            "[96]\tvalidation_0-auc:0.724591\n",
            "Stopping. Best iteration:\n",
            "[46]\tvalidation_0-auc:0.732223\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |        Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.6448979591836734 |      0.60625       | 0.46634615384615385 | 0.5271739130434783 |\n",
            "|      GRU 0.15     | 0.6428571428571429 | 0.6024844720496895 | 0.46634615384615385 | 0.5257452574525745 |\n",
            "|    XGBoost 0.15   | 0.5979591836734693 | 0.5214007782101168 |  0.6442307692307693 | 0.5763440860215053 |\n",
            "|    Logreg 0.15    | 0.636734693877551  | 0.5675675675675675 |  0.6057692307692307 | 0.5860465116279069 |\n",
            "|      SVM 0.15     | 0.6387755102040816 | 0.5803108808290155 |  0.5384615384615384 | 0.5586034912718204 |\n",
            "|   LSTM beta 0.15  | 0.7111597374179431 | 0.7121212121212122 |         0.5         |       0.5875       |\n",
            "|   GRU beta 0.15   | 0.7067833698030634 |       0.6875       |  0.526595744680851  | 0.5963855421686746 |\n",
            "| XGBoost beta 0.15 | 0.6980306345733042 | 0.7777777777777778 |  0.3723404255319149 | 0.5035971223021583 |\n",
            "|  logreg beta 0.15 | 0.6958424507658644 | 0.6713286713286714 |  0.5106382978723404 | 0.580060422960725  |\n",
            "|   svm beta 0.15   | 0.6936542669584245 |        0.74        | 0.39361702127659576 | 0.513888888888889  |\n",
            "+-------------------+--------------------+--------------------+---------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAd9FoNiUbov",
        "outputId": "2c2d78c0-bee9-491f-c923-0f2e0145a3d4"
      },
      "source": [
        "Result_cross.to_csv('DVN_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.577657</td>\n",
              "      <td>0.640816</td>\n",
              "      <td>0.706667</td>\n",
              "      <td>0.909871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.751553</td>\n",
              "      <td>0.689796</td>\n",
              "      <td>0.614213</td>\n",
              "      <td>0.519313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.682203</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.686567</td>\n",
              "      <td>0.690987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.665370</td>\n",
              "      <td>0.697959</td>\n",
              "      <td>0.697959</td>\n",
              "      <td>0.733906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.709360</td>\n",
              "      <td>0.697959</td>\n",
              "      <td>0.660550</td>\n",
              "      <td>0.618026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.855072</td>\n",
              "      <td>0.641138</td>\n",
              "      <td>0.418440</td>\n",
              "      <td>0.276995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.722101</td>\n",
              "      <td>0.670130</td>\n",
              "      <td>0.605634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.694215</td>\n",
              "      <td>0.636761</td>\n",
              "      <td>0.502994</td>\n",
              "      <td>0.394366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.705000</td>\n",
              "      <td>0.713348</td>\n",
              "      <td>0.682809</td>\n",
              "      <td>0.661972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.676149</td>\n",
              "      <td>0.548780</td>\n",
              "      <td>0.422535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.314741</td>\n",
              "      <td>0.577551</td>\n",
              "      <td>0.432877</td>\n",
              "      <td>0.692982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.320755</td>\n",
              "      <td>0.651020</td>\n",
              "      <td>0.373626</td>\n",
              "      <td>0.447368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.306620</td>\n",
              "      <td>0.540816</td>\n",
              "      <td>0.438903</td>\n",
              "      <td>0.771930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.327660</td>\n",
              "      <td>0.602041</td>\n",
              "      <td>0.441261</td>\n",
              "      <td>0.675439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.626531</td>\n",
              "      <td>0.429907</td>\n",
              "      <td>0.605263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.622642</td>\n",
              "      <td>0.822757</td>\n",
              "      <td>0.448980</td>\n",
              "      <td>0.351064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.367347</td>\n",
              "      <td>0.737418</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.382979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.481013</td>\n",
              "      <td>0.787746</td>\n",
              "      <td>0.439306</td>\n",
              "      <td>0.404255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.344828</td>\n",
              "      <td>0.695842</td>\n",
              "      <td>0.418410</td>\n",
              "      <td>0.531915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>0.750547</td>\n",
              "      <td>0.406250</td>\n",
              "      <td>0.414894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.606250</td>\n",
              "      <td>0.644898</td>\n",
              "      <td>0.527174</td>\n",
              "      <td>0.466346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.602484</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.525745</td>\n",
              "      <td>0.466346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.521401</td>\n",
              "      <td>0.597959</td>\n",
              "      <td>0.576344</td>\n",
              "      <td>0.644231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.567568</td>\n",
              "      <td>0.636735</td>\n",
              "      <td>0.586047</td>\n",
              "      <td>0.605769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.580311</td>\n",
              "      <td>0.638776</td>\n",
              "      <td>0.558603</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.712121</td>\n",
              "      <td>0.711160</td>\n",
              "      <td>0.587500</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.706783</td>\n",
              "      <td>0.596386</td>\n",
              "      <td>0.526596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.698031</td>\n",
              "      <td>0.503597</td>\n",
              "      <td>0.372340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.671329</td>\n",
              "      <td>0.695842</td>\n",
              "      <td>0.580060</td>\n",
              "      <td>0.510638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>0.693654</td>\n",
              "      <td>0.513889</td>\n",
              "      <td>0.393617</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  DVN  0.577657  0.640816  0.706667  0.909871\n",
              "1            GRU 0.1  DVN  0.751553  0.689796  0.614213  0.519313\n",
              "2        XGBoost 0.1  DVN  0.682203  0.700000  0.686567  0.690987\n",
              "3         Logreg 0.1  DVN  0.665370  0.697959  0.697959  0.733906\n",
              "4            SVM 0.1  DVN  0.709360  0.697959  0.660550  0.618026\n",
              "5      LSTM beta 0.1  DVN  0.855072  0.641138  0.418440  0.276995\n",
              "6       GRU beta 0.1  DVN  0.750000  0.722101  0.670130  0.605634\n",
              "7   XGBoost beta 0.1  DVN  0.694215  0.636761  0.502994  0.394366\n",
              "8    logreg beta 0.1  DVN  0.705000  0.713348  0.682809  0.661972\n",
              "9       svm beta 0.1  DVN  0.782609  0.676149  0.548780  0.422535\n",
              "0           LSTM 0.2  DVN  0.314741  0.577551  0.432877  0.692982\n",
              "1            GRU 0.2  DVN  0.320755  0.651020  0.373626  0.447368\n",
              "2        XGBoost 0.2  DVN  0.306620  0.540816  0.438903  0.771930\n",
              "3         Logreg 0.2  DVN  0.327660  0.602041  0.441261  0.675439\n",
              "4            SVM 0.2  DVN  0.333333  0.626531  0.429907  0.605263\n",
              "5      LSTM beta 0.2  DVN  0.622642  0.822757  0.448980  0.351064\n",
              "6       GRU beta 0.2  DVN  0.367347  0.737418  0.375000  0.382979\n",
              "7   XGBoost beta 0.2  DVN  0.481013  0.787746  0.439306  0.404255\n",
              "8    logreg beta 0.2  DVN  0.344828  0.695842  0.418410  0.531915\n",
              "9       svm beta 0.2  DVN  0.397959  0.750547  0.406250  0.414894\n",
              "0          LSTM 0.15  DVN  0.606250  0.644898  0.527174  0.466346\n",
              "1           GRU 0.15  DVN  0.602484  0.642857  0.525745  0.466346\n",
              "2       XGBoost 0.15  DVN  0.521401  0.597959  0.576344  0.644231\n",
              "3        Logreg 0.15  DVN  0.567568  0.636735  0.586047  0.605769\n",
              "4           SVM 0.15  DVN  0.580311  0.638776  0.558603  0.538462\n",
              "5     LSTM beta 0.15  DVN  0.712121  0.711160  0.587500  0.500000\n",
              "6      GRU beta 0.15  DVN  0.687500  0.706783  0.596386  0.526596\n",
              "7  XGBoost beta 0.15  DVN  0.777778  0.698031  0.503597  0.372340\n",
              "8   logreg beta 0.15  DVN  0.671329  0.695842  0.580060  0.510638\n",
              "9      svm beta 0.15  DVN  0.740000  0.693654  0.513889  0.393617"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6pGj-XwUbov"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crurrFJAUbov"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jTHp7w7Ubov",
        "outputId": "8df1b303-84df-4d51-e429-4f26e2b70c76"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"DVN\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6925 - accuracy: 0.5356 - val_loss: 0.6921 - val_accuracy: 0.4755\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6166 - accuracy: 0.6745 - val_loss: 0.6485 - val_accuracy: 0.6939\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5831 - accuracy: 0.7047 - val_loss: 0.6266 - val_accuracy: 0.6837\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5487 - accuracy: 0.7356 - val_loss: 0.7009 - val_accuracy: 0.5939\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5311 - accuracy: 0.7396 - val_loss: 0.6518 - val_accuracy: 0.5918\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6388 - accuracy: 0.6141 - val_loss: 0.6158 - val_accuracy: 0.7020\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5207 - accuracy: 0.7376 - val_loss: 0.5781 - val_accuracy: 0.6939\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5068 - accuracy: 0.7470 - val_loss: 0.5773 - val_accuracy: 0.7061\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4969 - accuracy: 0.7624 - val_loss: 0.5727 - val_accuracy: 0.7041\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4938 - accuracy: 0.7577 - val_loss: 0.5716 - val_accuracy: 0.6980\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.74117\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.757335\n",
            "[2]\tvalidation_0-auc:0.766712\n",
            "[3]\tvalidation_0-auc:0.76809\n",
            "[4]\tvalidation_0-auc:0.76809\n",
            "[5]\tvalidation_0-auc:0.770261\n",
            "[6]\tvalidation_0-auc:0.772382\n",
            "[7]\tvalidation_0-auc:0.773017\n",
            "[8]\tvalidation_0-auc:0.772749\n",
            "[9]\tvalidation_0-auc:0.772215\n",
            "[10]\tvalidation_0-auc:0.772983\n",
            "[11]\tvalidation_0-auc:0.773175\n",
            "[12]\tvalidation_0-auc:0.773685\n",
            "[13]\tvalidation_0-auc:0.775805\n",
            "[14]\tvalidation_0-auc:0.776181\n",
            "[15]\tvalidation_0-auc:0.776148\n",
            "[16]\tvalidation_0-auc:0.776874\n",
            "[17]\tvalidation_0-auc:0.776774\n",
            "[18]\tvalidation_0-auc:0.777066\n",
            "[19]\tvalidation_0-auc:0.777083\n",
            "[20]\tvalidation_0-auc:0.77796\n",
            "[21]\tvalidation_0-auc:0.777926\n",
            "[22]\tvalidation_0-auc:0.778494\n",
            "[23]\tvalidation_0-auc:0.779337\n",
            "[24]\tvalidation_0-auc:0.779521\n",
            "[25]\tvalidation_0-auc:0.778653\n",
            "[26]\tvalidation_0-auc:0.778628\n",
            "[27]\tvalidation_0-auc:0.778519\n",
            "[28]\tvalidation_0-auc:0.777091\n",
            "[29]\tvalidation_0-auc:0.777125\n",
            "[30]\tvalidation_0-auc:0.77624\n",
            "[31]\tvalidation_0-auc:0.77624\n",
            "[32]\tvalidation_0-auc:0.776507\n",
            "[33]\tvalidation_0-auc:0.774486\n",
            "[34]\tvalidation_0-auc:0.773968\n",
            "[35]\tvalidation_0-auc:0.773576\n",
            "[36]\tvalidation_0-auc:0.773526\n",
            "[37]\tvalidation_0-auc:0.773593\n",
            "[38]\tvalidation_0-auc:0.774077\n",
            "[39]\tvalidation_0-auc:0.774327\n",
            "[40]\tvalidation_0-auc:0.774361\n",
            "[41]\tvalidation_0-auc:0.773943\n",
            "[42]\tvalidation_0-auc:0.773943\n",
            "[43]\tvalidation_0-auc:0.774762\n",
            "[44]\tvalidation_0-auc:0.774553\n",
            "[45]\tvalidation_0-auc:0.774352\n",
            "[46]\tvalidation_0-auc:0.774052\n",
            "[47]\tvalidation_0-auc:0.775304\n",
            "[48]\tvalidation_0-auc:0.775213\n",
            "[49]\tvalidation_0-auc:0.775029\n",
            "[50]\tvalidation_0-auc:0.774962\n",
            "[51]\tvalidation_0-auc:0.774962\n",
            "[52]\tvalidation_0-auc:0.774979\n",
            "[53]\tvalidation_0-auc:0.775112\n",
            "[54]\tvalidation_0-auc:0.775238\n",
            "[55]\tvalidation_0-auc:0.775187\n",
            "[56]\tvalidation_0-auc:0.774678\n",
            "[57]\tvalidation_0-auc:0.774611\n",
            "[58]\tvalidation_0-auc:0.774578\n",
            "[59]\tvalidation_0-auc:0.768975\n",
            "[60]\tvalidation_0-auc:0.769125\n",
            "[61]\tvalidation_0-auc:0.770528\n",
            "[62]\tvalidation_0-auc:0.770679\n",
            "[63]\tvalidation_0-auc:0.770695\n",
            "[64]\tvalidation_0-auc:0.771012\n",
            "[65]\tvalidation_0-auc:0.77148\n",
            "[66]\tvalidation_0-auc:0.771447\n",
            "[67]\tvalidation_0-auc:0.767639\n",
            "[68]\tvalidation_0-auc:0.767589\n",
            "[69]\tvalidation_0-auc:0.767848\n",
            "[70]\tvalidation_0-auc:0.768374\n",
            "[71]\tvalidation_0-auc:0.768407\n",
            "[72]\tvalidation_0-auc:0.768524\n",
            "[73]\tvalidation_0-auc:0.768708\n",
            "[74]\tvalidation_0-auc:0.768391\n",
            "Stopping. Best iteration:\n",
            "[24]\tvalidation_0-auc:0.779521\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6799 - accuracy: 0.5511 - val_loss: 0.6700 - val_accuracy: 0.6236\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6314 - accuracy: 0.6445 - val_loss: 0.6589 - val_accuracy: 0.7090\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5856 - accuracy: 0.6980 - val_loss: 0.6062 - val_accuracy: 0.6915\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5229 - accuracy: 0.7399 - val_loss: 0.5924 - val_accuracy: 0.6740\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5104 - accuracy: 0.7495 - val_loss: 0.5892 - val_accuracy: 0.6937\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6269 - accuracy: 0.6328 - val_loss: 0.6188 - val_accuracy: 0.7002\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5252 - accuracy: 0.7378 - val_loss: 0.5972 - val_accuracy: 0.6543\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5100 - accuracy: 0.7570 - val_loss: 0.5753 - val_accuracy: 0.7046\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4991 - accuracy: 0.7653 - val_loss: 0.5571 - val_accuracy: 0.7068\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5067 - accuracy: 0.7639 - val_loss: 0.5665 - val_accuracy: 0.7090\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.570932\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.584488\n",
            "[2]\tvalidation_0-auc:0.723976\n",
            "[3]\tvalidation_0-auc:0.716097\n",
            "[4]\tvalidation_0-auc:0.726458\n",
            "[5]\tvalidation_0-auc:0.777342\n",
            "[6]\tvalidation_0-auc:0.77257\n",
            "[7]\tvalidation_0-auc:0.754483\n",
            "[8]\tvalidation_0-auc:0.739013\n",
            "[9]\tvalidation_0-auc:0.73656\n",
            "[10]\tvalidation_0-auc:0.747508\n",
            "[11]\tvalidation_0-auc:0.75101\n",
            "[12]\tvalidation_0-auc:0.752963\n",
            "[13]\tvalidation_0-auc:0.74238\n",
            "[14]\tvalidation_0-auc:0.741005\n",
            "[15]\tvalidation_0-auc:0.731615\n",
            "[16]\tvalidation_0-auc:0.732596\n",
            "[17]\tvalidation_0-auc:0.728652\n",
            "[18]\tvalidation_0-auc:0.729248\n",
            "[19]\tvalidation_0-auc:0.72971\n",
            "[20]\tvalidation_0-auc:0.725343\n",
            "[21]\tvalidation_0-auc:0.725314\n",
            "[22]\tvalidation_0-auc:0.72541\n",
            "[23]\tvalidation_0-auc:0.720869\n",
            "[24]\tvalidation_0-auc:0.720869\n",
            "[25]\tvalidation_0-auc:0.720638\n",
            "[26]\tvalidation_0-auc:0.723697\n",
            "[27]\tvalidation_0-auc:0.717598\n",
            "[28]\tvalidation_0-auc:0.717598\n",
            "[29]\tvalidation_0-auc:0.717598\n",
            "[30]\tvalidation_0-auc:0.717069\n",
            "[31]\tvalidation_0-auc:0.717069\n",
            "[32]\tvalidation_0-auc:0.717069\n",
            "[33]\tvalidation_0-auc:0.715683\n",
            "[34]\tvalidation_0-auc:0.714298\n",
            "[35]\tvalidation_0-auc:0.714298\n",
            "[36]\tvalidation_0-auc:0.71805\n",
            "[37]\tvalidation_0-auc:0.711268\n",
            "[38]\tvalidation_0-auc:0.705899\n",
            "[39]\tvalidation_0-auc:0.706034\n",
            "[40]\tvalidation_0-auc:0.706034\n",
            "[41]\tvalidation_0-auc:0.707862\n",
            "[42]\tvalidation_0-auc:0.707391\n",
            "[43]\tvalidation_0-auc:0.707391\n",
            "[44]\tvalidation_0-auc:0.707275\n",
            "[45]\tvalidation_0-auc:0.707304\n",
            "[46]\tvalidation_0-auc:0.707304\n",
            "[47]\tvalidation_0-auc:0.705361\n",
            "[48]\tvalidation_0-auc:0.705514\n",
            "[49]\tvalidation_0-auc:0.701243\n",
            "[50]\tvalidation_0-auc:0.693142\n",
            "[51]\tvalidation_0-auc:0.692796\n",
            "[52]\tvalidation_0-auc:0.692912\n",
            "[53]\tvalidation_0-auc:0.689525\n",
            "[54]\tvalidation_0-auc:0.688659\n",
            "[55]\tvalidation_0-auc:0.691719\n",
            "Stopping. Best iteration:\n",
            "[5]\tvalidation_0-auc:0.777342\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.5918367346938775 | 0.539568345323741  |  0.9656652360515021 | 0.6923076923076923 |\n",
            "|     GRU 0.1      | 0.6979591836734694 | 0.695852534562212  |  0.648068669527897  | 0.6711111111111111 |\n",
            "|   XGBoost 0.1    |        0.7         | 0.6822033898305084 |  0.6909871244635193 | 0.6865671641791045 |\n",
            "|    Logreg 0.1    | 0.6979591836734694 | 0.6653696498054474 |  0.7339055793991416 | 0.6979591836734694 |\n",
            "|     SVM 0.1      | 0.6979591836734694 | 0.7093596059113301 |  0.6180257510729614 | 0.6605504587155964 |\n",
            "|  LSTM beta 0.1   | 0.6936542669584245 |       0.792        |  0.4647887323943662 | 0.5857988165680473 |\n",
            "|   GRU beta 0.1   | 0.7089715536105032 | 0.7439024390243902 |  0.5727699530516432 | 0.6472148541114058 |\n",
            "| XGBoost beta 0.1 | 0.6367614879649891 | 0.6942148760330579 | 0.39436619718309857 | 0.5029940119760479 |\n",
            "| logreg beta 0.1  | 0.7133479212253829 |       0.705        |  0.6619718309859155 | 0.6828087167070217 |\n",
            "|   svm beta 0.1   | 0.6761487964989059 | 0.782608695652174  |  0.4225352112676056 | 0.548780487804878  |\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6880 - accuracy: 0.5671 - val_loss: 0.6087 - val_accuracy: 0.7653\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6352 - accuracy: 0.6195 - val_loss: 0.7046 - val_accuracy: 0.5694\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6059 - accuracy: 0.6933 - val_loss: 0.6070 - val_accuracy: 0.6898\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5975 - accuracy: 0.6953 - val_loss: 0.6793 - val_accuracy: 0.6265\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5772 - accuracy: 0.7067 - val_loss: 0.6595 - val_accuracy: 0.6347\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 17ms/step - loss: 0.6664 - accuracy: 0.5698 - val_loss: 0.7926 - val_accuracy: 0.4429\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5914 - accuracy: 0.6940 - val_loss: 0.5816 - val_accuracy: 0.7245\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5670 - accuracy: 0.7027 - val_loss: 0.6227 - val_accuracy: 0.6612\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5615 - accuracy: 0.7047 - val_loss: 0.6041 - val_accuracy: 0.6633\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5559 - accuracy: 0.7121 - val_loss: 0.6165 - val_accuracy: 0.6694\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.640782\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.640782\n",
            "[2]\tvalidation_0-auc:0.640782\n",
            "[3]\tvalidation_0-auc:0.638974\n",
            "[4]\tvalidation_0-auc:0.64854\n",
            "[5]\tvalidation_0-auc:0.658863\n",
            "[6]\tvalidation_0-auc:0.66424\n",
            "[7]\tvalidation_0-auc:0.66459\n",
            "[8]\tvalidation_0-auc:0.662479\n",
            "[9]\tvalidation_0-auc:0.661581\n",
            "[10]\tvalidation_0-auc:0.661674\n",
            "[11]\tvalidation_0-auc:0.65506\n",
            "[12]\tvalidation_0-auc:0.655317\n",
            "[13]\tvalidation_0-auc:0.648026\n",
            "[14]\tvalidation_0-auc:0.65373\n",
            "[15]\tvalidation_0-auc:0.653835\n",
            "[16]\tvalidation_0-auc:0.651152\n",
            "[17]\tvalidation_0-auc:0.655154\n",
            "[18]\tvalidation_0-auc:0.655119\n",
            "[19]\tvalidation_0-auc:0.651887\n",
            "[20]\tvalidation_0-auc:0.65317\n",
            "[21]\tvalidation_0-auc:0.653485\n",
            "[22]\tvalidation_0-auc:0.653462\n",
            "[23]\tvalidation_0-auc:0.652926\n",
            "[24]\tvalidation_0-auc:0.652984\n",
            "[25]\tvalidation_0-auc:0.653299\n",
            "[26]\tvalidation_0-auc:0.653089\n",
            "[27]\tvalidation_0-auc:0.653345\n",
            "[28]\tvalidation_0-auc:0.653369\n",
            "[29]\tvalidation_0-auc:0.653275\n",
            "[30]\tvalidation_0-auc:0.653182\n",
            "[31]\tvalidation_0-auc:0.652412\n",
            "[32]\tvalidation_0-auc:0.652237\n",
            "[33]\tvalidation_0-auc:0.652062\n",
            "[34]\tvalidation_0-auc:0.651992\n",
            "[35]\tvalidation_0-auc:0.650884\n",
            "[36]\tvalidation_0-auc:0.651607\n",
            "[37]\tvalidation_0-auc:0.651444\n",
            "[38]\tvalidation_0-auc:0.651257\n",
            "[39]\tvalidation_0-auc:0.649799\n",
            "[40]\tvalidation_0-auc:0.649683\n",
            "[41]\tvalidation_0-auc:0.648936\n",
            "[42]\tvalidation_0-auc:0.649099\n",
            "[43]\tvalidation_0-auc:0.649053\n",
            "[44]\tvalidation_0-auc:0.649006\n",
            "[45]\tvalidation_0-auc:0.649006\n",
            "[46]\tvalidation_0-auc:0.649006\n",
            "[47]\tvalidation_0-auc:0.649006\n",
            "[48]\tvalidation_0-auc:0.649006\n",
            "[49]\tvalidation_0-auc:0.648854\n",
            "[50]\tvalidation_0-auc:0.646592\n",
            "[51]\tvalidation_0-auc:0.646452\n",
            "[52]\tvalidation_0-auc:0.646662\n",
            "[53]\tvalidation_0-auc:0.646662\n",
            "[54]\tvalidation_0-auc:0.646662\n",
            "[55]\tvalidation_0-auc:0.646662\n",
            "[56]\tvalidation_0-auc:0.645052\n",
            "[57]\tvalidation_0-auc:0.644818\n",
            "Stopping. Best iteration:\n",
            "[7]\tvalidation_0-auc:0.66459\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6883 - accuracy: 0.5292 - val_loss: 0.7110 - val_accuracy: 0.3173\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6128 - accuracy: 0.6795 - val_loss: 0.6267 - val_accuracy: 0.7834\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5780 - accuracy: 0.7179 - val_loss: 0.5580 - val_accuracy: 0.7571\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5346 - accuracy: 0.7440 - val_loss: 0.5589 - val_accuracy: 0.7571\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4994 - accuracy: 0.7639 - val_loss: 0.6039 - val_accuracy: 0.6937\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6195 - accuracy: 0.6609 - val_loss: 0.5398 - val_accuracy: 0.7768\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5220 - accuracy: 0.7412 - val_loss: 0.5544 - val_accuracy: 0.7309\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5101 - accuracy: 0.7598 - val_loss: 0.6040 - val_accuracy: 0.7024\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5112 - accuracy: 0.7591 - val_loss: 0.4926 - val_accuracy: 0.7834\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4987 - accuracy: 0.7701 - val_loss: 0.5534 - val_accuracy: 0.7374\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.667297\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.675034\n",
            "[2]\tvalidation_0-auc:0.675195\n",
            "[3]\tvalidation_0-auc:0.675869\n",
            "[4]\tvalidation_0-auc:0.674521\n",
            "[5]\tvalidation_0-auc:0.666769\n",
            "[6]\tvalidation_0-auc:0.671634\n",
            "[7]\tvalidation_0-auc:0.679459\n",
            "[8]\tvalidation_0-auc:0.677525\n",
            "[9]\tvalidation_0-auc:0.675092\n",
            "[10]\tvalidation_0-auc:0.675092\n",
            "[11]\tvalidation_0-auc:0.676382\n",
            "[12]\tvalidation_0-auc:0.67159\n",
            "[13]\tvalidation_0-auc:0.67159\n",
            "[14]\tvalidation_0-auc:0.669436\n",
            "[15]\tvalidation_0-auc:0.670491\n",
            "[16]\tvalidation_0-auc:0.679825\n",
            "[17]\tvalidation_0-auc:0.6794\n",
            "[18]\tvalidation_0-auc:0.681891\n",
            "[19]\tvalidation_0-auc:0.673187\n",
            "[20]\tvalidation_0-auc:0.672748\n",
            "[21]\tvalidation_0-auc:0.673217\n",
            "[22]\tvalidation_0-auc:0.674125\n",
            "[23]\tvalidation_0-auc:0.674711\n",
            "[24]\tvalidation_0-auc:0.674711\n",
            "[25]\tvalidation_0-auc:0.672689\n",
            "[26]\tvalidation_0-auc:0.67351\n",
            "[27]\tvalidation_0-auc:0.674067\n",
            "[28]\tvalidation_0-auc:0.674096\n",
            "[29]\tvalidation_0-auc:0.674096\n",
            "[30]\tvalidation_0-auc:0.674917\n",
            "[31]\tvalidation_0-auc:0.679767\n",
            "[32]\tvalidation_0-auc:0.679767\n",
            "[33]\tvalidation_0-auc:0.680661\n",
            "[34]\tvalidation_0-auc:0.68217\n",
            "[35]\tvalidation_0-auc:0.677686\n",
            "[36]\tvalidation_0-auc:0.678213\n",
            "[37]\tvalidation_0-auc:0.678213\n",
            "[38]\tvalidation_0-auc:0.6788\n",
            "[39]\tvalidation_0-auc:0.679444\n",
            "[40]\tvalidation_0-auc:0.671209\n",
            "[41]\tvalidation_0-auc:0.674462\n",
            "[42]\tvalidation_0-auc:0.674521\n",
            "[43]\tvalidation_0-auc:0.674521\n",
            "[44]\tvalidation_0-auc:0.674682\n",
            "[45]\tvalidation_0-auc:0.674213\n",
            "[46]\tvalidation_0-auc:0.672806\n",
            "[47]\tvalidation_0-auc:0.673246\n",
            "[48]\tvalidation_0-auc:0.673246\n",
            "[49]\tvalidation_0-auc:0.673246\n",
            "[50]\tvalidation_0-auc:0.670315\n",
            "[51]\tvalidation_0-auc:0.670315\n",
            "[52]\tvalidation_0-auc:0.670315\n",
            "[53]\tvalidation_0-auc:0.671488\n",
            "[54]\tvalidation_0-auc:0.673363\n",
            "[55]\tvalidation_0-auc:0.672675\n",
            "[56]\tvalidation_0-auc:0.672616\n",
            "[57]\tvalidation_0-auc:0.672381\n",
            "[58]\tvalidation_0-auc:0.672059\n",
            "[59]\tvalidation_0-auc:0.670403\n",
            "[60]\tvalidation_0-auc:0.668703\n",
            "[61]\tvalidation_0-auc:0.669026\n",
            "[62]\tvalidation_0-auc:0.667487\n",
            "[63]\tvalidation_0-auc:0.667487\n",
            "[64]\tvalidation_0-auc:0.667106\n",
            "[65]\tvalidation_0-auc:0.667106\n",
            "[66]\tvalidation_0-auc:0.666989\n",
            "[67]\tvalidation_0-auc:0.666989\n",
            "[68]\tvalidation_0-auc:0.663384\n",
            "[69]\tvalidation_0-auc:0.66356\n",
            "[70]\tvalidation_0-auc:0.66441\n",
            "[71]\tvalidation_0-auc:0.66441\n",
            "[72]\tvalidation_0-auc:0.664586\n",
            "[73]\tvalidation_0-auc:0.664586\n",
            "[74]\tvalidation_0-auc:0.664014\n",
            "[75]\tvalidation_0-auc:0.664014\n",
            "[76]\tvalidation_0-auc:0.663751\n",
            "[77]\tvalidation_0-auc:0.663135\n",
            "[78]\tvalidation_0-auc:0.663135\n",
            "[79]\tvalidation_0-auc:0.661992\n",
            "[80]\tvalidation_0-auc:0.661699\n",
            "[81]\tvalidation_0-auc:0.662022\n",
            "[82]\tvalidation_0-auc:0.662315\n",
            "[83]\tvalidation_0-auc:0.662315\n",
            "[84]\tvalidation_0-auc:0.661494\n",
            "Stopping. Best iteration:\n",
            "[34]\tvalidation_0-auc:0.68217\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.2     | 0.6346938775510204 |  0.3224043715846995 |  0.5175438596491229 |  0.3973063973063974 |\n",
            "|     GRU 0.2      | 0.6693877551020408 | 0.33098591549295775 | 0.41228070175438597 |      0.3671875      |\n",
            "|   XGBoost 0.2    | 0.5408163265306123 | 0.30662020905923343 |  0.7719298245614035 |  0.4389027431421446 |\n",
            "|    Logreg 0.2    | 0.6020408163265306 |  0.3276595744680851 |  0.6754385964912281 |  0.4412607449856733 |\n",
            "|     SVM 0.2      | 0.6265306122448979 |  0.3333333333333333 |  0.6052631578947368 |  0.4299065420560747 |\n",
            "|  LSTM beta 0.2   | 0.6936542669584245 |  0.3380281690140845 |  0.5106382978723404 |  0.4067796610169492 |\n",
            "|   GRU beta 0.2   | 0.737417943107221  | 0.38392857142857145 |  0.4574468085106383 |  0.4174757281553398 |\n",
            "| XGBoost beta 0.2 | 0.787746170678337  |  0.4810126582278481 | 0.40425531914893614 |  0.4393063583815029 |\n",
            "| logreg beta 0.2  | 0.6958424507658644 |  0.3448275862068966 |  0.5319148936170213 | 0.41841004184100417 |\n",
            "|   svm beta 0.2   |  0.75054704595186  |  0.3979591836734694 |  0.4148936170212766 |       0.40625       |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6807 - accuracy: 0.5403 - val_loss: 0.6813 - val_accuracy: 0.5816\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6055 - accuracy: 0.6826 - val_loss: 0.6591 - val_accuracy: 0.6184\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5956 - accuracy: 0.6946 - val_loss: 0.6442 - val_accuracy: 0.6490\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5799 - accuracy: 0.7174 - val_loss: 0.6404 - val_accuracy: 0.6531\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5674 - accuracy: 0.7174 - val_loss: 0.6346 - val_accuracy: 0.6469\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6453 - accuracy: 0.6134 - val_loss: 0.7487 - val_accuracy: 0.4816\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5609 - accuracy: 0.7201 - val_loss: 0.6378 - val_accuracy: 0.6469\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5500 - accuracy: 0.7195 - val_loss: 0.6355 - val_accuracy: 0.6510\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5436 - accuracy: 0.7215 - val_loss: 0.6720 - val_accuracy: 0.5918\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5328 - accuracy: 0.7255 - val_loss: 0.6397 - val_accuracy: 0.6469\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.677697\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.677697\n",
            "[2]\tvalidation_0-auc:0.676435\n",
            "[3]\tvalidation_0-auc:0.679513\n",
            "[4]\tvalidation_0-auc:0.686895\n",
            "[5]\tvalidation_0-auc:0.685454\n",
            "[6]\tvalidation_0-auc:0.689034\n",
            "[7]\tvalidation_0-auc:0.689486\n",
            "[8]\tvalidation_0-auc:0.690066\n",
            "[9]\tvalidation_0-auc:0.689767\n",
            "[10]\tvalidation_0-auc:0.688813\n",
            "[11]\tvalidation_0-auc:0.688685\n",
            "[12]\tvalidation_0-auc:0.689836\n",
            "[13]\tvalidation_0-auc:0.684772\n",
            "[14]\tvalidation_0-auc:0.684397\n",
            "[15]\tvalidation_0-auc:0.685079\n",
            "[16]\tvalidation_0-auc:0.681738\n",
            "[17]\tvalidation_0-auc:0.682607\n",
            "[18]\tvalidation_0-auc:0.681942\n",
            "[19]\tvalidation_0-auc:0.682684\n",
            "[20]\tvalidation_0-auc:0.685735\n",
            "[21]\tvalidation_0-auc:0.685718\n",
            "[22]\tvalidation_0-auc:0.685429\n",
            "[23]\tvalidation_0-auc:0.683724\n",
            "[24]\tvalidation_0-auc:0.681917\n",
            "[25]\tvalidation_0-auc:0.679283\n",
            "[26]\tvalidation_0-auc:0.679973\n",
            "[27]\tvalidation_0-auc:0.679973\n",
            "[28]\tvalidation_0-auc:0.678967\n",
            "[29]\tvalidation_0-auc:0.680834\n",
            "[30]\tvalidation_0-auc:0.680834\n",
            "[31]\tvalidation_0-auc:0.682104\n",
            "[32]\tvalidation_0-auc:0.68161\n",
            "[33]\tvalidation_0-auc:0.681243\n",
            "[34]\tvalidation_0-auc:0.681286\n",
            "[35]\tvalidation_0-auc:0.680382\n",
            "[36]\tvalidation_0-auc:0.680263\n",
            "[37]\tvalidation_0-auc:0.679939\n",
            "[38]\tvalidation_0-auc:0.680041\n",
            "[39]\tvalidation_0-auc:0.680041\n",
            "[40]\tvalidation_0-auc:0.679982\n",
            "[41]\tvalidation_0-auc:0.679982\n",
            "[42]\tvalidation_0-auc:0.678959\n",
            "[43]\tvalidation_0-auc:0.679257\n",
            "[44]\tvalidation_0-auc:0.680621\n",
            "[45]\tvalidation_0-auc:0.681286\n",
            "[46]\tvalidation_0-auc:0.681294\n",
            "[47]\tvalidation_0-auc:0.680203\n",
            "[48]\tvalidation_0-auc:0.679291\n",
            "[49]\tvalidation_0-auc:0.679189\n",
            "[50]\tvalidation_0-auc:0.680561\n",
            "[51]\tvalidation_0-auc:0.680561\n",
            "[52]\tvalidation_0-auc:0.682044\n",
            "[53]\tvalidation_0-auc:0.682232\n",
            "[54]\tvalidation_0-auc:0.682616\n",
            "[55]\tvalidation_0-auc:0.682616\n",
            "[56]\tvalidation_0-auc:0.682581\n",
            "[57]\tvalidation_0-auc:0.682283\n",
            "[58]\tvalidation_0-auc:0.682283\n",
            "Stopping. Best iteration:\n",
            "[8]\tvalidation_0-auc:0.690066\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6770 - accuracy: 0.6012 - val_loss: 0.6555 - val_accuracy: 0.6324\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6237 - accuracy: 0.6767 - val_loss: 0.6332 - val_accuracy: 0.6630\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.5868 - accuracy: 0.7049 - val_loss: 0.6199 - val_accuracy: 0.6674\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5413 - accuracy: 0.7358 - val_loss: 0.6553 - val_accuracy: 0.6586\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.5250 - accuracy: 0.7522 - val_loss: 0.5859 - val_accuracy: 0.7133\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6266 - accuracy: 0.6541 - val_loss: 0.6082 - val_accuracy: 0.6958\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5235 - accuracy: 0.7632 - val_loss: 0.6021 - val_accuracy: 0.6980\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5037 - accuracy: 0.7694 - val_loss: 0.5961 - val_accuracy: 0.7046\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5049 - accuracy: 0.7701 - val_loss: 0.5918 - val_accuracy: 0.7243\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5030 - accuracy: 0.7612 - val_loss: 0.5903 - val_accuracy: 0.7177\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.686081\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.717966\n",
            "[2]\tvalidation_0-auc:0.725441\n",
            "[3]\tvalidation_0-auc:0.701366\n",
            "[4]\tvalidation_0-auc:0.705351\n",
            "[5]\tvalidation_0-auc:0.718026\n",
            "[6]\tvalidation_0-auc:0.707595\n",
            "[7]\tvalidation_0-auc:0.703759\n",
            "[8]\tvalidation_0-auc:0.712548\n",
            "[9]\tvalidation_0-auc:0.713557\n",
            "[10]\tvalidation_0-auc:0.701851\n",
            "[11]\tvalidation_0-auc:0.707585\n",
            "[12]\tvalidation_0-auc:0.71154\n",
            "[13]\tvalidation_0-auc:0.711441\n",
            "[14]\tvalidation_0-auc:0.711421\n",
            "[15]\tvalidation_0-auc:0.711362\n",
            "[16]\tvalidation_0-auc:0.712262\n",
            "[17]\tvalidation_0-auc:0.715633\n",
            "[18]\tvalidation_0-auc:0.715564\n",
            "[19]\tvalidation_0-auc:0.717136\n",
            "[20]\tvalidation_0-auc:0.718065\n",
            "[21]\tvalidation_0-auc:0.722247\n",
            "[22]\tvalidation_0-auc:0.722495\n",
            "[23]\tvalidation_0-auc:0.723721\n",
            "[24]\tvalidation_0-auc:0.724571\n",
            "[25]\tvalidation_0-auc:0.724571\n",
            "[26]\tvalidation_0-auc:0.727932\n",
            "[27]\tvalidation_0-auc:0.727379\n",
            "[28]\tvalidation_0-auc:0.727596\n",
            "[29]\tvalidation_0-auc:0.727932\n",
            "[30]\tvalidation_0-auc:0.728367\n",
            "[31]\tvalidation_0-auc:0.728367\n",
            "[32]\tvalidation_0-auc:0.723493\n",
            "[33]\tvalidation_0-auc:0.723493\n",
            "[34]\tvalidation_0-auc:0.723493\n",
            "[35]\tvalidation_0-auc:0.723493\n",
            "[36]\tvalidation_0-auc:0.726944\n",
            "[37]\tvalidation_0-auc:0.724739\n",
            "[38]\tvalidation_0-auc:0.724996\n",
            "[39]\tvalidation_0-auc:0.726786\n",
            "[40]\tvalidation_0-auc:0.728427\n",
            "[41]\tvalidation_0-auc:0.728427\n",
            "[42]\tvalidation_0-auc:0.728427\n",
            "[43]\tvalidation_0-auc:0.728269\n",
            "[44]\tvalidation_0-auc:0.730187\n",
            "[45]\tvalidation_0-auc:0.730187\n",
            "[46]\tvalidation_0-auc:0.732223\n",
            "[47]\tvalidation_0-auc:0.732223\n",
            "[48]\tvalidation_0-auc:0.731788\n",
            "[49]\tvalidation_0-auc:0.731848\n",
            "[50]\tvalidation_0-auc:0.72552\n",
            "[51]\tvalidation_0-auc:0.72552\n",
            "[52]\tvalidation_0-auc:0.72552\n",
            "[53]\tvalidation_0-auc:0.722327\n",
            "[54]\tvalidation_0-auc:0.722327\n",
            "[55]\tvalidation_0-auc:0.724126\n",
            "[56]\tvalidation_0-auc:0.724126\n",
            "[57]\tvalidation_0-auc:0.723582\n",
            "[58]\tvalidation_0-auc:0.724096\n",
            "[59]\tvalidation_0-auc:0.724225\n",
            "[60]\tvalidation_0-auc:0.724225\n",
            "[61]\tvalidation_0-auc:0.724225\n",
            "[62]\tvalidation_0-auc:0.722277\n",
            "[63]\tvalidation_0-auc:0.721625\n",
            "[64]\tvalidation_0-auc:0.724729\n",
            "[65]\tvalidation_0-auc:0.724156\n",
            "[66]\tvalidation_0-auc:0.725115\n",
            "[67]\tvalidation_0-auc:0.726707\n",
            "[68]\tvalidation_0-auc:0.723928\n",
            "[69]\tvalidation_0-auc:0.723958\n",
            "[70]\tvalidation_0-auc:0.724255\n",
            "[71]\tvalidation_0-auc:0.724255\n",
            "[72]\tvalidation_0-auc:0.724255\n",
            "[73]\tvalidation_0-auc:0.728041\n",
            "[74]\tvalidation_0-auc:0.728041\n",
            "[75]\tvalidation_0-auc:0.726815\n",
            "[76]\tvalidation_0-auc:0.722168\n",
            "[77]\tvalidation_0-auc:0.722168\n",
            "[78]\tvalidation_0-auc:0.721852\n",
            "[79]\tvalidation_0-auc:0.721832\n",
            "[80]\tvalidation_0-auc:0.72205\n",
            "[81]\tvalidation_0-auc:0.724136\n",
            "[82]\tvalidation_0-auc:0.724077\n",
            "[83]\tvalidation_0-auc:0.723819\n",
            "[84]\tvalidation_0-auc:0.725036\n",
            "[85]\tvalidation_0-auc:0.725036\n",
            "[86]\tvalidation_0-auc:0.725055\n",
            "[87]\tvalidation_0-auc:0.725134\n",
            "[88]\tvalidation_0-auc:0.725253\n",
            "[89]\tvalidation_0-auc:0.725471\n",
            "[90]\tvalidation_0-auc:0.725026\n",
            "[91]\tvalidation_0-auc:0.725836\n",
            "[92]\tvalidation_0-auc:0.725678\n",
            "[93]\tvalidation_0-auc:0.723187\n",
            "[94]\tvalidation_0-auc:0.724096\n",
            "[95]\tvalidation_0-auc:0.724591\n",
            "[96]\tvalidation_0-auc:0.724591\n",
            "Stopping. Best iteration:\n",
            "[46]\tvalidation_0-auc:0.732223\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |        Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.6469387755102041 | 0.6296296296296297 | 0.40865384615384615 | 0.4956268221574344 |\n",
            "|      GRU 0.15     | 0.6469387755102041 | 0.5879396984924623 |        0.5625       | 0.5749385749385749 |\n",
            "|    XGBoost 0.15   | 0.5979591836734693 | 0.5214007782101168 |  0.6442307692307693 | 0.5763440860215053 |\n",
            "|    Logreg 0.15    | 0.636734693877551  | 0.5675675675675675 |  0.6057692307692307 | 0.5860465116279069 |\n",
            "|      SVM 0.15     | 0.6387755102040816 | 0.5803108808290155 |  0.5384615384615384 | 0.5586034912718204 |\n",
            "|   LSTM beta 0.15  | 0.7133479212253829 | 0.7522123893805309 |  0.4521276595744681 | 0.5647840531561462 |\n",
            "|   GRU beta 0.15   | 0.7177242888402626 | 0.7092198581560284 |  0.5319148936170213 |  0.60790273556231  |\n",
            "| XGBoost beta 0.15 | 0.6980306345733042 | 0.7777777777777778 |  0.3723404255319149 | 0.5035971223021583 |\n",
            "|  logreg beta 0.15 | 0.6958424507658644 | 0.6713286713286714 |  0.5106382978723404 | 0.580060422960725  |\n",
            "|   svm beta 0.15   | 0.6936542669584245 |        0.74        | 0.39361702127659576 | 0.513888888888889  |\n",
            "+-------------------+--------------------+--------------------+---------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hc5BQh76Ubow",
        "outputId": "3c33b01e-b420-4e83-e64b-a86cb9988754"
      },
      "source": [
        "Result_purging.to_csv('DVN_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.539568</td>\n",
              "      <td>0.591837</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.965665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.695853</td>\n",
              "      <td>0.697959</td>\n",
              "      <td>0.671111</td>\n",
              "      <td>0.648069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.682203</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.686567</td>\n",
              "      <td>0.690987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.665370</td>\n",
              "      <td>0.697959</td>\n",
              "      <td>0.697959</td>\n",
              "      <td>0.733906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.709360</td>\n",
              "      <td>0.697959</td>\n",
              "      <td>0.660550</td>\n",
              "      <td>0.618026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.792000</td>\n",
              "      <td>0.693654</td>\n",
              "      <td>0.585799</td>\n",
              "      <td>0.464789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.743902</td>\n",
              "      <td>0.708972</td>\n",
              "      <td>0.647215</td>\n",
              "      <td>0.572770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.694215</td>\n",
              "      <td>0.636761</td>\n",
              "      <td>0.502994</td>\n",
              "      <td>0.394366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.705000</td>\n",
              "      <td>0.713348</td>\n",
              "      <td>0.682809</td>\n",
              "      <td>0.661972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.676149</td>\n",
              "      <td>0.548780</td>\n",
              "      <td>0.422535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.322404</td>\n",
              "      <td>0.634694</td>\n",
              "      <td>0.397306</td>\n",
              "      <td>0.517544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.330986</td>\n",
              "      <td>0.669388</td>\n",
              "      <td>0.367188</td>\n",
              "      <td>0.412281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.306620</td>\n",
              "      <td>0.540816</td>\n",
              "      <td>0.438903</td>\n",
              "      <td>0.771930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.327660</td>\n",
              "      <td>0.602041</td>\n",
              "      <td>0.441261</td>\n",
              "      <td>0.675439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.626531</td>\n",
              "      <td>0.429907</td>\n",
              "      <td>0.605263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.338028</td>\n",
              "      <td>0.693654</td>\n",
              "      <td>0.406780</td>\n",
              "      <td>0.510638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.383929</td>\n",
              "      <td>0.737418</td>\n",
              "      <td>0.417476</td>\n",
              "      <td>0.457447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.481013</td>\n",
              "      <td>0.787746</td>\n",
              "      <td>0.439306</td>\n",
              "      <td>0.404255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.344828</td>\n",
              "      <td>0.695842</td>\n",
              "      <td>0.418410</td>\n",
              "      <td>0.531915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>0.750547</td>\n",
              "      <td>0.406250</td>\n",
              "      <td>0.414894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.629630</td>\n",
              "      <td>0.646939</td>\n",
              "      <td>0.495627</td>\n",
              "      <td>0.408654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.587940</td>\n",
              "      <td>0.646939</td>\n",
              "      <td>0.574939</td>\n",
              "      <td>0.562500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.521401</td>\n",
              "      <td>0.597959</td>\n",
              "      <td>0.576344</td>\n",
              "      <td>0.644231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.567568</td>\n",
              "      <td>0.636735</td>\n",
              "      <td>0.586047</td>\n",
              "      <td>0.605769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.580311</td>\n",
              "      <td>0.638776</td>\n",
              "      <td>0.558603</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.752212</td>\n",
              "      <td>0.713348</td>\n",
              "      <td>0.564784</td>\n",
              "      <td>0.452128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.709220</td>\n",
              "      <td>0.717724</td>\n",
              "      <td>0.607903</td>\n",
              "      <td>0.531915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.698031</td>\n",
              "      <td>0.503597</td>\n",
              "      <td>0.372340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.671329</td>\n",
              "      <td>0.695842</td>\n",
              "      <td>0.580060</td>\n",
              "      <td>0.510638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>DVN</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>0.693654</td>\n",
              "      <td>0.513889</td>\n",
              "      <td>0.393617</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  DVN  0.539568  0.591837  0.692308  0.965665\n",
              "1            GRU 0.1  DVN  0.695853  0.697959  0.671111  0.648069\n",
              "2        XGBoost 0.1  DVN  0.682203  0.700000  0.686567  0.690987\n",
              "3         Logreg 0.1  DVN  0.665370  0.697959  0.697959  0.733906\n",
              "4            SVM 0.1  DVN  0.709360  0.697959  0.660550  0.618026\n",
              "5      LSTM beta 0.1  DVN  0.792000  0.693654  0.585799  0.464789\n",
              "6       GRU beta 0.1  DVN  0.743902  0.708972  0.647215  0.572770\n",
              "7   XGBoost beta 0.1  DVN  0.694215  0.636761  0.502994  0.394366\n",
              "8    logreg beta 0.1  DVN  0.705000  0.713348  0.682809  0.661972\n",
              "9       svm beta 0.1  DVN  0.782609  0.676149  0.548780  0.422535\n",
              "0           LSTM 0.2  DVN  0.322404  0.634694  0.397306  0.517544\n",
              "1            GRU 0.2  DVN  0.330986  0.669388  0.367188  0.412281\n",
              "2        XGBoost 0.2  DVN  0.306620  0.540816  0.438903  0.771930\n",
              "3         Logreg 0.2  DVN  0.327660  0.602041  0.441261  0.675439\n",
              "4            SVM 0.2  DVN  0.333333  0.626531  0.429907  0.605263\n",
              "5      LSTM beta 0.2  DVN  0.338028  0.693654  0.406780  0.510638\n",
              "6       GRU beta 0.2  DVN  0.383929  0.737418  0.417476  0.457447\n",
              "7   XGBoost beta 0.2  DVN  0.481013  0.787746  0.439306  0.404255\n",
              "8    logreg beta 0.2  DVN  0.344828  0.695842  0.418410  0.531915\n",
              "9       svm beta 0.2  DVN  0.397959  0.750547  0.406250  0.414894\n",
              "0          LSTM 0.15  DVN  0.629630  0.646939  0.495627  0.408654\n",
              "1           GRU 0.15  DVN  0.587940  0.646939  0.574939  0.562500\n",
              "2       XGBoost 0.15  DVN  0.521401  0.597959  0.576344  0.644231\n",
              "3        Logreg 0.15  DVN  0.567568  0.636735  0.586047  0.605769\n",
              "4           SVM 0.15  DVN  0.580311  0.638776  0.558603  0.538462\n",
              "5     LSTM beta 0.15  DVN  0.752212  0.713348  0.564784  0.452128\n",
              "6      GRU beta 0.15  DVN  0.709220  0.717724  0.607903  0.531915\n",
              "7  XGBoost beta 0.15  DVN  0.777778  0.698031  0.503597  0.372340\n",
              "8   logreg beta 0.15  DVN  0.671329  0.695842  0.580060  0.510638\n",
              "9      svm beta 0.15  DVN  0.740000  0.693654  0.513889  0.393617"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh962R4yUbow"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('DVN_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6Gma6DSUbox"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrOzpv5nVHfS"
      },
      "source": [
        "## FCX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKJb6Ao9VHfY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53a6f633-9c63-41e4-b4aa-87efd012a905"
      },
      "source": [
        "dfs = pd.read_csv(\"FCX.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2768</td>\n",
              "      <td>US1.FCX</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>33.00</td>\n",
              "      <td>33.100</td>\n",
              "      <td>31.8700</td>\n",
              "      <td>32.8400</td>\n",
              "      <td>805504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2767</td>\n",
              "      <td>US1.FCX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>32.69</td>\n",
              "      <td>33.750</td>\n",
              "      <td>32.5300</td>\n",
              "      <td>32.5400</td>\n",
              "      <td>1359849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.FCX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>34.00</td>\n",
              "      <td>34.010</td>\n",
              "      <td>32.9200</td>\n",
              "      <td>33.1000</td>\n",
              "      <td>589350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.FCX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>33.98</td>\n",
              "      <td>34.015</td>\n",
              "      <td>32.8200</td>\n",
              "      <td>33.5700</td>\n",
              "      <td>910310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.FCX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>32.29</td>\n",
              "      <td>33.950</td>\n",
              "      <td>32.2900</td>\n",
              "      <td>33.9000</td>\n",
              "      <td>1099291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.FCX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>92.31</td>\n",
              "      <td>95.910</td>\n",
              "      <td>92.0002</td>\n",
              "      <td>95.5200</td>\n",
              "      <td>12656521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.FCX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>93.63</td>\n",
              "      <td>93.870</td>\n",
              "      <td>89.8500</td>\n",
              "      <td>91.3975</td>\n",
              "      <td>13153770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.FCX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>91.51</td>\n",
              "      <td>94.360</td>\n",
              "      <td>91.4500</td>\n",
              "      <td>93.6100</td>\n",
              "      <td>13105657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.FCX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>88.71</td>\n",
              "      <td>91.800</td>\n",
              "      <td>88.6100</td>\n",
              "      <td>91.1800</td>\n",
              "      <td>12610936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.FCX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>88.44</td>\n",
              "      <td>88.590</td>\n",
              "      <td>86.6700</td>\n",
              "      <td>87.2500</td>\n",
              "      <td>11292152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2769 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...  <HIGH>    <LOW>  <CLOSE>     <VOL>\n",
              "0      2768  US1.FCX     D  20211001  ...  33.100  31.8700  32.8400    805504\n",
              "1      2767  US1.FCX     D  20210930  ...  33.750  32.5300  32.5400   1359849\n",
              "2      2766  US1.FCX     D  20210929  ...  34.010  32.9200  33.1000    589350\n",
              "3      2765  US1.FCX     D  20210928  ...  34.015  32.8200  33.5700    910310\n",
              "4      2764  US1.FCX     D  20210927  ...  33.950  32.2900  33.9000   1099291\n",
              "...     ...      ...   ...       ...  ...     ...      ...      ...       ...\n",
              "2764      4  US1.FCX     D  20101008  ...  95.910  92.0002  95.5200  12656521\n",
              "2765      3  US1.FCX     D  20101007  ...  93.870  89.8500  91.3975  13153770\n",
              "2766      2  US1.FCX     D  20101006  ...  94.360  91.4500  93.6100  13105657\n",
              "2767      1  US1.FCX     D  20101005  ...  91.800  88.6100  91.1800  12610936\n",
              "2768      0  US1.FCX     D  20101004  ...  88.590  86.6700  87.2500  11292152\n",
              "\n",
              "[2769 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVRuZZzqVHfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "140fd34b-cb3f-41b3-b9f8-47b5306585cf"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"f0a65365-f5cf-4654-bfa6-68f975d3084d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"f0a65365-f5cf-4654-bfa6-68f975d3084d\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'f0a65365-f5cf-4654-bfa6-68f975d3084d',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [8.5, 8.76, 8.82, 8.86, 8.88, 9.17, 9.57, 9.64, 9.795, 10.06, 10.01, 10.33, 10.47, 10.45, 10.36, 10.41, 10.47, 10.76, 10.38, 10.08, 10.12, 9.71, 9.42, 9.53, 9.29, 9.075, 9.18, 9.13, 8.95, 8.7, 8.85, 8.835, 9.13, 9.14, 9.19, 9.25, 9.1, 8.98, 9.36, 9.88, 9.51, 9.745, 10.13, 10.09, 10.14, 10.07, 10.3, 10.7, 11.05, 11.46, 11.61, 11.57, 11.43, 11.94, 11.66, 11.49, 11.49, 11.15, 11.05, 11.12, 11.15, 11.015, 11.1, 10.95, 10.87, 11.22, 11.29, 11.3046, 11.44, 11.44, 11.6, 11.56, 11.49, 11.33, 11.34, 11.34, 11.385, 11.19, 11.13, 10.74, 10.655, 10.83, 10.59, 10.625, 10.61, 10.34, 10.19, 10.1, 10.32, 9.91, 9.7, 9.82, 9.98, 10.03, 10.08, 9.95, 10.06, 10.35, 10.2, 10.37, 10.66, 10.86, 10.87, 10.71, 11.37, 11.28, 11.38, 11.44, 11.73, 11.95, 11.51, 11.71, 12.32, 12.43, 12.4963, 12.201, 13.58, 13.46, 13.64, 14.0, 14.18, 14.15, 13.69, 13.7, 13.41, 13.5, 13.385, 13.74, 13.33, 13.57, 13.155, 13.125, 13.1, 12.89, 12.725, 12.72, 12.7, 12.38, 12.18, 12.92, 12.825, 12.68, 12.625, 12.35, 12.323, 12.66, 12.66, 12.36, 12.08, 12.31, 12.54, 12.84, 12.77, 12.86, 12.9, 13.16, 13.18, 13.25, 13.22, 13.01, 13.35, 13.08, 12.28, 12.15, 12.2851, 11.49, 11.53, 11.61, 11.87, 11.91, 11.86, 11.86, 11.51, 11.64, 11.27, 10.45, 10.27, 11.24, 10.69, 12.3, 12.38, 12.55, 12.12, 11.89, 11.57, 11.555, 11.6, 11.51, 11.5, 11.21, 11.0, 10.81, 10.07, 10.37, 10.31, 10.46, 10.675, 10.4, 9.53, 10.05, 10.23, 10.185, 10.6, 10.6, 10.56, 10.92, 11.07, 10.76, 10.69, 10.93, 11.145, 11.49, 12.09, 11.94, 11.82, 11.96, 11.24, 11.09, 10.77, 11.4, 10.88, 11.515, 11.96, 11.96, 11.54, 11.25, 11.17, 11.35, 11.93, 12.31, 12.21, 12.23, 12.24, 12.28, 11.65, 11.415, 11.09, 11.5, 11.25, 10.71, 11.6, 12.0, 12.01, 12.14, 12.4, 12.67, 12.82, 12.73, 13.23, 12.77, 13.185, 13.295, 13.27, 13.63, 13.94, 13.89, 13.7, 13.92, 13.87, 14.01, 14.49, 14.24, 14.6, 14.39, 14.3, 14.01, 13.73, 13.68, 13.58, 13.64, 13.045, 13.2, 13.18, 13.57, 13.68, 13.48, 14.05, 14.15, 14.67, 14.66, 14.73, 14.47, 14.18, 14.53, 14.32, 14.12, 13.96, 13.78, 13.67, 14.8, 14.93, 15.105, 15.31, 15.39, 15.37, 15.4, 15.72, 15.43, 15.58, 16.5, 16.085, 15.99, 15.97, 15.8699, 16.07, 15.82, 15.75, 15.8, 17.09, 16.77, 16.77, 16.8, 17.29, 17.4, 18.11, 18.105, 17.51, 16.98, 17.07, 17.15, 17.27, 16.8, 16.9, 16.25, 15.74, 16.44, 16.35, 16.3, 16.26, 16.805, 16.9, 17.58, 17.82, 17.855, 18.18, 18.03, 17.93, 18.2, 17.66, 17.215, 17.12, 16.9, 17.13, 16.65, 17.095, 17.39, 17.35, 17.16, 16.7, 16.86, 16.77, 16.65, 15.96, 16.2, 16.26, 16.21, 15.67, 15.3, 15.26, 15.42, 15.22, 15.11, 14.93, 15.22, 15.325, 15.63, 15.37, 16.075, 18.8, 19.37, 19.58, 19.17, 18.315, 18.15, 17.865, 17.85, 17.97, 18.19, 17.43, 17.32, 18.105, 17.34, 17.38, 17.14, 17.56, 16.76, 17.24, 17.76, 17.52, 18.06, 19.19, 18.485, 18.055, 18.35, 18.41, 18.66, 18.49, 18.555, 18.43, 17.86, 18.12, 18.695, 18.2, 18.32, 18.52, 18.6, 19.08, 19.54, 19.56, 19.16, 18.73, 18.43, 18.73, 19.12, 19.12, 17.79, 17.58, 17.58, 17.16, 17.87, 18.725, 17.66, 17.97, 19.445, 19.5, 19.13, 19.64, 19.54, 19.83, 19.6, 19.56, 20.0, 19.96, 19.42, 19.76, 19.33, 19.74, 19.88, 19.51, 18.93, 19.81, 19.89, 19.74, 19.465, 19.775, 18.97, 19.26, 18.68, 18.66, 18.105, 18.19, 17.66, 17.33, 17.36, 16.99, 16.185, 16.34, 15.715, 15.03, 14.98, 14.85, 14.35, 14.115, 14.29, 14.12, 13.92, 14.0, 14.21, 14.1, 14.3461, 14.24, 14.15, 13.95, 13.86, 13.615, 13.63, 13.8, 14.43, 14.41, 14.715, 14.87, 14.54, 14.6358, 14.14, 14.23, 14.385, 13.98, 13.95, 14.13, 14.685, 14.71, 15.23, 14.81, 14.825, 14.82, 14.83, 14.925, 15.27, 14.75, 14.52, 14.425, 14.41, 14.33, 14.325, 14.75, 14.53, 14.65, 14.33, 14.035, 14.42, 14.0, 14.0, 13.89, 14.06, 13.98, 14.17, 14.49, 14.125, 13.88, 13.865, 13.93, 14.305, 14.39, 14.23, 15.21, 15.06, 14.78, 15.07, 14.77, 14.57, 15.2243, 15.52, 15.28, 15.48, 15.3, 15.07, 14.72, 14.1484, 14.2, 14.76, 13.965, 14.145, 13.945, 14.155, 14.58, 14.46, 14.69, 14.41, 14.37, 14.38, 14.485, 14.61, 14.595, 14.49, 15.06, 14.87, 12.9522, 13.015, 13.05, 13.12, 13.01, 13.01, 12.6, 12.49, 12.28, 12.585, 12.525, 11.905, 11.895, 12.16, 12.2, 12.0, 12.085, 11.99, 11.725, 11.83, 11.75, 11.45, 11.22, 11.245, 11.59, 11.415, 11.495, 12.085, 12.35, 12.34, 12.35, 12.07, 11.73, 11.84, 11.42, 11.29, 11.46, 11.485, 11.67, 11.67, 11.7, 11.9, 11.89, 11.83, 11.73, 11.29, 11.42, 11.76, 11.75, 11.5, 11.68, 11.72, 11.64, 11.66, 11.8, 11.69, 12.015, 12.715, 12.535, 12.74, 13.02, 13.5, 13.09, 12.21, 12.225, 12.51, 12.355, 12.48, 12.76, 12.71, 12.87, 13.615, 13.52, 13.37, 13.55, 13.45, 13.53, 13.245, 13.36, 13.305, 12.72, 12.57, 12.22, 12.8, 12.84, 12.86, 12.57, 12.74, 12.75, 12.81, 12.89, 12.27, 12.57, 12.375, 12.41, 12.435, 12.68, 12.89, 13.2, 13.3, 13.98, 13.39, 13.26, 13.26, 13.47, 13.73, 14.12, 14.905, 15.07, 15.36, 15.9, 15.95, 15.8, 15.38, 15.53, 15.51, 15.995, 15.82, 16.8, 16.85, 16.65, 16.24, 16.36, 15.85, 16.51, 17.05, 15.71, 15.5, 15.25, 15.23, 15.05, 15.19, 15.27, 15.86, 15.55, 14.68, 14.9, 14.62, 14.82, 13.77, 13.19, 13.479, 13.5648, 13.77, 13.8, 13.8, 14.04, 14.16, 13.55, 13.87, 14.61, 14.65, 15.04, 15.365, 15.747, 15.6, 15.42, 15.884, 15.87, 15.42, 15.03, 15.34, 14.97, 15.78, 15.95, 16.2099, 15.11, 14.51, 13.74, 13.79, 13.8, 14.0, 13.925, 13.93, 13.8, 13.065, 12.1, 11.28, 11.05, 10.73, 10.61, 11.15, 11.19, 10.94, 10.695, 10.575, 10.545, 10.18, 10.03, 10.21, 10.02, 9.73, 9.53, 9.66, 9.645, 10.05, 9.87, 10.2, 10.135, 10.31, 10.675, 10.385, 10.69, 10.87, 10.7, 10.89, 10.21, 10.52, 10.64, 10.97, 10.56, 9.96, 10.0, 9.93, 9.78, 9.8, 10.141, 11.08, 10.285, 10.62, 10.6, 10.715, 10.48, 10.34, 10.28, 10.55, 10.98, 10.89, 11.08, 11.09, 11.98, 11.82, 11.97, 12.22, 11.98, 12.07, 12.18, 11.81, 12.1, 12.02, 12.09, 12.29, 12.23, 12.311, 12.38, 12.4, 12.4799, 12.96, 12.9399, 13.06, 12.68, 12.385, 12.67, 12.87, 12.29, 12.45, 13.15, 13.1, 12.95, 12.95, 12.91, 11.65, 11.18, 10.6617, 10.88, 10.51, 11.36, 11.15, 10.7682, 10.68, 10.13, 10.5701, 11.765, 11.496, 11.62, 11.55, 11.14, 10.78, 11.0, 10.21, 10.41, 10.37, 10.91, 11.5889, 11.23, 11.655, 11.1, 10.66, 10.6, 11.08, 11.14, 11.35, 11.65, 11.1, 11.39, 11.07, 10.98, 10.54, 11.51, 11.04, 10.4, 11.04, 11.615, 10.85, 10.5101, 11.79, 11.28, 11.79, 12.01, 13.56, 14.01, 12.64, 12.65, 11.48, 11.35, 11.66, 11.51, 12.35, 12.01, 11.03, 10.85, 10.73, 10.8199, 10.42, 9.76, 9.32, 8.85, 9.61, 9.34, 9.42, 9.88, 10.33, 10.28, 10.15, 10.15, 10.11, 9.75, 10.99, 10.8, 10.77, 10.89, 10.23, 9.27, 9.97, 9.54, 9.59, 9.14, 8.655, 9.86, 9.73, 9.11, 8.98, 7.76, 7.6275, 7.42, 7.11, 7.19, 7.24, 7.92, 6.92, 7.14, 7.16, 6.37, 5.52, 4.87, 4.98, 5.0, 5.26, 5.67, 5.72, 4.84, 4.35, 4.725, 4.59, 4.41, 4.6589, 4.19, 3.94, 3.93, 4.32, 4.06, 3.96, 4.35, 4.21, 3.73, 4.11, 4.305, 5.405, 5.6, 6.175, 6.71, 6.5463, 6.76, 6.761, 6.97, 6.84, 7.57, 7.4599, 6.42, 6.24, 6.2, 6.12, 6.7, 6.53, 6.47, 6.92, 7.36, 6.99, 6.74, 7.23, 7.84, 7.69, 7.84, 8.33, 8.18, 8.05, 8.11, 8.31, 8.0, 8.25, 8.41, 8.77, 8.4, 8.86, 8.672, 8.77, 9.3001, 9.83, 10.49, 10.75, 11.48, 12.01, 12.4, 11.82, 11.76, 11.62, 11.77, 11.6, 12.0228, 12.13, 12.01, 11.95, 12.24, 12.03, 12.45, 13.0, 13.05, 12.73, 12.94, 13.48, 13.46, 13.015, 11.84, 11.185, 10.63, 9.82, 9.68, 9.115, 8.91, 9.79, 9.99, 9.99, 10.6, 10.53, 10.7399, 12.05, 11.84, 11.29, 11.17, 11.39, 11.28, 10.74, 10.4, 9.7, 10.12, 9.89, 9.76, 10.64, 10.49, 10.2, 7.92, 8.26, 8.67, 9.58, 9.7199, 9.73, 9.92, 10.23, 10.02, 10.07, 10.255, 10.21, 11.66, 10.53, 11.2, 10.93, 11.04, 11.2, 11.75, 11.8532, 12.5, 12.33, 11.36, 12.29, 13.64, 15.06, 15.7, 15.03, 15.88, 16.31, 16.41, 17.11, 16.92, 16.73, 16.775, 16.52, 17.25, 17.84, 18.385, 18.4, 18.62, 19.38, 19.95, 19.74, 20.55, 20.11, 19.41, 19.76, 20.12, 20.02, 19.79, 19.59, 19.83, 19.94, 20.55, 20.02, 19.42, 19.64, 19.53, 19.87, 20.46, 19.34, 19.66, 20.16, 20.14, 20.08, 21.0, 21.36, 21.08, 21.28, 22.14, 22.83, 22.79, 22.6, 22.72, 22.965, 23.29, 22.91, 23.27, 23.34, 23.4, 23.6601, 23.28, 22.9, 22.69, 21.82, 20.82, 20.07, 20.57, 20.24, 20.66, 20.66, 20.83, 20.66, 18.97, 18.27, 18.3, 18.79, 18.83, 18.975, 19.16, 19.01, 18.97, 18.955, 19.38, 18.79, 19.51, 19.1, 19.19, 19.34, 18.4, 17.265, 18.23, 17.42, 17.96, 18.14, 18.89, 18.82, 18.85, 19.615, 19.435, 20.18, 20.88, 20.99, 21.13, 21.635, 21.1, 21.49, 21.23, 20.58, 21.28, 21.1, 21.29, 20.97, 20.3, 19.47, 18.6045, 18.7, 19.51, 18.86, 19.57, 18.29, 18.985, 17.46, 16.81, 16.835, 17.42, 18.3772, 19.54, 19.24, 20.02, 19.84, 19.26, 19.23, 18.32, 18.74, 21.06, 23.06, 23.45, 23.375, 22.84, 22.53, 22.145, 23.55, 23.36, 23.55, 23.26, 23.52, 22.8, 22.83, 22.545, 23.405, 22.82, 22.13, 21.2, 21.0399, 21.79, 22.98, 23.96, 25.13, 25.13, 26.0, 26.43, 26.68, 25.88, 26.1999, 26.99, 29.33, 29.28, 29.1, 29.58, 28.58, 28.08, 28.19, 28.42, 28.31, 27.9099, 28.55, 28.44, 28.0101, 28.33, 27.57, 27.13, 27.79, 27.81, 28.49, 28.09, 28.99, 29.01, 30.27, 30.8, 30.945, 30.9, 31.51, 30.75, 30.3499, 30.24, 30.03, 30.8, 30.72, 30.66, 31.13, 32.29, 31.59, 32.5, 32.31, 32.08, 31.87, 32.66, 32.41, 32.86, 32.66, 33.27, 32.96, 33.17, 34.05, 34.31, 34.51, 34.9, 34.31, 34.25, 34.56, 34.56, 34.42, 34.9, 35.01, 35.21, 35.29, 35.48, 36.37, 36.15, 36.43, 36.38, 36.48, 36.44, 36.67, 36.975, 36.29, 36.36, 36.1, 36.04, 36.4, 36.89, 36.89, 36.49, 36.14, 36.6962, 36.92, 37.24, 36.79, 37.23, 37.89, 37.91, 38.07, 37.97, 37.53, 38.56, 38.72, 38.665, 38.35, 38.51, 38.83, 38.29, 38.69, 38.72, 38.87, 39.05, 38.75, 38.6799, 38.44, 37.843, 36.85, 36.49, 36.06, 35.69, 35.78, 35.65, 36.04, 34.86, 34.63, 34.8, 34.05, 34.01, 33.98, 33.655, 34.09, 34.37, 34.65, 34.94, 34.765, 34.51, 34.08, 34.11, 34.04, 34.275, 33.81, 33.96, 34.44, 34.305, 34.26, 34.44, 35.06, 34.99, 35.22, 35.495, 35.57, 34.9, 33.91, 33.83, 34.0, 33.85, 34.07, 34.68, 34.16, 34.38, 34.44, 33.9, 34.01, 33.95, 33.5, 33.3, 32.97, 33.01, 33.0, 33.02, 33.29, 32.56, 33.19, 33.99, 33.62, 33.08, 33.33, 33.4, 33.33, 32.91, 33.06, 32.86, 32.41, 31.68, 32.08, 31.61, 32.29, 30.89, 31.08, 31.65, 31.5, 31.06, 30.63, 30.78, 30.72, 31.38, 32.17, 33.82, 33.92, 33.51, 32.82, 32.62, 33.46, 33.425, 32.76, 33.21, 33.35, 33.76, 33.17, 33.74, 33.75, 33.17, 32.77, 33.18, 32.23, 32.35, 31.54, 31.08, 30.95, 31.09, 32.4, 32.42, 32.35, 32.565, 32.4, 32.76, 33.67, 34.51, 35.26, 36.18, 36.91, 36.61, 36.11, 35.62, 36.18, 35.72, 36.19, 36.66, 37.02, 37.31, 37.62, 37.7499, 37.68, 37.51, 36.97, 36.31, 35.74, 35.68, 35.2, 35.19, 34.9, 34.59, 34.35, 34.3, 34.46, 34.89, 34.64, 34.25, 34.28, 34.53, 34.21, 34.25, 34.6999, 34.68, 35.33, 35.8, 36.1078, 36.145, 36.18, 36.4, 36.4849, 36.8375, 36.35, 36.32, 35.88, 36.47, 36.59, 35.92, 37.05, 37.18, 37.41, 36.78, 36.7501, 37.27, 37.68, 37.27, 37.45, 37.41, 36.64]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f0a65365-f5cf-4654-bfa6-68f975d3084d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"685da041-ffd5-44f5-8356-4fa1446d0ecc\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"685da041-ffd5-44f5-8356-4fa1446d0ecc\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '685da041-ffd5-44f5-8356-4fa1446d0ecc',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('685da041-ffd5-44f5-8356-4fa1446d0ecc');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYJvaA2uVHfZ"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABW1Wkq-VHfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d2e284-dd6b-4514-9065-82ac5031691b"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .2, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"FCX\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6893 - accuracy: 0.5477 - val_loss: 0.7204 - val_accuracy: 0.3918\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6373 - accuracy: 0.6376 - val_loss: 0.7106 - val_accuracy: 0.5143\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5980 - accuracy: 0.6799 - val_loss: 0.6251 - val_accuracy: 0.6327\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5338 - accuracy: 0.7463 - val_loss: 0.5662 - val_accuracy: 0.7041\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5255 - accuracy: 0.7510 - val_loss: 0.5858 - val_accuracy: 0.6816\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6122 - accuracy: 0.6530 - val_loss: 0.5663 - val_accuracy: 0.7143\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5236 - accuracy: 0.7557 - val_loss: 0.5753 - val_accuracy: 0.6816\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5053 - accuracy: 0.7544 - val_loss: 0.5878 - val_accuracy: 0.6633\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5089 - accuracy: 0.7631 - val_loss: 0.5365 - val_accuracy: 0.7388\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5032 - accuracy: 0.7591 - val_loss: 0.6367 - val_accuracy: 0.6224\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.740754\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.744835\n",
            "[2]\tvalidation_0-auc:0.754046\n",
            "[3]\tvalidation_0-auc:0.755628\n",
            "[4]\tvalidation_0-auc:0.760312\n",
            "[5]\tvalidation_0-auc:0.760696\n",
            "[6]\tvalidation_0-auc:0.773079\n",
            "[7]\tvalidation_0-auc:0.760976\n",
            "[8]\tvalidation_0-auc:0.759586\n",
            "[9]\tvalidation_0-auc:0.759351\n",
            "[10]\tvalidation_0-auc:0.758381\n",
            "[11]\tvalidation_0-auc:0.757507\n",
            "[12]\tvalidation_0-auc:0.758704\n",
            "[13]\tvalidation_0-auc:0.758616\n",
            "[14]\tvalidation_0-auc:0.757987\n",
            "[15]\tvalidation_0-auc:0.758879\n",
            "[16]\tvalidation_0-auc:0.758354\n",
            "[17]\tvalidation_0-auc:0.757856\n",
            "[18]\tvalidation_0-auc:0.757279\n",
            "[19]\tvalidation_0-auc:0.757341\n",
            "[20]\tvalidation_0-auc:0.75298\n",
            "[21]\tvalidation_0-auc:0.753067\n",
            "[22]\tvalidation_0-auc:0.753015\n",
            "[23]\tvalidation_0-auc:0.752945\n",
            "[24]\tvalidation_0-auc:0.757795\n",
            "[25]\tvalidation_0-auc:0.758389\n",
            "[26]\tvalidation_0-auc:0.75818\n",
            "[27]\tvalidation_0-auc:0.756502\n",
            "[28]\tvalidation_0-auc:0.762094\n",
            "[29]\tvalidation_0-auc:0.762872\n",
            "[30]\tvalidation_0-auc:0.7624\n",
            "[31]\tvalidation_0-auc:0.762453\n",
            "[32]\tvalidation_0-auc:0.762418\n",
            "[33]\tvalidation_0-auc:0.762732\n",
            "[34]\tvalidation_0-auc:0.76275\n",
            "[35]\tvalidation_0-auc:0.759683\n",
            "[36]\tvalidation_0-auc:0.759683\n",
            "[37]\tvalidation_0-auc:0.759683\n",
            "[38]\tvalidation_0-auc:0.761535\n",
            "[39]\tvalidation_0-auc:0.762619\n",
            "[40]\tvalidation_0-auc:0.763493\n",
            "[41]\tvalidation_0-auc:0.763528\n",
            "[42]\tvalidation_0-auc:0.763528\n",
            "[43]\tvalidation_0-auc:0.763702\n",
            "[44]\tvalidation_0-auc:0.76386\n",
            "[45]\tvalidation_0-auc:0.763982\n",
            "[46]\tvalidation_0-auc:0.765066\n",
            "[47]\tvalidation_0-auc:0.766656\n",
            "[48]\tvalidation_0-auc:0.767207\n",
            "[49]\tvalidation_0-auc:0.767259\n",
            "[50]\tvalidation_0-auc:0.768483\n",
            "[51]\tvalidation_0-auc:0.768133\n",
            "[52]\tvalidation_0-auc:0.767329\n",
            "[53]\tvalidation_0-auc:0.767941\n",
            "[54]\tvalidation_0-auc:0.768561\n",
            "[55]\tvalidation_0-auc:0.768762\n",
            "[56]\tvalidation_0-auc:0.768587\n",
            "Stopping. Best iteration:\n",
            "[6]\tvalidation_0-auc:0.773079\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6870 - accuracy: 0.5635 - val_loss: 0.7365 - val_accuracy: 0.3917\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6684 - accuracy: 0.5882 - val_loss: 0.6831 - val_accuracy: 0.4551\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5966 - accuracy: 0.6905 - val_loss: 0.6233 - val_accuracy: 0.6171\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5687 - accuracy: 0.7152 - val_loss: 0.6959 - val_accuracy: 0.5536\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5520 - accuracy: 0.7152 - val_loss: 0.6308 - val_accuracy: 0.6193\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6768 - accuracy: 0.5724 - val_loss: 0.6609 - val_accuracy: 0.6389\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5956 - accuracy: 0.6788 - val_loss: 0.6684 - val_accuracy: 0.5383\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5536 - accuracy: 0.7241 - val_loss: 0.6669 - val_accuracy: 0.5646\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5415 - accuracy: 0.7255 - val_loss: 0.6147 - val_accuracy: 0.6149\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5348 - accuracy: 0.7433 - val_loss: 0.6474 - val_accuracy: 0.5886\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.628602\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.689572\n",
            "[2]\tvalidation_0-auc:0.694355\n",
            "[3]\tvalidation_0-auc:0.685463\n",
            "[4]\tvalidation_0-auc:0.678409\n",
            "[5]\tvalidation_0-auc:0.707879\n",
            "[6]\tvalidation_0-auc:0.70589\n",
            "[7]\tvalidation_0-auc:0.708412\n",
            "[8]\tvalidation_0-auc:0.695742\n",
            "[9]\tvalidation_0-auc:0.695089\n",
            "[10]\tvalidation_0-auc:0.702162\n",
            "[11]\tvalidation_0-auc:0.706865\n",
            "[12]\tvalidation_0-auc:0.713155\n",
            "[13]\tvalidation_0-auc:0.720208\n",
            "[14]\tvalidation_0-auc:0.719012\n",
            "[15]\tvalidation_0-auc:0.722941\n",
            "[16]\tvalidation_0-auc:0.729512\n",
            "[17]\tvalidation_0-auc:0.72682\n",
            "[18]\tvalidation_0-auc:0.731623\n",
            "[19]\tvalidation_0-auc:0.727865\n",
            "[20]\tvalidation_0-auc:0.73323\n",
            "[21]\tvalidation_0-auc:0.717817\n",
            "[22]\tvalidation_0-auc:0.720047\n",
            "[23]\tvalidation_0-auc:0.724288\n",
            "[24]\tvalidation_0-auc:0.724288\n",
            "[25]\tvalidation_0-auc:0.725363\n",
            "[26]\tvalidation_0-auc:0.728859\n",
            "[27]\tvalidation_0-auc:0.728739\n",
            "[28]\tvalidation_0-auc:0.731673\n",
            "[29]\tvalidation_0-auc:0.72279\n",
            "[30]\tvalidation_0-auc:0.713135\n",
            "[31]\tvalidation_0-auc:0.70809\n",
            "[32]\tvalidation_0-auc:0.712813\n",
            "[33]\tvalidation_0-auc:0.712662\n",
            "[34]\tvalidation_0-auc:0.712662\n",
            "[35]\tvalidation_0-auc:0.712783\n",
            "[36]\tvalidation_0-auc:0.717787\n",
            "[37]\tvalidation_0-auc:0.725483\n",
            "[38]\tvalidation_0-auc:0.72474\n",
            "[39]\tvalidation_0-auc:0.721565\n",
            "[40]\tvalidation_0-auc:0.721565\n",
            "[41]\tvalidation_0-auc:0.721565\n",
            "[42]\tvalidation_0-auc:0.721565\n",
            "[43]\tvalidation_0-auc:0.717375\n",
            "[44]\tvalidation_0-auc:0.715596\n",
            "[45]\tvalidation_0-auc:0.714833\n",
            "[46]\tvalidation_0-auc:0.714139\n",
            "[47]\tvalidation_0-auc:0.713094\n",
            "[48]\tvalidation_0-auc:0.713938\n",
            "[49]\tvalidation_0-auc:0.713938\n",
            "[50]\tvalidation_0-auc:0.714461\n",
            "[51]\tvalidation_0-auc:0.714461\n",
            "[52]\tvalidation_0-auc:0.714461\n",
            "[53]\tvalidation_0-auc:0.710201\n",
            "[54]\tvalidation_0-auc:0.710201\n",
            "[55]\tvalidation_0-auc:0.708894\n",
            "[56]\tvalidation_0-auc:0.70385\n",
            "[57]\tvalidation_0-auc:0.698987\n",
            "[58]\tvalidation_0-auc:0.696144\n",
            "[59]\tvalidation_0-auc:0.696114\n",
            "[60]\tvalidation_0-auc:0.694184\n",
            "[61]\tvalidation_0-auc:0.693823\n",
            "[62]\tvalidation_0-auc:0.693823\n",
            "[63]\tvalidation_0-auc:0.693823\n",
            "[64]\tvalidation_0-auc:0.693762\n",
            "[65]\tvalidation_0-auc:0.699168\n",
            "[66]\tvalidation_0-auc:0.702223\n",
            "[67]\tvalidation_0-auc:0.702002\n",
            "[68]\tvalidation_0-auc:0.693501\n",
            "[69]\tvalidation_0-auc:0.694375\n",
            "[70]\tvalidation_0-auc:0.691723\n",
            "Stopping. Best iteration:\n",
            "[20]\tvalidation_0-auc:0.73323\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.6816326530612244 | 0.5708661417322834 | 0.7552083333333334 | 0.6502242152466368 |\n",
            "|     GRU 0.1      | 0.6224489795918368 | 0.5103857566765578 | 0.8958333333333334 | 0.6502835538752363 |\n",
            "|   XGBoost 0.1    | 0.6489795918367347 | 0.5331125827814569 | 0.8385416666666666 | 0.6518218623481782 |\n",
            "|    Logreg 0.1    | 0.6551020408163265 | 0.5384615384615384 | 0.8385416666666666 | 0.6558044806517311 |\n",
            "|     SVM 0.1      | 0.6632653061224489 | 0.5454545454545454 |      0.84375       | 0.6625766871165644 |\n",
            "|  LSTM beta 0.1   | 0.6192560175054704 | 0.508833922261484  | 0.8044692737430168 | 0.6233766233766234 |\n",
            "|   GRU beta 0.1   | 0.5886214442013129 | 0.4840989399293286 | 0.7653631284916201 | 0.5930735930735931 |\n",
            "| XGBoost beta 0.1 | 0.5820568927789934 | 0.4794520547945205 | 0.7821229050279329 | 0.5944798301486199 |\n",
            "| logreg beta 0.1  | 0.6214442013129103 | 0.5103448275862069 | 0.8268156424581006 | 0.6311300639658849 |\n",
            "|   svm beta 0.1   | 0.6345733041575492 | 0.5211267605633803 | 0.8268156424581006 | 0.6393088552915767 |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6466 - accuracy: 0.6369 - val_loss: 0.6606 - val_accuracy: 0.5837\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5746 - accuracy: 0.7007 - val_loss: 0.6237 - val_accuracy: 0.6020\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.5729 - accuracy: 0.7020 - val_loss: 0.6466 - val_accuracy: 0.6020\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5562 - accuracy: 0.6899 - val_loss: 0.6513 - val_accuracy: 0.6143\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5565 - accuracy: 0.7047 - val_loss: 0.6644 - val_accuracy: 0.5673\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6257 - accuracy: 0.6537 - val_loss: 0.7340 - val_accuracy: 0.5347\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5582 - accuracy: 0.7034 - val_loss: 0.6223 - val_accuracy: 0.6245\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5318 - accuracy: 0.7336 - val_loss: 0.6284 - val_accuracy: 0.6061\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5324 - accuracy: 0.7201 - val_loss: 0.6151 - val_accuracy: 0.6224\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5357 - accuracy: 0.7248 - val_loss: 0.6404 - val_accuracy: 0.5959\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.659647\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.687099\n",
            "[2]\tvalidation_0-auc:0.685121\n",
            "[3]\tvalidation_0-auc:0.687008\n",
            "[4]\tvalidation_0-auc:0.704519\n",
            "[5]\tvalidation_0-auc:0.701973\n",
            "[6]\tvalidation_0-auc:0.702432\n",
            "[7]\tvalidation_0-auc:0.704335\n",
            "[8]\tvalidation_0-auc:0.704836\n",
            "[9]\tvalidation_0-auc:0.708459\n",
            "[10]\tvalidation_0-auc:0.708225\n",
            "[11]\tvalidation_0-auc:0.708208\n",
            "[12]\tvalidation_0-auc:0.708392\n",
            "[13]\tvalidation_0-auc:0.708225\n",
            "[14]\tvalidation_0-auc:0.709076\n",
            "[15]\tvalidation_0-auc:0.709293\n",
            "[16]\tvalidation_0-auc:0.709143\n",
            "[17]\tvalidation_0-auc:0.709435\n",
            "[18]\tvalidation_0-auc:0.709435\n",
            "[19]\tvalidation_0-auc:0.711956\n",
            "[20]\tvalidation_0-auc:0.712006\n",
            "[21]\tvalidation_0-auc:0.713358\n",
            "[22]\tvalidation_0-auc:0.710387\n",
            "[23]\tvalidation_0-auc:0.709969\n",
            "[24]\tvalidation_0-auc:0.707791\n",
            "[25]\tvalidation_0-auc:0.703451\n",
            "[26]\tvalidation_0-auc:0.705721\n",
            "[27]\tvalidation_0-auc:0.703968\n",
            "[28]\tvalidation_0-auc:0.704093\n",
            "[29]\tvalidation_0-auc:0.705629\n",
            "[30]\tvalidation_0-auc:0.705746\n",
            "[31]\tvalidation_0-auc:0.705746\n",
            "[32]\tvalidation_0-auc:0.705946\n",
            "[33]\tvalidation_0-auc:0.705946\n",
            "[34]\tvalidation_0-auc:0.700212\n",
            "[35]\tvalidation_0-auc:0.700846\n",
            "[36]\tvalidation_0-auc:0.70098\n",
            "[37]\tvalidation_0-auc:0.698317\n",
            "[38]\tvalidation_0-auc:0.699027\n",
            "[39]\tvalidation_0-auc:0.699786\n",
            "[40]\tvalidation_0-auc:0.699786\n",
            "[41]\tvalidation_0-auc:0.699786\n",
            "[42]\tvalidation_0-auc:0.698868\n",
            "[43]\tvalidation_0-auc:0.696898\n",
            "[44]\tvalidation_0-auc:0.695179\n",
            "[45]\tvalidation_0-auc:0.693343\n",
            "[46]\tvalidation_0-auc:0.693159\n",
            "[47]\tvalidation_0-auc:0.693226\n",
            "[48]\tvalidation_0-auc:0.693226\n",
            "[49]\tvalidation_0-auc:0.692717\n",
            "[50]\tvalidation_0-auc:0.693326\n",
            "[51]\tvalidation_0-auc:0.693326\n",
            "[52]\tvalidation_0-auc:0.691815\n",
            "[53]\tvalidation_0-auc:0.691206\n",
            "[54]\tvalidation_0-auc:0.68982\n",
            "[55]\tvalidation_0-auc:0.689002\n",
            "[56]\tvalidation_0-auc:0.688902\n",
            "[57]\tvalidation_0-auc:0.688744\n",
            "[58]\tvalidation_0-auc:0.688727\n",
            "[59]\tvalidation_0-auc:0.68881\n",
            "[60]\tvalidation_0-auc:0.687225\n",
            "[61]\tvalidation_0-auc:0.686749\n",
            "[62]\tvalidation_0-auc:0.686832\n",
            "[63]\tvalidation_0-auc:0.688318\n",
            "[64]\tvalidation_0-auc:0.687867\n",
            "[65]\tvalidation_0-auc:0.686574\n",
            "[66]\tvalidation_0-auc:0.684829\n",
            "[67]\tvalidation_0-auc:0.684529\n",
            "[68]\tvalidation_0-auc:0.684529\n",
            "[69]\tvalidation_0-auc:0.683844\n",
            "[70]\tvalidation_0-auc:0.683327\n",
            "[71]\tvalidation_0-auc:0.683293\n",
            "Stopping. Best iteration:\n",
            "[21]\tvalidation_0-auc:0.713358\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6533 - accuracy: 0.6424 - val_loss: 0.6272 - val_accuracy: 0.7462\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5899 - accuracy: 0.6863 - val_loss: 0.6376 - val_accuracy: 0.5361\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.5568 - accuracy: 0.7056 - val_loss: 0.5845 - val_accuracy: 0.6630\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.5269 - accuracy: 0.7220 - val_loss: 0.5578 - val_accuracy: 0.6783\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5151 - accuracy: 0.7406 - val_loss: 0.5721 - val_accuracy: 0.7309\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5966 - accuracy: 0.6767 - val_loss: 0.6150 - val_accuracy: 0.5667\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5226 - accuracy: 0.7392 - val_loss: 0.5496 - val_accuracy: 0.7330\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5065 - accuracy: 0.7474 - val_loss: 0.7274 - val_accuracy: 0.5580\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5002 - accuracy: 0.7509 - val_loss: 0.5735 - val_accuracy: 0.6455\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5076 - accuracy: 0.7433 - val_loss: 0.5760 - val_accuracy: 0.6477\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.768663\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.688724\n",
            "[2]\tvalidation_0-auc:0.692387\n",
            "[3]\tvalidation_0-auc:0.729097\n",
            "[4]\tvalidation_0-auc:0.705069\n",
            "[5]\tvalidation_0-auc:0.700252\n",
            "[6]\tvalidation_0-auc:0.70804\n",
            "[7]\tvalidation_0-auc:0.690118\n",
            "[8]\tvalidation_0-auc:0.697367\n",
            "[9]\tvalidation_0-auc:0.692435\n",
            "[10]\tvalidation_0-auc:0.674762\n",
            "[11]\tvalidation_0-auc:0.689002\n",
            "[12]\tvalidation_0-auc:0.688541\n",
            "[13]\tvalidation_0-auc:0.67458\n",
            "[14]\tvalidation_0-auc:0.669599\n",
            "[15]\tvalidation_0-auc:0.679743\n",
            "[16]\tvalidation_0-auc:0.692993\n",
            "[17]\tvalidation_0-auc:0.691185\n",
            "[18]\tvalidation_0-auc:0.690916\n",
            "[19]\tvalidation_0-auc:0.687858\n",
            "[20]\tvalidation_0-auc:0.685079\n",
            "[21]\tvalidation_0-auc:0.688897\n",
            "[22]\tvalidation_0-auc:0.68955\n",
            "[23]\tvalidation_0-auc:0.69032\n",
            "[24]\tvalidation_0-auc:0.692589\n",
            "[25]\tvalidation_0-auc:0.692666\n",
            "[26]\tvalidation_0-auc:0.694262\n",
            "[27]\tvalidation_0-auc:0.694627\n",
            "[28]\tvalidation_0-auc:0.689973\n",
            "[29]\tvalidation_0-auc:0.690166\n",
            "[30]\tvalidation_0-auc:0.686022\n",
            "[31]\tvalidation_0-auc:0.68606\n",
            "[32]\tvalidation_0-auc:0.687925\n",
            "[33]\tvalidation_0-auc:0.685118\n",
            "[34]\tvalidation_0-auc:0.685983\n",
            "[35]\tvalidation_0-auc:0.686349\n",
            "[36]\tvalidation_0-auc:0.683185\n",
            "[37]\tvalidation_0-auc:0.680512\n",
            "[38]\tvalidation_0-auc:0.680512\n",
            "[39]\tvalidation_0-auc:0.680512\n",
            "[40]\tvalidation_0-auc:0.681743\n",
            "[41]\tvalidation_0-auc:0.682483\n",
            "[42]\tvalidation_0-auc:0.683964\n",
            "[43]\tvalidation_0-auc:0.685868\n",
            "[44]\tvalidation_0-auc:0.686022\n",
            "[45]\tvalidation_0-auc:0.686445\n",
            "[46]\tvalidation_0-auc:0.681676\n",
            "[47]\tvalidation_0-auc:0.681753\n",
            "[48]\tvalidation_0-auc:0.678791\n",
            "[49]\tvalidation_0-auc:0.67633\n",
            "[50]\tvalidation_0-auc:0.677099\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.768663\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.2     | 0.5673469387755102 | 0.5478260869565217 |      0.984375      | 0.7039106145251397 |\n",
            "|     GRU 0.2      | 0.5959183673469388 | 0.5747422680412371 |     0.87109375     | 0.6925465838509317 |\n",
            "|   XGBoost 0.2    | 0.5897959183673469 | 0.5685785536159601 |      0.890625      | 0.6940639269406392 |\n",
            "|    Logreg 0.2    | 0.5959183673469388 | 0.5736040609137056 |     0.8828125      | 0.6953846153846155 |\n",
            "|     SVM 0.2      | 0.563265306122449  | 0.5462555066079295 |      0.96875       | 0.6985915492957746 |\n",
            "|  LSTM beta 0.2   | 0.7308533916849015 | 0.7362204724409449 | 0.7695473251028807 | 0.7525150905432596 |\n",
            "|   GRU beta 0.2   | 0.6477024070021882 | 0.6120218579234973 | 0.9218106995884774 | 0.7356321839080461 |\n",
            "| XGBoost beta 0.2 | 0.5251641137855579 | 0.5326633165829145 | 0.8724279835390947 | 0.6614664586583464 |\n",
            "| logreg beta 0.2  | 0.6389496717724289 | 0.6077348066298343 | 0.9053497942386831 | 0.7272727272727274 |\n",
            "|   svm beta 0.2   | 0.6608315098468271 | 0.6271676300578035 | 0.8930041152263375 | 0.736842105263158  |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6613 - accuracy: 0.6020 - val_loss: 0.6492 - val_accuracy: 0.6163\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.5958 - accuracy: 0.6960 - val_loss: 0.6342 - val_accuracy: 0.6143\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5777 - accuracy: 0.7067 - val_loss: 0.8095 - val_accuracy: 0.5837\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5679 - accuracy: 0.7228 - val_loss: 0.6305 - val_accuracy: 0.6082\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.5594 - accuracy: 0.7208 - val_loss: 0.6251 - val_accuracy: 0.6347\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6496 - accuracy: 0.6289 - val_loss: 0.6416 - val_accuracy: 0.6102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5634 - accuracy: 0.7181 - val_loss: 0.6193 - val_accuracy: 0.6204\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5361 - accuracy: 0.7376 - val_loss: 0.6154 - val_accuracy: 0.6449\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5274 - accuracy: 0.7423 - val_loss: 0.6128 - val_accuracy: 0.6469\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5251 - accuracy: 0.7389 - val_loss: 0.6251 - val_accuracy: 0.6184\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.701022\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.699845\n",
            "[2]\tvalidation_0-auc:0.697675\n",
            "[3]\tvalidation_0-auc:0.696064\n",
            "[4]\tvalidation_0-auc:0.698242\n",
            "[5]\tvalidation_0-auc:0.699436\n",
            "[6]\tvalidation_0-auc:0.699018\n",
            "[7]\tvalidation_0-auc:0.70007\n",
            "[8]\tvalidation_0-auc:0.701898\n",
            "[9]\tvalidation_0-auc:0.704285\n",
            "[10]\tvalidation_0-auc:0.705596\n",
            "[11]\tvalidation_0-auc:0.704744\n",
            "[12]\tvalidation_0-auc:0.70391\n",
            "[13]\tvalidation_0-auc:0.705879\n",
            "[14]\tvalidation_0-auc:0.707098\n",
            "[15]\tvalidation_0-auc:0.708016\n",
            "[16]\tvalidation_0-auc:0.707924\n",
            "[17]\tvalidation_0-auc:0.708191\n",
            "[18]\tvalidation_0-auc:0.708242\n",
            "[19]\tvalidation_0-auc:0.708191\n",
            "[20]\tvalidation_0-auc:0.7083\n",
            "[21]\tvalidation_0-auc:0.708784\n",
            "[22]\tvalidation_0-auc:0.710395\n",
            "[23]\tvalidation_0-auc:0.710579\n",
            "[24]\tvalidation_0-auc:0.70946\n",
            "[25]\tvalidation_0-auc:0.711013\n",
            "[26]\tvalidation_0-auc:0.711146\n",
            "[27]\tvalidation_0-auc:0.710103\n",
            "[28]\tvalidation_0-auc:0.707933\n",
            "[29]\tvalidation_0-auc:0.708066\n",
            "[30]\tvalidation_0-auc:0.707382\n",
            "[31]\tvalidation_0-auc:0.706923\n",
            "[32]\tvalidation_0-auc:0.706956\n",
            "[33]\tvalidation_0-auc:0.707357\n",
            "[34]\tvalidation_0-auc:0.707732\n",
            "[35]\tvalidation_0-auc:0.708434\n",
            "[36]\tvalidation_0-auc:0.708233\n",
            "[37]\tvalidation_0-auc:0.707816\n",
            "[38]\tvalidation_0-auc:0.708233\n",
            "[39]\tvalidation_0-auc:0.708492\n",
            "[40]\tvalidation_0-auc:0.708459\n",
            "[41]\tvalidation_0-auc:0.708893\n",
            "[42]\tvalidation_0-auc:0.708592\n",
            "[43]\tvalidation_0-auc:0.708792\n",
            "[44]\tvalidation_0-auc:0.708809\n",
            "[45]\tvalidation_0-auc:0.708692\n",
            "[46]\tvalidation_0-auc:0.709302\n",
            "[47]\tvalidation_0-auc:0.709302\n",
            "[48]\tvalidation_0-auc:0.709118\n",
            "[49]\tvalidation_0-auc:0.708976\n",
            "[50]\tvalidation_0-auc:0.708976\n",
            "[51]\tvalidation_0-auc:0.709193\n",
            "[52]\tvalidation_0-auc:0.708692\n",
            "[53]\tvalidation_0-auc:0.708759\n",
            "[54]\tvalidation_0-auc:0.708292\n",
            "[55]\tvalidation_0-auc:0.708292\n",
            "[56]\tvalidation_0-auc:0.708392\n",
            "[57]\tvalidation_0-auc:0.708408\n",
            "[58]\tvalidation_0-auc:0.708325\n",
            "[59]\tvalidation_0-auc:0.708692\n",
            "[60]\tvalidation_0-auc:0.708726\n",
            "[61]\tvalidation_0-auc:0.708709\n",
            "[62]\tvalidation_0-auc:0.708258\n",
            "[63]\tvalidation_0-auc:0.707991\n",
            "[64]\tvalidation_0-auc:0.707874\n",
            "[65]\tvalidation_0-auc:0.707774\n",
            "[66]\tvalidation_0-auc:0.707565\n",
            "[67]\tvalidation_0-auc:0.709619\n",
            "[68]\tvalidation_0-auc:0.709569\n",
            "[69]\tvalidation_0-auc:0.709602\n",
            "[70]\tvalidation_0-auc:0.709059\n",
            "[71]\tvalidation_0-auc:0.709059\n",
            "[72]\tvalidation_0-auc:0.709059\n",
            "[73]\tvalidation_0-auc:0.70911\n",
            "[74]\tvalidation_0-auc:0.709076\n",
            "[75]\tvalidation_0-auc:0.708926\n",
            "[76]\tvalidation_0-auc:0.708976\n",
            "Stopping. Best iteration:\n",
            "[26]\tvalidation_0-auc:0.711146\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6712 - accuracy: 0.5806 - val_loss: 0.6404 - val_accuracy: 0.7287\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6299 - accuracy: 0.6239 - val_loss: 0.6363 - val_accuracy: 0.5317\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.5861 - accuracy: 0.6815 - val_loss: 0.6079 - val_accuracy: 0.5908\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5693 - accuracy: 0.7104 - val_loss: 0.5632 - val_accuracy: 0.7352\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5352 - accuracy: 0.7385 - val_loss: 0.5825 - val_accuracy: 0.6958\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6395 - accuracy: 0.6156 - val_loss: 0.5966 - val_accuracy: 0.6214\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5440 - accuracy: 0.7234 - val_loss: 0.5677 - val_accuracy: 0.6849\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5409 - accuracy: 0.7255 - val_loss: 0.5657 - val_accuracy: 0.7462\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5260 - accuracy: 0.7351 - val_loss: 0.5922 - val_accuracy: 0.6565\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5317 - accuracy: 0.7296 - val_loss: 0.5626 - val_accuracy: 0.7177\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.65334\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.666263\n",
            "[2]\tvalidation_0-auc:0.674157\n",
            "[3]\tvalidation_0-auc:0.679532\n",
            "[4]\tvalidation_0-auc:0.68182\n",
            "[5]\tvalidation_0-auc:0.67582\n",
            "[6]\tvalidation_0-auc:0.667263\n",
            "[7]\tvalidation_0-auc:0.671859\n",
            "[8]\tvalidation_0-auc:0.670359\n",
            "[9]\tvalidation_0-auc:0.677493\n",
            "[10]\tvalidation_0-auc:0.665542\n",
            "[11]\tvalidation_0-auc:0.669388\n",
            "[12]\tvalidation_0-auc:0.669369\n",
            "[13]\tvalidation_0-auc:0.668917\n",
            "[14]\tvalidation_0-auc:0.676301\n",
            "[15]\tvalidation_0-auc:0.676301\n",
            "[16]\tvalidation_0-auc:0.67857\n",
            "[17]\tvalidation_0-auc:0.669493\n",
            "[18]\tvalidation_0-auc:0.670301\n",
            "[19]\tvalidation_0-auc:0.669926\n",
            "[20]\tvalidation_0-auc:0.671801\n",
            "[21]\tvalidation_0-auc:0.675166\n",
            "[22]\tvalidation_0-auc:0.671397\n",
            "[23]\tvalidation_0-auc:0.676628\n",
            "[24]\tvalidation_0-auc:0.677945\n",
            "[25]\tvalidation_0-auc:0.680339\n",
            "[26]\tvalidation_0-auc:0.680339\n",
            "[27]\tvalidation_0-auc:0.681416\n",
            "[28]\tvalidation_0-auc:0.681416\n",
            "[29]\tvalidation_0-auc:0.682128\n",
            "[30]\tvalidation_0-auc:0.679262\n",
            "[31]\tvalidation_0-auc:0.678878\n",
            "[32]\tvalidation_0-auc:0.675551\n",
            "[33]\tvalidation_0-auc:0.676166\n",
            "[34]\tvalidation_0-auc:0.676166\n",
            "[35]\tvalidation_0-auc:0.676109\n",
            "[36]\tvalidation_0-auc:0.682445\n",
            "[37]\tvalidation_0-auc:0.679195\n",
            "[38]\tvalidation_0-auc:0.676359\n",
            "[39]\tvalidation_0-auc:0.673436\n",
            "[40]\tvalidation_0-auc:0.670138\n",
            "[41]\tvalidation_0-auc:0.670138\n",
            "[42]\tvalidation_0-auc:0.670138\n",
            "[43]\tvalidation_0-auc:0.670138\n",
            "[44]\tvalidation_0-auc:0.672945\n",
            "[45]\tvalidation_0-auc:0.672945\n",
            "[46]\tvalidation_0-auc:0.67558\n",
            "[47]\tvalidation_0-auc:0.672205\n",
            "[48]\tvalidation_0-auc:0.673705\n",
            "[49]\tvalidation_0-auc:0.675282\n",
            "[50]\tvalidation_0-auc:0.675282\n",
            "[51]\tvalidation_0-auc:0.675282\n",
            "[52]\tvalidation_0-auc:0.674628\n",
            "[53]\tvalidation_0-auc:0.669763\n",
            "[54]\tvalidation_0-auc:0.671484\n",
            "[55]\tvalidation_0-auc:0.670916\n",
            "[56]\tvalidation_0-auc:0.669205\n",
            "[57]\tvalidation_0-auc:0.670282\n",
            "[58]\tvalidation_0-auc:0.671474\n",
            "[59]\tvalidation_0-auc:0.671416\n",
            "[60]\tvalidation_0-auc:0.672705\n",
            "[61]\tvalidation_0-auc:0.672743\n",
            "[62]\tvalidation_0-auc:0.674388\n",
            "[63]\tvalidation_0-auc:0.674407\n",
            "[64]\tvalidation_0-auc:0.672445\n",
            "[65]\tvalidation_0-auc:0.672272\n",
            "[66]\tvalidation_0-auc:0.669244\n",
            "[67]\tvalidation_0-auc:0.669244\n",
            "[68]\tvalidation_0-auc:0.670378\n",
            "[69]\tvalidation_0-auc:0.665801\n",
            "[70]\tvalidation_0-auc:0.666628\n",
            "[71]\tvalidation_0-auc:0.666282\n",
            "[72]\tvalidation_0-auc:0.666003\n",
            "[73]\tvalidation_0-auc:0.66708\n",
            "[74]\tvalidation_0-auc:0.66708\n",
            "[75]\tvalidation_0-auc:0.66933\n",
            "[76]\tvalidation_0-auc:0.668657\n",
            "[77]\tvalidation_0-auc:0.668311\n",
            "[78]\tvalidation_0-auc:0.665369\n",
            "[79]\tvalidation_0-auc:0.66233\n",
            "[80]\tvalidation_0-auc:0.662407\n",
            "[81]\tvalidation_0-auc:0.662003\n",
            "[82]\tvalidation_0-auc:0.662176\n",
            "[83]\tvalidation_0-auc:0.662176\n",
            "[84]\tvalidation_0-auc:0.661407\n",
            "[85]\tvalidation_0-auc:0.659984\n",
            "[86]\tvalidation_0-auc:0.660003\n",
            "Stopping. Best iteration:\n",
            "[36]\tvalidation_0-auc:0.682445\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.6346938775510204 | 0.7894736842105263 |     0.41015625     | 0.5398457583547558 |\n",
            "|      GRU 0.15     | 0.6183673469387755 | 0.6011730205278593 |     0.80078125     | 0.6867671691792295 |\n",
            "|    XGBoost 0.15   | 0.636734693877551  |        0.65        |     0.66015625     | 0.6550387596899225 |\n",
            "|    Logreg 0.15    | 0.6204081632653061 | 0.6067073170731707 |     0.77734375     | 0.6815068493150686 |\n",
            "|      SVM 0.15     | 0.6612244897959184 | 0.6642335766423357 |     0.7109375      | 0.6867924528301886 |\n",
            "|   LSTM beta 0.15  | 0.6958424507658644 | 0.6925925925925925 | 0.7695473251028807 | 0.7290448343079923 |\n",
            "|   GRU beta 0.15   | 0.7177242888402626 | 0.7261904761904762 | 0.7530864197530864 | 0.7393939393939394 |\n",
            "| XGBoost beta 0.15 | 0.5951859956236324 | 0.6006944444444444 | 0.7119341563786008 | 0.6516007532956686 |\n",
            "|  logreg beta 0.15 | 0.687089715536105  | 0.6623376623376623 | 0.8395061728395061 | 0.7404718693284936 |\n",
            "|   svm beta 0.15   | 0.7396061269146609 | 0.7649572649572649 | 0.7366255144032922 | 0.7505241090146751 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC81KPR2VHfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad6ff538-6c84-4f70-a5f7-d767c7628988"
      },
      "source": [
        "Result_cross.to_csv('FCX_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.570866</td>\n",
              "      <td>0.681633</td>\n",
              "      <td>0.650224</td>\n",
              "      <td>0.755208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.510386</td>\n",
              "      <td>0.622449</td>\n",
              "      <td>0.650284</td>\n",
              "      <td>0.895833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.533113</td>\n",
              "      <td>0.648980</td>\n",
              "      <td>0.651822</td>\n",
              "      <td>0.838542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.655102</td>\n",
              "      <td>0.655804</td>\n",
              "      <td>0.838542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.663265</td>\n",
              "      <td>0.662577</td>\n",
              "      <td>0.843750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.508834</td>\n",
              "      <td>0.619256</td>\n",
              "      <td>0.623377</td>\n",
              "      <td>0.804469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.484099</td>\n",
              "      <td>0.588621</td>\n",
              "      <td>0.593074</td>\n",
              "      <td>0.765363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.479452</td>\n",
              "      <td>0.582057</td>\n",
              "      <td>0.594480</td>\n",
              "      <td>0.782123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.510345</td>\n",
              "      <td>0.621444</td>\n",
              "      <td>0.631130</td>\n",
              "      <td>0.826816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.521127</td>\n",
              "      <td>0.634573</td>\n",
              "      <td>0.639309</td>\n",
              "      <td>0.826816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.547826</td>\n",
              "      <td>0.567347</td>\n",
              "      <td>0.703911</td>\n",
              "      <td>0.984375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.574742</td>\n",
              "      <td>0.595918</td>\n",
              "      <td>0.692547</td>\n",
              "      <td>0.871094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.568579</td>\n",
              "      <td>0.589796</td>\n",
              "      <td>0.694064</td>\n",
              "      <td>0.890625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.573604</td>\n",
              "      <td>0.595918</td>\n",
              "      <td>0.695385</td>\n",
              "      <td>0.882812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.546256</td>\n",
              "      <td>0.563265</td>\n",
              "      <td>0.698592</td>\n",
              "      <td>0.968750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.736220</td>\n",
              "      <td>0.730853</td>\n",
              "      <td>0.752515</td>\n",
              "      <td>0.769547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.612022</td>\n",
              "      <td>0.647702</td>\n",
              "      <td>0.735632</td>\n",
              "      <td>0.921811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.532663</td>\n",
              "      <td>0.525164</td>\n",
              "      <td>0.661466</td>\n",
              "      <td>0.872428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.607735</td>\n",
              "      <td>0.638950</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.905350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.627168</td>\n",
              "      <td>0.660832</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.893004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.789474</td>\n",
              "      <td>0.634694</td>\n",
              "      <td>0.539846</td>\n",
              "      <td>0.410156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.601173</td>\n",
              "      <td>0.618367</td>\n",
              "      <td>0.686767</td>\n",
              "      <td>0.800781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.636735</td>\n",
              "      <td>0.655039</td>\n",
              "      <td>0.660156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.606707</td>\n",
              "      <td>0.620408</td>\n",
              "      <td>0.681507</td>\n",
              "      <td>0.777344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.664234</td>\n",
              "      <td>0.661224</td>\n",
              "      <td>0.686792</td>\n",
              "      <td>0.710938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.692593</td>\n",
              "      <td>0.695842</td>\n",
              "      <td>0.729045</td>\n",
              "      <td>0.769547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.726190</td>\n",
              "      <td>0.717724</td>\n",
              "      <td>0.739394</td>\n",
              "      <td>0.753086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.600694</td>\n",
              "      <td>0.595186</td>\n",
              "      <td>0.651601</td>\n",
              "      <td>0.711934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.662338</td>\n",
              "      <td>0.687090</td>\n",
              "      <td>0.740472</td>\n",
              "      <td>0.839506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.764957</td>\n",
              "      <td>0.739606</td>\n",
              "      <td>0.750524</td>\n",
              "      <td>0.736626</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  FCX  0.570866  0.681633  0.650224  0.755208\n",
              "1            GRU 0.1  FCX  0.510386  0.622449  0.650284  0.895833\n",
              "2        XGBoost 0.1  FCX  0.533113  0.648980  0.651822  0.838542\n",
              "3         Logreg 0.1  FCX  0.538462  0.655102  0.655804  0.838542\n",
              "4            SVM 0.1  FCX  0.545455  0.663265  0.662577  0.843750\n",
              "5      LSTM beta 0.1  FCX  0.508834  0.619256  0.623377  0.804469\n",
              "6       GRU beta 0.1  FCX  0.484099  0.588621  0.593074  0.765363\n",
              "7   XGBoost beta 0.1  FCX  0.479452  0.582057  0.594480  0.782123\n",
              "8    logreg beta 0.1  FCX  0.510345  0.621444  0.631130  0.826816\n",
              "9       svm beta 0.1  FCX  0.521127  0.634573  0.639309  0.826816\n",
              "0           LSTM 0.2  FCX  0.547826  0.567347  0.703911  0.984375\n",
              "1            GRU 0.2  FCX  0.574742  0.595918  0.692547  0.871094\n",
              "2        XGBoost 0.2  FCX  0.568579  0.589796  0.694064  0.890625\n",
              "3         Logreg 0.2  FCX  0.573604  0.595918  0.695385  0.882812\n",
              "4            SVM 0.2  FCX  0.546256  0.563265  0.698592  0.968750\n",
              "5      LSTM beta 0.2  FCX  0.736220  0.730853  0.752515  0.769547\n",
              "6       GRU beta 0.2  FCX  0.612022  0.647702  0.735632  0.921811\n",
              "7   XGBoost beta 0.2  FCX  0.532663  0.525164  0.661466  0.872428\n",
              "8    logreg beta 0.2  FCX  0.607735  0.638950  0.727273  0.905350\n",
              "9       svm beta 0.2  FCX  0.627168  0.660832  0.736842  0.893004\n",
              "0          LSTM 0.15  FCX  0.789474  0.634694  0.539846  0.410156\n",
              "1           GRU 0.15  FCX  0.601173  0.618367  0.686767  0.800781\n",
              "2       XGBoost 0.15  FCX  0.650000  0.636735  0.655039  0.660156\n",
              "3        Logreg 0.15  FCX  0.606707  0.620408  0.681507  0.777344\n",
              "4           SVM 0.15  FCX  0.664234  0.661224  0.686792  0.710938\n",
              "5     LSTM beta 0.15  FCX  0.692593  0.695842  0.729045  0.769547\n",
              "6      GRU beta 0.15  FCX  0.726190  0.717724  0.739394  0.753086\n",
              "7  XGBoost beta 0.15  FCX  0.600694  0.595186  0.651601  0.711934\n",
              "8   logreg beta 0.15  FCX  0.662338  0.687090  0.740472  0.839506\n",
              "9      svm beta 0.15  FCX  0.764957  0.739606  0.750524  0.736626"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orbR1f_1VHfZ"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjZnF0AjVHfZ"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF7y8DU6VHfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea474e86-8917-45e2-b48d-bd485f72d25e"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"FCX\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6878 - accuracy: 0.5483 - val_loss: 0.7010 - val_accuracy: 0.3918\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6269 - accuracy: 0.6718 - val_loss: 0.5939 - val_accuracy: 0.6633\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5853 - accuracy: 0.7074 - val_loss: 0.5955 - val_accuracy: 0.6592\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5343 - accuracy: 0.7403 - val_loss: 0.5591 - val_accuracy: 0.7163\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5134 - accuracy: 0.7557 - val_loss: 0.6455 - val_accuracy: 0.6224\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6332 - accuracy: 0.6430 - val_loss: 0.6509 - val_accuracy: 0.5776\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5276 - accuracy: 0.7342 - val_loss: 0.6214 - val_accuracy: 0.6367\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5149 - accuracy: 0.7537 - val_loss: 0.5878 - val_accuracy: 0.6633\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5120 - accuracy: 0.7584 - val_loss: 0.6281 - val_accuracy: 0.6306\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5163 - accuracy: 0.7409 - val_loss: 0.6397 - val_accuracy: 0.6163\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.740754\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.744835\n",
            "[2]\tvalidation_0-auc:0.754046\n",
            "[3]\tvalidation_0-auc:0.755628\n",
            "[4]\tvalidation_0-auc:0.760312\n",
            "[5]\tvalidation_0-auc:0.760696\n",
            "[6]\tvalidation_0-auc:0.773079\n",
            "[7]\tvalidation_0-auc:0.760976\n",
            "[8]\tvalidation_0-auc:0.759586\n",
            "[9]\tvalidation_0-auc:0.759351\n",
            "[10]\tvalidation_0-auc:0.758381\n",
            "[11]\tvalidation_0-auc:0.757507\n",
            "[12]\tvalidation_0-auc:0.758704\n",
            "[13]\tvalidation_0-auc:0.758616\n",
            "[14]\tvalidation_0-auc:0.757987\n",
            "[15]\tvalidation_0-auc:0.758879\n",
            "[16]\tvalidation_0-auc:0.758354\n",
            "[17]\tvalidation_0-auc:0.757856\n",
            "[18]\tvalidation_0-auc:0.757279\n",
            "[19]\tvalidation_0-auc:0.757341\n",
            "[20]\tvalidation_0-auc:0.75298\n",
            "[21]\tvalidation_0-auc:0.753067\n",
            "[22]\tvalidation_0-auc:0.753015\n",
            "[23]\tvalidation_0-auc:0.752945\n",
            "[24]\tvalidation_0-auc:0.757795\n",
            "[25]\tvalidation_0-auc:0.758389\n",
            "[26]\tvalidation_0-auc:0.75818\n",
            "[27]\tvalidation_0-auc:0.756502\n",
            "[28]\tvalidation_0-auc:0.762094\n",
            "[29]\tvalidation_0-auc:0.762872\n",
            "[30]\tvalidation_0-auc:0.7624\n",
            "[31]\tvalidation_0-auc:0.762453\n",
            "[32]\tvalidation_0-auc:0.762418\n",
            "[33]\tvalidation_0-auc:0.762732\n",
            "[34]\tvalidation_0-auc:0.76275\n",
            "[35]\tvalidation_0-auc:0.759683\n",
            "[36]\tvalidation_0-auc:0.759683\n",
            "[37]\tvalidation_0-auc:0.759683\n",
            "[38]\tvalidation_0-auc:0.761535\n",
            "[39]\tvalidation_0-auc:0.762619\n",
            "[40]\tvalidation_0-auc:0.763493\n",
            "[41]\tvalidation_0-auc:0.763528\n",
            "[42]\tvalidation_0-auc:0.763528\n",
            "[43]\tvalidation_0-auc:0.763702\n",
            "[44]\tvalidation_0-auc:0.76386\n",
            "[45]\tvalidation_0-auc:0.763982\n",
            "[46]\tvalidation_0-auc:0.765066\n",
            "[47]\tvalidation_0-auc:0.766656\n",
            "[48]\tvalidation_0-auc:0.767207\n",
            "[49]\tvalidation_0-auc:0.767259\n",
            "[50]\tvalidation_0-auc:0.768483\n",
            "[51]\tvalidation_0-auc:0.768133\n",
            "[52]\tvalidation_0-auc:0.767329\n",
            "[53]\tvalidation_0-auc:0.767941\n",
            "[54]\tvalidation_0-auc:0.768561\n",
            "[55]\tvalidation_0-auc:0.768762\n",
            "[56]\tvalidation_0-auc:0.768587\n",
            "Stopping. Best iteration:\n",
            "[6]\tvalidation_0-auc:0.773079\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6875 - accuracy: 0.5532 - val_loss: 0.7315 - val_accuracy: 0.3917\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6712 - accuracy: 0.5752 - val_loss: 0.7153 - val_accuracy: 0.3917\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6076 - accuracy: 0.6671 - val_loss: 0.6864 - val_accuracy: 0.5164\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5621 - accuracy: 0.7351 - val_loss: 0.6415 - val_accuracy: 0.6018\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5517 - accuracy: 0.7303 - val_loss: 0.6158 - val_accuracy: 0.6346\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6729 - accuracy: 0.5820 - val_loss: 0.6617 - val_accuracy: 0.5908\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5806 - accuracy: 0.6994 - val_loss: 0.6711 - val_accuracy: 0.5449\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5457 - accuracy: 0.7268 - val_loss: 0.6459 - val_accuracy: 0.5996\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5256 - accuracy: 0.7351 - val_loss: 0.6666 - val_accuracy: 0.5864\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5342 - accuracy: 0.7433 - val_loss: 0.6244 - val_accuracy: 0.6214\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.628602\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.689572\n",
            "[2]\tvalidation_0-auc:0.694355\n",
            "[3]\tvalidation_0-auc:0.685463\n",
            "[4]\tvalidation_0-auc:0.678409\n",
            "[5]\tvalidation_0-auc:0.707879\n",
            "[6]\tvalidation_0-auc:0.70589\n",
            "[7]\tvalidation_0-auc:0.708412\n",
            "[8]\tvalidation_0-auc:0.695742\n",
            "[9]\tvalidation_0-auc:0.695089\n",
            "[10]\tvalidation_0-auc:0.702162\n",
            "[11]\tvalidation_0-auc:0.706865\n",
            "[12]\tvalidation_0-auc:0.713155\n",
            "[13]\tvalidation_0-auc:0.720208\n",
            "[14]\tvalidation_0-auc:0.719012\n",
            "[15]\tvalidation_0-auc:0.722941\n",
            "[16]\tvalidation_0-auc:0.729512\n",
            "[17]\tvalidation_0-auc:0.72682\n",
            "[18]\tvalidation_0-auc:0.731623\n",
            "[19]\tvalidation_0-auc:0.727865\n",
            "[20]\tvalidation_0-auc:0.73323\n",
            "[21]\tvalidation_0-auc:0.717817\n",
            "[22]\tvalidation_0-auc:0.720047\n",
            "[23]\tvalidation_0-auc:0.724288\n",
            "[24]\tvalidation_0-auc:0.724288\n",
            "[25]\tvalidation_0-auc:0.725363\n",
            "[26]\tvalidation_0-auc:0.728859\n",
            "[27]\tvalidation_0-auc:0.728739\n",
            "[28]\tvalidation_0-auc:0.731673\n",
            "[29]\tvalidation_0-auc:0.72279\n",
            "[30]\tvalidation_0-auc:0.713135\n",
            "[31]\tvalidation_0-auc:0.70809\n",
            "[32]\tvalidation_0-auc:0.712813\n",
            "[33]\tvalidation_0-auc:0.712662\n",
            "[34]\tvalidation_0-auc:0.712662\n",
            "[35]\tvalidation_0-auc:0.712783\n",
            "[36]\tvalidation_0-auc:0.717787\n",
            "[37]\tvalidation_0-auc:0.725483\n",
            "[38]\tvalidation_0-auc:0.72474\n",
            "[39]\tvalidation_0-auc:0.721565\n",
            "[40]\tvalidation_0-auc:0.721565\n",
            "[41]\tvalidation_0-auc:0.721565\n",
            "[42]\tvalidation_0-auc:0.721565\n",
            "[43]\tvalidation_0-auc:0.717375\n",
            "[44]\tvalidation_0-auc:0.715596\n",
            "[45]\tvalidation_0-auc:0.714833\n",
            "[46]\tvalidation_0-auc:0.714139\n",
            "[47]\tvalidation_0-auc:0.713094\n",
            "[48]\tvalidation_0-auc:0.713938\n",
            "[49]\tvalidation_0-auc:0.713938\n",
            "[50]\tvalidation_0-auc:0.714461\n",
            "[51]\tvalidation_0-auc:0.714461\n",
            "[52]\tvalidation_0-auc:0.714461\n",
            "[53]\tvalidation_0-auc:0.710201\n",
            "[54]\tvalidation_0-auc:0.710201\n",
            "[55]\tvalidation_0-auc:0.708894\n",
            "[56]\tvalidation_0-auc:0.70385\n",
            "[57]\tvalidation_0-auc:0.698987\n",
            "[58]\tvalidation_0-auc:0.696144\n",
            "[59]\tvalidation_0-auc:0.696114\n",
            "[60]\tvalidation_0-auc:0.694184\n",
            "[61]\tvalidation_0-auc:0.693823\n",
            "[62]\tvalidation_0-auc:0.693823\n",
            "[63]\tvalidation_0-auc:0.693823\n",
            "[64]\tvalidation_0-auc:0.693762\n",
            "[65]\tvalidation_0-auc:0.699168\n",
            "[66]\tvalidation_0-auc:0.702223\n",
            "[67]\tvalidation_0-auc:0.702002\n",
            "[68]\tvalidation_0-auc:0.693501\n",
            "[69]\tvalidation_0-auc:0.694375\n",
            "[70]\tvalidation_0-auc:0.691723\n",
            "Stopping. Best iteration:\n",
            "[20]\tvalidation_0-auc:0.73323\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.6224489795918368 | 0.5106382978723404 |       0.875        | 0.6449136276391555 |\n",
            "|     GRU 0.1      | 0.6163265306122448 | 0.5059523809523809 | 0.8854166666666666 | 0.6439393939393939 |\n",
            "|   XGBoost 0.1    | 0.6489795918367347 | 0.5331125827814569 | 0.8385416666666666 | 0.6518218623481782 |\n",
            "|    Logreg 0.1    | 0.6551020408163265 | 0.5384615384615384 | 0.8385416666666666 | 0.6558044806517311 |\n",
            "|     SVM 0.1      | 0.6632653061224489 | 0.5454545454545454 |      0.84375       | 0.6625766871165644 |\n",
            "|  LSTM beta 0.1   | 0.6345733041575492 | 0.5288461538461539 | 0.6145251396648045 | 0.5684754521963824 |\n",
            "|   GRU beta 0.1   | 0.6214442013129103 | 0.5126050420168067 | 0.6815642458100558 | 0.5851318944844125 |\n",
            "| XGBoost beta 0.1 | 0.5820568927789934 | 0.4794520547945205 | 0.7821229050279329 | 0.5944798301486199 |\n",
            "| logreg beta 0.1  | 0.6214442013129103 | 0.5103448275862069 | 0.8268156424581006 | 0.6311300639658849 |\n",
            "|   svm beta 0.1   | 0.6345733041575492 | 0.5211267605633803 | 0.8268156424581006 | 0.6393088552915767 |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6560 - accuracy: 0.6329 - val_loss: 0.6419 - val_accuracy: 0.6143\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5908 - accuracy: 0.6772 - val_loss: 0.6453 - val_accuracy: 0.5980\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5772 - accuracy: 0.6919 - val_loss: 0.6397 - val_accuracy: 0.6204\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5601 - accuracy: 0.7034 - val_loss: 0.6408 - val_accuracy: 0.5980\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.5561 - accuracy: 0.7087 - val_loss: 0.6523 - val_accuracy: 0.5878\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6146 - accuracy: 0.6584 - val_loss: 0.6328 - val_accuracy: 0.6245\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5485 - accuracy: 0.7228 - val_loss: 0.6387 - val_accuracy: 0.5980\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5317 - accuracy: 0.7336 - val_loss: 0.6624 - val_accuracy: 0.5776\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5356 - accuracy: 0.7282 - val_loss: 0.6965 - val_accuracy: 0.5694\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5290 - accuracy: 0.7262 - val_loss: 0.6682 - val_accuracy: 0.5796\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.659647\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.687099\n",
            "[2]\tvalidation_0-auc:0.685121\n",
            "[3]\tvalidation_0-auc:0.687008\n",
            "[4]\tvalidation_0-auc:0.704519\n",
            "[5]\tvalidation_0-auc:0.701973\n",
            "[6]\tvalidation_0-auc:0.702432\n",
            "[7]\tvalidation_0-auc:0.704335\n",
            "[8]\tvalidation_0-auc:0.704836\n",
            "[9]\tvalidation_0-auc:0.708459\n",
            "[10]\tvalidation_0-auc:0.708225\n",
            "[11]\tvalidation_0-auc:0.708208\n",
            "[12]\tvalidation_0-auc:0.708392\n",
            "[13]\tvalidation_0-auc:0.708225\n",
            "[14]\tvalidation_0-auc:0.709076\n",
            "[15]\tvalidation_0-auc:0.709293\n",
            "[16]\tvalidation_0-auc:0.709143\n",
            "[17]\tvalidation_0-auc:0.709435\n",
            "[18]\tvalidation_0-auc:0.709435\n",
            "[19]\tvalidation_0-auc:0.711956\n",
            "[20]\tvalidation_0-auc:0.712006\n",
            "[21]\tvalidation_0-auc:0.713358\n",
            "[22]\tvalidation_0-auc:0.710387\n",
            "[23]\tvalidation_0-auc:0.709969\n",
            "[24]\tvalidation_0-auc:0.707791\n",
            "[25]\tvalidation_0-auc:0.703451\n",
            "[26]\tvalidation_0-auc:0.705721\n",
            "[27]\tvalidation_0-auc:0.703968\n",
            "[28]\tvalidation_0-auc:0.704093\n",
            "[29]\tvalidation_0-auc:0.705629\n",
            "[30]\tvalidation_0-auc:0.705746\n",
            "[31]\tvalidation_0-auc:0.705746\n",
            "[32]\tvalidation_0-auc:0.705946\n",
            "[33]\tvalidation_0-auc:0.705946\n",
            "[34]\tvalidation_0-auc:0.700212\n",
            "[35]\tvalidation_0-auc:0.700846\n",
            "[36]\tvalidation_0-auc:0.70098\n",
            "[37]\tvalidation_0-auc:0.698317\n",
            "[38]\tvalidation_0-auc:0.699027\n",
            "[39]\tvalidation_0-auc:0.699786\n",
            "[40]\tvalidation_0-auc:0.699786\n",
            "[41]\tvalidation_0-auc:0.699786\n",
            "[42]\tvalidation_0-auc:0.698868\n",
            "[43]\tvalidation_0-auc:0.696898\n",
            "[44]\tvalidation_0-auc:0.695179\n",
            "[45]\tvalidation_0-auc:0.693343\n",
            "[46]\tvalidation_0-auc:0.693159\n",
            "[47]\tvalidation_0-auc:0.693226\n",
            "[48]\tvalidation_0-auc:0.693226\n",
            "[49]\tvalidation_0-auc:0.692717\n",
            "[50]\tvalidation_0-auc:0.693326\n",
            "[51]\tvalidation_0-auc:0.693326\n",
            "[52]\tvalidation_0-auc:0.691815\n",
            "[53]\tvalidation_0-auc:0.691206\n",
            "[54]\tvalidation_0-auc:0.68982\n",
            "[55]\tvalidation_0-auc:0.689002\n",
            "[56]\tvalidation_0-auc:0.688902\n",
            "[57]\tvalidation_0-auc:0.688744\n",
            "[58]\tvalidation_0-auc:0.688727\n",
            "[59]\tvalidation_0-auc:0.68881\n",
            "[60]\tvalidation_0-auc:0.687225\n",
            "[61]\tvalidation_0-auc:0.686749\n",
            "[62]\tvalidation_0-auc:0.686832\n",
            "[63]\tvalidation_0-auc:0.688318\n",
            "[64]\tvalidation_0-auc:0.687867\n",
            "[65]\tvalidation_0-auc:0.686574\n",
            "[66]\tvalidation_0-auc:0.684829\n",
            "[67]\tvalidation_0-auc:0.684529\n",
            "[68]\tvalidation_0-auc:0.684529\n",
            "[69]\tvalidation_0-auc:0.683844\n",
            "[70]\tvalidation_0-auc:0.683327\n",
            "[71]\tvalidation_0-auc:0.683293\n",
            "Stopping. Best iteration:\n",
            "[21]\tvalidation_0-auc:0.713358\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6485 - accuracy: 0.6356 - val_loss: 0.7622 - val_accuracy: 0.5317\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6375 - accuracy: 0.6417 - val_loss: 0.6376 - val_accuracy: 0.5317\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.5916 - accuracy: 0.6809 - val_loss: 0.6086 - val_accuracy: 0.7571\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5666 - accuracy: 0.7186 - val_loss: 0.5885 - val_accuracy: 0.6980\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5236 - accuracy: 0.7474 - val_loss: 0.5826 - val_accuracy: 0.6477\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6047 - accuracy: 0.6733 - val_loss: 0.6377 - val_accuracy: 0.5558\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5147 - accuracy: 0.7447 - val_loss: 0.5661 - val_accuracy: 0.7177\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5142 - accuracy: 0.7467 - val_loss: 0.5565 - val_accuracy: 0.7046\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5081 - accuracy: 0.7461 - val_loss: 0.5962 - val_accuracy: 0.6018\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5072 - accuracy: 0.7426 - val_loss: 0.6091 - val_accuracy: 0.5952\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.768663\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.688724\n",
            "[2]\tvalidation_0-auc:0.692387\n",
            "[3]\tvalidation_0-auc:0.729097\n",
            "[4]\tvalidation_0-auc:0.705069\n",
            "[5]\tvalidation_0-auc:0.700252\n",
            "[6]\tvalidation_0-auc:0.70804\n",
            "[7]\tvalidation_0-auc:0.690118\n",
            "[8]\tvalidation_0-auc:0.697367\n",
            "[9]\tvalidation_0-auc:0.692435\n",
            "[10]\tvalidation_0-auc:0.674762\n",
            "[11]\tvalidation_0-auc:0.689002\n",
            "[12]\tvalidation_0-auc:0.688541\n",
            "[13]\tvalidation_0-auc:0.67458\n",
            "[14]\tvalidation_0-auc:0.669599\n",
            "[15]\tvalidation_0-auc:0.679743\n",
            "[16]\tvalidation_0-auc:0.692993\n",
            "[17]\tvalidation_0-auc:0.691185\n",
            "[18]\tvalidation_0-auc:0.690916\n",
            "[19]\tvalidation_0-auc:0.687858\n",
            "[20]\tvalidation_0-auc:0.685079\n",
            "[21]\tvalidation_0-auc:0.688897\n",
            "[22]\tvalidation_0-auc:0.68955\n",
            "[23]\tvalidation_0-auc:0.69032\n",
            "[24]\tvalidation_0-auc:0.692589\n",
            "[25]\tvalidation_0-auc:0.692666\n",
            "[26]\tvalidation_0-auc:0.694262\n",
            "[27]\tvalidation_0-auc:0.694627\n",
            "[28]\tvalidation_0-auc:0.689973\n",
            "[29]\tvalidation_0-auc:0.690166\n",
            "[30]\tvalidation_0-auc:0.686022\n",
            "[31]\tvalidation_0-auc:0.68606\n",
            "[32]\tvalidation_0-auc:0.687925\n",
            "[33]\tvalidation_0-auc:0.685118\n",
            "[34]\tvalidation_0-auc:0.685983\n",
            "[35]\tvalidation_0-auc:0.686349\n",
            "[36]\tvalidation_0-auc:0.683185\n",
            "[37]\tvalidation_0-auc:0.680512\n",
            "[38]\tvalidation_0-auc:0.680512\n",
            "[39]\tvalidation_0-auc:0.680512\n",
            "[40]\tvalidation_0-auc:0.681743\n",
            "[41]\tvalidation_0-auc:0.682483\n",
            "[42]\tvalidation_0-auc:0.683964\n",
            "[43]\tvalidation_0-auc:0.685868\n",
            "[44]\tvalidation_0-auc:0.686022\n",
            "[45]\tvalidation_0-auc:0.686445\n",
            "[46]\tvalidation_0-auc:0.681676\n",
            "[47]\tvalidation_0-auc:0.681753\n",
            "[48]\tvalidation_0-auc:0.678791\n",
            "[49]\tvalidation_0-auc:0.67633\n",
            "[50]\tvalidation_0-auc:0.677099\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.768663\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.2     | 0.5877551020408164 | 0.5678391959798995 |     0.8828125      | 0.691131498470948  |\n",
            "|     GRU 0.2      | 0.5795918367346938 | 0.5595238095238095 |     0.91796875     | 0.6952662721893491 |\n",
            "|   XGBoost 0.2    | 0.5897959183673469 | 0.5685785536159601 |      0.890625      | 0.6940639269406392 |\n",
            "|    Logreg 0.2    | 0.5959183673469388 | 0.5736040609137056 |     0.8828125      | 0.6953846153846155 |\n",
            "|     SVM 0.2      | 0.563265306122449  | 0.5462555066079295 |      0.96875       | 0.6985915492957746 |\n",
            "|  LSTM beta 0.2   | 0.6477024070021882 | 0.6164772727272727 | 0.8930041152263375 | 0.7294117647058823 |\n",
            "|   GRU beta 0.2   | 0.5951859956236324 |       0.5725       | 0.9423868312757202 | 0.7122861586314152 |\n",
            "| XGBoost beta 0.2 | 0.5251641137855579 | 0.5326633165829145 | 0.8724279835390947 | 0.6614664586583464 |\n",
            "| logreg beta 0.2  | 0.6389496717724289 | 0.6077348066298343 | 0.9053497942386831 | 0.7272727272727274 |\n",
            "|   svm beta 0.2   | 0.6608315098468271 | 0.6271676300578035 | 0.8930041152263375 | 0.736842105263158  |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 19ms/step - loss: 0.6502 - accuracy: 0.5980 - val_loss: 0.6750 - val_accuracy: 0.5592\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5993 - accuracy: 0.6812 - val_loss: 0.6337 - val_accuracy: 0.6286\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.5849 - accuracy: 0.6960 - val_loss: 0.6259 - val_accuracy: 0.6204\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5750 - accuracy: 0.7087 - val_loss: 0.6278 - val_accuracy: 0.6122\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.5555 - accuracy: 0.7154 - val_loss: 0.6296 - val_accuracy: 0.6347\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6373 - accuracy: 0.6356 - val_loss: 0.6705 - val_accuracy: 0.5735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5577 - accuracy: 0.7248 - val_loss: 0.6306 - val_accuracy: 0.6122\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5430 - accuracy: 0.7456 - val_loss: 0.6444 - val_accuracy: 0.6041\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5317 - accuracy: 0.7262 - val_loss: 0.6205 - val_accuracy: 0.6347\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5359 - accuracy: 0.7349 - val_loss: 0.6297 - val_accuracy: 0.6102\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.701022\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.699845\n",
            "[2]\tvalidation_0-auc:0.697675\n",
            "[3]\tvalidation_0-auc:0.696064\n",
            "[4]\tvalidation_0-auc:0.698242\n",
            "[5]\tvalidation_0-auc:0.699436\n",
            "[6]\tvalidation_0-auc:0.699018\n",
            "[7]\tvalidation_0-auc:0.70007\n",
            "[8]\tvalidation_0-auc:0.701898\n",
            "[9]\tvalidation_0-auc:0.704285\n",
            "[10]\tvalidation_0-auc:0.705596\n",
            "[11]\tvalidation_0-auc:0.704744\n",
            "[12]\tvalidation_0-auc:0.70391\n",
            "[13]\tvalidation_0-auc:0.705879\n",
            "[14]\tvalidation_0-auc:0.707098\n",
            "[15]\tvalidation_0-auc:0.708016\n",
            "[16]\tvalidation_0-auc:0.707924\n",
            "[17]\tvalidation_0-auc:0.708191\n",
            "[18]\tvalidation_0-auc:0.708242\n",
            "[19]\tvalidation_0-auc:0.708191\n",
            "[20]\tvalidation_0-auc:0.7083\n",
            "[21]\tvalidation_0-auc:0.708784\n",
            "[22]\tvalidation_0-auc:0.710395\n",
            "[23]\tvalidation_0-auc:0.710579\n",
            "[24]\tvalidation_0-auc:0.70946\n",
            "[25]\tvalidation_0-auc:0.711013\n",
            "[26]\tvalidation_0-auc:0.711146\n",
            "[27]\tvalidation_0-auc:0.710103\n",
            "[28]\tvalidation_0-auc:0.707933\n",
            "[29]\tvalidation_0-auc:0.708066\n",
            "[30]\tvalidation_0-auc:0.707382\n",
            "[31]\tvalidation_0-auc:0.706923\n",
            "[32]\tvalidation_0-auc:0.706956\n",
            "[33]\tvalidation_0-auc:0.707357\n",
            "[34]\tvalidation_0-auc:0.707732\n",
            "[35]\tvalidation_0-auc:0.708434\n",
            "[36]\tvalidation_0-auc:0.708233\n",
            "[37]\tvalidation_0-auc:0.707816\n",
            "[38]\tvalidation_0-auc:0.708233\n",
            "[39]\tvalidation_0-auc:0.708492\n",
            "[40]\tvalidation_0-auc:0.708459\n",
            "[41]\tvalidation_0-auc:0.708893\n",
            "[42]\tvalidation_0-auc:0.708592\n",
            "[43]\tvalidation_0-auc:0.708792\n",
            "[44]\tvalidation_0-auc:0.708809\n",
            "[45]\tvalidation_0-auc:0.708692\n",
            "[46]\tvalidation_0-auc:0.709302\n",
            "[47]\tvalidation_0-auc:0.709302\n",
            "[48]\tvalidation_0-auc:0.709118\n",
            "[49]\tvalidation_0-auc:0.708976\n",
            "[50]\tvalidation_0-auc:0.708976\n",
            "[51]\tvalidation_0-auc:0.709193\n",
            "[52]\tvalidation_0-auc:0.708692\n",
            "[53]\tvalidation_0-auc:0.708759\n",
            "[54]\tvalidation_0-auc:0.708292\n",
            "[55]\tvalidation_0-auc:0.708292\n",
            "[56]\tvalidation_0-auc:0.708392\n",
            "[57]\tvalidation_0-auc:0.708408\n",
            "[58]\tvalidation_0-auc:0.708325\n",
            "[59]\tvalidation_0-auc:0.708692\n",
            "[60]\tvalidation_0-auc:0.708726\n",
            "[61]\tvalidation_0-auc:0.708709\n",
            "[62]\tvalidation_0-auc:0.708258\n",
            "[63]\tvalidation_0-auc:0.707991\n",
            "[64]\tvalidation_0-auc:0.707874\n",
            "[65]\tvalidation_0-auc:0.707774\n",
            "[66]\tvalidation_0-auc:0.707565\n",
            "[67]\tvalidation_0-auc:0.709619\n",
            "[68]\tvalidation_0-auc:0.709569\n",
            "[69]\tvalidation_0-auc:0.709602\n",
            "[70]\tvalidation_0-auc:0.709059\n",
            "[71]\tvalidation_0-auc:0.709059\n",
            "[72]\tvalidation_0-auc:0.709059\n",
            "[73]\tvalidation_0-auc:0.70911\n",
            "[74]\tvalidation_0-auc:0.709076\n",
            "[75]\tvalidation_0-auc:0.708926\n",
            "[76]\tvalidation_0-auc:0.708976\n",
            "Stopping. Best iteration:\n",
            "[26]\tvalidation_0-auc:0.711146\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6686 - accuracy: 0.5889 - val_loss: 0.6660 - val_accuracy: 0.5317\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6329 - accuracy: 0.6259 - val_loss: 0.6475 - val_accuracy: 0.5405\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5844 - accuracy: 0.6987 - val_loss: 0.5659 - val_accuracy: 0.7330\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5561 - accuracy: 0.7090 - val_loss: 0.6044 - val_accuracy: 0.6433\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5417 - accuracy: 0.7344 - val_loss: 0.5922 - val_accuracy: 0.6740\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6238 - accuracy: 0.6404 - val_loss: 0.5753 - val_accuracy: 0.7418\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5473 - accuracy: 0.7303 - val_loss: 0.5869 - val_accuracy: 0.6433\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5287 - accuracy: 0.7330 - val_loss: 0.5681 - val_accuracy: 0.7484\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5209 - accuracy: 0.7358 - val_loss: 0.5631 - val_accuracy: 0.6958\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5284 - accuracy: 0.7392 - val_loss: 0.5664 - val_accuracy: 0.6827\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.65334\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.666263\n",
            "[2]\tvalidation_0-auc:0.674157\n",
            "[3]\tvalidation_0-auc:0.679532\n",
            "[4]\tvalidation_0-auc:0.68182\n",
            "[5]\tvalidation_0-auc:0.67582\n",
            "[6]\tvalidation_0-auc:0.667263\n",
            "[7]\tvalidation_0-auc:0.671859\n",
            "[8]\tvalidation_0-auc:0.670359\n",
            "[9]\tvalidation_0-auc:0.677493\n",
            "[10]\tvalidation_0-auc:0.665542\n",
            "[11]\tvalidation_0-auc:0.669388\n",
            "[12]\tvalidation_0-auc:0.669369\n",
            "[13]\tvalidation_0-auc:0.668917\n",
            "[14]\tvalidation_0-auc:0.676301\n",
            "[15]\tvalidation_0-auc:0.676301\n",
            "[16]\tvalidation_0-auc:0.67857\n",
            "[17]\tvalidation_0-auc:0.669493\n",
            "[18]\tvalidation_0-auc:0.670301\n",
            "[19]\tvalidation_0-auc:0.669926\n",
            "[20]\tvalidation_0-auc:0.671801\n",
            "[21]\tvalidation_0-auc:0.675166\n",
            "[22]\tvalidation_0-auc:0.671397\n",
            "[23]\tvalidation_0-auc:0.676628\n",
            "[24]\tvalidation_0-auc:0.677945\n",
            "[25]\tvalidation_0-auc:0.680339\n",
            "[26]\tvalidation_0-auc:0.680339\n",
            "[27]\tvalidation_0-auc:0.681416\n",
            "[28]\tvalidation_0-auc:0.681416\n",
            "[29]\tvalidation_0-auc:0.682128\n",
            "[30]\tvalidation_0-auc:0.679262\n",
            "[31]\tvalidation_0-auc:0.678878\n",
            "[32]\tvalidation_0-auc:0.675551\n",
            "[33]\tvalidation_0-auc:0.676166\n",
            "[34]\tvalidation_0-auc:0.676166\n",
            "[35]\tvalidation_0-auc:0.676109\n",
            "[36]\tvalidation_0-auc:0.682445\n",
            "[37]\tvalidation_0-auc:0.679195\n",
            "[38]\tvalidation_0-auc:0.676359\n",
            "[39]\tvalidation_0-auc:0.673436\n",
            "[40]\tvalidation_0-auc:0.670138\n",
            "[41]\tvalidation_0-auc:0.670138\n",
            "[42]\tvalidation_0-auc:0.670138\n",
            "[43]\tvalidation_0-auc:0.670138\n",
            "[44]\tvalidation_0-auc:0.672945\n",
            "[45]\tvalidation_0-auc:0.672945\n",
            "[46]\tvalidation_0-auc:0.67558\n",
            "[47]\tvalidation_0-auc:0.672205\n",
            "[48]\tvalidation_0-auc:0.673705\n",
            "[49]\tvalidation_0-auc:0.675282\n",
            "[50]\tvalidation_0-auc:0.675282\n",
            "[51]\tvalidation_0-auc:0.675282\n",
            "[52]\tvalidation_0-auc:0.674628\n",
            "[53]\tvalidation_0-auc:0.669763\n",
            "[54]\tvalidation_0-auc:0.671484\n",
            "[55]\tvalidation_0-auc:0.670916\n",
            "[56]\tvalidation_0-auc:0.669205\n",
            "[57]\tvalidation_0-auc:0.670282\n",
            "[58]\tvalidation_0-auc:0.671474\n",
            "[59]\tvalidation_0-auc:0.671416\n",
            "[60]\tvalidation_0-auc:0.672705\n",
            "[61]\tvalidation_0-auc:0.672743\n",
            "[62]\tvalidation_0-auc:0.674388\n",
            "[63]\tvalidation_0-auc:0.674407\n",
            "[64]\tvalidation_0-auc:0.672445\n",
            "[65]\tvalidation_0-auc:0.672272\n",
            "[66]\tvalidation_0-auc:0.669244\n",
            "[67]\tvalidation_0-auc:0.669244\n",
            "[68]\tvalidation_0-auc:0.670378\n",
            "[69]\tvalidation_0-auc:0.665801\n",
            "[70]\tvalidation_0-auc:0.666628\n",
            "[71]\tvalidation_0-auc:0.666282\n",
            "[72]\tvalidation_0-auc:0.666003\n",
            "[73]\tvalidation_0-auc:0.66708\n",
            "[74]\tvalidation_0-auc:0.66708\n",
            "[75]\tvalidation_0-auc:0.66933\n",
            "[76]\tvalidation_0-auc:0.668657\n",
            "[77]\tvalidation_0-auc:0.668311\n",
            "[78]\tvalidation_0-auc:0.665369\n",
            "[79]\tvalidation_0-auc:0.66233\n",
            "[80]\tvalidation_0-auc:0.662407\n",
            "[81]\tvalidation_0-auc:0.662003\n",
            "[82]\tvalidation_0-auc:0.662176\n",
            "[83]\tvalidation_0-auc:0.662176\n",
            "[84]\tvalidation_0-auc:0.661407\n",
            "[85]\tvalidation_0-auc:0.659984\n",
            "[86]\tvalidation_0-auc:0.660003\n",
            "Stopping. Best iteration:\n",
            "[36]\tvalidation_0-auc:0.682445\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.6346938775510204 | 0.6878048780487804 |     0.55078125     | 0.6117136659436009 |\n",
            "|      GRU 0.15     | 0.610204081632653  | 0.5942028985507246 |     0.80078125     | 0.6821963394342762 |\n",
            "|    XGBoost 0.15   | 0.636734693877551  |        0.65        |     0.66015625     | 0.6550387596899225 |\n",
            "|    Logreg 0.15    | 0.6204081632653061 | 0.6067073170731707 |     0.77734375     | 0.6815068493150686 |\n",
            "|      SVM 0.15     | 0.6612244897959184 | 0.6642335766423357 |     0.7109375      | 0.6867924528301886 |\n",
            "|   LSTM beta 0.15  | 0.6739606126914661 | 0.6654929577464789 | 0.7777777777777778 | 0.7172675521821632 |\n",
            "|   GRU beta 0.15   | 0.6827133479212254 | 0.6590909090909091 | 0.8353909465020576 | 0.7368421052631579 |\n",
            "| XGBoost beta 0.15 | 0.5951859956236324 | 0.6006944444444444 | 0.7119341563786008 | 0.6516007532956686 |\n",
            "|  logreg beta 0.15 | 0.687089715536105  | 0.6623376623376623 | 0.8395061728395061 | 0.7404718693284936 |\n",
            "|   svm beta 0.15   | 0.7396061269146609 | 0.7649572649572649 | 0.7366255144032922 | 0.7505241090146751 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcgFnHPeVHfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "023f5da4-f117-4ffb-a2fb-62a8344da699"
      },
      "source": [
        "Result_purging.to_csv('FCX_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.510638</td>\n",
              "      <td>0.622449</td>\n",
              "      <td>0.644914</td>\n",
              "      <td>0.875000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.505952</td>\n",
              "      <td>0.616327</td>\n",
              "      <td>0.643939</td>\n",
              "      <td>0.885417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.533113</td>\n",
              "      <td>0.648980</td>\n",
              "      <td>0.651822</td>\n",
              "      <td>0.838542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.655102</td>\n",
              "      <td>0.655804</td>\n",
              "      <td>0.838542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.663265</td>\n",
              "      <td>0.662577</td>\n",
              "      <td>0.843750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.528846</td>\n",
              "      <td>0.634573</td>\n",
              "      <td>0.568475</td>\n",
              "      <td>0.614525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.512605</td>\n",
              "      <td>0.621444</td>\n",
              "      <td>0.585132</td>\n",
              "      <td>0.681564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.479452</td>\n",
              "      <td>0.582057</td>\n",
              "      <td>0.594480</td>\n",
              "      <td>0.782123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.510345</td>\n",
              "      <td>0.621444</td>\n",
              "      <td>0.631130</td>\n",
              "      <td>0.826816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.521127</td>\n",
              "      <td>0.634573</td>\n",
              "      <td>0.639309</td>\n",
              "      <td>0.826816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.567839</td>\n",
              "      <td>0.587755</td>\n",
              "      <td>0.691131</td>\n",
              "      <td>0.882812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.559524</td>\n",
              "      <td>0.579592</td>\n",
              "      <td>0.695266</td>\n",
              "      <td>0.917969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.568579</td>\n",
              "      <td>0.589796</td>\n",
              "      <td>0.694064</td>\n",
              "      <td>0.890625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.573604</td>\n",
              "      <td>0.595918</td>\n",
              "      <td>0.695385</td>\n",
              "      <td>0.882812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.546256</td>\n",
              "      <td>0.563265</td>\n",
              "      <td>0.698592</td>\n",
              "      <td>0.968750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.616477</td>\n",
              "      <td>0.647702</td>\n",
              "      <td>0.729412</td>\n",
              "      <td>0.893004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.572500</td>\n",
              "      <td>0.595186</td>\n",
              "      <td>0.712286</td>\n",
              "      <td>0.942387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.532663</td>\n",
              "      <td>0.525164</td>\n",
              "      <td>0.661466</td>\n",
              "      <td>0.872428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.607735</td>\n",
              "      <td>0.638950</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.905350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.627168</td>\n",
              "      <td>0.660832</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.893004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.687805</td>\n",
              "      <td>0.634694</td>\n",
              "      <td>0.611714</td>\n",
              "      <td>0.550781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.594203</td>\n",
              "      <td>0.610204</td>\n",
              "      <td>0.682196</td>\n",
              "      <td>0.800781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.636735</td>\n",
              "      <td>0.655039</td>\n",
              "      <td>0.660156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.606707</td>\n",
              "      <td>0.620408</td>\n",
              "      <td>0.681507</td>\n",
              "      <td>0.777344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.664234</td>\n",
              "      <td>0.661224</td>\n",
              "      <td>0.686792</td>\n",
              "      <td>0.710938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.665493</td>\n",
              "      <td>0.673961</td>\n",
              "      <td>0.717268</td>\n",
              "      <td>0.777778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.659091</td>\n",
              "      <td>0.682713</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.835391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.600694</td>\n",
              "      <td>0.595186</td>\n",
              "      <td>0.651601</td>\n",
              "      <td>0.711934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.662338</td>\n",
              "      <td>0.687090</td>\n",
              "      <td>0.740472</td>\n",
              "      <td>0.839506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>FCX</td>\n",
              "      <td>0.764957</td>\n",
              "      <td>0.739606</td>\n",
              "      <td>0.750524</td>\n",
              "      <td>0.736626</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  FCX  0.510638  0.622449  0.644914  0.875000\n",
              "1            GRU 0.1  FCX  0.505952  0.616327  0.643939  0.885417\n",
              "2        XGBoost 0.1  FCX  0.533113  0.648980  0.651822  0.838542\n",
              "3         Logreg 0.1  FCX  0.538462  0.655102  0.655804  0.838542\n",
              "4            SVM 0.1  FCX  0.545455  0.663265  0.662577  0.843750\n",
              "5      LSTM beta 0.1  FCX  0.528846  0.634573  0.568475  0.614525\n",
              "6       GRU beta 0.1  FCX  0.512605  0.621444  0.585132  0.681564\n",
              "7   XGBoost beta 0.1  FCX  0.479452  0.582057  0.594480  0.782123\n",
              "8    logreg beta 0.1  FCX  0.510345  0.621444  0.631130  0.826816\n",
              "9       svm beta 0.1  FCX  0.521127  0.634573  0.639309  0.826816\n",
              "0           LSTM 0.2  FCX  0.567839  0.587755  0.691131  0.882812\n",
              "1            GRU 0.2  FCX  0.559524  0.579592  0.695266  0.917969\n",
              "2        XGBoost 0.2  FCX  0.568579  0.589796  0.694064  0.890625\n",
              "3         Logreg 0.2  FCX  0.573604  0.595918  0.695385  0.882812\n",
              "4            SVM 0.2  FCX  0.546256  0.563265  0.698592  0.968750\n",
              "5      LSTM beta 0.2  FCX  0.616477  0.647702  0.729412  0.893004\n",
              "6       GRU beta 0.2  FCX  0.572500  0.595186  0.712286  0.942387\n",
              "7   XGBoost beta 0.2  FCX  0.532663  0.525164  0.661466  0.872428\n",
              "8    logreg beta 0.2  FCX  0.607735  0.638950  0.727273  0.905350\n",
              "9       svm beta 0.2  FCX  0.627168  0.660832  0.736842  0.893004\n",
              "0          LSTM 0.15  FCX  0.687805  0.634694  0.611714  0.550781\n",
              "1           GRU 0.15  FCX  0.594203  0.610204  0.682196  0.800781\n",
              "2       XGBoost 0.15  FCX  0.650000  0.636735  0.655039  0.660156\n",
              "3        Logreg 0.15  FCX  0.606707  0.620408  0.681507  0.777344\n",
              "4           SVM 0.15  FCX  0.664234  0.661224  0.686792  0.710938\n",
              "5     LSTM beta 0.15  FCX  0.665493  0.673961  0.717268  0.777778\n",
              "6      GRU beta 0.15  FCX  0.659091  0.682713  0.736842  0.835391\n",
              "7  XGBoost beta 0.15  FCX  0.600694  0.595186  0.651601  0.711934\n",
              "8   logreg beta 0.15  FCX  0.662338  0.687090  0.740472  0.839506\n",
              "9      svm beta 0.15  FCX  0.764957  0.739606  0.750524  0.736626"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzlJgQkNVHfa"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FCX_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFhdh53cVHfa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRIVh23HVrKZ"
      },
      "source": [
        "## FOX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ8N410FVrKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c336ad-fa30-4167-a048-30a30c491bbc"
      },
      "source": [
        "dfs = pd.read_csv(\"FOX.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2756</td>\n",
              "      <td>US1.FOX</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>37.48</td>\n",
              "      <td>37.92</td>\n",
              "      <td>37.13</td>\n",
              "      <td>37.69</td>\n",
              "      <td>25772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2755</td>\n",
              "      <td>US1.FOX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>37.90</td>\n",
              "      <td>37.99</td>\n",
              "      <td>37.11</td>\n",
              "      <td>37.13</td>\n",
              "      <td>49472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2754</td>\n",
              "      <td>US1.FOX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>36.79</td>\n",
              "      <td>37.32</td>\n",
              "      <td>36.66</td>\n",
              "      <td>37.25</td>\n",
              "      <td>29082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2753</td>\n",
              "      <td>US1.FOX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>36.99</td>\n",
              "      <td>37.35</td>\n",
              "      <td>36.76</td>\n",
              "      <td>36.86</td>\n",
              "      <td>31023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2752</td>\n",
              "      <td>US1.FOX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>36.38</td>\n",
              "      <td>37.19</td>\n",
              "      <td>36.38</td>\n",
              "      <td>36.93</td>\n",
              "      <td>34743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2752</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.FOX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>15.64</td>\n",
              "      <td>15.85</td>\n",
              "      <td>15.54</td>\n",
              "      <td>15.79</td>\n",
              "      <td>1297198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2753</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.FOX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>15.61</td>\n",
              "      <td>15.71</td>\n",
              "      <td>15.33</td>\n",
              "      <td>15.62</td>\n",
              "      <td>2236400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2754</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.FOX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>15.56</td>\n",
              "      <td>15.67</td>\n",
              "      <td>15.41</td>\n",
              "      <td>15.56</td>\n",
              "      <td>2451013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2755</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.FOX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>15.34</td>\n",
              "      <td>15.58</td>\n",
              "      <td>15.24</td>\n",
              "      <td>15.49</td>\n",
              "      <td>2117362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2756</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.FOX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>15.14</td>\n",
              "      <td>15.24</td>\n",
              "      <td>14.90</td>\n",
              "      <td>15.04</td>\n",
              "      <td>1805146</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2757 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...  <HIGH>  <LOW>  <CLOSE>    <VOL>\n",
              "0      2756  US1.FOX     D  20211001  ...   37.92  37.13    37.69    25772\n",
              "1      2755  US1.FOX     D  20210930  ...   37.99  37.11    37.13    49472\n",
              "2      2754  US1.FOX     D  20210929  ...   37.32  36.66    37.25    29082\n",
              "3      2753  US1.FOX     D  20210928  ...   37.35  36.76    36.86    31023\n",
              "4      2752  US1.FOX     D  20210927  ...   37.19  36.38    36.93    34743\n",
              "...     ...      ...   ...       ...  ...     ...    ...      ...      ...\n",
              "2752      4  US1.FOX     D  20101008  ...   15.85  15.54    15.79  1297198\n",
              "2753      3  US1.FOX     D  20101007  ...   15.71  15.33    15.62  2236400\n",
              "2754      2  US1.FOX     D  20101006  ...   15.67  15.41    15.56  2451013\n",
              "2755      1  US1.FOX     D  20101005  ...   15.58  15.24    15.49  2117362\n",
              "2756      0  US1.FOX     D  20101004  ...   15.24  14.90    15.04  1805146\n",
              "\n",
              "[2757 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBQaX_uAVrKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdcd4adb-77f6-4dc2-d1f7-672d99540b4d"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"83bdae3c-30a0-4b4a-88c2-6d09339197d6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"83bdae3c-30a0-4b4a-88c2-6d09339197d6\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '83bdae3c-30a0-4b4a-88c2-6d09339197d6',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [30.04, 30.49, 30.48, 30.4, 30.325, 31.19, 31.53, 31.47, 31.49, 32.0, 31.53, 32.18, 32.09, 31.98, 32.49, 32.815, 33.48, 33.71, 33.69, 33.97, 34.21, 33.99, 33.88, 33.91, 33.2, 32.43, 32.78, 32.44, 31.54, 31.61, 31.69, 31.45, 32.83, 33.14, 33.35, 33.61, 32.98, 32.8, 33.66, 35.005, 34.8, 35.39, 35.0, 36.78, 36.39, 36.0, 36.58, 36.86, 37.2, 37.87, 37.68, 37.48, 37.44, 36.89, 36.51, 36.31, 36.46, 36.6, 36.17, 36.69, 37.03, 36.96, 36.3, 36.12, 36.01, 36.13, 36.57, 36.35, 36.53, 35.96, 36.52, 35.75, 35.31, 35.18, 35.55, 35.87, 35.33, 35.74, 35.86, 35.8, 34.95, 34.94, 34.7, 33.83, 33.75, 33.68, 33.74, 34.02, 34.08, 33.78, 34.71, 34.82, 35.29, 36.17, 36.47, 36.52, 36.67, 37.18, 36.67, 36.83, 36.28, 37.24, 37.46, 37.51, 38.05, 37.97, 36.83, 36.7, 37.31, 37.01, 36.85, 38.75, 38.5, 38.73, 37.89, 37.63, 37.47, 37.6, 37.24, 37.92, 38.21, 37.24, 36.98, 36.95, 36.41, 35.93, 36.87, 36.86, 36.98, 36.02, 35.5, 35.7, 36.15, 35.88, 35.67, 36.2, 36.71, 36.44, 37.46, 38.89, 37.6269, 39.57, 51.05, 51.69, 51.32, 51.08, 50.69, 50.26, 50.22, 50.27, 50.38, 50.3, 50.41, 50.31, 50.15, 50.12, 50.59, 50.41, 50.82, 50.46, 50.37, 50.17, 50.01, 49.63, 49.26, 49.23, 49.23, 49.34, 49.18, 49.14, 49.03, 49.08, 49.09, 49.05, 48.75, 48.8, 48.91, 48.96, 48.64, 48.46, 48.4, 48.37, 48.07, 48.06, 48.33, 48.34, 48.4, 48.48, 48.49, 48.34, 47.93, 47.78, 47.19, 47.47, 47.8, 47.63, 47.75, 47.53, 45.79, 46.57, 47.72, 48.19, 48.49, 48.78, 48.79, 48.89, 48.91, 49.005, 49.03, 48.99, 49.22, 48.93, 49.39, 49.04, 49.3, 49.25, 48.88, 48.54, 48.66, 48.55, 48.31, 48.55, 47.74, 47.83, 47.44, 47.26, 47.57, 47.71, 47.47, 47.13, 46.79, 46.41, 46.08, 46.2, 45.17, 45.3, 45.21, 44.79, 44.78, 44.3, 45.35, 45.44, 45.5, 45.46, 45.45, 45.49, 45.13, 44.93, 44.79, 44.43, 45.25, 45.46, 45.81, 46.24, 46.49, 46.18, 45.83, 45.85, 45.49, 45.27, 44.8, 44.65, 43.98, 44.19, 44.25, 44.17, 44.34, 44.41, 44.37, 44.2, 44.3, 44.47, 44.76, 44.88, 44.74, 44.86, 44.87, 44.73, 45.0, 44.6, 44.61, 44.57, 44.67, 44.76, 44.85, 44.8, 44.83, 45.12, 44.83, 44.96, 44.72, 44.9, 45.02, 44.95, 45.07, 45.26, 44.92, 44.6, 44.53, 44.43, 44.53, 44.7, 45.09, 44.91, 44.915, 45.53, 45.69, 46.21, 46.33, 46.19, 46.42, 47.19, 47.02, 47.4, 49.34, 49.08, 48.44, 48.16, 48.28, 48.67, 49.28, 49.31, 48.31, 47.43, 47.89, 48.28, 47.98, 47.56, 44.28, 44.36, 44.57, 44.41, 43.4, 40.32, 39.92, 39.52, 39.23, 38.94, 38.84, 38.42, 38.26, 38.19, 38.25, 38.26, 38.5, 38.5, 38.25, 37.75, 37.45, 37.27, 37.29, 37.71, 37.35, 37.29, 37.48, 37.46, 37.24, 37.45, 37.49, 37.15, 36.34, 36.13, 36.11, 36.07, 35.99, 35.84, 36.14, 35.505, 36.03, 36.19, 36.68, 36.82, 37.06, 36.51, 36.41, 36.06, 35.66, 35.8, 35.33, 35.34, 35.99, 35.63, 35.61, 35.68, 36.33, 35.5, 36.08, 36.57, 35.45, 36.191, 36.3, 36.18, 36.249, 36.63, 36.7, 37.21, 36.86, 37.52, 37.22, 36.78, 36.52, 37.01, 36.16, 35.92, 35.53, 36.42, 37.15, 38.4, 37.61, 36.73, 36.55, 36.62, 37.05, 36.64, 36.18, 36.2, 35.95, 35.19, 34.05, 35.65, 36.41, 35.57, 36.22, 36.69, 36.49, 37.855, 38.15, 38.31, 37.73, 37.49, 37.18, 36.96, 36.26, 35.74, 36.49, 35.78, 36.36, 35.79, 35.31, 36.18, 35.85, 36.31, 36.19, 35.93, 35.36, 34.12, 34.08, 34.12, 34.16, 34.73, 34.52, 33.86, 34.37, 34.52, 34.22, 34.23, 32.38, 33.6, 33.26, 32.96, 33.77, 32.7, 32.47, 32.295, 31.48, 31.14, 31.31, 30.18, 29.7, 29.6, 29.95, 30.1, 29.9, 30.49, 28.61, 27.95, 27.32, 27.82, 28.06, 28.1, 27.36, 27.04, 26.65, 24.43, 25.15, 25.37, 25.46, 25.75, 25.74, 25.69, 25.435, 25.56, 26.02, 26.84, 26.7, 26.58, 26.23, 26.095, 25.83, 25.48, 25.49, 26.14, 25.98, 26.46, 26.31, 26.45, 26.01, 26.045, 25.79, 26.02, 26.77, 26.43, 26.61, 26.425, 26.33, 26.37, 26.26, 26.03, 26.61, 26.0, 26.14, 25.47, 25.66, 25.38, 25.38, 25.965, 26.09, 26.97, 27.1, 27.0, 26.91, 26.77, 26.71, 26.62, 26.66, 26.97, 26.88, 26.8, 26.89, 27.555, 27.45, 27.76, 27.85, 27.48, 27.52, 27.71, 27.83, 28.05, 28.5, 28.27, 28.92, 28.69, 28.91, 29.23, 27.92, 27.725, 27.51, 27.65, 27.6, 27.46, 27.19, 27.22, 27.53, 27.385, 28.33, 27.72, 27.84, 27.55, 27.52, 27.94, 28.13, 27.86, 27.73, 27.77, 27.22, 27.63, 26.93, 26.63, 26.86, 26.74, 27.42, 27.12, 27.41, 27.13, 27.66, 28.18, 27.69, 27.0, 27.19, 27.21, 27.36, 27.6, 27.54, 26.9, 26.96, 26.94, 27.23, 26.67, 26.53, 27.0, 27.01, 26.77, 26.76, 27.11, 27.66, 27.9, 28.32, 27.67, 27.96, 28.01, 28.59, 28.51, 28.35, 29.85, 29.695, 29.85, 29.93, 30.03, 29.9, 29.74, 30.22, 30.55, 29.81, 30.05, 30.14, 29.86, 30.14, 30.465, 30.59, 30.535, 30.55, 30.84, 31.2, 31.565, 31.78, 31.67, 31.52, 31.82, 31.255, 30.89, 30.74, 30.42, 30.09, 30.09, 30.26, 30.195, 30.31, 29.915, 29.95, 29.98, 30.27, 29.97, 30.1, 30.2, 29.95, 29.7, 29.86, 29.35, 29.995, 30.23, 30.0, 30.15, 30.05, 29.92, 30.08, 30.32, 30.01, 29.48, 29.65, 29.69, 29.81, 30.16, 30.715, 31.09, 31.15, 31.23, 31.01, 30.97, 30.85, 30.59, 29.8, 29.79, 29.5, 29.55, 29.21, 29.52, 29.235, 29.25, 29.08, 29.44, 28.92, 28.77, 28.85, 28.35, 28.24, 27.995, 27.26, 27.48, 27.66, 27.84, 27.695, 27.745, 27.74, 27.59, 27.62, 27.74, 27.385, 27.61, 26.895, 26.2, 27.92, 28.48, 27.95, 27.37, 27.53, 27.36, 28.08, 28.06, 28.35, 28.19, 28.26, 28.28, 28.13, 27.72, 27.705, 27.81, 27.48, 27.39, 27.33, 27.0, 27.16, 27.33, 27.08, 27.18, 26.69, 27.53, 25.765, 26.185, 26.385, 26.24, 26.235, 26.51, 25.71, 25.665, 25.905, 25.355, 25.32, 25.07, 24.8, 24.865, 24.76, 24.77, 24.82, 25.0, 24.67, 24.9, 25.01, 25.15, 25.18, 24.74, 24.62, 24.89, 24.8, 24.31, 24.485, 24.74, 24.3, 24.235, 24.29, 24.28, 24.33, 24.19, 24.2, 24.66, 24.12, 25.04, 24.57, 24.7, 24.75, 24.75, 24.855, 24.87, 25.06, 25.015, 25.11, 25.335, 25.445, 25.7, 25.81, 25.82, 26.19, 26.14, 26.3, 26.58, 26.63, 26.19, 26.175, 26.225, 26.43, 26.21, 27.415, 27.035, 27.13, 27.03, 27.07, 27.42, 27.41, 27.44, 27.76, 27.72, 27.8, 28.45, 28.57, 28.58, 28.61, 28.41, 28.08, 28.08, 28.0, 27.26, 27.12, 26.88, 27.51, 27.25, 27.07, 26.62, 26.26, 26.77, 28.97, 28.68, 28.59, 28.94, 28.695, 29.02, 28.97, 28.98, 29.08, 28.995, 29.41, 29.68, 29.68, 29.73, 29.6, 29.845, 29.69, 29.24, 29.25, 28.97, 29.2, 28.83, 28.44, 28.45, 28.25, 28.48, 28.83, 29.13, 29.24, 29.35, 29.58, 30.02, 29.4, 29.52, 29.81, 29.58, 29.69, 30.25, 30.12, 30.575, 30.79, 30.87, 30.94, 30.74, 30.09, 29.97, 30.04, 30.08, 29.84, 29.79, 29.88, 29.42, 28.69, 28.78, 28.83, 29.37, 28.8, 29.015, 29.12, 28.21, 28.2, 28.175, 27.99, 28.01, 28.0, 28.0, 28.03, 28.2, 28.2, 28.2, 28.11, 28.135, 28.19, 27.99, 28.08, 27.84, 28.14, 27.98, 27.99, 27.67, 28.025, 27.17, 27.21, 27.39, 27.325, 27.03, 27.31, 26.69, 26.41, 26.465, 25.63, 24.67, 24.71, 24.47, 24.21, 24.54, 25.04, 26.24, 26.5, 26.06, 27.11, 27.095, 26.23, 26.585, 26.95, 26.015, 26.41, 25.63, 25.75, 26.58, 26.16, 26.78, 25.82, 26.46, 26.0, 25.93, 25.92, 26.505, 26.59, 26.6, 27.24, 27.35, 27.6, 27.45, 28.1, 27.6, 27.6, 27.57, 27.83, 28.24, 28.5, 28.11, 27.99, 28.16, 29.06, 28.98, 29.5, 30.03, 30.0, 29.71, 29.94, 30.26, 29.97, 29.9, 30.31, 30.25, 30.38, 30.535, 30.31, 30.63, 30.44, 30.6, 30.02, 30.36, 30.34, 30.34, 29.7, 30.09, 29.87, 29.92, 31.495, 31.11, 30.88, 30.64, 30.41, 30.29, 30.58, 30.41, 30.1, 29.55, 29.82, 29.67, 29.66, 29.44, 28.73, 28.95, 28.835, 28.89, 28.9, 28.51, 28.455, 28.685, 28.3, 27.485, 27.06, 25.88, 25.41, 25.985, 26.215, 26.52, 26.57, 26.6, 26.49, 27.04, 27.065, 26.88, 26.67, 26.89, 27.01, 26.98, 27.625, 26.91, 27.18, 27.17, 26.76, 27.65, 28.08, 27.94, 27.49, 26.9, 26.865, 27.92, 28.67, 29.81, 30.06, 30.15, 29.91, 29.65, 29.49, 29.73, 30.15, 30.37, 29.07, 31.18, 33.39, 33.38, 33.53, 33.06, 33.0, 32.73, 32.29, 32.83, 32.49, 32.96, 32.75, 33.14, 33.195, 33.465, 33.18, 33.13, 33.1, 32.53, 31.935, 31.81, 32.46, 32.27, 32.21, 32.45, 32.22, 31.89, 32.79, 32.6, 32.51, 33.07, 32.99, 32.725, 32.78, 32.49, 32.17, 32.26, 32.59, 32.82, 32.85, 32.72, 33.0, 33.11, 33.52, 33.58, 33.35, 33.46, 33.44, 33.77, 34.01, 33.8, 34.1, 34.43, 34.23, 34.19, 33.47, 33.625, 33.06, 32.66, 32.36, 32.51, 32.43, 32.34, 33.02, 33.2, 33.57, 33.65, 33.36, 33.78, 33.79, 33.81, 33.96, 33.65, 33.31, 33.12, 33.11, 32.62, 33.18, 33.18, 33.13, 33.18, 33.37, 33.44, 33.225, 33.3, 33.48, 33.235, 32.69, 32.89, 33.15, 32.74, 33.02, 33.26, 34.13, 34.66, 34.4, 33.59, 33.88, 32.98, 33.33, 32.96, 33.49, 33.17, 33.22, 33.65, 33.6, 34.04, 34.07, 34.62, 34.25, 34.045, 33.8, 33.95, 34.06, 34.4, 34.27, 33.84, 33.62, 33.99, 33.65, 33.31, 33.58, 33.27, 33.01, 32.57, 31.77, 33.42, 33.32, 32.46, 31.86, 32.58, 32.36, 32.99, 33.41, 33.24, 33.38, 32.64, 32.78, 33.58, 33.11, 33.29, 33.57, 33.81, 33.835, 34.11, 33.96, 35.26, 35.76, 36.52, 36.9, 37.18, 37.5, 37.31, 37.33, 37.31, 37.485, 37.05, 36.38, 35.8, 34.81, 35.16, 35.46, 35.75, 35.615, 35.77, 36.19, 36.52, 36.15, 35.66, 35.79, 35.72, 35.361, 34.86, 35.15, 34.45, 33.96, 33.8, 33.66, 33.77, 33.82, 33.7, 33.56, 33.19, 33.41, 33.3, 33.24, 33.95, 33.42, 32.01, 33.12, 33.16, 33.3, 33.11, 33.1, 32.64, 32.23, 32.38, 31.64, 32.54, 31.96, 31.63, 31.56, 30.88, 30.96, 30.73, 31.35, 32.2, 33.11, 32.69, 32.99, 33.03, 32.39, 32.87, 33.31, 33.25, 33.4, 32.76, 33.46, 33.2, 33.5, 34.14, 33.93, 33.62, 33.75, 33.92, 34.08, 34.26, 34.49, 34.65, 34.9, 35.115, 35.14, 35.29, 34.92, 34.43, 34.6, 34.83, 34.51, 34.56, 34.7, 34.645, 34.61, 34.73, 34.87, 34.73, 34.62, 34.25, 34.02, 33.69, 33.545, 33.08, 31.68, 31.02, 31.37, 31.87, 31.66, 31.9557, 32.0, 32.24, 32.55, 32.58, 32.26, 32.73, 32.49, 32.77, 32.47, 32.57, 34.155, 34.42, 34.5, 34.62, 34.84, 34.105, 34.62, 35.15, 34.67, 34.34, 34.22, 34.34, 34.22, 34.09, 33.35, 33.515, 33.58, 34.2, 34.34, 34.38, 34.48, 34.39, 34.305, 35.02, 35.05, 35.37, 35.08, 35.11, 34.52, 34.41, 34.625, 34.46, 34.5, 34.16, 33.81, 34.05, 33.64, 33.72, 33.26, 33.28, 33.21, 33.26, 33.46, 34.07, 34.35, 33.45, 33.49, 31.41, 31.66, 32.0, 31.775, 31.69, 31.32, 31.275, 31.03, 31.79, 32.02, 31.9, 32.0, 31.69, 31.7, 31.32, 30.7725, 31.09, 31.02, 31.54, 32.29, 31.65, 31.19, 32.18, 32.77, 32.52, 32.06, 31.13, 30.73, 30.99, 31.31, 31.63, 31.485, 31.82, 31.79, 32.01, 32.25, 31.8, 31.635, 31.45, 32.06, 32.27, 32.93, 33.14, 33.44, 33.01, 32.74, 32.08, 32.55, 32.11, 31.955, 32.23, 32.31, 32.13, 31.8, 31.89, 32.14, 31.835, 31.61, 31.83, 31.51, 31.44, 31.58, 31.56, 31.155, 31.05, 30.45, 31.25, 31.305, 30.21, 30.68, 30.54, 30.66, 31.04, 31.36, 31.245, 31.45, 31.815, 31.97, 31.8075, 31.925, 32.85, 32.59, 33.8, 34.37, 34.27, 34.66, 35.05, 34.6, 34.45, 34.38, 34.41, 34.14, 33.93, 33.59, 33.235, 32.58, 31.96, 32.01, 32.07, 31.66, 31.75, 32.31, 32.1, 32.16, 31.98, 32.315, 32.74, 32.98, 33.0322, 32.875, 32.68, 32.7, 32.96, 32.89, 32.81, 32.955, 33.41, 33.845, 33.79, 33.35, 32.92, 33.25, 33.6, 32.67, 33.77, 33.87, 34.01, 34.24, 34.0, 33.84, 34.58, 34.62, 34.64, 34.85, 34.5]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('83bdae3c-30a0-4b4a-88c2-6d09339197d6');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"ca2b6955-577d-4460-82c7-1c3fe5c17896\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"ca2b6955-577d-4460-82c7-1c3fe5c17896\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'ca2b6955-577d-4460-82c7-1c3fe5c17896',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ca2b6955-577d-4460-82c7-1c3fe5c17896');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D28g1T1BVrKf"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksGC5RYAVrKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "493a33c5-b36f-4f59-fb87-7a72db3d2c9a"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .2, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"FOX\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6940 - accuracy: 0.5148 - val_loss: 0.7265 - val_accuracy: 0.1469\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6924 - accuracy: 0.5161 - val_loss: 0.6894 - val_accuracy: 0.5531\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6539 - accuracy: 0.6154 - val_loss: 0.5665 - val_accuracy: 0.7918\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6514 - accuracy: 0.6450 - val_loss: 0.6896 - val_accuracy: 0.6224\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6312 - accuracy: 0.6664 - val_loss: 0.7353 - val_accuracy: 0.6510\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6920 - accuracy: 0.5174 - val_loss: 0.7236 - val_accuracy: 0.2102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6400 - accuracy: 0.6624 - val_loss: 0.8450 - val_accuracy: 0.3327\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6117 - accuracy: 0.6758 - val_loss: 0.5557 - val_accuracy: 0.7571\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6114 - accuracy: 0.6859 - val_loss: 0.6684 - val_accuracy: 0.6367\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5926 - accuracy: 0.7101 - val_loss: 0.6350 - val_accuracy: 0.6816\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.697867\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.707486\n",
            "[2]\tvalidation_0-auc:0.722023\n",
            "[3]\tvalidation_0-auc:0.750748\n",
            "[4]\tvalidation_0-auc:0.720195\n",
            "[5]\tvalidation_0-auc:0.720096\n",
            "[6]\tvalidation_0-auc:0.746478\n",
            "[7]\tvalidation_0-auc:0.742507\n",
            "[8]\tvalidation_0-auc:0.741893\n",
            "[9]\tvalidation_0-auc:0.742109\n",
            "[10]\tvalidation_0-auc:0.741145\n",
            "[11]\tvalidation_0-auc:0.744086\n",
            "[12]\tvalidation_0-auc:0.754934\n",
            "[13]\tvalidation_0-auc:0.759652\n",
            "[14]\tvalidation_0-auc:0.761247\n",
            "[15]\tvalidation_0-auc:0.760201\n",
            "[16]\tvalidation_0-auc:0.75927\n",
            "[17]\tvalidation_0-auc:0.760516\n",
            "[18]\tvalidation_0-auc:0.76369\n",
            "[19]\tvalidation_0-auc:0.76359\n",
            "[20]\tvalidation_0-auc:0.762892\n",
            "[21]\tvalidation_0-auc:0.763175\n",
            "[22]\tvalidation_0-auc:0.761945\n",
            "[23]\tvalidation_0-auc:0.758639\n",
            "[24]\tvalidation_0-auc:0.755948\n",
            "[25]\tvalidation_0-auc:0.756911\n",
            "[26]\tvalidation_0-auc:0.757011\n",
            "[27]\tvalidation_0-auc:0.756247\n",
            "[28]\tvalidation_0-auc:0.753921\n",
            "[29]\tvalidation_0-auc:0.754685\n",
            "[30]\tvalidation_0-auc:0.752841\n",
            "[31]\tvalidation_0-auc:0.748937\n",
            "[32]\tvalidation_0-auc:0.749502\n",
            "[33]\tvalidation_0-auc:0.747574\n",
            "[34]\tvalidation_0-auc:0.748505\n",
            "[35]\tvalidation_0-auc:0.747807\n",
            "[36]\tvalidation_0-auc:0.745016\n",
            "[37]\tvalidation_0-auc:0.741826\n",
            "[38]\tvalidation_0-auc:0.737141\n",
            "[39]\tvalidation_0-auc:0.735879\n",
            "[40]\tvalidation_0-auc:0.734981\n",
            "[41]\tvalidation_0-auc:0.734317\n",
            "[42]\tvalidation_0-auc:0.734948\n",
            "[43]\tvalidation_0-auc:0.732024\n",
            "[44]\tvalidation_0-auc:0.729034\n",
            "[45]\tvalidation_0-auc:0.727638\n",
            "[46]\tvalidation_0-auc:0.726974\n",
            "[47]\tvalidation_0-auc:0.729648\n",
            "[48]\tvalidation_0-auc:0.73131\n",
            "[49]\tvalidation_0-auc:0.733137\n",
            "[50]\tvalidation_0-auc:0.731376\n",
            "[51]\tvalidation_0-auc:0.731675\n",
            "[52]\tvalidation_0-auc:0.732838\n",
            "[53]\tvalidation_0-auc:0.735496\n",
            "[54]\tvalidation_0-auc:0.734034\n",
            "[55]\tvalidation_0-auc:0.7344\n",
            "[56]\tvalidation_0-auc:0.73327\n",
            "[57]\tvalidation_0-auc:0.733968\n",
            "[58]\tvalidation_0-auc:0.731509\n",
            "[59]\tvalidation_0-auc:0.734034\n",
            "[60]\tvalidation_0-auc:0.733071\n",
            "[61]\tvalidation_0-auc:0.732041\n",
            "[62]\tvalidation_0-auc:0.732805\n",
            "[63]\tvalidation_0-auc:0.730712\n",
            "[64]\tvalidation_0-auc:0.729748\n",
            "[65]\tvalidation_0-auc:0.728901\n",
            "[66]\tvalidation_0-auc:0.723983\n",
            "[67]\tvalidation_0-auc:0.722986\n",
            "[68]\tvalidation_0-auc:0.722222\n",
            "Stopping. Best iteration:\n",
            "[18]\tvalidation_0-auc:0.76369\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6954 - accuracy: 0.4997 - val_loss: 0.7330 - val_accuracy: 0.0853\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6923 - accuracy: 0.5148 - val_loss: 0.7572 - val_accuracy: 0.0678\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6736 - accuracy: 0.5868 - val_loss: 0.7602 - val_accuracy: 0.3195\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6410 - accuracy: 0.6376 - val_loss: 0.8867 - val_accuracy: 0.2516\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6246 - accuracy: 0.6651 - val_loss: 0.5798 - val_accuracy: 0.7221\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6916 - accuracy: 0.5443 - val_loss: 0.7687 - val_accuracy: 0.0875\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6682 - accuracy: 0.5978 - val_loss: 0.6745 - val_accuracy: 0.6696\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6216 - accuracy: 0.6712 - val_loss: 0.6694 - val_accuracy: 0.6389\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6061 - accuracy: 0.6905 - val_loss: 0.5796 - val_accuracy: 0.7549\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5965 - accuracy: 0.6953 - val_loss: 0.6179 - val_accuracy: 0.7068\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.697246\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.66912\n",
            "[2]\tvalidation_0-auc:0.665593\n",
            "[3]\tvalidation_0-auc:0.678322\n",
            "[4]\tvalidation_0-auc:0.694363\n",
            "[5]\tvalidation_0-auc:0.6874\n",
            "[6]\tvalidation_0-auc:0.697951\n",
            "[7]\tvalidation_0-auc:0.705435\n",
            "[8]\tvalidation_0-auc:0.707797\n",
            "[9]\tvalidation_0-auc:0.71252\n",
            "[10]\tvalidation_0-auc:0.717427\n",
            "[11]\tvalidation_0-auc:0.739326\n",
            "[12]\tvalidation_0-auc:0.739541\n",
            "[13]\tvalidation_0-auc:0.735401\n",
            "[14]\tvalidation_0-auc:0.741903\n",
            "[15]\tvalidation_0-auc:0.75595\n",
            "[16]\tvalidation_0-auc:0.753926\n",
            "[17]\tvalidation_0-auc:0.754754\n",
            "[18]\tvalidation_0-auc:0.752055\n",
            "[19]\tvalidation_0-auc:0.748834\n",
            "[20]\tvalidation_0-auc:0.756196\n",
            "[21]\tvalidation_0-auc:0.756318\n",
            "[22]\tvalidation_0-auc:0.756165\n",
            "[23]\tvalidation_0-auc:0.751564\n",
            "[24]\tvalidation_0-auc:0.752699\n",
            "[25]\tvalidation_0-auc:0.748282\n",
            "[26]\tvalidation_0-auc:0.7427\n",
            "[27]\tvalidation_0-auc:0.751718\n",
            "[28]\tvalidation_0-auc:0.758987\n",
            "[29]\tvalidation_0-auc:0.761072\n",
            "[30]\tvalidation_0-auc:0.769783\n",
            "[31]\tvalidation_0-auc:0.760766\n",
            "[32]\tvalidation_0-auc:0.769906\n",
            "[33]\tvalidation_0-auc:0.768617\n",
            "[34]\tvalidation_0-auc:0.774353\n",
            "[35]\tvalidation_0-auc:0.778524\n",
            "[36]\tvalidation_0-auc:0.776622\n",
            "[37]\tvalidation_0-auc:0.779414\n",
            "[38]\tvalidation_0-auc:0.776653\n",
            "[39]\tvalidation_0-auc:0.77607\n",
            "[40]\tvalidation_0-auc:0.774844\n",
            "[41]\tvalidation_0-auc:0.779444\n",
            "[42]\tvalidation_0-auc:0.779475\n",
            "[43]\tvalidation_0-auc:0.779414\n",
            "[44]\tvalidation_0-auc:0.777389\n",
            "[45]\tvalidation_0-auc:0.775058\n",
            "[46]\tvalidation_0-auc:0.771286\n",
            "[47]\tvalidation_0-auc:0.770304\n",
            "[48]\tvalidation_0-auc:0.769752\n",
            "[49]\tvalidation_0-auc:0.771654\n",
            "[50]\tvalidation_0-auc:0.772083\n",
            "[51]\tvalidation_0-auc:0.776009\n",
            "[52]\tvalidation_0-auc:0.776561\n",
            "[53]\tvalidation_0-auc:0.774537\n",
            "[54]\tvalidation_0-auc:0.770243\n",
            "[55]\tvalidation_0-auc:0.769752\n",
            "[56]\tvalidation_0-auc:0.773157\n",
            "[57]\tvalidation_0-auc:0.774322\n",
            "[58]\tvalidation_0-auc:0.773954\n",
            "[59]\tvalidation_0-auc:0.771132\n",
            "[60]\tvalidation_0-auc:0.770335\n",
            "[61]\tvalidation_0-auc:0.771194\n",
            "[62]\tvalidation_0-auc:0.772175\n",
            "[63]\tvalidation_0-auc:0.771194\n",
            "[64]\tvalidation_0-auc:0.772114\n",
            "[65]\tvalidation_0-auc:0.772237\n",
            "[66]\tvalidation_0-auc:0.772421\n",
            "[67]\tvalidation_0-auc:0.770274\n",
            "[68]\tvalidation_0-auc:0.774445\n",
            "[69]\tvalidation_0-auc:0.769906\n",
            "[70]\tvalidation_0-auc:0.769047\n",
            "[71]\tvalidation_0-auc:0.769353\n",
            "[72]\tvalidation_0-auc:0.766961\n",
            "[73]\tvalidation_0-auc:0.766164\n",
            "[74]\tvalidation_0-auc:0.767145\n",
            "[75]\tvalidation_0-auc:0.770764\n",
            "[76]\tvalidation_0-auc:0.77058\n",
            "[77]\tvalidation_0-auc:0.772237\n",
            "[78]\tvalidation_0-auc:0.774138\n",
            "[79]\tvalidation_0-auc:0.778064\n",
            "[80]\tvalidation_0-auc:0.777573\n",
            "[81]\tvalidation_0-auc:0.775672\n",
            "[82]\tvalidation_0-auc:0.775304\n",
            "[83]\tvalidation_0-auc:0.776592\n",
            "[84]\tvalidation_0-auc:0.774813\n",
            "[85]\tvalidation_0-auc:0.780456\n",
            "[86]\tvalidation_0-auc:0.780272\n",
            "[87]\tvalidation_0-auc:0.779291\n",
            "[88]\tvalidation_0-auc:0.778187\n",
            "[89]\tvalidation_0-auc:0.77972\n",
            "[90]\tvalidation_0-auc:0.780334\n",
            "[91]\tvalidation_0-auc:0.781131\n",
            "[92]\tvalidation_0-auc:0.781683\n",
            "[93]\tvalidation_0-auc:0.78199\n",
            "[94]\tvalidation_0-auc:0.784076\n",
            "[95]\tvalidation_0-auc:0.780947\n",
            "[96]\tvalidation_0-auc:0.779475\n",
            "[97]\tvalidation_0-auc:0.77972\n",
            "[98]\tvalidation_0-auc:0.781929\n",
            "[99]\tvalidation_0-auc:0.782174\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.6510204081632653 |  0.2631578947368421 | 0.7638888888888888 | 0.39145907473309605 |\n",
            "|     GRU 0.1      | 0.6816326530612244 |       0.28125       |        0.75        |  0.4090909090909091 |\n",
            "|   XGBoost 0.1    | 0.6877551020408164 |  0.2786885245901639 | 0.7083333333333334 | 0.39999999999999997 |\n",
            "|    Logreg 0.1    | 0.6346938775510204 | 0.25116279069767444 |        0.75        |  0.3763066202090593 |\n",
            "|     SVM 0.1      | 0.6918367346938775 | 0.29949238578680204 | 0.8194444444444444 |  0.4386617100371747 |\n",
            "|  LSTM beta 0.1   | 0.7221006564551422 | 0.18115942028985507 | 0.6410256410256411 |  0.2824858757062147 |\n",
            "|   GRU beta 0.1   | 0.7067833698030634 |  0.1724137931034483 | 0.6410256410256411 | 0.27173913043478265 |\n",
            "| XGBoost beta 0.1 | 0.6477024070021882 | 0.14942528735632185 | 0.6666666666666666 | 0.24413145539906106 |\n",
            "| logreg beta 0.1  | 0.6608315098468271 |  0.1588235294117647 | 0.6923076923076923 | 0.25837320574162675 |\n",
            "|   svm beta 0.1   | 0.6258205689277899 | 0.16666666666666666 | 0.8461538461538461 | 0.27848101265822783 |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6938 - accuracy: 0.5275 - val_loss: 0.6202 - val_accuracy: 0.9633\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6909 - accuracy: 0.5336 - val_loss: 0.7155 - val_accuracy: 0.0939\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6859 - accuracy: 0.5544 - val_loss: 0.6377 - val_accuracy: 0.7204\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6602 - accuracy: 0.6376 - val_loss: 0.6342 - val_accuracy: 0.6816\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6535 - accuracy: 0.6430 - val_loss: 0.7665 - val_accuracy: 0.4898\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6931 - accuracy: 0.5181 - val_loss: 0.5735 - val_accuracy: 0.9776\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6665 - accuracy: 0.6027 - val_loss: 0.7242 - val_accuracy: 0.5286\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6526 - accuracy: 0.6295 - val_loss: 0.6973 - val_accuracy: 0.5980\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6435 - accuracy: 0.6550 - val_loss: 0.6024 - val_accuracy: 0.7286\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6482 - accuracy: 0.6403 - val_loss: 0.6544 - val_accuracy: 0.6531\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.555851\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.47883\n",
            "[2]\tvalidation_0-auc:0.524521\n",
            "[3]\tvalidation_0-auc:0.525798\n",
            "[4]\tvalidation_0-auc:0.475957\n",
            "[5]\tvalidation_0-auc:0.491489\n",
            "[6]\tvalidation_0-auc:0.483777\n",
            "[7]\tvalidation_0-auc:0.481915\n",
            "[8]\tvalidation_0-auc:0.472979\n",
            "[9]\tvalidation_0-auc:0.46766\n",
            "[10]\tvalidation_0-auc:0.448191\n",
            "[11]\tvalidation_0-auc:0.455957\n",
            "[12]\tvalidation_0-auc:0.455106\n",
            "[13]\tvalidation_0-auc:0.446277\n",
            "[14]\tvalidation_0-auc:0.447128\n",
            "[15]\tvalidation_0-auc:0.441702\n",
            "[16]\tvalidation_0-auc:0.41234\n",
            "[17]\tvalidation_0-auc:0.420638\n",
            "[18]\tvalidation_0-auc:0.422128\n",
            "[19]\tvalidation_0-auc:0.408617\n",
            "[20]\tvalidation_0-auc:0.404149\n",
            "[21]\tvalidation_0-auc:0.396809\n",
            "[22]\tvalidation_0-auc:0.395\n",
            "[23]\tvalidation_0-auc:0.394149\n",
            "[24]\tvalidation_0-auc:0.381809\n",
            "[25]\tvalidation_0-auc:0.383936\n",
            "[26]\tvalidation_0-auc:0.376489\n",
            "[27]\tvalidation_0-auc:0.380851\n",
            "[28]\tvalidation_0-auc:0.383723\n",
            "[29]\tvalidation_0-auc:0.388191\n",
            "[30]\tvalidation_0-auc:0.375851\n",
            "[31]\tvalidation_0-auc:0.38\n",
            "[32]\tvalidation_0-auc:0.377872\n",
            "[33]\tvalidation_0-auc:0.368511\n",
            "[34]\tvalidation_0-auc:0.371808\n",
            "[35]\tvalidation_0-auc:0.367128\n",
            "[36]\tvalidation_0-auc:0.359787\n",
            "[37]\tvalidation_0-auc:0.361809\n",
            "[38]\tvalidation_0-auc:0.36383\n",
            "[39]\tvalidation_0-auc:0.37266\n",
            "[40]\tvalidation_0-auc:0.371383\n",
            "[41]\tvalidation_0-auc:0.367553\n",
            "[42]\tvalidation_0-auc:0.364894\n",
            "[43]\tvalidation_0-auc:0.35883\n",
            "[44]\tvalidation_0-auc:0.363404\n",
            "[45]\tvalidation_0-auc:0.364681\n",
            "[46]\tvalidation_0-auc:0.364043\n",
            "[47]\tvalidation_0-auc:0.361277\n",
            "[48]\tvalidation_0-auc:0.35234\n",
            "[49]\tvalidation_0-auc:0.346064\n",
            "[50]\tvalidation_0-auc:0.344681\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.555851\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6965 - accuracy: 0.4942 - val_loss: 0.7045 - val_accuracy: 0.0438\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6895 - accuracy: 0.5079 - val_loss: 0.7310 - val_accuracy: 0.4836\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6701 - accuracy: 0.6012 - val_loss: 0.7222 - val_accuracy: 0.5886\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6469 - accuracy: 0.6356 - val_loss: 0.7288 - val_accuracy: 0.5383\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6511 - accuracy: 0.6513 - val_loss: 0.5943 - val_accuracy: 0.7374\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6795 - accuracy: 0.5649 - val_loss: 0.5565 - val_accuracy: 0.9190\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6433 - accuracy: 0.6431 - val_loss: 0.5664 - val_accuracy: 0.8249\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6390 - accuracy: 0.6596 - val_loss: 0.6492 - val_accuracy: 0.7002\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6366 - accuracy: 0.6568 - val_loss: 0.6663 - val_accuracy: 0.6630\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6199 - accuracy: 0.6671 - val_loss: 0.6513 - val_accuracy: 0.6565\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.407208\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.320881\n",
            "[2]\tvalidation_0-auc:0.566762\n",
            "[3]\tvalidation_0-auc:0.606007\n",
            "[4]\tvalidation_0-auc:0.661613\n",
            "[5]\tvalidation_0-auc:0.691934\n",
            "[6]\tvalidation_0-auc:0.691247\n",
            "[7]\tvalidation_0-auc:0.701087\n",
            "[8]\tvalidation_0-auc:0.708181\n",
            "[9]\tvalidation_0-auc:0.724313\n",
            "[10]\tvalidation_0-auc:0.726201\n",
            "[11]\tvalidation_0-auc:0.733181\n",
            "[12]\tvalidation_0-auc:0.742105\n",
            "[13]\tvalidation_0-auc:0.757208\n",
            "[14]\tvalidation_0-auc:0.740503\n",
            "[15]\tvalidation_0-auc:0.759725\n",
            "[16]\tvalidation_0-auc:0.767391\n",
            "[17]\tvalidation_0-auc:0.774485\n",
            "[18]\tvalidation_0-auc:0.756064\n",
            "[19]\tvalidation_0-auc:0.768535\n",
            "[20]\tvalidation_0-auc:0.750915\n",
            "[21]\tvalidation_0-auc:0.766705\n",
            "[22]\tvalidation_0-auc:0.775858\n",
            "[23]\tvalidation_0-auc:0.760755\n",
            "[24]\tvalidation_0-auc:0.76762\n",
            "[25]\tvalidation_0-auc:0.757895\n",
            "[26]\tvalidation_0-auc:0.738215\n",
            "[27]\tvalidation_0-auc:0.735355\n",
            "[28]\tvalidation_0-auc:0.74119\n",
            "[29]\tvalidation_0-auc:0.731693\n",
            "[30]\tvalidation_0-auc:0.730549\n",
            "[31]\tvalidation_0-auc:0.70881\n",
            "[32]\tvalidation_0-auc:0.722654\n",
            "[33]\tvalidation_0-auc:0.733524\n",
            "[34]\tvalidation_0-auc:0.739588\n",
            "[35]\tvalidation_0-auc:0.756178\n",
            "[36]\tvalidation_0-auc:0.759611\n",
            "[37]\tvalidation_0-auc:0.763616\n",
            "[38]\tvalidation_0-auc:0.770366\n",
            "[39]\tvalidation_0-auc:0.771053\n",
            "[40]\tvalidation_0-auc:0.774714\n",
            "[41]\tvalidation_0-auc:0.76476\n",
            "[42]\tvalidation_0-auc:0.775744\n",
            "[43]\tvalidation_0-auc:0.774485\n",
            "[44]\tvalidation_0-auc:0.77151\n",
            "[45]\tvalidation_0-auc:0.7746\n",
            "[46]\tvalidation_0-auc:0.783753\n",
            "[47]\tvalidation_0-auc:0.789016\n",
            "[48]\tvalidation_0-auc:0.79714\n",
            "[49]\tvalidation_0-auc:0.797483\n",
            "[50]\tvalidation_0-auc:0.798169\n",
            "[51]\tvalidation_0-auc:0.799542\n",
            "[52]\tvalidation_0-auc:0.804691\n",
            "[53]\tvalidation_0-auc:0.813272\n",
            "[54]\tvalidation_0-auc:0.815904\n",
            "[55]\tvalidation_0-auc:0.8246\n",
            "[56]\tvalidation_0-auc:0.820137\n",
            "[57]\tvalidation_0-auc:0.828947\n",
            "[58]\tvalidation_0-auc:0.828604\n",
            "[59]\tvalidation_0-auc:0.827346\n",
            "[60]\tvalidation_0-auc:0.828261\n",
            "[61]\tvalidation_0-auc:0.825515\n",
            "[62]\tvalidation_0-auc:0.819222\n",
            "[63]\tvalidation_0-auc:0.827803\n",
            "[64]\tvalidation_0-auc:0.824828\n",
            "[65]\tvalidation_0-auc:0.825172\n",
            "[66]\tvalidation_0-auc:0.829062\n",
            "[67]\tvalidation_0-auc:0.846911\n",
            "[68]\tvalidation_0-auc:0.853204\n",
            "[69]\tvalidation_0-auc:0.862815\n",
            "[70]\tvalidation_0-auc:0.861785\n",
            "[71]\tvalidation_0-auc:0.858009\n",
            "[72]\tvalidation_0-auc:0.86476\n",
            "[73]\tvalidation_0-auc:0.863844\n",
            "[74]\tvalidation_0-auc:0.864531\n",
            "[75]\tvalidation_0-auc:0.865103\n",
            "[76]\tvalidation_0-auc:0.865561\n",
            "[77]\tvalidation_0-auc:0.862815\n",
            "[78]\tvalidation_0-auc:0.860526\n",
            "[79]\tvalidation_0-auc:0.852975\n",
            "[80]\tvalidation_0-auc:0.854005\n",
            "[81]\tvalidation_0-auc:0.855835\n",
            "[82]\tvalidation_0-auc:0.860526\n",
            "[83]\tvalidation_0-auc:0.858124\n",
            "[84]\tvalidation_0-auc:0.859497\n",
            "[85]\tvalidation_0-auc:0.860298\n",
            "[86]\tvalidation_0-auc:0.864416\n",
            "[87]\tvalidation_0-auc:0.871625\n",
            "[88]\tvalidation_0-auc:0.869565\n",
            "[89]\tvalidation_0-auc:0.871396\n",
            "[90]\tvalidation_0-auc:0.872197\n",
            "[91]\tvalidation_0-auc:0.869794\n",
            "[92]\tvalidation_0-auc:0.869908\n",
            "[93]\tvalidation_0-auc:0.868993\n",
            "[94]\tvalidation_0-auc:0.869908\n",
            "[95]\tvalidation_0-auc:0.870938\n",
            "[96]\tvalidation_0-auc:0.864416\n",
            "[97]\tvalidation_0-auc:0.871854\n",
            "[98]\tvalidation_0-auc:0.868421\n",
            "[99]\tvalidation_0-auc:0.86476\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+----------------------+--------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision       | Recall |       F1 score      |\n",
            "+------------------+--------------------+----------------------+--------+---------------------+\n",
            "|     LSTM 0.2     | 0.4897959183673469 | 0.047244094488188976 |  0.6   | 0.08759124087591241 |\n",
            "|     GRU 0.2      | 0.6530612244897959 | 0.07386363636363637  |  0.65  |  0.1326530612244898 |\n",
            "|   XGBoost 0.2    | 0.7428571428571429 | 0.07936507936507936  |  0.5   | 0.13698630136986303 |\n",
            "|    Logreg 0.2    | 0.6551020408163265 | 0.07428571428571429  |  0.65  | 0.13333333333333333 |\n",
            "|     SVM 0.2      | 0.6673469387755102 | 0.020134228187919462 |  0.15  | 0.03550295857988166 |\n",
            "|  LSTM beta 0.2   | 0.737417943107221  | 0.07627118644067797  |  0.45  | 0.13043478260869565 |\n",
            "|   GRU beta 0.2   | 0.6564551422319475 | 0.07975460122699386  |  0.65  | 0.14207650273224043 |\n",
            "| XGBoost beta 0.2 | 0.7155361050328227 | 0.10714285714285714  |  0.75  |        0.1875       |\n",
            "| logreg beta 0.2  | 0.7352297592997812 | 0.10236220472440945  |  0.65  |  0.1768707482993197 |\n",
            "|   svm beta 0.2   | 0.6739606126914661 | 0.11834319526627218  |  1.0   | 0.21164021164021163 |\n",
            "+------------------+--------------------+----------------------+--------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6968 - accuracy: 0.5060 - val_loss: 0.7242 - val_accuracy: 0.1102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6937 - accuracy: 0.5101 - val_loss: 0.6826 - val_accuracy: 0.8980\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6784 - accuracy: 0.5631 - val_loss: 0.7482 - val_accuracy: 0.4796\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6574 - accuracy: 0.6309 - val_loss: 0.6773 - val_accuracy: 0.5837\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6348 - accuracy: 0.6624 - val_loss: 0.6810 - val_accuracy: 0.6347\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6891 - accuracy: 0.5342 - val_loss: 0.7273 - val_accuracy: 0.3429\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6515 - accuracy: 0.6430 - val_loss: 0.6410 - val_accuracy: 0.7367\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6345 - accuracy: 0.6604 - val_loss: 0.7435 - val_accuracy: 0.5408\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6235 - accuracy: 0.6591 - val_loss: 0.6221 - val_accuracy: 0.6837\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6198 - accuracy: 0.6611 - val_loss: 0.5682 - val_accuracy: 0.7490\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.645897\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.646704\n",
            "[2]\tvalidation_0-auc:0.649507\n",
            "[3]\tvalidation_0-auc:0.644984\n",
            "[4]\tvalidation_0-auc:0.650548\n",
            "[5]\tvalidation_0-auc:0.65866\n",
            "[6]\tvalidation_0-auc:0.654073\n",
            "[7]\tvalidation_0-auc:0.651567\n",
            "[8]\tvalidation_0-auc:0.646088\n",
            "[9]\tvalidation_0-auc:0.651058\n",
            "[10]\tvalidation_0-auc:0.651822\n",
            "[11]\tvalidation_0-auc:0.645345\n",
            "[12]\tvalidation_0-auc:0.646109\n",
            "[13]\tvalidation_0-auc:0.64732\n",
            "[14]\tvalidation_0-auc:0.646683\n",
            "[15]\tvalidation_0-auc:0.648594\n",
            "[16]\tvalidation_0-auc:0.649168\n",
            "[17]\tvalidation_0-auc:0.644602\n",
            "[18]\tvalidation_0-auc:0.63925\n",
            "[19]\tvalidation_0-auc:0.638783\n",
            "[20]\tvalidation_0-auc:0.635682\n",
            "[21]\tvalidation_0-auc:0.634557\n",
            "[22]\tvalidation_0-auc:0.63067\n",
            "[23]\tvalidation_0-auc:0.632454\n",
            "[24]\tvalidation_0-auc:0.628079\n",
            "[25]\tvalidation_0-auc:0.627548\n",
            "[26]\tvalidation_0-auc:0.624745\n",
            "[27]\tvalidation_0-auc:0.621517\n",
            "[28]\tvalidation_0-auc:0.621262\n",
            "[29]\tvalidation_0-auc:0.622069\n",
            "[30]\tvalidation_0-auc:0.620328\n",
            "[31]\tvalidation_0-auc:0.620477\n",
            "[32]\tvalidation_0-auc:0.623428\n",
            "[33]\tvalidation_0-auc:0.622027\n",
            "[34]\tvalidation_0-auc:0.620413\n",
            "[35]\tvalidation_0-auc:0.616081\n",
            "[36]\tvalidation_0-auc:0.613575\n",
            "[37]\tvalidation_0-auc:0.611961\n",
            "[38]\tvalidation_0-auc:0.610665\n",
            "[39]\tvalidation_0-auc:0.608541\n",
            "[40]\tvalidation_0-auc:0.610453\n",
            "[41]\tvalidation_0-auc:0.609773\n",
            "[42]\tvalidation_0-auc:0.609688\n",
            "[43]\tvalidation_0-auc:0.6068\n",
            "[44]\tvalidation_0-auc:0.602892\n",
            "[45]\tvalidation_0-auc:0.601363\n",
            "[46]\tvalidation_0-auc:0.601172\n",
            "[47]\tvalidation_0-auc:0.602446\n",
            "[48]\tvalidation_0-auc:0.603678\n",
            "[49]\tvalidation_0-auc:0.603126\n",
            "[50]\tvalidation_0-auc:0.606057\n",
            "[51]\tvalidation_0-auc:0.603551\n",
            "[52]\tvalidation_0-auc:0.601894\n",
            "[53]\tvalidation_0-auc:0.603041\n",
            "[54]\tvalidation_0-auc:0.601894\n",
            "[55]\tvalidation_0-auc:0.599558\n",
            "Stopping. Best iteration:\n",
            "[5]\tvalidation_0-auc:0.65866\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6973 - accuracy: 0.4900 - val_loss: 0.6579 - val_accuracy: 0.9540\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6864 - accuracy: 0.5724 - val_loss: 0.5927 - val_accuracy: 0.7877\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6472 - accuracy: 0.6500 - val_loss: 0.8015 - val_accuracy: 0.5098\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6449 - accuracy: 0.6328 - val_loss: 0.6329 - val_accuracy: 0.7155\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6175 - accuracy: 0.6740 - val_loss: 0.7343 - val_accuracy: 0.6018\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6802 - accuracy: 0.5772 - val_loss: 0.7726 - val_accuracy: 0.3239\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6381 - accuracy: 0.6671 - val_loss: 0.6657 - val_accuracy: 0.6718\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6080 - accuracy: 0.6953 - val_loss: 0.6455 - val_accuracy: 0.6127\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5953 - accuracy: 0.7042 - val_loss: 0.6839 - val_accuracy: 0.6521\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5751 - accuracy: 0.7104 - val_loss: 0.6356 - val_accuracy: 0.6718\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.432613\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.539155\n",
            "[2]\tvalidation_0-auc:0.573285\n",
            "[3]\tvalidation_0-auc:0.609436\n",
            "[4]\tvalidation_0-auc:0.578528\n",
            "[5]\tvalidation_0-auc:0.578855\n",
            "[6]\tvalidation_0-auc:0.595347\n",
            "[7]\tvalidation_0-auc:0.613914\n",
            "[8]\tvalidation_0-auc:0.621232\n",
            "[9]\tvalidation_0-auc:0.644932\n",
            "[10]\tvalidation_0-auc:0.623198\n",
            "[11]\tvalidation_0-auc:0.669561\n",
            "[12]\tvalidation_0-auc:0.640181\n",
            "[13]\tvalidation_0-auc:0.619321\n",
            "[14]\tvalidation_0-auc:0.6564\n",
            "[15]\tvalidation_0-auc:0.681575\n",
            "[16]\tvalidation_0-auc:0.662789\n",
            "[17]\tvalidation_0-auc:0.691186\n",
            "[18]\tvalidation_0-auc:0.691295\n",
            "[19]\tvalidation_0-auc:0.698504\n",
            "[20]\tvalidation_0-auc:0.705931\n",
            "[21]\tvalidation_0-auc:0.719037\n",
            "[22]\tvalidation_0-auc:0.725917\n",
            "[23]\tvalidation_0-auc:0.732798\n",
            "[24]\tvalidation_0-auc:0.735965\n",
            "[25]\tvalidation_0-auc:0.733453\n",
            "[26]\tvalidation_0-auc:0.723023\n",
            "[27]\tvalidation_0-auc:0.724661\n",
            "[28]\tvalidation_0-auc:0.714613\n",
            "[29]\tvalidation_0-auc:0.703255\n",
            "[30]\tvalidation_0-auc:0.698558\n",
            "[31]\tvalidation_0-auc:0.69692\n",
            "[32]\tvalidation_0-auc:0.714941\n",
            "[33]\tvalidation_0-auc:0.703146\n",
            "[34]\tvalidation_0-auc:0.701398\n",
            "[35]\tvalidation_0-auc:0.69834\n",
            "[36]\tvalidation_0-auc:0.69976\n",
            "[37]\tvalidation_0-auc:0.710135\n",
            "[38]\tvalidation_0-auc:0.704456\n",
            "[39]\tvalidation_0-auc:0.690804\n",
            "[40]\tvalidation_0-auc:0.70533\n",
            "[41]\tvalidation_0-auc:0.709043\n",
            "[42]\tvalidation_0-auc:0.712101\n",
            "[43]\tvalidation_0-auc:0.719747\n",
            "[44]\tvalidation_0-auc:0.718654\n",
            "[45]\tvalidation_0-auc:0.724771\n",
            "[46]\tvalidation_0-auc:0.721931\n",
            "[47]\tvalidation_0-auc:0.721822\n",
            "[48]\tvalidation_0-auc:0.72346\n",
            "[49]\tvalidation_0-auc:0.725754\n",
            "[50]\tvalidation_0-auc:0.731979\n",
            "[51]\tvalidation_0-auc:0.742901\n",
            "[52]\tvalidation_0-auc:0.754478\n",
            "[53]\tvalidation_0-auc:0.752621\n",
            "[54]\tvalidation_0-auc:0.756225\n",
            "[55]\tvalidation_0-auc:0.756662\n",
            "[56]\tvalidation_0-auc:0.760813\n",
            "[57]\tvalidation_0-auc:0.767693\n",
            "[58]\tvalidation_0-auc:0.766383\n",
            "[59]\tvalidation_0-auc:0.76813\n",
            "[60]\tvalidation_0-auc:0.778397\n",
            "[61]\tvalidation_0-auc:0.783639\n",
            "[62]\tvalidation_0-auc:0.784622\n",
            "[63]\tvalidation_0-auc:0.778615\n",
            "[64]\tvalidation_0-auc:0.783858\n",
            "[65]\tvalidation_0-auc:0.783311\n",
            "[66]\tvalidation_0-auc:0.781673\n",
            "[67]\tvalidation_0-auc:0.782984\n",
            "[68]\tvalidation_0-auc:0.788117\n",
            "[69]\tvalidation_0-auc:0.797182\n",
            "[70]\tvalidation_0-auc:0.793141\n",
            "[71]\tvalidation_0-auc:0.792813\n",
            "[72]\tvalidation_0-auc:0.796745\n",
            "[73]\tvalidation_0-auc:0.79194\n",
            "[74]\tvalidation_0-auc:0.797728\n",
            "[75]\tvalidation_0-auc:0.794561\n",
            "[76]\tvalidation_0-auc:0.793141\n",
            "[77]\tvalidation_0-auc:0.797947\n",
            "[78]\tvalidation_0-auc:0.799585\n",
            "[79]\tvalidation_0-auc:0.797837\n",
            "[80]\tvalidation_0-auc:0.79882\n",
            "[81]\tvalidation_0-auc:0.79882\n",
            "[82]\tvalidation_0-auc:0.799694\n",
            "[83]\tvalidation_0-auc:0.797182\n",
            "[84]\tvalidation_0-auc:0.795544\n",
            "[85]\tvalidation_0-auc:0.795544\n",
            "[86]\tvalidation_0-auc:0.796636\n",
            "[87]\tvalidation_0-auc:0.794233\n",
            "[88]\tvalidation_0-auc:0.789428\n",
            "[89]\tvalidation_0-auc:0.789646\n",
            "[90]\tvalidation_0-auc:0.790301\n",
            "[91]\tvalidation_0-auc:0.787462\n",
            "[92]\tvalidation_0-auc:0.790192\n",
            "[93]\tvalidation_0-auc:0.788663\n",
            "[94]\tvalidation_0-auc:0.784622\n",
            "[95]\tvalidation_0-auc:0.780253\n",
            "[96]\tvalidation_0-auc:0.781673\n",
            "[97]\tvalidation_0-auc:0.782329\n",
            "[98]\tvalidation_0-auc:0.785605\n",
            "[99]\tvalidation_0-auc:0.787025\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+----------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision       |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+----------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.6346938775510204 | 0.18592964824120603  |  0.6851851851851852 | 0.29249011857707513 |\n",
            "|      GRU 0.15     | 0.7489795918367347 |  0.2553191489361702  |  0.6666666666666666 | 0.36923076923076914 |\n",
            "|    XGBoost 0.15   | 0.6428571428571429 | 0.18324607329842932  |  0.6481481481481481 |  0.2857142857142857 |\n",
            "|    Logreg 0.15    | 0.6326530612244898 | 0.19117647058823528  |  0.7222222222222222 |  0.3023255813953488 |\n",
            "|      SVM 0.15     | 0.6775510204081633 | 0.22916666666666666  |  0.8148148148148148 |  0.3577235772357723 |\n",
            "|   LSTM beta 0.15  | 0.6017505470459519 | 0.055248618784530384 | 0.47619047619047616 | 0.09900990099009901 |\n",
            "|   GRU beta 0.15   | 0.6717724288840262 | 0.08917197452229299  |  0.6666666666666666 | 0.15730337078651685 |\n",
            "| XGBoost beta 0.15 | 0.6958424507658644 |  0.0958904109589041  |  0.6666666666666666 | 0.16766467065868262 |\n",
            "|  logreg beta 0.15 | 0.6761487964989059 | 0.09032258064516129  |  0.6666666666666666 |  0.1590909090909091 |\n",
            "|   svm beta 0.15   | 0.6608315098468271 | 0.11931818181818182  |         1.0         |  0.2131979695431472 |\n",
            "+-------------------+--------------------+----------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUtL3PM7VrKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e65ba94-eb5e-49bf-d345-ff89ef75c0d7"
      },
      "source": [
        "Result_cross.to_csv('FOX_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.263158</td>\n",
              "      <td>0.651020</td>\n",
              "      <td>0.391459</td>\n",
              "      <td>0.763889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.281250</td>\n",
              "      <td>0.681633</td>\n",
              "      <td>0.409091</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.278689</td>\n",
              "      <td>0.687755</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.708333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.251163</td>\n",
              "      <td>0.634694</td>\n",
              "      <td>0.376307</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.299492</td>\n",
              "      <td>0.691837</td>\n",
              "      <td>0.438662</td>\n",
              "      <td>0.819444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.181159</td>\n",
              "      <td>0.722101</td>\n",
              "      <td>0.282486</td>\n",
              "      <td>0.641026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.172414</td>\n",
              "      <td>0.706783</td>\n",
              "      <td>0.271739</td>\n",
              "      <td>0.641026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.149425</td>\n",
              "      <td>0.647702</td>\n",
              "      <td>0.244131</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.158824</td>\n",
              "      <td>0.660832</td>\n",
              "      <td>0.258373</td>\n",
              "      <td>0.692308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.625821</td>\n",
              "      <td>0.278481</td>\n",
              "      <td>0.846154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.047244</td>\n",
              "      <td>0.489796</td>\n",
              "      <td>0.087591</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.073864</td>\n",
              "      <td>0.653061</td>\n",
              "      <td>0.132653</td>\n",
              "      <td>0.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.079365</td>\n",
              "      <td>0.742857</td>\n",
              "      <td>0.136986</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.074286</td>\n",
              "      <td>0.655102</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.020134</td>\n",
              "      <td>0.667347</td>\n",
              "      <td>0.035503</td>\n",
              "      <td>0.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.076271</td>\n",
              "      <td>0.737418</td>\n",
              "      <td>0.130435</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.079755</td>\n",
              "      <td>0.656455</td>\n",
              "      <td>0.142077</td>\n",
              "      <td>0.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.107143</td>\n",
              "      <td>0.715536</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.102362</td>\n",
              "      <td>0.735230</td>\n",
              "      <td>0.176871</td>\n",
              "      <td>0.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.118343</td>\n",
              "      <td>0.673961</td>\n",
              "      <td>0.211640</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.185930</td>\n",
              "      <td>0.634694</td>\n",
              "      <td>0.292490</td>\n",
              "      <td>0.685185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.255319</td>\n",
              "      <td>0.748980</td>\n",
              "      <td>0.369231</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.183246</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.648148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.191176</td>\n",
              "      <td>0.632653</td>\n",
              "      <td>0.302326</td>\n",
              "      <td>0.722222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.229167</td>\n",
              "      <td>0.677551</td>\n",
              "      <td>0.357724</td>\n",
              "      <td>0.814815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.055249</td>\n",
              "      <td>0.601751</td>\n",
              "      <td>0.099010</td>\n",
              "      <td>0.476190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.089172</td>\n",
              "      <td>0.671772</td>\n",
              "      <td>0.157303</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.095890</td>\n",
              "      <td>0.695842</td>\n",
              "      <td>0.167665</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.090323</td>\n",
              "      <td>0.676149</td>\n",
              "      <td>0.159091</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.119318</td>\n",
              "      <td>0.660832</td>\n",
              "      <td>0.213198</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  FOX  0.263158  0.651020  0.391459  0.763889\n",
              "1            GRU 0.1  FOX  0.281250  0.681633  0.409091  0.750000\n",
              "2        XGBoost 0.1  FOX  0.278689  0.687755  0.400000  0.708333\n",
              "3         Logreg 0.1  FOX  0.251163  0.634694  0.376307  0.750000\n",
              "4            SVM 0.1  FOX  0.299492  0.691837  0.438662  0.819444\n",
              "5      LSTM beta 0.1  FOX  0.181159  0.722101  0.282486  0.641026\n",
              "6       GRU beta 0.1  FOX  0.172414  0.706783  0.271739  0.641026\n",
              "7   XGBoost beta 0.1  FOX  0.149425  0.647702  0.244131  0.666667\n",
              "8    logreg beta 0.1  FOX  0.158824  0.660832  0.258373  0.692308\n",
              "9       svm beta 0.1  FOX  0.166667  0.625821  0.278481  0.846154\n",
              "0           LSTM 0.2  FOX  0.047244  0.489796  0.087591  0.600000\n",
              "1            GRU 0.2  FOX  0.073864  0.653061  0.132653  0.650000\n",
              "2        XGBoost 0.2  FOX  0.079365  0.742857  0.136986  0.500000\n",
              "3         Logreg 0.2  FOX  0.074286  0.655102  0.133333  0.650000\n",
              "4            SVM 0.2  FOX  0.020134  0.667347  0.035503  0.150000\n",
              "5      LSTM beta 0.2  FOX  0.076271  0.737418  0.130435  0.450000\n",
              "6       GRU beta 0.2  FOX  0.079755  0.656455  0.142077  0.650000\n",
              "7   XGBoost beta 0.2  FOX  0.107143  0.715536  0.187500  0.750000\n",
              "8    logreg beta 0.2  FOX  0.102362  0.735230  0.176871  0.650000\n",
              "9       svm beta 0.2  FOX  0.118343  0.673961  0.211640  1.000000\n",
              "0          LSTM 0.15  FOX  0.185930  0.634694  0.292490  0.685185\n",
              "1           GRU 0.15  FOX  0.255319  0.748980  0.369231  0.666667\n",
              "2       XGBoost 0.15  FOX  0.183246  0.642857  0.285714  0.648148\n",
              "3        Logreg 0.15  FOX  0.191176  0.632653  0.302326  0.722222\n",
              "4           SVM 0.15  FOX  0.229167  0.677551  0.357724  0.814815\n",
              "5     LSTM beta 0.15  FOX  0.055249  0.601751  0.099010  0.476190\n",
              "6      GRU beta 0.15  FOX  0.089172  0.671772  0.157303  0.666667\n",
              "7  XGBoost beta 0.15  FOX  0.095890  0.695842  0.167665  0.666667\n",
              "8   logreg beta 0.15  FOX  0.090323  0.676149  0.159091  0.666667\n",
              "9      svm beta 0.15  FOX  0.119318  0.660832  0.213198  1.000000"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEuqZeD1VrKf"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_logreg_beta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1Bih0EGVrKf"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umveLR73VrKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e4f46c-6e65-4e3b-c1b4-2b796eb39be2"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"FOX\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6968 - accuracy: 0.4893 - val_loss: 0.7350 - val_accuracy: 0.1408\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6894 - accuracy: 0.5262 - val_loss: 0.8239 - val_accuracy: 0.1327\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6544 - accuracy: 0.6396 - val_loss: 0.5856 - val_accuracy: 0.7735\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6273 - accuracy: 0.6732 - val_loss: 0.7838 - val_accuracy: 0.4959\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6338 - accuracy: 0.6658 - val_loss: 0.6133 - val_accuracy: 0.7041\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6896 - accuracy: 0.5309 - val_loss: 0.7469 - val_accuracy: 0.2000\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6531 - accuracy: 0.6154 - val_loss: 0.8913 - val_accuracy: 0.2490\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6222 - accuracy: 0.6644 - val_loss: 0.7317 - val_accuracy: 0.5388\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6041 - accuracy: 0.6906 - val_loss: 0.6294 - val_accuracy: 0.6980\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5951 - accuracy: 0.6973 - val_loss: 0.7271 - val_accuracy: 0.6224\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.697867\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.707486\n",
            "[2]\tvalidation_0-auc:0.722023\n",
            "[3]\tvalidation_0-auc:0.750748\n",
            "[4]\tvalidation_0-auc:0.720195\n",
            "[5]\tvalidation_0-auc:0.720096\n",
            "[6]\tvalidation_0-auc:0.746478\n",
            "[7]\tvalidation_0-auc:0.742507\n",
            "[8]\tvalidation_0-auc:0.741893\n",
            "[9]\tvalidation_0-auc:0.742109\n",
            "[10]\tvalidation_0-auc:0.741145\n",
            "[11]\tvalidation_0-auc:0.744086\n",
            "[12]\tvalidation_0-auc:0.754934\n",
            "[13]\tvalidation_0-auc:0.759652\n",
            "[14]\tvalidation_0-auc:0.761247\n",
            "[15]\tvalidation_0-auc:0.760201\n",
            "[16]\tvalidation_0-auc:0.75927\n",
            "[17]\tvalidation_0-auc:0.760516\n",
            "[18]\tvalidation_0-auc:0.76369\n",
            "[19]\tvalidation_0-auc:0.76359\n",
            "[20]\tvalidation_0-auc:0.762892\n",
            "[21]\tvalidation_0-auc:0.763175\n",
            "[22]\tvalidation_0-auc:0.761945\n",
            "[23]\tvalidation_0-auc:0.758639\n",
            "[24]\tvalidation_0-auc:0.755948\n",
            "[25]\tvalidation_0-auc:0.756911\n",
            "[26]\tvalidation_0-auc:0.757011\n",
            "[27]\tvalidation_0-auc:0.756247\n",
            "[28]\tvalidation_0-auc:0.753921\n",
            "[29]\tvalidation_0-auc:0.754685\n",
            "[30]\tvalidation_0-auc:0.752841\n",
            "[31]\tvalidation_0-auc:0.748937\n",
            "[32]\tvalidation_0-auc:0.749502\n",
            "[33]\tvalidation_0-auc:0.747574\n",
            "[34]\tvalidation_0-auc:0.748505\n",
            "[35]\tvalidation_0-auc:0.747807\n",
            "[36]\tvalidation_0-auc:0.745016\n",
            "[37]\tvalidation_0-auc:0.741826\n",
            "[38]\tvalidation_0-auc:0.737141\n",
            "[39]\tvalidation_0-auc:0.735879\n",
            "[40]\tvalidation_0-auc:0.734981\n",
            "[41]\tvalidation_0-auc:0.734317\n",
            "[42]\tvalidation_0-auc:0.734948\n",
            "[43]\tvalidation_0-auc:0.732024\n",
            "[44]\tvalidation_0-auc:0.729034\n",
            "[45]\tvalidation_0-auc:0.727638\n",
            "[46]\tvalidation_0-auc:0.726974\n",
            "[47]\tvalidation_0-auc:0.729648\n",
            "[48]\tvalidation_0-auc:0.73131\n",
            "[49]\tvalidation_0-auc:0.733137\n",
            "[50]\tvalidation_0-auc:0.731376\n",
            "[51]\tvalidation_0-auc:0.731675\n",
            "[52]\tvalidation_0-auc:0.732838\n",
            "[53]\tvalidation_0-auc:0.735496\n",
            "[54]\tvalidation_0-auc:0.734034\n",
            "[55]\tvalidation_0-auc:0.7344\n",
            "[56]\tvalidation_0-auc:0.73327\n",
            "[57]\tvalidation_0-auc:0.733968\n",
            "[58]\tvalidation_0-auc:0.731509\n",
            "[59]\tvalidation_0-auc:0.734034\n",
            "[60]\tvalidation_0-auc:0.733071\n",
            "[61]\tvalidation_0-auc:0.732041\n",
            "[62]\tvalidation_0-auc:0.732805\n",
            "[63]\tvalidation_0-auc:0.730712\n",
            "[64]\tvalidation_0-auc:0.729748\n",
            "[65]\tvalidation_0-auc:0.728901\n",
            "[66]\tvalidation_0-auc:0.723983\n",
            "[67]\tvalidation_0-auc:0.722986\n",
            "[68]\tvalidation_0-auc:0.722222\n",
            "Stopping. Best iteration:\n",
            "[18]\tvalidation_0-auc:0.76369\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6946 - accuracy: 0.5168 - val_loss: 0.6576 - val_accuracy: 0.9147\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6909 - accuracy: 0.5251 - val_loss: 0.7427 - val_accuracy: 0.0635\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6720 - accuracy: 0.5992 - val_loss: 0.7575 - val_accuracy: 0.3720\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6398 - accuracy: 0.6644 - val_loss: 0.6265 - val_accuracy: 0.7243\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6328 - accuracy: 0.6541 - val_loss: 0.7363 - val_accuracy: 0.5492\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6854 - accuracy: 0.5511 - val_loss: 0.5899 - val_accuracy: 0.8796\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6538 - accuracy: 0.6095 - val_loss: 0.5880 - val_accuracy: 0.8009\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6227 - accuracy: 0.6726 - val_loss: 0.8424 - val_accuracy: 0.4026\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6050 - accuracy: 0.6891 - val_loss: 0.6661 - val_accuracy: 0.6455\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6004 - accuracy: 0.6973 - val_loss: 0.6063 - val_accuracy: 0.7243\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.697246\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.66912\n",
            "[2]\tvalidation_0-auc:0.665593\n",
            "[3]\tvalidation_0-auc:0.678322\n",
            "[4]\tvalidation_0-auc:0.694363\n",
            "[5]\tvalidation_0-auc:0.6874\n",
            "[6]\tvalidation_0-auc:0.697951\n",
            "[7]\tvalidation_0-auc:0.705435\n",
            "[8]\tvalidation_0-auc:0.707797\n",
            "[9]\tvalidation_0-auc:0.71252\n",
            "[10]\tvalidation_0-auc:0.717427\n",
            "[11]\tvalidation_0-auc:0.739326\n",
            "[12]\tvalidation_0-auc:0.739541\n",
            "[13]\tvalidation_0-auc:0.735401\n",
            "[14]\tvalidation_0-auc:0.741903\n",
            "[15]\tvalidation_0-auc:0.75595\n",
            "[16]\tvalidation_0-auc:0.753926\n",
            "[17]\tvalidation_0-auc:0.754754\n",
            "[18]\tvalidation_0-auc:0.752055\n",
            "[19]\tvalidation_0-auc:0.748834\n",
            "[20]\tvalidation_0-auc:0.756196\n",
            "[21]\tvalidation_0-auc:0.756318\n",
            "[22]\tvalidation_0-auc:0.756165\n",
            "[23]\tvalidation_0-auc:0.751564\n",
            "[24]\tvalidation_0-auc:0.752699\n",
            "[25]\tvalidation_0-auc:0.748282\n",
            "[26]\tvalidation_0-auc:0.7427\n",
            "[27]\tvalidation_0-auc:0.751718\n",
            "[28]\tvalidation_0-auc:0.758987\n",
            "[29]\tvalidation_0-auc:0.761072\n",
            "[30]\tvalidation_0-auc:0.769783\n",
            "[31]\tvalidation_0-auc:0.760766\n",
            "[32]\tvalidation_0-auc:0.769906\n",
            "[33]\tvalidation_0-auc:0.768617\n",
            "[34]\tvalidation_0-auc:0.774353\n",
            "[35]\tvalidation_0-auc:0.778524\n",
            "[36]\tvalidation_0-auc:0.776622\n",
            "[37]\tvalidation_0-auc:0.779414\n",
            "[38]\tvalidation_0-auc:0.776653\n",
            "[39]\tvalidation_0-auc:0.77607\n",
            "[40]\tvalidation_0-auc:0.774844\n",
            "[41]\tvalidation_0-auc:0.779444\n",
            "[42]\tvalidation_0-auc:0.779475\n",
            "[43]\tvalidation_0-auc:0.779414\n",
            "[44]\tvalidation_0-auc:0.777389\n",
            "[45]\tvalidation_0-auc:0.775058\n",
            "[46]\tvalidation_0-auc:0.771286\n",
            "[47]\tvalidation_0-auc:0.770304\n",
            "[48]\tvalidation_0-auc:0.769752\n",
            "[49]\tvalidation_0-auc:0.771654\n",
            "[50]\tvalidation_0-auc:0.772083\n",
            "[51]\tvalidation_0-auc:0.776009\n",
            "[52]\tvalidation_0-auc:0.776561\n",
            "[53]\tvalidation_0-auc:0.774537\n",
            "[54]\tvalidation_0-auc:0.770243\n",
            "[55]\tvalidation_0-auc:0.769752\n",
            "[56]\tvalidation_0-auc:0.773157\n",
            "[57]\tvalidation_0-auc:0.774322\n",
            "[58]\tvalidation_0-auc:0.773954\n",
            "[59]\tvalidation_0-auc:0.771132\n",
            "[60]\tvalidation_0-auc:0.770335\n",
            "[61]\tvalidation_0-auc:0.771194\n",
            "[62]\tvalidation_0-auc:0.772175\n",
            "[63]\tvalidation_0-auc:0.771194\n",
            "[64]\tvalidation_0-auc:0.772114\n",
            "[65]\tvalidation_0-auc:0.772237\n",
            "[66]\tvalidation_0-auc:0.772421\n",
            "[67]\tvalidation_0-auc:0.770274\n",
            "[68]\tvalidation_0-auc:0.774445\n",
            "[69]\tvalidation_0-auc:0.769906\n",
            "[70]\tvalidation_0-auc:0.769047\n",
            "[71]\tvalidation_0-auc:0.769353\n",
            "[72]\tvalidation_0-auc:0.766961\n",
            "[73]\tvalidation_0-auc:0.766164\n",
            "[74]\tvalidation_0-auc:0.767145\n",
            "[75]\tvalidation_0-auc:0.770764\n",
            "[76]\tvalidation_0-auc:0.77058\n",
            "[77]\tvalidation_0-auc:0.772237\n",
            "[78]\tvalidation_0-auc:0.774138\n",
            "[79]\tvalidation_0-auc:0.778064\n",
            "[80]\tvalidation_0-auc:0.777573\n",
            "[81]\tvalidation_0-auc:0.775672\n",
            "[82]\tvalidation_0-auc:0.775304\n",
            "[83]\tvalidation_0-auc:0.776592\n",
            "[84]\tvalidation_0-auc:0.774813\n",
            "[85]\tvalidation_0-auc:0.780456\n",
            "[86]\tvalidation_0-auc:0.780272\n",
            "[87]\tvalidation_0-auc:0.779291\n",
            "[88]\tvalidation_0-auc:0.778187\n",
            "[89]\tvalidation_0-auc:0.77972\n",
            "[90]\tvalidation_0-auc:0.780334\n",
            "[91]\tvalidation_0-auc:0.781131\n",
            "[92]\tvalidation_0-auc:0.781683\n",
            "[93]\tvalidation_0-auc:0.78199\n",
            "[94]\tvalidation_0-auc:0.784076\n",
            "[95]\tvalidation_0-auc:0.780947\n",
            "[96]\tvalidation_0-auc:0.779475\n",
            "[97]\tvalidation_0-auc:0.77972\n",
            "[98]\tvalidation_0-auc:0.781929\n",
            "[99]\tvalidation_0-auc:0.782174\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.7040816326530612 | 0.28402366863905326 | 0.6666666666666666 | 0.39834024896265563 |\n",
            "|     GRU 0.1      | 0.6224489795918368 | 0.24663677130044842 | 0.7638888888888888 |  0.3728813559322034 |\n",
            "|   XGBoost 0.1    | 0.6877551020408164 |  0.2786885245901639 | 0.7083333333333334 | 0.39999999999999997 |\n",
            "|    Logreg 0.1    | 0.6346938775510204 | 0.25116279069767444 |        0.75        |  0.3763066202090593 |\n",
            "|     SVM 0.1      | 0.6918367346938775 | 0.29949238578680204 | 0.8194444444444444 |  0.4386617100371747 |\n",
            "|  LSTM beta 0.1   | 0.5492341356673961 |  0.1187214611872146 | 0.6666666666666666 | 0.20155038759689922 |\n",
            "|   GRU beta 0.1   | 0.7242888402625821 | 0.18705035971223022 | 0.6666666666666666 | 0.29213483146067415 |\n",
            "| XGBoost beta 0.1 | 0.6477024070021882 | 0.14942528735632185 | 0.6666666666666666 | 0.24413145539906106 |\n",
            "| logreg beta 0.1  | 0.6608315098468271 |  0.1588235294117647 | 0.6923076923076923 | 0.25837320574162675 |\n",
            "|   svm beta 0.1   | 0.6258205689277899 | 0.16666666666666666 | 0.8461538461538461 | 0.27848101265822783 |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6957 - accuracy: 0.4899 - val_loss: 0.7192 - val_accuracy: 0.0408\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6951 - accuracy: 0.4953 - val_loss: 0.6540 - val_accuracy: 0.9592\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6943 - accuracy: 0.5000 - val_loss: 0.6944 - val_accuracy: 0.4184\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6806 - accuracy: 0.5792 - val_loss: 0.6979 - val_accuracy: 0.5510\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6700 - accuracy: 0.6074 - val_loss: 0.8706 - val_accuracy: 0.1143\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6881 - accuracy: 0.5369 - val_loss: 0.7488 - val_accuracy: 0.1735\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6613 - accuracy: 0.6067 - val_loss: 0.6169 - val_accuracy: 0.7776\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6516 - accuracy: 0.6275 - val_loss: 0.7026 - val_accuracy: 0.6184\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6429 - accuracy: 0.6456 - val_loss: 0.7235 - val_accuracy: 0.5796\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6440 - accuracy: 0.6409 - val_loss: 0.6462 - val_accuracy: 0.6918\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.555851\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.47883\n",
            "[2]\tvalidation_0-auc:0.524521\n",
            "[3]\tvalidation_0-auc:0.525798\n",
            "[4]\tvalidation_0-auc:0.475957\n",
            "[5]\tvalidation_0-auc:0.491489\n",
            "[6]\tvalidation_0-auc:0.483777\n",
            "[7]\tvalidation_0-auc:0.481915\n",
            "[8]\tvalidation_0-auc:0.472979\n",
            "[9]\tvalidation_0-auc:0.46766\n",
            "[10]\tvalidation_0-auc:0.448191\n",
            "[11]\tvalidation_0-auc:0.455957\n",
            "[12]\tvalidation_0-auc:0.455106\n",
            "[13]\tvalidation_0-auc:0.446277\n",
            "[14]\tvalidation_0-auc:0.447128\n",
            "[15]\tvalidation_0-auc:0.441702\n",
            "[16]\tvalidation_0-auc:0.41234\n",
            "[17]\tvalidation_0-auc:0.420638\n",
            "[18]\tvalidation_0-auc:0.422128\n",
            "[19]\tvalidation_0-auc:0.408617\n",
            "[20]\tvalidation_0-auc:0.404149\n",
            "[21]\tvalidation_0-auc:0.396809\n",
            "[22]\tvalidation_0-auc:0.395\n",
            "[23]\tvalidation_0-auc:0.394149\n",
            "[24]\tvalidation_0-auc:0.381809\n",
            "[25]\tvalidation_0-auc:0.383936\n",
            "[26]\tvalidation_0-auc:0.376489\n",
            "[27]\tvalidation_0-auc:0.380851\n",
            "[28]\tvalidation_0-auc:0.383723\n",
            "[29]\tvalidation_0-auc:0.388191\n",
            "[30]\tvalidation_0-auc:0.375851\n",
            "[31]\tvalidation_0-auc:0.38\n",
            "[32]\tvalidation_0-auc:0.377872\n",
            "[33]\tvalidation_0-auc:0.368511\n",
            "[34]\tvalidation_0-auc:0.371808\n",
            "[35]\tvalidation_0-auc:0.367128\n",
            "[36]\tvalidation_0-auc:0.359787\n",
            "[37]\tvalidation_0-auc:0.361809\n",
            "[38]\tvalidation_0-auc:0.36383\n",
            "[39]\tvalidation_0-auc:0.37266\n",
            "[40]\tvalidation_0-auc:0.371383\n",
            "[41]\tvalidation_0-auc:0.367553\n",
            "[42]\tvalidation_0-auc:0.364894\n",
            "[43]\tvalidation_0-auc:0.35883\n",
            "[44]\tvalidation_0-auc:0.363404\n",
            "[45]\tvalidation_0-auc:0.364681\n",
            "[46]\tvalidation_0-auc:0.364043\n",
            "[47]\tvalidation_0-auc:0.361277\n",
            "[48]\tvalidation_0-auc:0.35234\n",
            "[49]\tvalidation_0-auc:0.346064\n",
            "[50]\tvalidation_0-auc:0.344681\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.555851\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6915 - accuracy: 0.5065 - val_loss: 0.6505 - val_accuracy: 0.7440\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6711 - accuracy: 0.6095 - val_loss: 0.6372 - val_accuracy: 0.6586\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6616 - accuracy: 0.6253 - val_loss: 0.5745 - val_accuracy: 0.7309\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6547 - accuracy: 0.6266 - val_loss: 0.7031 - val_accuracy: 0.5974\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6523 - accuracy: 0.6500 - val_loss: 0.6002 - val_accuracy: 0.7287\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6782 - accuracy: 0.5710 - val_loss: 0.6704 - val_accuracy: 0.7112\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6490 - accuracy: 0.6225 - val_loss: 0.7608 - val_accuracy: 0.4967\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6392 - accuracy: 0.6616 - val_loss: 0.6041 - val_accuracy: 0.7702\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6303 - accuracy: 0.6513 - val_loss: 0.6410 - val_accuracy: 0.7243\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6243 - accuracy: 0.6699 - val_loss: 0.6192 - val_accuracy: 0.7462\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.407208\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.320881\n",
            "[2]\tvalidation_0-auc:0.566762\n",
            "[3]\tvalidation_0-auc:0.606007\n",
            "[4]\tvalidation_0-auc:0.661613\n",
            "[5]\tvalidation_0-auc:0.691934\n",
            "[6]\tvalidation_0-auc:0.691247\n",
            "[7]\tvalidation_0-auc:0.701087\n",
            "[8]\tvalidation_0-auc:0.708181\n",
            "[9]\tvalidation_0-auc:0.724313\n",
            "[10]\tvalidation_0-auc:0.726201\n",
            "[11]\tvalidation_0-auc:0.733181\n",
            "[12]\tvalidation_0-auc:0.742105\n",
            "[13]\tvalidation_0-auc:0.757208\n",
            "[14]\tvalidation_0-auc:0.740503\n",
            "[15]\tvalidation_0-auc:0.759725\n",
            "[16]\tvalidation_0-auc:0.767391\n",
            "[17]\tvalidation_0-auc:0.774485\n",
            "[18]\tvalidation_0-auc:0.756064\n",
            "[19]\tvalidation_0-auc:0.768535\n",
            "[20]\tvalidation_0-auc:0.750915\n",
            "[21]\tvalidation_0-auc:0.766705\n",
            "[22]\tvalidation_0-auc:0.775858\n",
            "[23]\tvalidation_0-auc:0.760755\n",
            "[24]\tvalidation_0-auc:0.76762\n",
            "[25]\tvalidation_0-auc:0.757895\n",
            "[26]\tvalidation_0-auc:0.738215\n",
            "[27]\tvalidation_0-auc:0.735355\n",
            "[28]\tvalidation_0-auc:0.74119\n",
            "[29]\tvalidation_0-auc:0.731693\n",
            "[30]\tvalidation_0-auc:0.730549\n",
            "[31]\tvalidation_0-auc:0.70881\n",
            "[32]\tvalidation_0-auc:0.722654\n",
            "[33]\tvalidation_0-auc:0.733524\n",
            "[34]\tvalidation_0-auc:0.739588\n",
            "[35]\tvalidation_0-auc:0.756178\n",
            "[36]\tvalidation_0-auc:0.759611\n",
            "[37]\tvalidation_0-auc:0.763616\n",
            "[38]\tvalidation_0-auc:0.770366\n",
            "[39]\tvalidation_0-auc:0.771053\n",
            "[40]\tvalidation_0-auc:0.774714\n",
            "[41]\tvalidation_0-auc:0.76476\n",
            "[42]\tvalidation_0-auc:0.775744\n",
            "[43]\tvalidation_0-auc:0.774485\n",
            "[44]\tvalidation_0-auc:0.77151\n",
            "[45]\tvalidation_0-auc:0.7746\n",
            "[46]\tvalidation_0-auc:0.783753\n",
            "[47]\tvalidation_0-auc:0.789016\n",
            "[48]\tvalidation_0-auc:0.79714\n",
            "[49]\tvalidation_0-auc:0.797483\n",
            "[50]\tvalidation_0-auc:0.798169\n",
            "[51]\tvalidation_0-auc:0.799542\n",
            "[52]\tvalidation_0-auc:0.804691\n",
            "[53]\tvalidation_0-auc:0.813272\n",
            "[54]\tvalidation_0-auc:0.815904\n",
            "[55]\tvalidation_0-auc:0.8246\n",
            "[56]\tvalidation_0-auc:0.820137\n",
            "[57]\tvalidation_0-auc:0.828947\n",
            "[58]\tvalidation_0-auc:0.828604\n",
            "[59]\tvalidation_0-auc:0.827346\n",
            "[60]\tvalidation_0-auc:0.828261\n",
            "[61]\tvalidation_0-auc:0.825515\n",
            "[62]\tvalidation_0-auc:0.819222\n",
            "[63]\tvalidation_0-auc:0.827803\n",
            "[64]\tvalidation_0-auc:0.824828\n",
            "[65]\tvalidation_0-auc:0.825172\n",
            "[66]\tvalidation_0-auc:0.829062\n",
            "[67]\tvalidation_0-auc:0.846911\n",
            "[68]\tvalidation_0-auc:0.853204\n",
            "[69]\tvalidation_0-auc:0.862815\n",
            "[70]\tvalidation_0-auc:0.861785\n",
            "[71]\tvalidation_0-auc:0.858009\n",
            "[72]\tvalidation_0-auc:0.86476\n",
            "[73]\tvalidation_0-auc:0.863844\n",
            "[74]\tvalidation_0-auc:0.864531\n",
            "[75]\tvalidation_0-auc:0.865103\n",
            "[76]\tvalidation_0-auc:0.865561\n",
            "[77]\tvalidation_0-auc:0.862815\n",
            "[78]\tvalidation_0-auc:0.860526\n",
            "[79]\tvalidation_0-auc:0.852975\n",
            "[80]\tvalidation_0-auc:0.854005\n",
            "[81]\tvalidation_0-auc:0.855835\n",
            "[82]\tvalidation_0-auc:0.860526\n",
            "[83]\tvalidation_0-auc:0.858124\n",
            "[84]\tvalidation_0-auc:0.859497\n",
            "[85]\tvalidation_0-auc:0.860298\n",
            "[86]\tvalidation_0-auc:0.864416\n",
            "[87]\tvalidation_0-auc:0.871625\n",
            "[88]\tvalidation_0-auc:0.869565\n",
            "[89]\tvalidation_0-auc:0.871396\n",
            "[90]\tvalidation_0-auc:0.872197\n",
            "[91]\tvalidation_0-auc:0.869794\n",
            "[92]\tvalidation_0-auc:0.869908\n",
            "[93]\tvalidation_0-auc:0.868993\n",
            "[94]\tvalidation_0-auc:0.869908\n",
            "[95]\tvalidation_0-auc:0.870938\n",
            "[96]\tvalidation_0-auc:0.864416\n",
            "[97]\tvalidation_0-auc:0.871854\n",
            "[98]\tvalidation_0-auc:0.868421\n",
            "[99]\tvalidation_0-auc:0.86476\n",
            "end training. \n",
            "\n",
            "+------------------+---------------------+----------------------+--------+---------------------+\n",
            "|      Model       |       Accuracy      |      Precision       | Recall |       F1 score      |\n",
            "+------------------+---------------------+----------------------+--------+---------------------+\n",
            "|     LSTM 0.2     | 0.11428571428571428 |  0.0273972602739726  |  0.6   | 0.05240174672489083 |\n",
            "|     GRU 0.2      |  0.6918367346938775 | 0.07741935483870968  |  0.6   | 0.13714285714285715 |\n",
            "|   XGBoost 0.2    |  0.7428571428571429 | 0.07936507936507936  |  0.5   | 0.13698630136986303 |\n",
            "|    Logreg 0.2    |  0.6551020408163265 | 0.07428571428571429  |  0.65  | 0.13333333333333333 |\n",
            "|     SVM 0.2      |  0.6673469387755102 | 0.020134228187919462 |  0.15  | 0.03550295857988166 |\n",
            "|  LSTM beta 0.2   |  0.7286652078774617 | 0.07377049180327869  |  0.45  |  0.1267605633802817 |\n",
            "|   GRU beta 0.2   |  0.7461706783369803 |         0.1          |  0.6   | 0.17142857142857143 |\n",
            "| XGBoost beta 0.2 |  0.7155361050328227 | 0.10714285714285714  |  0.75  |        0.1875       |\n",
            "| logreg beta 0.2  |  0.7352297592997812 | 0.10236220472440945  |  0.65  |  0.1768707482993197 |\n",
            "|   svm beta 0.2   |  0.6739606126914661 | 0.11834319526627218  |  1.0   | 0.21164021164021163 |\n",
            "+------------------+---------------------+----------------------+--------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6938 - accuracy: 0.5322 - val_loss: 0.7686 - val_accuracy: 0.1102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6911 - accuracy: 0.5228 - val_loss: 0.7093 - val_accuracy: 0.0939\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6665 - accuracy: 0.5953 - val_loss: 0.6726 - val_accuracy: 0.7122\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6700 - accuracy: 0.5779 - val_loss: 0.6764 - val_accuracy: 0.6429\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6448 - accuracy: 0.6530 - val_loss: 0.8857 - val_accuracy: 0.3306\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 17ms/step - loss: 0.6880 - accuracy: 0.5430 - val_loss: 0.6521 - val_accuracy: 0.8082\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6566 - accuracy: 0.6268 - val_loss: 0.6216 - val_accuracy: 0.7735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6291 - accuracy: 0.6698 - val_loss: 0.6825 - val_accuracy: 0.6347\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6342 - accuracy: 0.6758 - val_loss: 0.7914 - val_accuracy: 0.4367\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6206 - accuracy: 0.6852 - val_loss: 0.7014 - val_accuracy: 0.6102\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.645897\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.646704\n",
            "[2]\tvalidation_0-auc:0.649507\n",
            "[3]\tvalidation_0-auc:0.644984\n",
            "[4]\tvalidation_0-auc:0.650548\n",
            "[5]\tvalidation_0-auc:0.65866\n",
            "[6]\tvalidation_0-auc:0.654073\n",
            "[7]\tvalidation_0-auc:0.651567\n",
            "[8]\tvalidation_0-auc:0.646088\n",
            "[9]\tvalidation_0-auc:0.651058\n",
            "[10]\tvalidation_0-auc:0.651822\n",
            "[11]\tvalidation_0-auc:0.645345\n",
            "[12]\tvalidation_0-auc:0.646109\n",
            "[13]\tvalidation_0-auc:0.64732\n",
            "[14]\tvalidation_0-auc:0.646683\n",
            "[15]\tvalidation_0-auc:0.648594\n",
            "[16]\tvalidation_0-auc:0.649168\n",
            "[17]\tvalidation_0-auc:0.644602\n",
            "[18]\tvalidation_0-auc:0.63925\n",
            "[19]\tvalidation_0-auc:0.638783\n",
            "[20]\tvalidation_0-auc:0.635682\n",
            "[21]\tvalidation_0-auc:0.634557\n",
            "[22]\tvalidation_0-auc:0.63067\n",
            "[23]\tvalidation_0-auc:0.632454\n",
            "[24]\tvalidation_0-auc:0.628079\n",
            "[25]\tvalidation_0-auc:0.627548\n",
            "[26]\tvalidation_0-auc:0.624745\n",
            "[27]\tvalidation_0-auc:0.621517\n",
            "[28]\tvalidation_0-auc:0.621262\n",
            "[29]\tvalidation_0-auc:0.622069\n",
            "[30]\tvalidation_0-auc:0.620328\n",
            "[31]\tvalidation_0-auc:0.620477\n",
            "[32]\tvalidation_0-auc:0.623428\n",
            "[33]\tvalidation_0-auc:0.622027\n",
            "[34]\tvalidation_0-auc:0.620413\n",
            "[35]\tvalidation_0-auc:0.616081\n",
            "[36]\tvalidation_0-auc:0.613575\n",
            "[37]\tvalidation_0-auc:0.611961\n",
            "[38]\tvalidation_0-auc:0.610665\n",
            "[39]\tvalidation_0-auc:0.608541\n",
            "[40]\tvalidation_0-auc:0.610453\n",
            "[41]\tvalidation_0-auc:0.609773\n",
            "[42]\tvalidation_0-auc:0.609688\n",
            "[43]\tvalidation_0-auc:0.6068\n",
            "[44]\tvalidation_0-auc:0.602892\n",
            "[45]\tvalidation_0-auc:0.601363\n",
            "[46]\tvalidation_0-auc:0.601172\n",
            "[47]\tvalidation_0-auc:0.602446\n",
            "[48]\tvalidation_0-auc:0.603678\n",
            "[49]\tvalidation_0-auc:0.603126\n",
            "[50]\tvalidation_0-auc:0.606057\n",
            "[51]\tvalidation_0-auc:0.603551\n",
            "[52]\tvalidation_0-auc:0.601894\n",
            "[53]\tvalidation_0-auc:0.603041\n",
            "[54]\tvalidation_0-auc:0.601894\n",
            "[55]\tvalidation_0-auc:0.599558\n",
            "Stopping. Best iteration:\n",
            "[5]\tvalidation_0-auc:0.65866\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6943 - accuracy: 0.5285 - val_loss: 0.7117 - val_accuracy: 0.0284\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6841 - accuracy: 0.5717 - val_loss: 0.7321 - val_accuracy: 0.5514\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6480 - accuracy: 0.6589 - val_loss: 0.6773 - val_accuracy: 0.6630\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6409 - accuracy: 0.6486 - val_loss: 0.7533 - val_accuracy: 0.4989\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6074 - accuracy: 0.6809 - val_loss: 0.7351 - val_accuracy: 0.5295\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6752 - accuracy: 0.5834 - val_loss: 0.6553 - val_accuracy: 0.7681\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6313 - accuracy: 0.6747 - val_loss: 0.6024 - val_accuracy: 0.7571\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6064 - accuracy: 0.6925 - val_loss: 0.6208 - val_accuracy: 0.7330\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5928 - accuracy: 0.7104 - val_loss: 0.4592 - val_accuracy: 0.8490\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5892 - accuracy: 0.7076 - val_loss: 0.7534 - val_accuracy: 0.5514\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.432613\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.539155\n",
            "[2]\tvalidation_0-auc:0.573285\n",
            "[3]\tvalidation_0-auc:0.609436\n",
            "[4]\tvalidation_0-auc:0.578528\n",
            "[5]\tvalidation_0-auc:0.578855\n",
            "[6]\tvalidation_0-auc:0.595347\n",
            "[7]\tvalidation_0-auc:0.613914\n",
            "[8]\tvalidation_0-auc:0.621232\n",
            "[9]\tvalidation_0-auc:0.644932\n",
            "[10]\tvalidation_0-auc:0.623198\n",
            "[11]\tvalidation_0-auc:0.669561\n",
            "[12]\tvalidation_0-auc:0.640181\n",
            "[13]\tvalidation_0-auc:0.619321\n",
            "[14]\tvalidation_0-auc:0.6564\n",
            "[15]\tvalidation_0-auc:0.681575\n",
            "[16]\tvalidation_0-auc:0.662789\n",
            "[17]\tvalidation_0-auc:0.691186\n",
            "[18]\tvalidation_0-auc:0.691295\n",
            "[19]\tvalidation_0-auc:0.698504\n",
            "[20]\tvalidation_0-auc:0.705931\n",
            "[21]\tvalidation_0-auc:0.719037\n",
            "[22]\tvalidation_0-auc:0.725917\n",
            "[23]\tvalidation_0-auc:0.732798\n",
            "[24]\tvalidation_0-auc:0.735965\n",
            "[25]\tvalidation_0-auc:0.733453\n",
            "[26]\tvalidation_0-auc:0.723023\n",
            "[27]\tvalidation_0-auc:0.724661\n",
            "[28]\tvalidation_0-auc:0.714613\n",
            "[29]\tvalidation_0-auc:0.703255\n",
            "[30]\tvalidation_0-auc:0.698558\n",
            "[31]\tvalidation_0-auc:0.69692\n",
            "[32]\tvalidation_0-auc:0.714941\n",
            "[33]\tvalidation_0-auc:0.703146\n",
            "[34]\tvalidation_0-auc:0.701398\n",
            "[35]\tvalidation_0-auc:0.69834\n",
            "[36]\tvalidation_0-auc:0.69976\n",
            "[37]\tvalidation_0-auc:0.710135\n",
            "[38]\tvalidation_0-auc:0.704456\n",
            "[39]\tvalidation_0-auc:0.690804\n",
            "[40]\tvalidation_0-auc:0.70533\n",
            "[41]\tvalidation_0-auc:0.709043\n",
            "[42]\tvalidation_0-auc:0.712101\n",
            "[43]\tvalidation_0-auc:0.719747\n",
            "[44]\tvalidation_0-auc:0.718654\n",
            "[45]\tvalidation_0-auc:0.724771\n",
            "[46]\tvalidation_0-auc:0.721931\n",
            "[47]\tvalidation_0-auc:0.721822\n",
            "[48]\tvalidation_0-auc:0.72346\n",
            "[49]\tvalidation_0-auc:0.725754\n",
            "[50]\tvalidation_0-auc:0.731979\n",
            "[51]\tvalidation_0-auc:0.742901\n",
            "[52]\tvalidation_0-auc:0.754478\n",
            "[53]\tvalidation_0-auc:0.752621\n",
            "[54]\tvalidation_0-auc:0.756225\n",
            "[55]\tvalidation_0-auc:0.756662\n",
            "[56]\tvalidation_0-auc:0.760813\n",
            "[57]\tvalidation_0-auc:0.767693\n",
            "[58]\tvalidation_0-auc:0.766383\n",
            "[59]\tvalidation_0-auc:0.76813\n",
            "[60]\tvalidation_0-auc:0.778397\n",
            "[61]\tvalidation_0-auc:0.783639\n",
            "[62]\tvalidation_0-auc:0.784622\n",
            "[63]\tvalidation_0-auc:0.778615\n",
            "[64]\tvalidation_0-auc:0.783858\n",
            "[65]\tvalidation_0-auc:0.783311\n",
            "[66]\tvalidation_0-auc:0.781673\n",
            "[67]\tvalidation_0-auc:0.782984\n",
            "[68]\tvalidation_0-auc:0.788117\n",
            "[69]\tvalidation_0-auc:0.797182\n",
            "[70]\tvalidation_0-auc:0.793141\n",
            "[71]\tvalidation_0-auc:0.792813\n",
            "[72]\tvalidation_0-auc:0.796745\n",
            "[73]\tvalidation_0-auc:0.79194\n",
            "[74]\tvalidation_0-auc:0.797728\n",
            "[75]\tvalidation_0-auc:0.794561\n",
            "[76]\tvalidation_0-auc:0.793141\n",
            "[77]\tvalidation_0-auc:0.797947\n",
            "[78]\tvalidation_0-auc:0.799585\n",
            "[79]\tvalidation_0-auc:0.797837\n",
            "[80]\tvalidation_0-auc:0.79882\n",
            "[81]\tvalidation_0-auc:0.79882\n",
            "[82]\tvalidation_0-auc:0.799694\n",
            "[83]\tvalidation_0-auc:0.797182\n",
            "[84]\tvalidation_0-auc:0.795544\n",
            "[85]\tvalidation_0-auc:0.795544\n",
            "[86]\tvalidation_0-auc:0.796636\n",
            "[87]\tvalidation_0-auc:0.794233\n",
            "[88]\tvalidation_0-auc:0.789428\n",
            "[89]\tvalidation_0-auc:0.789646\n",
            "[90]\tvalidation_0-auc:0.790301\n",
            "[91]\tvalidation_0-auc:0.787462\n",
            "[92]\tvalidation_0-auc:0.790192\n",
            "[93]\tvalidation_0-auc:0.788663\n",
            "[94]\tvalidation_0-auc:0.784622\n",
            "[95]\tvalidation_0-auc:0.780253\n",
            "[96]\tvalidation_0-auc:0.781673\n",
            "[97]\tvalidation_0-auc:0.782329\n",
            "[98]\tvalidation_0-auc:0.785605\n",
            "[99]\tvalidation_0-auc:0.787025\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.3306122448979592 | 0.12771739130434784 | 0.8703703703703703 |  0.2227488151658768 |\n",
            "|      GRU 0.15     | 0.610204081632653  |  0.1813953488372093 | 0.7222222222222222 |  0.2899628252788104 |\n",
            "|    XGBoost 0.15   | 0.6428571428571429 | 0.18324607329842932 | 0.6481481481481481 |  0.2857142857142857 |\n",
            "|    Logreg 0.15    | 0.6326530612244898 | 0.19117647058823528 | 0.7222222222222222 |  0.3023255813953488 |\n",
            "|      SVM 0.15     | 0.6775510204081633 | 0.22916666666666666 | 0.8148148148148148 |  0.3577235772357723 |\n",
            "|   LSTM beta 0.15  | 0.5295404814004376 | 0.06306306306306306 | 0.6666666666666666 | 0.11522633744855966 |\n",
            "|   GRU beta 0.15   | 0.5514223194748359 | 0.06190476190476191 | 0.6190476190476191 | 0.11255411255411256 |\n",
            "| XGBoost beta 0.15 | 0.6958424507658644 |  0.0958904109589041 | 0.6666666666666666 | 0.16766467065868262 |\n",
            "|  logreg beta 0.15 | 0.6761487964989059 | 0.09032258064516129 | 0.6666666666666666 |  0.1590909090909091 |\n",
            "|   svm beta 0.15   | 0.6608315098468271 | 0.11931818181818182 |        1.0         |  0.2131979695431472 |\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4ZDRP2MVrKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6874019b-423d-4d04-e8e8-0c25fae13761"
      },
      "source": [
        "Result_purging.to_csv('FOX_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.284024</td>\n",
              "      <td>0.704082</td>\n",
              "      <td>0.398340</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.246637</td>\n",
              "      <td>0.622449</td>\n",
              "      <td>0.372881</td>\n",
              "      <td>0.763889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.278689</td>\n",
              "      <td>0.687755</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.708333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.251163</td>\n",
              "      <td>0.634694</td>\n",
              "      <td>0.376307</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.299492</td>\n",
              "      <td>0.691837</td>\n",
              "      <td>0.438662</td>\n",
              "      <td>0.819444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.118721</td>\n",
              "      <td>0.549234</td>\n",
              "      <td>0.201550</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.187050</td>\n",
              "      <td>0.724289</td>\n",
              "      <td>0.292135</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.149425</td>\n",
              "      <td>0.647702</td>\n",
              "      <td>0.244131</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.158824</td>\n",
              "      <td>0.660832</td>\n",
              "      <td>0.258373</td>\n",
              "      <td>0.692308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.625821</td>\n",
              "      <td>0.278481</td>\n",
              "      <td>0.846154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.027397</td>\n",
              "      <td>0.114286</td>\n",
              "      <td>0.052402</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.077419</td>\n",
              "      <td>0.691837</td>\n",
              "      <td>0.137143</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.079365</td>\n",
              "      <td>0.742857</td>\n",
              "      <td>0.136986</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.074286</td>\n",
              "      <td>0.655102</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.020134</td>\n",
              "      <td>0.667347</td>\n",
              "      <td>0.035503</td>\n",
              "      <td>0.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.073770</td>\n",
              "      <td>0.728665</td>\n",
              "      <td>0.126761</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.746171</td>\n",
              "      <td>0.171429</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.107143</td>\n",
              "      <td>0.715536</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.102362</td>\n",
              "      <td>0.735230</td>\n",
              "      <td>0.176871</td>\n",
              "      <td>0.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.118343</td>\n",
              "      <td>0.673961</td>\n",
              "      <td>0.211640</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.127717</td>\n",
              "      <td>0.330612</td>\n",
              "      <td>0.222749</td>\n",
              "      <td>0.870370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.181395</td>\n",
              "      <td>0.610204</td>\n",
              "      <td>0.289963</td>\n",
              "      <td>0.722222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.183246</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.648148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.191176</td>\n",
              "      <td>0.632653</td>\n",
              "      <td>0.302326</td>\n",
              "      <td>0.722222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.229167</td>\n",
              "      <td>0.677551</td>\n",
              "      <td>0.357724</td>\n",
              "      <td>0.814815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.063063</td>\n",
              "      <td>0.529540</td>\n",
              "      <td>0.115226</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.061905</td>\n",
              "      <td>0.551422</td>\n",
              "      <td>0.112554</td>\n",
              "      <td>0.619048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.095890</td>\n",
              "      <td>0.695842</td>\n",
              "      <td>0.167665</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.090323</td>\n",
              "      <td>0.676149</td>\n",
              "      <td>0.159091</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>FOX</td>\n",
              "      <td>0.119318</td>\n",
              "      <td>0.660832</td>\n",
              "      <td>0.213198</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  FOX  0.284024  0.704082  0.398340  0.666667\n",
              "1            GRU 0.1  FOX  0.246637  0.622449  0.372881  0.763889\n",
              "2        XGBoost 0.1  FOX  0.278689  0.687755  0.400000  0.708333\n",
              "3         Logreg 0.1  FOX  0.251163  0.634694  0.376307  0.750000\n",
              "4            SVM 0.1  FOX  0.299492  0.691837  0.438662  0.819444\n",
              "5      LSTM beta 0.1  FOX  0.118721  0.549234  0.201550  0.666667\n",
              "6       GRU beta 0.1  FOX  0.187050  0.724289  0.292135  0.666667\n",
              "7   XGBoost beta 0.1  FOX  0.149425  0.647702  0.244131  0.666667\n",
              "8    logreg beta 0.1  FOX  0.158824  0.660832  0.258373  0.692308\n",
              "9       svm beta 0.1  FOX  0.166667  0.625821  0.278481  0.846154\n",
              "0           LSTM 0.2  FOX  0.027397  0.114286  0.052402  0.600000\n",
              "1            GRU 0.2  FOX  0.077419  0.691837  0.137143  0.600000\n",
              "2        XGBoost 0.2  FOX  0.079365  0.742857  0.136986  0.500000\n",
              "3         Logreg 0.2  FOX  0.074286  0.655102  0.133333  0.650000\n",
              "4            SVM 0.2  FOX  0.020134  0.667347  0.035503  0.150000\n",
              "5      LSTM beta 0.2  FOX  0.073770  0.728665  0.126761  0.450000\n",
              "6       GRU beta 0.2  FOX  0.100000  0.746171  0.171429  0.600000\n",
              "7   XGBoost beta 0.2  FOX  0.107143  0.715536  0.187500  0.750000\n",
              "8    logreg beta 0.2  FOX  0.102362  0.735230  0.176871  0.650000\n",
              "9       svm beta 0.2  FOX  0.118343  0.673961  0.211640  1.000000\n",
              "0          LSTM 0.15  FOX  0.127717  0.330612  0.222749  0.870370\n",
              "1           GRU 0.15  FOX  0.181395  0.610204  0.289963  0.722222\n",
              "2       XGBoost 0.15  FOX  0.183246  0.642857  0.285714  0.648148\n",
              "3        Logreg 0.15  FOX  0.191176  0.632653  0.302326  0.722222\n",
              "4           SVM 0.15  FOX  0.229167  0.677551  0.357724  0.814815\n",
              "5     LSTM beta 0.15  FOX  0.063063  0.529540  0.115226  0.666667\n",
              "6      GRU beta 0.15  FOX  0.061905  0.551422  0.112554  0.619048\n",
              "7  XGBoost beta 0.15  FOX  0.095890  0.695842  0.167665  0.666667\n",
              "8   logreg beta 0.15  FOX  0.090323  0.676149  0.159091  0.666667\n",
              "9      svm beta 0.15  FOX  0.119318  0.660832  0.213198  1.000000"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u3EKgRCVrKg"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOX_logreg_beta_p.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9pDBGNZVrKg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqt4rK47WILc"
      },
      "source": [
        "## FOXA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixgyea-lWILi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "33641b1e-f498-41a5-ab01-c690c17e75ae"
      },
      "source": [
        "dfs = pd.read_csv(\"FOXA.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "dfs"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2768</td>\n",
              "      <td>US1.FOXA</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>40.44</td>\n",
              "      <td>41.065</td>\n",
              "      <td>40.1200</td>\n",
              "      <td>40.79</td>\n",
              "      <td>76891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2767</td>\n",
              "      <td>US1.FOXA</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>40.75</td>\n",
              "      <td>41.060</td>\n",
              "      <td>40.1100</td>\n",
              "      <td>40.11</td>\n",
              "      <td>158785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.FOXA</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>39.81</td>\n",
              "      <td>40.310</td>\n",
              "      <td>39.6500</td>\n",
              "      <td>40.24</td>\n",
              "      <td>91517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.FOXA</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>39.87</td>\n",
              "      <td>40.420</td>\n",
              "      <td>39.7000</td>\n",
              "      <td>39.84</td>\n",
              "      <td>163999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.FOXA</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>39.10</td>\n",
              "      <td>40.370</td>\n",
              "      <td>39.1000</td>\n",
              "      <td>40.11</td>\n",
              "      <td>180216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.FOXA</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>13.68</td>\n",
              "      <td>13.950</td>\n",
              "      <td>13.5675</td>\n",
              "      <td>13.88</td>\n",
              "      <td>20812401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.FOXA</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>13.48</td>\n",
              "      <td>13.750</td>\n",
              "      <td>13.3100</td>\n",
              "      <td>13.64</td>\n",
              "      <td>28318775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.FOXA</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>13.58</td>\n",
              "      <td>13.650</td>\n",
              "      <td>13.3200</td>\n",
              "      <td>13.48</td>\n",
              "      <td>16218875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.FOXA</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>13.28</td>\n",
              "      <td>13.580</td>\n",
              "      <td>13.1700</td>\n",
              "      <td>13.56</td>\n",
              "      <td>17732763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.FOXA</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>13.16</td>\n",
              "      <td>13.250</td>\n",
              "      <td>12.8800</td>\n",
              "      <td>12.97</td>\n",
              "      <td>15096088</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2769 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  <TICKER> <PER>    <DATE>  ...  <HIGH>    <LOW>  <CLOSE>     <VOL>\n",
              "0      2768  US1.FOXA     D  20211001  ...  41.065  40.1200    40.79     76891\n",
              "1      2767  US1.FOXA     D  20210930  ...  41.060  40.1100    40.11    158785\n",
              "2      2766  US1.FOXA     D  20210929  ...  40.310  39.6500    40.24     91517\n",
              "3      2765  US1.FOXA     D  20210928  ...  40.420  39.7000    39.84    163999\n",
              "4      2764  US1.FOXA     D  20210927  ...  40.370  39.1000    40.11    180216\n",
              "...     ...       ...   ...       ...  ...     ...      ...      ...       ...\n",
              "2764      4  US1.FOXA     D  20101008  ...  13.950  13.5675    13.88  20812401\n",
              "2765      3  US1.FOXA     D  20101007  ...  13.750  13.3100    13.64  28318775\n",
              "2766      2  US1.FOXA     D  20101006  ...  13.650  13.3200    13.48  16218875\n",
              "2767      1  US1.FOXA     D  20101005  ...  13.580  13.1700    13.56  17732763\n",
              "2768      0  US1.FOXA     D  20101004  ...  13.250  12.8800    12.97  15096088\n",
              "\n",
              "[2769 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQEWUYhuWILi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "df59a0f9-17ff-4a7c-d770-f5abcecdaf02"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"e84bc31c-8410-4550-ac59-53259207602c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"e84bc31c-8410-4550-ac59-53259207602c\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'e84bc31c-8410-4550-ac59-53259207602c',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [30.06, 30.605, 30.57, 30.46, 30.41, 31.25, 31.54, 31.47, 31.455, 32.01, 31.57, 32.26, 32.25, 32.05, 32.65, 32.87, 33.59, 33.82, 33.77, 34.0, 34.29, 34.3, 34.24, 34.25, 33.555, 32.77, 33.14, 32.74, 31.78, 31.94, 32.06, 31.76, 33.05, 33.41, 33.57, 33.88, 32.98, 32.75, 33.69, 35.07, 34.93, 35.41, 35.14, 37.03, 36.45, 36.06, 36.61, 36.91, 37.31, 37.83, 37.5, 37.2, 37.19, 36.61, 36.22, 36.11, 36.44, 36.56, 36.17, 36.67, 37.01, 36.96, 36.29, 36.14, 36.02, 36.13, 36.6, 36.35, 36.59, 35.92, 36.63, 35.99, 35.47, 35.36, 35.8, 36.14, 35.57, 36.0, 36.2, 36.15, 35.24, 35.3, 35.06, 34.17, 34.03, 34.01, 34.12, 34.36, 34.47, 34.22, 35.23, 35.21, 35.8, 36.62, 36.94, 36.91, 37.13, 37.66, 37.14, 37.3, 36.74, 37.72, 37.91, 37.97, 38.56, 38.48, 37.42, 37.28, 37.91, 37.46, 37.27, 39.43, 38.98, 39.21, 38.49, 38.15, 37.96, 38.06, 37.72, 38.44, 38.84, 37.76, 37.53, 37.52, 37.08, 36.49, 37.3, 37.47, 37.69, 36.76, 36.05, 36.29, 36.86, 36.72, 36.39, 36.93, 37.54, 37.45, 38.71, 40.04, 38.59, 40.38, 51.39, 51.95, 51.77, 51.21, 50.95, 50.53, 50.46, 50.58, 50.64, 50.53, 50.66, 50.61, 50.45, 50.42, 50.86, 50.73, 51.16, 50.75, 50.79, 50.525, 50.4, 49.985, 49.65, 49.54, 49.55, 49.68, 49.5, 49.43, 49.26, 49.3, 49.3, 49.23, 49.04, 49.1, 49.23, 49.24, 48.89, 48.64, 48.71, 48.74, 48.405, 48.36, 48.62, 48.68, 48.75, 48.83, 48.78, 48.58, 48.18, 48.07, 47.47, 47.79, 48.12, 47.97, 48.01, 47.84, 46.16, 46.95, 48.01, 48.51, 48.79, 49.0, 49.05, 49.135, 49.14, 49.255, 49.35, 49.26, 49.5, 49.19, 49.675, 49.48, 49.55, 49.47, 49.14, 48.86, 48.94, 48.86, 48.58, 48.91, 48.19, 48.28, 47.93, 47.72, 48.07, 48.14, 47.85, 47.515, 47.17, 46.84, 46.52, 46.56, 45.5, 45.65, 45.5, 45.14, 45.25, 44.705, 45.59, 45.72, 45.76, 45.81, 45.82, 45.86, 45.53, 45.35, 45.12, 44.7, 45.49, 45.72, 46.1, 46.56, 46.82, 46.58, 46.19, 46.34, 45.82, 45.68, 45.21, 45.04, 44.33, 44.59, 44.6, 44.54, 44.71, 44.8, 44.8, 44.63, 44.73, 44.95, 45.23, 45.35, 45.27, 45.32, 45.38, 45.31, 45.61, 45.22, 45.16, 45.15, 45.27, 45.34, 45.36, 45.39, 45.37, 45.71, 45.35, 45.48, 45.31, 45.48, 45.54, 45.46, 45.51, 45.69, 45.42, 45.02, 45.02, 45.01, 45.1, 45.15, 45.42, 45.18, 45.25, 45.92, 46.01, 46.55, 46.68, 46.47, 46.72, 47.56, 47.38, 47.8, 49.77, 49.55, 48.94, 48.73, 48.78, 49.2, 49.76, 49.79, 48.88, 47.68, 48.15, 48.61, 48.35, 48.1, 44.7, 44.55, 44.59, 44.58, 43.69, 40.51, 40.13, 39.9825, 39.65, 39.365, 39.09, 38.64, 38.54, 38.58, 38.65, 38.64, 38.95, 38.92, 38.74, 38.17, 37.92, 37.67, 37.75, 38.16, 37.89, 37.77, 37.94, 37.91, 37.69, 37.99, 38.05, 37.66, 36.88, 36.74, 36.57, 36.57, 36.38, 36.26, 36.59, 36.02, 36.49, 36.69, 37.2, 37.32, 37.54, 37.03, 36.79, 36.48, 36.205, 36.27, 35.7, 35.68, 36.35, 36.14, 36.15, 36.15, 36.65, 35.89, 36.49, 37.03, 36.11, 36.76, 36.87, 36.7, 36.79, 37.14, 37.15, 37.7, 37.4, 37.98, 37.74, 37.19, 37.02, 37.54, 36.65, 36.37, 35.93, 36.83, 37.63, 38.82, 38.04, 37.18, 37.05, 37.13, 37.46, 36.97, 36.55, 36.58, 36.41, 35.69, 34.58, 36.07, 36.8, 36.15, 36.72, 37.01, 36.91, 38.46, 38.74, 38.8, 38.19, 38.01, 37.52, 37.3, 36.66, 36.19, 37.11, 36.3, 36.72, 36.2, 35.56, 36.54, 36.24, 36.75, 36.82, 36.55, 35.87, 34.54, 34.48, 34.56, 34.61, 35.24, 35.12, 34.27, 35.06, 35.2, 34.84, 34.88, 32.75, 34.08, 33.66, 33.3, 34.19, 33.21, 33.0, 33.08, 32.19, 31.93, 32.1, 30.87, 30.39, 30.4, 30.6, 30.88, 30.67, 31.15, 29.33, 28.74, 28.04, 28.55, 28.83, 28.7, 28.125, 27.76, 27.45, 24.96, 25.81, 25.98, 26.145, 26.4, 26.4, 26.32, 26.13, 26.24, 26.7, 27.5, 27.37, 27.2, 26.875, 26.69, 26.52, 26.145, 26.12, 26.77, 26.57, 26.99, 26.95, 27.095, 26.64, 26.6, 26.375, 26.555, 27.33, 26.96, 27.125, 26.88, 26.76, 26.86, 26.7, 26.505, 27.06, 26.51, 26.59, 25.91, 26.055, 25.78, 25.79, 26.43, 26.57, 27.49, 27.6, 27.46, 27.36, 27.26, 27.24, 27.09, 27.155, 27.43, 27.36, 27.27, 27.35, 27.99, 27.86, 28.2, 28.23, 27.88, 27.9, 28.02, 28.19, 28.46, 28.82, 28.67, 29.23, 29.1, 29.37, 29.62, 28.18, 27.94, 27.69, 27.83, 27.8, 27.64, 27.36, 27.38, 27.73, 27.58, 28.65, 28.11, 28.29, 27.9, 27.95, 28.37, 28.52, 28.33, 28.18, 28.11, 27.61, 28.05, 27.19, 26.965, 27.25, 27.17, 27.8, 27.45, 27.74, 27.61, 28.06, 28.75, 28.17, 27.395, 27.58, 27.53, 27.74, 27.93, 27.82, 27.12, 27.045, 27.02, 27.33, 26.87, 26.75, 27.255, 27.11, 26.86, 26.85, 27.33, 27.71, 27.925, 28.51, 27.89, 28.23, 28.4, 29.14, 29.04, 28.88, 30.43, 30.39, 30.535, 30.56, 30.74, 30.615, 30.35, 30.71, 31.07, 30.39, 30.66, 30.73, 30.43, 30.65, 31.15, 31.23, 31.07, 31.12, 31.38, 31.75, 32.15, 32.4, 32.37, 32.12, 32.45, 31.9, 31.52, 31.33, 30.97, 30.56, 30.63, 30.81, 30.74, 30.74, 30.515, 30.5, 30.56, 30.7, 30.355, 30.51, 30.69, 30.46, 30.2, 30.4, 29.92, 30.435, 30.61, 30.35, 30.47, 30.54, 30.35, 30.51, 30.7, 30.31, 29.72, 29.91, 30.12, 30.2, 30.48, 31.06, 31.4, 31.46, 31.62, 31.39, 31.38, 31.295, 31.01, 30.29, 30.27, 30.01, 30.09, 29.81, 30.27, 29.94, 30.03, 29.81, 30.08, 29.68, 29.47, 29.52, 29.0, 29.02, 28.715, 28.04, 28.27, 28.46, 28.53, 28.385, 28.44, 28.44, 28.175, 28.13, 28.03, 28.0, 28.02, 27.03, 26.34, 28.21, 28.64, 28.11, 27.5, 27.56, 27.395, 28.21, 28.12, 28.47, 28.31, 28.36, 28.4, 28.27, 27.8, 27.81, 27.92, 27.54, 27.45, 27.52, 27.13, 27.16, 27.45, 27.08, 27.39, 26.94, 27.72, 25.825, 26.18, 26.27, 26.26, 26.11, 26.4, 25.56, 25.56, 25.83, 25.275, 25.13, 24.96, 24.685, 24.735, 24.57, 24.54, 24.655, 24.71, 24.35, 24.53, 24.65, 24.75, 24.73, 24.22, 24.03, 24.31, 24.21, 23.8, 23.93, 24.3, 24.0, 23.89, 23.94, 23.89, 23.84, 23.7, 23.72, 23.95, 23.57, 24.74, 24.38, 24.42, 24.55, 24.53, 24.54, 24.55, 24.74, 24.55, 24.585, 24.81, 24.89, 25.12, 25.15, 25.075, 25.48, 25.47, 25.845, 26.015, 26.16, 25.58, 25.47, 25.41, 25.63, 25.6, 27.06, 26.7, 26.62, 26.645, 26.75, 26.75, 26.74, 26.85, 27.1, 27.18, 26.995, 27.76, 28.02, 28.03, 28.12, 27.92, 27.83, 27.77, 27.7, 26.93, 26.75, 26.65, 27.26, 27.06, 26.995, 26.65, 26.37, 26.93, 29.2, 28.92, 28.805, 29.2, 28.81, 29.13, 28.96, 28.87, 28.91, 28.93, 29.32, 29.44, 29.31, 29.45, 29.38, 29.55, 29.34, 28.875, 28.93, 28.76, 28.89, 28.58, 28.2, 28.14, 27.92, 28.15, 28.69, 28.93, 29.0, 29.09, 29.41, 29.77, 29.41, 29.51, 29.83, 29.8, 29.96, 30.36, 30.26, 30.7, 30.83, 30.96, 31.06, 30.94, 30.33, 30.09, 30.31, 30.39, 30.0, 29.85, 29.8, 29.295, 28.53, 28.59, 28.55, 28.88, 28.36, 28.585, 28.58, 27.89, 27.91, 27.83, 27.89, 27.74, 27.79, 27.96, 27.97, 28.215, 28.18, 28.13, 27.83, 27.82, 27.91, 27.46, 27.7, 27.45, 27.9, 27.8, 27.84, 27.585, 27.8, 27.015, 27.08, 27.38, 27.32, 26.955, 27.35, 26.62, 26.4, 26.46, 25.61, 24.55, 24.69, 24.33, 24.13, 24.59, 25.065, 26.08, 26.5, 25.99, 26.97, 26.94, 26.13, 26.59, 26.9, 25.99, 26.38, 25.5202, 25.655, 26.37, 26.1, 26.55, 25.875, 26.42, 25.88, 25.9, 25.89, 26.725, 26.58, 26.58, 27.16, 27.29, 27.565, 27.42, 27.43, 27.4, 27.365, 27.06, 27.07, 27.48, 27.89, 27.33, 27.255, 27.455, 28.235, 28.145, 28.725, 29.315, 29.335, 29.02, 29.32, 29.74, 29.53, 29.88, 29.91, 29.86, 29.99, 30.07, 29.95, 30.29, 30.19, 30.31, 29.7, 30.025, 30.01, 30.09, 29.48, 29.835, 29.58, 29.65, 31.29, 30.88, 30.68, 30.41, 30.13, 29.96, 30.23, 30.11, 29.785, 29.25, 29.565, 29.48, 29.47, 29.26, 28.56, 28.695, 28.665, 28.67, 28.58, 28.2, 28.17, 28.35, 28.05, 27.2, 26.97, 25.8, 25.19, 25.735, 25.95, 26.28, 26.355, 26.445, 26.32, 26.78, 26.74, 26.58, 26.33, 26.45, 26.52, 26.48, 27.05, 26.46, 26.87, 26.95, 26.6, 27.39, 27.91, 27.79, 27.34, 26.76, 26.67, 27.94, 28.7, 29.96, 30.2, 30.53, 30.3, 30.12, 30.09, 30.06, 30.67, 30.72, 29.86, 32.07, 34.34, 34.38, 34.49, 34.04, 34.0, 33.63, 33.19, 33.68, 33.23, 33.69, 33.52, 33.845, 33.99, 34.25, 33.92, 33.65, 33.66, 33.1, 32.49, 32.22, 32.68, 32.555, 32.33, 32.6, 32.54, 32.3, 32.995, 32.96, 32.985, 33.42, 33.275, 32.89, 33.02, 32.69, 32.26, 32.48, 32.69, 32.915, 32.95, 32.8, 33.07, 33.31, 33.59, 33.63, 33.4, 33.59, 33.605, 33.96, 34.16, 33.97, 34.265, 34.61, 34.565, 34.36, 33.675, 34.0, 33.32, 32.925, 32.61, 32.8, 32.79, 32.77, 33.67, 33.92, 34.47, 34.5, 34.09, 34.63, 34.53, 34.515, 34.65, 34.35, 34.025, 33.95, 33.93, 33.44, 34.0, 34.04, 34.12, 34.075, 34.33, 34.36, 34.12, 34.24, 34.45, 34.04, 33.58, 33.85, 34.24, 33.7, 33.94, 34.26, 35.07, 35.55, 35.32, 34.46, 34.92, 33.88, 34.36, 34.005, 34.57, 34.19, 34.3, 34.71, 34.5, 34.87, 34.98, 35.64, 35.16, 35.0, 34.81, 34.9, 35.24, 35.36, 35.29, 34.795, 34.67, 35.06, 34.72, 34.41, 34.82, 34.45, 34.04, 33.58, 32.79, 34.65, 34.6, 33.76, 33.17, 33.86, 33.56, 34.165, 34.53, 34.29, 34.55, 33.91, 34.12, 34.94, 34.59, 34.86, 35.13, 35.38, 35.37, 35.68, 35.32, 36.61, 37.09, 37.84, 38.41, 38.67, 39.0, 38.9, 38.8249, 38.75, 38.98, 38.5, 37.75, 37.23, 36.215, 36.5, 36.91, 37.1, 36.99, 37.11, 37.4, 37.81, 37.625, 37.24, 37.19, 37.28, 36.799, 36.31, 36.56, 35.73, 35.205, 35.055, 35.005, 35.32, 35.35, 35.11, 34.99, 34.63, 34.93, 34.74, 34.6, 35.3, 34.83, 33.32, 34.34, 34.48, 34.57, 34.34, 34.31, 33.78, 33.35, 33.54, 32.765, 33.65, 33.05, 32.7, 32.5, 31.78, 31.91, 31.825, 32.43, 33.11, 34.1, 33.65, 34.1, 34.055, 33.4, 33.7, 34.29, 34.15, 34.44, 33.75, 34.49, 34.22, 34.53, 35.16, 34.905, 34.66, 34.745, 34.92, 35.06, 35.3, 35.55, 35.67, 35.88, 36.15, 36.13, 36.305, 35.94, 35.42, 35.6, 35.85, 35.56, 35.58, 35.7, 35.73, 35.68, 35.89, 35.96, 35.87, 35.76, 35.4, 35.12, 34.76, 34.37, 33.96, 32.32, 31.3, 31.51, 31.94, 31.68, 32.0, 32.15, 32.45, 32.815, 32.91, 32.7, 33.34, 32.89, 33.01, 32.78, 33.0, 35.18, 35.54, 35.65, 35.71, 35.91, 35.1, 35.685, 36.21, 35.695, 35.3, 35.15, 35.26, 35.15, 34.88, 34.22, 34.35, 34.53, 35.36, 35.37, 35.51, 35.73, 35.515, 35.41, 36.12, 36.07, 36.21, 36.04, 36.09, 35.5, 35.39, 35.55, 35.41, 35.51, 35.18, 34.78, 34.96, 34.53, 34.55, 34.1, 34.12, 34.06, 34.03, 34.28, 34.9, 35.19, 34.14, 34.21, 32.12, 32.41, 32.82, 32.52, 32.47, 32.02, 32.01, 31.78, 32.655, 32.883, 32.73, 32.81, 32.58, 32.52, 32.27, 31.65, 31.89, 31.92, 32.435, 33.13, 32.36, 31.99, 32.88, 33.74, 33.58, 33.04, 31.96, 31.6, 31.67, 32.075, 32.51, 32.27, 32.65, 32.75, 33.0, 33.04, 32.39, 32.29, 32.11, 32.685, 32.98, 33.7, 33.9, 34.35, 33.93, 33.785, 33.06, 33.54, 33.05, 32.83, 33.2, 33.17, 32.96, 32.67, 32.77, 33.02, 32.58, 32.23, 32.57, 32.15, 32.14, 32.24, 32.14, 31.73, 31.56, 30.89, 31.83, 32.01, 30.735, 31.28, 31.05, 31.27, 31.74, 32.065, 31.92, 32.025, 32.465, 32.62, 32.3, 32.35, 33.47, 33.5, 34.4, 34.96, 34.79, 35.2, 35.63, 35.17, 35.01, 34.965, 34.98, 34.79, 34.6, 34.29, 33.97, 33.46, 32.85, 32.83, 32.92, 32.67, 32.68, 33.17, 33.11, 33.13, 32.75, 32.89, 33.23, 33.54, 33.49, 33.495, 33.2, 33.2, 33.4, 33.26, 33.11, 33.25, 33.76, 34.18, 34.13, 33.69, 33.17, 33.45, 33.83, 32.8, 33.93, 34.125, 34.16, 34.36, 34.09, 33.94, 34.82, 34.85, 34.85, 35.17, 34.78]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e84bc31c-8410-4550-ac59-53259207602c');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"9190afdb-059a-49e1-9f21-a9a630d4523a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"9190afdb-059a-49e1-9f21-a9a630d4523a\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '9190afdb-059a-49e1-9f21-a9a630d4523a',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9190afdb-059a-49e1-9f21-a9a630d4523a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce6zzprTWILi"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCfkaYDhWILi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd81d3d5-06b3-4ba1-8224-1a761f56dbfd"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"FOXA\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 18ms/step - loss: 0.6935 - accuracy: 0.5336 - val_loss: 0.6800 - val_accuracy: 0.8776\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6735 - accuracy: 0.5899 - val_loss: 0.7676 - val_accuracy: 0.1980\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6342 - accuracy: 0.6423 - val_loss: 0.5570 - val_accuracy: 0.7878\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6346 - accuracy: 0.6631 - val_loss: 0.6406 - val_accuracy: 0.6837\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6171 - accuracy: 0.6624 - val_loss: 0.5673 - val_accuracy: 0.7633\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6896 - accuracy: 0.5221 - val_loss: 0.8035 - val_accuracy: 0.1388\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6544 - accuracy: 0.6174 - val_loss: 0.7432 - val_accuracy: 0.4816\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6171 - accuracy: 0.6839 - val_loss: 0.4671 - val_accuracy: 0.8551\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6190 - accuracy: 0.6752 - val_loss: 0.6396 - val_accuracy: 0.6531\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6039 - accuracy: 0.6879 - val_loss: 0.6468 - val_accuracy: 0.6469\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.834245\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.848881\n",
            "[2]\tvalidation_0-auc:0.847244\n",
            "[3]\tvalidation_0-auc:0.843009\n",
            "[4]\tvalidation_0-auc:0.851739\n",
            "[5]\tvalidation_0-auc:0.855171\n",
            "[6]\tvalidation_0-auc:0.855799\n",
            "[7]\tvalidation_0-auc:0.856792\n",
            "[8]\tvalidation_0-auc:0.85498\n",
            "[9]\tvalidation_0-auc:0.853185\n",
            "[10]\tvalidation_0-auc:0.853882\n",
            "[11]\tvalidation_0-auc:0.850885\n",
            "[12]\tvalidation_0-auc:0.853534\n",
            "[13]\tvalidation_0-auc:0.853081\n",
            "[14]\tvalidation_0-auc:0.855241\n",
            "[15]\tvalidation_0-auc:0.855311\n",
            "[16]\tvalidation_0-auc:0.857245\n",
            "[17]\tvalidation_0-auc:0.861462\n",
            "[18]\tvalidation_0-auc:0.860974\n",
            "[19]\tvalidation_0-auc:0.860782\n",
            "[20]\tvalidation_0-auc:0.859911\n",
            "[21]\tvalidation_0-auc:0.858918\n",
            "[22]\tvalidation_0-auc:0.858743\n",
            "[23]\tvalidation_0-auc:0.858064\n",
            "[24]\tvalidation_0-auc:0.857611\n",
            "[25]\tvalidation_0-auc:0.857924\n",
            "[26]\tvalidation_0-auc:0.85836\n",
            "[27]\tvalidation_0-auc:0.857803\n",
            "[28]\tvalidation_0-auc:0.858621\n",
            "[29]\tvalidation_0-auc:0.858447\n",
            "[30]\tvalidation_0-auc:0.859284\n",
            "[31]\tvalidation_0-auc:0.857977\n",
            "[32]\tvalidation_0-auc:0.85829\n",
            "[33]\tvalidation_0-auc:0.858256\n",
            "[34]\tvalidation_0-auc:0.856652\n",
            "[35]\tvalidation_0-auc:0.856687\n",
            "[36]\tvalidation_0-auc:0.8566\n",
            "[37]\tvalidation_0-auc:0.856513\n",
            "[38]\tvalidation_0-auc:0.856234\n",
            "[39]\tvalidation_0-auc:0.856356\n",
            "[40]\tvalidation_0-auc:0.856635\n",
            "[41]\tvalidation_0-auc:0.856008\n",
            "[42]\tvalidation_0-auc:0.855729\n",
            "[43]\tvalidation_0-auc:0.855729\n",
            "[44]\tvalidation_0-auc:0.854021\n",
            "[45]\tvalidation_0-auc:0.855311\n",
            "[46]\tvalidation_0-auc:0.854474\n",
            "[47]\tvalidation_0-auc:0.854997\n",
            "[48]\tvalidation_0-auc:0.855137\n",
            "[49]\tvalidation_0-auc:0.855171\n",
            "[50]\tvalidation_0-auc:0.85545\n",
            "[51]\tvalidation_0-auc:0.85545\n",
            "[52]\tvalidation_0-auc:0.854962\n",
            "[53]\tvalidation_0-auc:0.855171\n",
            "[54]\tvalidation_0-auc:0.855485\n",
            "[55]\tvalidation_0-auc:0.85444\n",
            "[56]\tvalidation_0-auc:0.854021\n",
            "[57]\tvalidation_0-auc:0.852279\n",
            "[58]\tvalidation_0-auc:0.850362\n",
            "[59]\tvalidation_0-auc:0.851547\n",
            "[60]\tvalidation_0-auc:0.851512\n",
            "[61]\tvalidation_0-auc:0.851478\n",
            "[62]\tvalidation_0-auc:0.851025\n",
            "[63]\tvalidation_0-auc:0.850641\n",
            "[64]\tvalidation_0-auc:0.85099\n",
            "[65]\tvalidation_0-auc:0.850676\n",
            "[66]\tvalidation_0-auc:0.850362\n",
            "[67]\tvalidation_0-auc:0.850397\n",
            "Stopping. Best iteration:\n",
            "[17]\tvalidation_0-auc:0.861462\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6948 - accuracy: 0.5079 - val_loss: 0.7631 - val_accuracy: 0.1138\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6922 - accuracy: 0.5340 - val_loss: 0.6714 - val_accuracy: 0.8753\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6653 - accuracy: 0.6239 - val_loss: 0.7337 - val_accuracy: 0.4201\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6275 - accuracy: 0.6616 - val_loss: 0.6037 - val_accuracy: 0.6893\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6177 - accuracy: 0.6726 - val_loss: 0.7330 - val_accuracy: 0.5383\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6784 - accuracy: 0.5717 - val_loss: 0.6698 - val_accuracy: 0.6149\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6251 - accuracy: 0.6829 - val_loss: 0.6707 - val_accuracy: 0.6039\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6167 - accuracy: 0.6980 - val_loss: 0.7584 - val_accuracy: 0.4661\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6122 - accuracy: 0.6925 - val_loss: 0.6311 - val_accuracy: 0.6805\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5907 - accuracy: 0.7049 - val_loss: 0.6111 - val_accuracy: 0.6543\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.792213\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.786087\n",
            "[2]\tvalidation_0-auc:0.816287\n",
            "[3]\tvalidation_0-auc:0.786491\n",
            "[4]\tvalidation_0-auc:0.786396\n",
            "[5]\tvalidation_0-auc:0.784663\n",
            "[6]\tvalidation_0-auc:0.795014\n",
            "[7]\tvalidation_0-auc:0.801235\n",
            "[8]\tvalidation_0-auc:0.808025\n",
            "[9]\tvalidation_0-auc:0.802683\n",
            "[10]\tvalidation_0-auc:0.802303\n",
            "[11]\tvalidation_0-auc:0.796178\n",
            "[12]\tvalidation_0-auc:0.80254\n",
            "[13]\tvalidation_0-auc:0.798837\n",
            "[14]\tvalidation_0-auc:0.805318\n",
            "[15]\tvalidation_0-auc:0.800997\n",
            "[16]\tvalidation_0-auc:0.800475\n",
            "[17]\tvalidation_0-auc:0.80133\n",
            "[18]\tvalidation_0-auc:0.801852\n",
            "[19]\tvalidation_0-auc:0.796771\n",
            "[20]\tvalidation_0-auc:0.798267\n",
            "[21]\tvalidation_0-auc:0.798552\n",
            "[22]\tvalidation_0-auc:0.796273\n",
            "[23]\tvalidation_0-auc:0.794539\n",
            "[24]\tvalidation_0-auc:0.791263\n",
            "[25]\tvalidation_0-auc:0.793495\n",
            "[26]\tvalidation_0-auc:0.794539\n",
            "[27]\tvalidation_0-auc:0.794658\n",
            "[28]\tvalidation_0-auc:0.794658\n",
            "[29]\tvalidation_0-auc:0.795679\n",
            "[30]\tvalidation_0-auc:0.795916\n",
            "[31]\tvalidation_0-auc:0.79632\n",
            "[32]\tvalidation_0-auc:0.796486\n",
            "[33]\tvalidation_0-auc:0.7967\n",
            "[34]\tvalidation_0-auc:0.797175\n",
            "[35]\tvalidation_0-auc:0.795489\n",
            "[36]\tvalidation_0-auc:0.795394\n",
            "[37]\tvalidation_0-auc:0.795893\n",
            "[38]\tvalidation_0-auc:0.796937\n",
            "[39]\tvalidation_0-auc:0.797317\n",
            "[40]\tvalidation_0-auc:0.797365\n",
            "[41]\tvalidation_0-auc:0.794967\n",
            "[42]\tvalidation_0-auc:0.795726\n",
            "[43]\tvalidation_0-auc:0.796629\n",
            "[44]\tvalidation_0-auc:0.79518\n",
            "[45]\tvalidation_0-auc:0.79518\n",
            "[46]\tvalidation_0-auc:0.795133\n",
            "[47]\tvalidation_0-auc:0.794041\n",
            "[48]\tvalidation_0-auc:0.792141\n",
            "[49]\tvalidation_0-auc:0.792806\n",
            "[50]\tvalidation_0-auc:0.79188\n",
            "[51]\tvalidation_0-auc:0.79378\n",
            "[52]\tvalidation_0-auc:0.795275\n",
            "Stopping. Best iteration:\n",
            "[2]\tvalidation_0-auc:0.816287\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.763265306122449  |         0.34        |        0.75        |  0.4678899082568807 |\n",
            "|     GRU 0.1      | 0.6469387755102041 | 0.26666666666666666 | 0.8823529411764706 |  0.4095563139931741 |\n",
            "|   XGBoost 0.1    | 0.7224489795918367 | 0.31521739130434784 | 0.8529411764705882 | 0.46031746031746035 |\n",
            "|    Logreg 0.1    | 0.6306122448979592 | 0.25957446808510637 | 0.8970588235294118 |  0.4026402640264027 |\n",
            "|     SVM 0.1      | 0.6938775510204082 | 0.29292929292929293 | 0.8529411764705882 | 0.43609022556390975 |\n",
            "|  LSTM beta 0.1   | 0.5382932166301969 | 0.18823529411764706 | 0.9230769230769231 |  0.3127035830618893 |\n",
            "|   GRU beta 0.1   | 0.6542669584245077 |  0.2268041237113402 | 0.8461538461538461 | 0.35772357723577236 |\n",
            "| XGBoost beta 0.1 | 0.6914660831509847 | 0.24277456647398843 | 0.8076923076923077 | 0.37333333333333335 |\n",
            "| logreg beta 0.1  | 0.7089715536105032 |  0.2711864406779661 | 0.9230769230769231 | 0.41921397379912667 |\n",
            "|   svm beta 0.1   | 0.6761487964989059 | 0.23626373626373626 | 0.8269230769230769 |  0.3675213675213675 |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6956 - accuracy: 0.5094 - val_loss: 0.6761 - val_accuracy: 0.9306\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6946 - accuracy: 0.5101 - val_loss: 0.6489 - val_accuracy: 0.9306\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6906 - accuracy: 0.5141 - val_loss: 0.6791 - val_accuracy: 0.7633\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6732 - accuracy: 0.5772 - val_loss: 0.4925 - val_accuracy: 0.8265\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6582 - accuracy: 0.6309 - val_loss: 0.5044 - val_accuracy: 0.8102\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6917 - accuracy: 0.5336 - val_loss: 0.6914 - val_accuracy: 0.5408\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6569 - accuracy: 0.6094 - val_loss: 0.5989 - val_accuracy: 0.7510\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6351 - accuracy: 0.6624 - val_loss: 0.5092 - val_accuracy: 0.8429\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6358 - accuracy: 0.6517 - val_loss: 0.5781 - val_accuracy: 0.7286\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6332 - accuracy: 0.6477 - val_loss: 0.5287 - val_accuracy: 0.7673\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.824207\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.843105\n",
            "[2]\tvalidation_0-auc:0.856037\n",
            "[3]\tvalidation_0-auc:0.865422\n",
            "[4]\tvalidation_0-auc:0.875839\n",
            "[5]\tvalidation_0-auc:0.872872\n",
            "[6]\tvalidation_0-auc:0.869614\n",
            "[7]\tvalidation_0-auc:0.872485\n",
            "[8]\tvalidation_0-auc:0.875419\n",
            "[9]\tvalidation_0-auc:0.87945\n",
            "[10]\tvalidation_0-auc:0.881192\n",
            "[11]\tvalidation_0-auc:0.880128\n",
            "[12]\tvalidation_0-auc:0.882643\n",
            "[13]\tvalidation_0-auc:0.879257\n",
            "[14]\tvalidation_0-auc:0.882482\n",
            "[15]\tvalidation_0-auc:0.881482\n",
            "[16]\tvalidation_0-auc:0.882579\n",
            "[17]\tvalidation_0-auc:0.882417\n",
            "[18]\tvalidation_0-auc:0.883578\n",
            "[19]\tvalidation_0-auc:0.883804\n",
            "[20]\tvalidation_0-auc:0.88532\n",
            "[21]\tvalidation_0-auc:0.886094\n",
            "[22]\tvalidation_0-auc:0.885997\n",
            "[23]\tvalidation_0-auc:0.886094\n",
            "[24]\tvalidation_0-auc:0.889125\n",
            "[25]\tvalidation_0-auc:0.888867\n",
            "[26]\tvalidation_0-auc:0.89048\n",
            "[27]\tvalidation_0-auc:0.892221\n",
            "[28]\tvalidation_0-auc:0.892157\n",
            "[29]\tvalidation_0-auc:0.89364\n",
            "[30]\tvalidation_0-auc:0.892286\n",
            "[31]\tvalidation_0-auc:0.893189\n",
            "[32]\tvalidation_0-auc:0.892608\n",
            "[33]\tvalidation_0-auc:0.892673\n",
            "[34]\tvalidation_0-auc:0.891576\n",
            "[35]\tvalidation_0-auc:0.890738\n",
            "[36]\tvalidation_0-auc:0.891254\n",
            "[37]\tvalidation_0-auc:0.893318\n",
            "[38]\tvalidation_0-auc:0.892673\n",
            "[39]\tvalidation_0-auc:0.892415\n",
            "[40]\tvalidation_0-auc:0.893382\n",
            "[41]\tvalidation_0-auc:0.892737\n",
            "[42]\tvalidation_0-auc:0.890931\n",
            "[43]\tvalidation_0-auc:0.89177\n",
            "[44]\tvalidation_0-auc:0.89048\n",
            "[45]\tvalidation_0-auc:0.889125\n",
            "[46]\tvalidation_0-auc:0.889899\n",
            "[47]\tvalidation_0-auc:0.890738\n",
            "[48]\tvalidation_0-auc:0.890028\n",
            "[49]\tvalidation_0-auc:0.888609\n",
            "[50]\tvalidation_0-auc:0.888996\n",
            "[51]\tvalidation_0-auc:0.889641\n",
            "[52]\tvalidation_0-auc:0.889254\n",
            "[53]\tvalidation_0-auc:0.889706\n",
            "[54]\tvalidation_0-auc:0.889641\n",
            "[55]\tvalidation_0-auc:0.888416\n",
            "[56]\tvalidation_0-auc:0.888867\n",
            "[57]\tvalidation_0-auc:0.887255\n",
            "[58]\tvalidation_0-auc:0.887771\n",
            "[59]\tvalidation_0-auc:0.887835\n",
            "[60]\tvalidation_0-auc:0.886868\n",
            "[61]\tvalidation_0-auc:0.887126\n",
            "[62]\tvalidation_0-auc:0.885578\n",
            "[63]\tvalidation_0-auc:0.885707\n",
            "[64]\tvalidation_0-auc:0.8859\n",
            "[65]\tvalidation_0-auc:0.884804\n",
            "[66]\tvalidation_0-auc:0.882482\n",
            "[67]\tvalidation_0-auc:0.883127\n",
            "[68]\tvalidation_0-auc:0.883127\n",
            "[69]\tvalidation_0-auc:0.883514\n",
            "[70]\tvalidation_0-auc:0.883836\n",
            "[71]\tvalidation_0-auc:0.883385\n",
            "[72]\tvalidation_0-auc:0.883578\n",
            "[73]\tvalidation_0-auc:0.882933\n",
            "[74]\tvalidation_0-auc:0.881127\n",
            "[75]\tvalidation_0-auc:0.881321\n",
            "[76]\tvalidation_0-auc:0.880095\n",
            "[77]\tvalidation_0-auc:0.880353\n",
            "[78]\tvalidation_0-auc:0.880289\n",
            "[79]\tvalidation_0-auc:0.878741\n",
            "Stopping. Best iteration:\n",
            "[29]\tvalidation_0-auc:0.89364\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6932 - accuracy: 0.5216 - val_loss: 0.6723 - val_accuracy: 0.9606\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6719 - accuracy: 0.5964 - val_loss: 0.4886 - val_accuracy: 0.8796\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6613 - accuracy: 0.6239 - val_loss: 0.6602 - val_accuracy: 0.6280\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6474 - accuracy: 0.6328 - val_loss: 0.5115 - val_accuracy: 0.8446\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6201 - accuracy: 0.6740 - val_loss: 0.6009 - val_accuracy: 0.6696\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 17ms/step - loss: 0.6771 - accuracy: 0.5669 - val_loss: 0.5726 - val_accuracy: 0.8884\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6329 - accuracy: 0.6754 - val_loss: 0.5474 - val_accuracy: 0.8337\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6239 - accuracy: 0.6774 - val_loss: 0.7012 - val_accuracy: 0.5558\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6208 - accuracy: 0.6767 - val_loss: 0.6305 - val_accuracy: 0.6652\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6088 - accuracy: 0.6898 - val_loss: 0.4842 - val_accuracy: 0.8009\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.608643\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.633637\n",
            "[2]\tvalidation_0-auc:0.662237\n",
            "[3]\tvalidation_0-auc:0.675588\n",
            "[4]\tvalidation_0-auc:0.66306\n",
            "[5]\tvalidation_0-auc:0.660213\n",
            "[6]\tvalidation_0-auc:0.65262\n",
            "[7]\tvalidation_0-auc:0.652746\n",
            "[8]\tvalidation_0-auc:0.661541\n",
            "[9]\tvalidation_0-auc:0.658631\n",
            "[10]\tvalidation_0-auc:0.655973\n",
            "[11]\tvalidation_0-auc:0.663946\n",
            "[12]\tvalidation_0-auc:0.67426\n",
            "[13]\tvalidation_0-auc:0.66616\n",
            "[14]\tvalidation_0-auc:0.664768\n",
            "[15]\tvalidation_0-auc:0.657049\n",
            "[16]\tvalidation_0-auc:0.650089\n",
            "[17]\tvalidation_0-auc:0.650215\n",
            "[18]\tvalidation_0-auc:0.650974\n",
            "[19]\tvalidation_0-auc:0.648317\n",
            "[20]\tvalidation_0-auc:0.653126\n",
            "[21]\tvalidation_0-auc:0.65224\n",
            "[22]\tvalidation_0-auc:0.647937\n",
            "[23]\tvalidation_0-auc:0.647558\n",
            "[24]\tvalidation_0-auc:0.643824\n",
            "[25]\tvalidation_0-auc:0.649456\n",
            "[26]\tvalidation_0-auc:0.649456\n",
            "[27]\tvalidation_0-auc:0.649456\n",
            "[28]\tvalidation_0-auc:0.649835\n",
            "[29]\tvalidation_0-auc:0.649835\n",
            "[30]\tvalidation_0-auc:0.652556\n",
            "[31]\tvalidation_0-auc:0.650278\n",
            "[32]\tvalidation_0-auc:0.650278\n",
            "[33]\tvalidation_0-auc:0.651038\n",
            "[34]\tvalidation_0-auc:0.654455\n",
            "[35]\tvalidation_0-auc:0.655847\n",
            "[36]\tvalidation_0-auc:0.658125\n",
            "[37]\tvalidation_0-auc:0.664199\n",
            "[38]\tvalidation_0-auc:0.662807\n",
            "[39]\tvalidation_0-auc:0.666287\n",
            "[40]\tvalidation_0-auc:0.665654\n",
            "[41]\tvalidation_0-auc:0.665528\n",
            "[42]\tvalidation_0-auc:0.665591\n",
            "[43]\tvalidation_0-auc:0.665591\n",
            "[44]\tvalidation_0-auc:0.665211\n",
            "[45]\tvalidation_0-auc:0.664199\n",
            "[46]\tvalidation_0-auc:0.664199\n",
            "[47]\tvalidation_0-auc:0.664705\n",
            "[48]\tvalidation_0-auc:0.664705\n",
            "[49]\tvalidation_0-auc:0.663819\n",
            "[50]\tvalidation_0-auc:0.663313\n",
            "[51]\tvalidation_0-auc:0.661415\n",
            "[52]\tvalidation_0-auc:0.661541\n",
            "[53]\tvalidation_0-auc:0.661541\n",
            "Stopping. Best iteration:\n",
            "[3]\tvalidation_0-auc:0.675588\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+----------------------+--------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision       |       Recall       |       F1 score      |\n",
            "+-------------------+--------------------+----------------------+--------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.810204081632653  | 0.22429906542056074  | 0.7058823529411765 |  0.3404255319148936 |\n",
            "|      GRU 0.15     | 0.7673469387755102 | 0.20588235294117646  | 0.8235294117647058 | 0.32941176470588235 |\n",
            "|    XGBoost 0.15   | 0.6408163265306123 |  0.1485148514851485  | 0.8823529411764706 | 0.25423728813559326 |\n",
            "|    Logreg 0.15    | 0.7204081632653061 | 0.18012422360248448  | 0.8529411764705882 | 0.29743589743589743 |\n",
            "|      SVM 0.15     | 0.7040816326530612 | 0.17543859649122806  | 0.8823529411764706 |  0.2926829268292682 |\n",
            "|   LSTM beta 0.15  | 0.6695842450765864 | 0.059602649006622516 |        0.5         | 0.10650887573964497 |\n",
            "|   GRU beta 0.15   | 0.8008752735229759 | 0.08045977011494253  | 0.3888888888888889 | 0.13333333333333333 |\n",
            "| XGBoost beta 0.15 | 0.6148796498905909 | 0.056179775280898875 | 0.5555555555555556 |  0.1020408163265306 |\n",
            "|  logreg beta 0.15 | 0.7636761487964989 | 0.08333333333333333  |        0.5         | 0.14285714285714285 |\n",
            "|   svm beta 0.15   | 0.6739606126914661 | 0.06040268456375839  |        0.5         | 0.10778443113772455 |\n",
            "+-------------------+--------------------+----------------------+--------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzc3B4L-WILk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3f2ddd43-eafa-40c4-ff71-2d06a043ebba"
      },
      "source": [
        "Result_cross.to_csv('FOXA_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.763265</td>\n",
              "      <td>0.467890</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.646939</td>\n",
              "      <td>0.409556</td>\n",
              "      <td>0.882353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.315217</td>\n",
              "      <td>0.722449</td>\n",
              "      <td>0.460317</td>\n",
              "      <td>0.852941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.259574</td>\n",
              "      <td>0.630612</td>\n",
              "      <td>0.402640</td>\n",
              "      <td>0.897059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.292929</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>0.436090</td>\n",
              "      <td>0.852941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.188235</td>\n",
              "      <td>0.538293</td>\n",
              "      <td>0.312704</td>\n",
              "      <td>0.923077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.226804</td>\n",
              "      <td>0.654267</td>\n",
              "      <td>0.357724</td>\n",
              "      <td>0.846154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.242775</td>\n",
              "      <td>0.691466</td>\n",
              "      <td>0.373333</td>\n",
              "      <td>0.807692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.271186</td>\n",
              "      <td>0.708972</td>\n",
              "      <td>0.419214</td>\n",
              "      <td>0.923077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.236264</td>\n",
              "      <td>0.676149</td>\n",
              "      <td>0.367521</td>\n",
              "      <td>0.826923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.224299</td>\n",
              "      <td>0.810204</td>\n",
              "      <td>0.340426</td>\n",
              "      <td>0.705882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.205882</td>\n",
              "      <td>0.767347</td>\n",
              "      <td>0.329412</td>\n",
              "      <td>0.823529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.148515</td>\n",
              "      <td>0.640816</td>\n",
              "      <td>0.254237</td>\n",
              "      <td>0.882353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.180124</td>\n",
              "      <td>0.720408</td>\n",
              "      <td>0.297436</td>\n",
              "      <td>0.852941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.175439</td>\n",
              "      <td>0.704082</td>\n",
              "      <td>0.292683</td>\n",
              "      <td>0.882353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.059603</td>\n",
              "      <td>0.669584</td>\n",
              "      <td>0.106509</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.080460</td>\n",
              "      <td>0.800875</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.388889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.056180</td>\n",
              "      <td>0.614880</td>\n",
              "      <td>0.102041</td>\n",
              "      <td>0.555556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.763676</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.060403</td>\n",
              "      <td>0.673961</td>\n",
              "      <td>0.107784</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  FOXA  0.340000  0.763265  0.467890  0.750000\n",
              "1            GRU 0.1  FOXA  0.266667  0.646939  0.409556  0.882353\n",
              "2        XGBoost 0.1  FOXA  0.315217  0.722449  0.460317  0.852941\n",
              "3         Logreg 0.1  FOXA  0.259574  0.630612  0.402640  0.897059\n",
              "4            SVM 0.1  FOXA  0.292929  0.693878  0.436090  0.852941\n",
              "5      LSTM beta 0.1  FOXA  0.188235  0.538293  0.312704  0.923077\n",
              "6       GRU beta 0.1  FOXA  0.226804  0.654267  0.357724  0.846154\n",
              "7   XGBoost beta 0.1  FOXA  0.242775  0.691466  0.373333  0.807692\n",
              "8    logreg beta 0.1  FOXA  0.271186  0.708972  0.419214  0.923077\n",
              "9       svm beta 0.1  FOXA  0.236264  0.676149  0.367521  0.826923\n",
              "0          LSTM 0.15  FOXA  0.224299  0.810204  0.340426  0.705882\n",
              "1           GRU 0.15  FOXA  0.205882  0.767347  0.329412  0.823529\n",
              "2       XGBoost 0.15  FOXA  0.148515  0.640816  0.254237  0.882353\n",
              "3        Logreg 0.15  FOXA  0.180124  0.720408  0.297436  0.852941\n",
              "4           SVM 0.15  FOXA  0.175439  0.704082  0.292683  0.882353\n",
              "5     LSTM beta 0.15  FOXA  0.059603  0.669584  0.106509  0.500000\n",
              "6      GRU beta 0.15  FOXA  0.080460  0.800875  0.133333  0.388889\n",
              "7  XGBoost beta 0.15  FOXA  0.056180  0.614880  0.102041  0.555556\n",
              "8   logreg beta 0.15  FOXA  0.083333  0.763676  0.142857  0.500000\n",
              "9      svm beta 0.15  FOXA  0.060403  0.673961  0.107784  0.500000"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRx5B2JfWILk"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_logreg_beta.csv')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_ZpxDYiWILk"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4MQOtwtWILk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6cf726b-ee6a-4520-e45e-91e8f80d677f"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"FOXA\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6940 - accuracy: 0.5148 - val_loss: 0.7334 - val_accuracy: 0.1388\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6830 - accuracy: 0.5497 - val_loss: 0.6837 - val_accuracy: 0.6204\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6416 - accuracy: 0.6477 - val_loss: 0.6966 - val_accuracy: 0.6286\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6243 - accuracy: 0.6577 - val_loss: 0.6314 - val_accuracy: 0.7102\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6286 - accuracy: 0.6631 - val_loss: 0.6205 - val_accuracy: 0.7143\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6856 - accuracy: 0.5477 - val_loss: 0.5595 - val_accuracy: 0.8816\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6441 - accuracy: 0.6477 - val_loss: 0.6059 - val_accuracy: 0.7082\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6177 - accuracy: 0.6785 - val_loss: 0.6222 - val_accuracy: 0.6878\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6036 - accuracy: 0.6953 - val_loss: 0.6938 - val_accuracy: 0.6122\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6070 - accuracy: 0.6919 - val_loss: 0.5837 - val_accuracy: 0.7367\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.834245\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.848881\n",
            "[2]\tvalidation_0-auc:0.847244\n",
            "[3]\tvalidation_0-auc:0.843009\n",
            "[4]\tvalidation_0-auc:0.851739\n",
            "[5]\tvalidation_0-auc:0.855171\n",
            "[6]\tvalidation_0-auc:0.855799\n",
            "[7]\tvalidation_0-auc:0.856792\n",
            "[8]\tvalidation_0-auc:0.85498\n",
            "[9]\tvalidation_0-auc:0.853185\n",
            "[10]\tvalidation_0-auc:0.853882\n",
            "[11]\tvalidation_0-auc:0.850885\n",
            "[12]\tvalidation_0-auc:0.853534\n",
            "[13]\tvalidation_0-auc:0.853081\n",
            "[14]\tvalidation_0-auc:0.855241\n",
            "[15]\tvalidation_0-auc:0.855311\n",
            "[16]\tvalidation_0-auc:0.857245\n",
            "[17]\tvalidation_0-auc:0.861462\n",
            "[18]\tvalidation_0-auc:0.860974\n",
            "[19]\tvalidation_0-auc:0.860782\n",
            "[20]\tvalidation_0-auc:0.859911\n",
            "[21]\tvalidation_0-auc:0.858918\n",
            "[22]\tvalidation_0-auc:0.858743\n",
            "[23]\tvalidation_0-auc:0.858064\n",
            "[24]\tvalidation_0-auc:0.857611\n",
            "[25]\tvalidation_0-auc:0.857924\n",
            "[26]\tvalidation_0-auc:0.85836\n",
            "[27]\tvalidation_0-auc:0.857803\n",
            "[28]\tvalidation_0-auc:0.858621\n",
            "[29]\tvalidation_0-auc:0.858447\n",
            "[30]\tvalidation_0-auc:0.859284\n",
            "[31]\tvalidation_0-auc:0.857977\n",
            "[32]\tvalidation_0-auc:0.85829\n",
            "[33]\tvalidation_0-auc:0.858256\n",
            "[34]\tvalidation_0-auc:0.856652\n",
            "[35]\tvalidation_0-auc:0.856687\n",
            "[36]\tvalidation_0-auc:0.8566\n",
            "[37]\tvalidation_0-auc:0.856513\n",
            "[38]\tvalidation_0-auc:0.856234\n",
            "[39]\tvalidation_0-auc:0.856356\n",
            "[40]\tvalidation_0-auc:0.856635\n",
            "[41]\tvalidation_0-auc:0.856008\n",
            "[42]\tvalidation_0-auc:0.855729\n",
            "[43]\tvalidation_0-auc:0.855729\n",
            "[44]\tvalidation_0-auc:0.854021\n",
            "[45]\tvalidation_0-auc:0.855311\n",
            "[46]\tvalidation_0-auc:0.854474\n",
            "[47]\tvalidation_0-auc:0.854997\n",
            "[48]\tvalidation_0-auc:0.855137\n",
            "[49]\tvalidation_0-auc:0.855171\n",
            "[50]\tvalidation_0-auc:0.85545\n",
            "[51]\tvalidation_0-auc:0.85545\n",
            "[52]\tvalidation_0-auc:0.854962\n",
            "[53]\tvalidation_0-auc:0.855171\n",
            "[54]\tvalidation_0-auc:0.855485\n",
            "[55]\tvalidation_0-auc:0.85444\n",
            "[56]\tvalidation_0-auc:0.854021\n",
            "[57]\tvalidation_0-auc:0.852279\n",
            "[58]\tvalidation_0-auc:0.850362\n",
            "[59]\tvalidation_0-auc:0.851547\n",
            "[60]\tvalidation_0-auc:0.851512\n",
            "[61]\tvalidation_0-auc:0.851478\n",
            "[62]\tvalidation_0-auc:0.851025\n",
            "[63]\tvalidation_0-auc:0.850641\n",
            "[64]\tvalidation_0-auc:0.85099\n",
            "[65]\tvalidation_0-auc:0.850676\n",
            "[66]\tvalidation_0-auc:0.850362\n",
            "[67]\tvalidation_0-auc:0.850397\n",
            "Stopping. Best iteration:\n",
            "[17]\tvalidation_0-auc:0.861462\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 19ms/step - loss: 0.6930 - accuracy: 0.5264 - val_loss: 0.6889 - val_accuracy: 0.6674\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6831 - accuracy: 0.5566 - val_loss: 0.7265 - val_accuracy: 0.4792\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6633 - accuracy: 0.6342 - val_loss: 0.7032 - val_accuracy: 0.5120\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6426 - accuracy: 0.6568 - val_loss: 0.6898 - val_accuracy: 0.5974\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6216 - accuracy: 0.6699 - val_loss: 0.5190 - val_accuracy: 0.8053\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6853 - accuracy: 0.5463 - val_loss: 0.7593 - val_accuracy: 0.1641\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6405 - accuracy: 0.6555 - val_loss: 0.6959 - val_accuracy: 0.5470\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6245 - accuracy: 0.6822 - val_loss: 0.6733 - val_accuracy: 0.6018\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6002 - accuracy: 0.6747 - val_loss: 0.5653 - val_accuracy: 0.7352\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5878 - accuracy: 0.7111 - val_loss: 0.5208 - val_accuracy: 0.7987\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.792213\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.786087\n",
            "[2]\tvalidation_0-auc:0.816287\n",
            "[3]\tvalidation_0-auc:0.786491\n",
            "[4]\tvalidation_0-auc:0.786396\n",
            "[5]\tvalidation_0-auc:0.784663\n",
            "[6]\tvalidation_0-auc:0.795014\n",
            "[7]\tvalidation_0-auc:0.801235\n",
            "[8]\tvalidation_0-auc:0.808025\n",
            "[9]\tvalidation_0-auc:0.802683\n",
            "[10]\tvalidation_0-auc:0.802303\n",
            "[11]\tvalidation_0-auc:0.796178\n",
            "[12]\tvalidation_0-auc:0.80254\n",
            "[13]\tvalidation_0-auc:0.798837\n",
            "[14]\tvalidation_0-auc:0.805318\n",
            "[15]\tvalidation_0-auc:0.800997\n",
            "[16]\tvalidation_0-auc:0.800475\n",
            "[17]\tvalidation_0-auc:0.80133\n",
            "[18]\tvalidation_0-auc:0.801852\n",
            "[19]\tvalidation_0-auc:0.796771\n",
            "[20]\tvalidation_0-auc:0.798267\n",
            "[21]\tvalidation_0-auc:0.798552\n",
            "[22]\tvalidation_0-auc:0.796273\n",
            "[23]\tvalidation_0-auc:0.794539\n",
            "[24]\tvalidation_0-auc:0.791263\n",
            "[25]\tvalidation_0-auc:0.793495\n",
            "[26]\tvalidation_0-auc:0.794539\n",
            "[27]\tvalidation_0-auc:0.794658\n",
            "[28]\tvalidation_0-auc:0.794658\n",
            "[29]\tvalidation_0-auc:0.795679\n",
            "[30]\tvalidation_0-auc:0.795916\n",
            "[31]\tvalidation_0-auc:0.79632\n",
            "[32]\tvalidation_0-auc:0.796486\n",
            "[33]\tvalidation_0-auc:0.7967\n",
            "[34]\tvalidation_0-auc:0.797175\n",
            "[35]\tvalidation_0-auc:0.795489\n",
            "[36]\tvalidation_0-auc:0.795394\n",
            "[37]\tvalidation_0-auc:0.795893\n",
            "[38]\tvalidation_0-auc:0.796937\n",
            "[39]\tvalidation_0-auc:0.797317\n",
            "[40]\tvalidation_0-auc:0.797365\n",
            "[41]\tvalidation_0-auc:0.794967\n",
            "[42]\tvalidation_0-auc:0.795726\n",
            "[43]\tvalidation_0-auc:0.796629\n",
            "[44]\tvalidation_0-auc:0.79518\n",
            "[45]\tvalidation_0-auc:0.79518\n",
            "[46]\tvalidation_0-auc:0.795133\n",
            "[47]\tvalidation_0-auc:0.794041\n",
            "[48]\tvalidation_0-auc:0.792141\n",
            "[49]\tvalidation_0-auc:0.792806\n",
            "[50]\tvalidation_0-auc:0.79188\n",
            "[51]\tvalidation_0-auc:0.79378\n",
            "[52]\tvalidation_0-auc:0.795275\n",
            "Stopping. Best iteration:\n",
            "[2]\tvalidation_0-auc:0.816287\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.7142857142857143 | 0.29775280898876405 | 0.7794117647058824 |  0.4308943089430894 |\n",
            "|     GRU 0.1      | 0.736734693877551  |  0.327683615819209  | 0.8529411764705882 |  0.473469387755102  |\n",
            "|   XGBoost 0.1    | 0.7224489795918367 | 0.31521739130434784 | 0.8529411764705882 | 0.46031746031746035 |\n",
            "|    Logreg 0.1    | 0.6306122448979592 | 0.25957446808510637 | 0.8970588235294118 |  0.4026402640264027 |\n",
            "|     SVM 0.1      | 0.6938775510204082 | 0.29292929292929293 | 0.8529411764705882 | 0.43609022556390975 |\n",
            "|  LSTM beta 0.1   | 0.8052516411378556 |  0.3418803418803419 | 0.7692307692307693 | 0.47337278106508873 |\n",
            "|   GRU beta 0.1   | 0.7986870897155361 | 0.32456140350877194 | 0.7115384615384616 | 0.44578313253012053 |\n",
            "| XGBoost beta 0.1 | 0.6914660831509847 | 0.24277456647398843 | 0.8076923076923077 | 0.37333333333333335 |\n",
            "| logreg beta 0.1  | 0.7089715536105032 |  0.2711864406779661 | 0.9230769230769231 | 0.41921397379912667 |\n",
            "|   svm beta 0.1   | 0.6761487964989059 | 0.23626373626373626 | 0.8269230769230769 |  0.3675213675213675 |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6948 - accuracy: 0.5020 - val_loss: 0.7211 - val_accuracy: 0.0694\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6821 - accuracy: 0.5671 - val_loss: 0.5609 - val_accuracy: 0.8490\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6511 - accuracy: 0.6235 - val_loss: 0.6390 - val_accuracy: 0.6816\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6459 - accuracy: 0.6517 - val_loss: 0.5611 - val_accuracy: 0.7857\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6454 - accuracy: 0.6416 - val_loss: 0.5606 - val_accuracy: 0.7959\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6906 - accuracy: 0.5383 - val_loss: 0.6905 - val_accuracy: 0.5327\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6510 - accuracy: 0.6268 - val_loss: 0.6026 - val_accuracy: 0.6694\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6324 - accuracy: 0.6591 - val_loss: 0.6028 - val_accuracy: 0.6898\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6248 - accuracy: 0.6678 - val_loss: 0.5525 - val_accuracy: 0.7531\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6214 - accuracy: 0.6671 - val_loss: 0.5756 - val_accuracy: 0.7367\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.824207\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.843105\n",
            "[2]\tvalidation_0-auc:0.856037\n",
            "[3]\tvalidation_0-auc:0.865422\n",
            "[4]\tvalidation_0-auc:0.875839\n",
            "[5]\tvalidation_0-auc:0.872872\n",
            "[6]\tvalidation_0-auc:0.869614\n",
            "[7]\tvalidation_0-auc:0.872485\n",
            "[8]\tvalidation_0-auc:0.875419\n",
            "[9]\tvalidation_0-auc:0.87945\n",
            "[10]\tvalidation_0-auc:0.881192\n",
            "[11]\tvalidation_0-auc:0.880128\n",
            "[12]\tvalidation_0-auc:0.882643\n",
            "[13]\tvalidation_0-auc:0.879257\n",
            "[14]\tvalidation_0-auc:0.882482\n",
            "[15]\tvalidation_0-auc:0.881482\n",
            "[16]\tvalidation_0-auc:0.882579\n",
            "[17]\tvalidation_0-auc:0.882417\n",
            "[18]\tvalidation_0-auc:0.883578\n",
            "[19]\tvalidation_0-auc:0.883804\n",
            "[20]\tvalidation_0-auc:0.88532\n",
            "[21]\tvalidation_0-auc:0.886094\n",
            "[22]\tvalidation_0-auc:0.885997\n",
            "[23]\tvalidation_0-auc:0.886094\n",
            "[24]\tvalidation_0-auc:0.889125\n",
            "[25]\tvalidation_0-auc:0.888867\n",
            "[26]\tvalidation_0-auc:0.89048\n",
            "[27]\tvalidation_0-auc:0.892221\n",
            "[28]\tvalidation_0-auc:0.892157\n",
            "[29]\tvalidation_0-auc:0.89364\n",
            "[30]\tvalidation_0-auc:0.892286\n",
            "[31]\tvalidation_0-auc:0.893189\n",
            "[32]\tvalidation_0-auc:0.892608\n",
            "[33]\tvalidation_0-auc:0.892673\n",
            "[34]\tvalidation_0-auc:0.891576\n",
            "[35]\tvalidation_0-auc:0.890738\n",
            "[36]\tvalidation_0-auc:0.891254\n",
            "[37]\tvalidation_0-auc:0.893318\n",
            "[38]\tvalidation_0-auc:0.892673\n",
            "[39]\tvalidation_0-auc:0.892415\n",
            "[40]\tvalidation_0-auc:0.893382\n",
            "[41]\tvalidation_0-auc:0.892737\n",
            "[42]\tvalidation_0-auc:0.890931\n",
            "[43]\tvalidation_0-auc:0.89177\n",
            "[44]\tvalidation_0-auc:0.89048\n",
            "[45]\tvalidation_0-auc:0.889125\n",
            "[46]\tvalidation_0-auc:0.889899\n",
            "[47]\tvalidation_0-auc:0.890738\n",
            "[48]\tvalidation_0-auc:0.890028\n",
            "[49]\tvalidation_0-auc:0.888609\n",
            "[50]\tvalidation_0-auc:0.888996\n",
            "[51]\tvalidation_0-auc:0.889641\n",
            "[52]\tvalidation_0-auc:0.889254\n",
            "[53]\tvalidation_0-auc:0.889706\n",
            "[54]\tvalidation_0-auc:0.889641\n",
            "[55]\tvalidation_0-auc:0.888416\n",
            "[56]\tvalidation_0-auc:0.888867\n",
            "[57]\tvalidation_0-auc:0.887255\n",
            "[58]\tvalidation_0-auc:0.887771\n",
            "[59]\tvalidation_0-auc:0.887835\n",
            "[60]\tvalidation_0-auc:0.886868\n",
            "[61]\tvalidation_0-auc:0.887126\n",
            "[62]\tvalidation_0-auc:0.885578\n",
            "[63]\tvalidation_0-auc:0.885707\n",
            "[64]\tvalidation_0-auc:0.8859\n",
            "[65]\tvalidation_0-auc:0.884804\n",
            "[66]\tvalidation_0-auc:0.882482\n",
            "[67]\tvalidation_0-auc:0.883127\n",
            "[68]\tvalidation_0-auc:0.883127\n",
            "[69]\tvalidation_0-auc:0.883514\n",
            "[70]\tvalidation_0-auc:0.883836\n",
            "[71]\tvalidation_0-auc:0.883385\n",
            "[72]\tvalidation_0-auc:0.883578\n",
            "[73]\tvalidation_0-auc:0.882933\n",
            "[74]\tvalidation_0-auc:0.881127\n",
            "[75]\tvalidation_0-auc:0.881321\n",
            "[76]\tvalidation_0-auc:0.880095\n",
            "[77]\tvalidation_0-auc:0.880353\n",
            "[78]\tvalidation_0-auc:0.880289\n",
            "[79]\tvalidation_0-auc:0.878741\n",
            "Stopping. Best iteration:\n",
            "[29]\tvalidation_0-auc:0.89364\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6954 - accuracy: 0.5072 - val_loss: 0.6702 - val_accuracy: 0.9606\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6937 - accuracy: 0.5161 - val_loss: 0.6487 - val_accuracy: 0.9606\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6934 - accuracy: 0.5106 - val_loss: 0.6276 - val_accuracy: 0.9606\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6820 - accuracy: 0.5559 - val_loss: 0.4576 - val_accuracy: 0.8621\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6531 - accuracy: 0.6239 - val_loss: 0.6244 - val_accuracy: 0.6630\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6819 - accuracy: 0.5875 - val_loss: 0.6790 - val_accuracy: 0.5908\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6364 - accuracy: 0.6507 - val_loss: 0.5995 - val_accuracy: 0.7309\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6214 - accuracy: 0.6719 - val_loss: 0.6161 - val_accuracy: 0.6871\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6160 - accuracy: 0.6884 - val_loss: 0.6356 - val_accuracy: 0.6521\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6168 - accuracy: 0.6726 - val_loss: 0.4869 - val_accuracy: 0.7746\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.608643\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.633637\n",
            "[2]\tvalidation_0-auc:0.662237\n",
            "[3]\tvalidation_0-auc:0.675588\n",
            "[4]\tvalidation_0-auc:0.66306\n",
            "[5]\tvalidation_0-auc:0.660213\n",
            "[6]\tvalidation_0-auc:0.65262\n",
            "[7]\tvalidation_0-auc:0.652746\n",
            "[8]\tvalidation_0-auc:0.661541\n",
            "[9]\tvalidation_0-auc:0.658631\n",
            "[10]\tvalidation_0-auc:0.655973\n",
            "[11]\tvalidation_0-auc:0.663946\n",
            "[12]\tvalidation_0-auc:0.67426\n",
            "[13]\tvalidation_0-auc:0.66616\n",
            "[14]\tvalidation_0-auc:0.664768\n",
            "[15]\tvalidation_0-auc:0.657049\n",
            "[16]\tvalidation_0-auc:0.650089\n",
            "[17]\tvalidation_0-auc:0.650215\n",
            "[18]\tvalidation_0-auc:0.650974\n",
            "[19]\tvalidation_0-auc:0.648317\n",
            "[20]\tvalidation_0-auc:0.653126\n",
            "[21]\tvalidation_0-auc:0.65224\n",
            "[22]\tvalidation_0-auc:0.647937\n",
            "[23]\tvalidation_0-auc:0.647558\n",
            "[24]\tvalidation_0-auc:0.643824\n",
            "[25]\tvalidation_0-auc:0.649456\n",
            "[26]\tvalidation_0-auc:0.649456\n",
            "[27]\tvalidation_0-auc:0.649456\n",
            "[28]\tvalidation_0-auc:0.649835\n",
            "[29]\tvalidation_0-auc:0.649835\n",
            "[30]\tvalidation_0-auc:0.652556\n",
            "[31]\tvalidation_0-auc:0.650278\n",
            "[32]\tvalidation_0-auc:0.650278\n",
            "[33]\tvalidation_0-auc:0.651038\n",
            "[34]\tvalidation_0-auc:0.654455\n",
            "[35]\tvalidation_0-auc:0.655847\n",
            "[36]\tvalidation_0-auc:0.658125\n",
            "[37]\tvalidation_0-auc:0.664199\n",
            "[38]\tvalidation_0-auc:0.662807\n",
            "[39]\tvalidation_0-auc:0.666287\n",
            "[40]\tvalidation_0-auc:0.665654\n",
            "[41]\tvalidation_0-auc:0.665528\n",
            "[42]\tvalidation_0-auc:0.665591\n",
            "[43]\tvalidation_0-auc:0.665591\n",
            "[44]\tvalidation_0-auc:0.665211\n",
            "[45]\tvalidation_0-auc:0.664199\n",
            "[46]\tvalidation_0-auc:0.664199\n",
            "[47]\tvalidation_0-auc:0.664705\n",
            "[48]\tvalidation_0-auc:0.664705\n",
            "[49]\tvalidation_0-auc:0.663819\n",
            "[50]\tvalidation_0-auc:0.663313\n",
            "[51]\tvalidation_0-auc:0.661415\n",
            "[52]\tvalidation_0-auc:0.661541\n",
            "[53]\tvalidation_0-auc:0.661541\n",
            "Stopping. Best iteration:\n",
            "[3]\tvalidation_0-auc:0.675588\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+----------------------+--------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision       |       Recall       |       F1 score      |\n",
            "+-------------------+--------------------+----------------------+--------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.7959183673469388 | 0.22033898305084745  | 0.7647058823529411 |  0.3421052631578947 |\n",
            "|      GRU 0.15     | 0.736734693877551  |  0.1895424836601307  | 0.8529411764705882 | 0.31016042780748665 |\n",
            "|    XGBoost 0.15   | 0.6408163265306123 |  0.1485148514851485  | 0.8823529411764706 | 0.25423728813559326 |\n",
            "|    Logreg 0.15    | 0.7204081632653061 | 0.18012422360248448  | 0.8529411764705882 | 0.29743589743589743 |\n",
            "|      SVM 0.15     | 0.7040816326530612 | 0.17543859649122806  | 0.8823529411764706 |  0.2926829268292682 |\n",
            "|   LSTM beta 0.15  | 0.6630196936542669 | 0.04054054054054054  | 0.3333333333333333 | 0.07228915662650603 |\n",
            "|   GRU beta 0.15   | 0.774617067833698  |  0.0707070707070707  | 0.3888888888888889 | 0.11965811965811965 |\n",
            "| XGBoost beta 0.15 | 0.6148796498905909 | 0.056179775280898875 | 0.5555555555555556 |  0.1020408163265306 |\n",
            "|  logreg beta 0.15 | 0.7636761487964989 | 0.08333333333333333  |        0.5         | 0.14285714285714285 |\n",
            "|   svm beta 0.15   | 0.6739606126914661 | 0.06040268456375839  |        0.5         | 0.10778443113772455 |\n",
            "+-------------------+--------------------+----------------------+--------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOPYypMLWILk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9f492ea9-bc05-48df-f391-3061fb4fc8af"
      },
      "source": [
        "Result_purging.to_csv('FOXA_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.297753</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.430894</td>\n",
              "      <td>0.779412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.327684</td>\n",
              "      <td>0.736735</td>\n",
              "      <td>0.473469</td>\n",
              "      <td>0.852941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.315217</td>\n",
              "      <td>0.722449</td>\n",
              "      <td>0.460317</td>\n",
              "      <td>0.852941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.259574</td>\n",
              "      <td>0.630612</td>\n",
              "      <td>0.402640</td>\n",
              "      <td>0.897059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.292929</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>0.436090</td>\n",
              "      <td>0.852941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.341880</td>\n",
              "      <td>0.805252</td>\n",
              "      <td>0.473373</td>\n",
              "      <td>0.769231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.324561</td>\n",
              "      <td>0.798687</td>\n",
              "      <td>0.445783</td>\n",
              "      <td>0.711538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.242775</td>\n",
              "      <td>0.691466</td>\n",
              "      <td>0.373333</td>\n",
              "      <td>0.807692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.271186</td>\n",
              "      <td>0.708972</td>\n",
              "      <td>0.419214</td>\n",
              "      <td>0.923077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.236264</td>\n",
              "      <td>0.676149</td>\n",
              "      <td>0.367521</td>\n",
              "      <td>0.826923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.220339</td>\n",
              "      <td>0.795918</td>\n",
              "      <td>0.342105</td>\n",
              "      <td>0.764706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.189542</td>\n",
              "      <td>0.736735</td>\n",
              "      <td>0.310160</td>\n",
              "      <td>0.852941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.148515</td>\n",
              "      <td>0.640816</td>\n",
              "      <td>0.254237</td>\n",
              "      <td>0.882353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.180124</td>\n",
              "      <td>0.720408</td>\n",
              "      <td>0.297436</td>\n",
              "      <td>0.852941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.175439</td>\n",
              "      <td>0.704082</td>\n",
              "      <td>0.292683</td>\n",
              "      <td>0.882353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.040541</td>\n",
              "      <td>0.663020</td>\n",
              "      <td>0.072289</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.070707</td>\n",
              "      <td>0.774617</td>\n",
              "      <td>0.119658</td>\n",
              "      <td>0.388889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.056180</td>\n",
              "      <td>0.614880</td>\n",
              "      <td>0.102041</td>\n",
              "      <td>0.555556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.763676</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>FOXA</td>\n",
              "      <td>0.060403</td>\n",
              "      <td>0.673961</td>\n",
              "      <td>0.107784</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  FOXA  0.297753  0.714286  0.430894  0.779412\n",
              "1            GRU 0.1  FOXA  0.327684  0.736735  0.473469  0.852941\n",
              "2        XGBoost 0.1  FOXA  0.315217  0.722449  0.460317  0.852941\n",
              "3         Logreg 0.1  FOXA  0.259574  0.630612  0.402640  0.897059\n",
              "4            SVM 0.1  FOXA  0.292929  0.693878  0.436090  0.852941\n",
              "5      LSTM beta 0.1  FOXA  0.341880  0.805252  0.473373  0.769231\n",
              "6       GRU beta 0.1  FOXA  0.324561  0.798687  0.445783  0.711538\n",
              "7   XGBoost beta 0.1  FOXA  0.242775  0.691466  0.373333  0.807692\n",
              "8    logreg beta 0.1  FOXA  0.271186  0.708972  0.419214  0.923077\n",
              "9       svm beta 0.1  FOXA  0.236264  0.676149  0.367521  0.826923\n",
              "0          LSTM 0.15  FOXA  0.220339  0.795918  0.342105  0.764706\n",
              "1           GRU 0.15  FOXA  0.189542  0.736735  0.310160  0.852941\n",
              "2       XGBoost 0.15  FOXA  0.148515  0.640816  0.254237  0.882353\n",
              "3        Logreg 0.15  FOXA  0.180124  0.720408  0.297436  0.852941\n",
              "4           SVM 0.15  FOXA  0.175439  0.704082  0.292683  0.882353\n",
              "5     LSTM beta 0.15  FOXA  0.040541  0.663020  0.072289  0.333333\n",
              "6      GRU beta 0.15  FOXA  0.070707  0.774617  0.119658  0.388889\n",
              "7  XGBoost beta 0.15  FOXA  0.056180  0.614880  0.102041  0.555556\n",
              "8   logreg beta 0.15  FOXA  0.083333  0.763676  0.142857  0.500000\n",
              "9      svm beta 0.15  FOXA  0.060403  0.673961  0.107784  0.500000"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32qoUKvzWILk"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('FOXA_logreg_beta_p.csv')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bslUV9G5WILk"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KbGsMvQWlIW"
      },
      "source": [
        "## M"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNEWq2i_WlId",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9d65ef6a-1cec-4599-e7ec-b1a4a347fb13"
      },
      "source": [
        "dfs = pd.read_csv(\"M.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "dfs"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2768</td>\n",
              "      <td>US1.M</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>22.64</td>\n",
              "      <td>23.1400</td>\n",
              "      <td>22.21</td>\n",
              "      <td>22.87</td>\n",
              "      <td>703439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2767</td>\n",
              "      <td>US1.M</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>23.90</td>\n",
              "      <td>23.9000</td>\n",
              "      <td>22.34</td>\n",
              "      <td>22.60</td>\n",
              "      <td>1239730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.M</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>25.28</td>\n",
              "      <td>25.4600</td>\n",
              "      <td>24.67</td>\n",
              "      <td>24.70</td>\n",
              "      <td>386601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.M</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>24.69</td>\n",
              "      <td>25.4500</td>\n",
              "      <td>24.58</td>\n",
              "      <td>25.12</td>\n",
              "      <td>854636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.M</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>23.91</td>\n",
              "      <td>25.1500</td>\n",
              "      <td>23.91</td>\n",
              "      <td>24.33</td>\n",
              "      <td>738222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.M</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>23.93</td>\n",
              "      <td>24.5025</td>\n",
              "      <td>23.83</td>\n",
              "      <td>24.37</td>\n",
              "      <td>8712993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.M</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>23.44</td>\n",
              "      <td>23.9800</td>\n",
              "      <td>23.25</td>\n",
              "      <td>23.85</td>\n",
              "      <td>9219821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.M</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>23.47</td>\n",
              "      <td>23.7300</td>\n",
              "      <td>23.09</td>\n",
              "      <td>23.67</td>\n",
              "      <td>9486082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.M</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>23.27</td>\n",
              "      <td>23.7000</td>\n",
              "      <td>23.08</td>\n",
              "      <td>23.52</td>\n",
              "      <td>10283908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.M</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>22.71</td>\n",
              "      <td>22.7700</td>\n",
              "      <td>22.01</td>\n",
              "      <td>22.76</td>\n",
              "      <td>11287243</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2769 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...   <HIGH>  <LOW>  <CLOSE>     <VOL>\n",
              "0      2768    US1.M     D  20211001  ...  23.1400  22.21    22.87    703439\n",
              "1      2767    US1.M     D  20210930  ...  23.9000  22.34    22.60   1239730\n",
              "2      2766    US1.M     D  20210929  ...  25.4600  24.67    24.70    386601\n",
              "3      2765    US1.M     D  20210928  ...  25.4500  24.58    25.12    854636\n",
              "4      2764    US1.M     D  20210927  ...  25.1500  23.91    24.33    738222\n",
              "...     ...      ...   ...       ...  ...      ...    ...      ...       ...\n",
              "2764      4    US1.M     D  20101008  ...  24.5025  23.83    24.37   8712993\n",
              "2765      3    US1.M     D  20101007  ...  23.9800  23.25    23.85   9219821\n",
              "2766      2    US1.M     D  20101006  ...  23.7300  23.09    23.67   9486082\n",
              "2767      1    US1.M     D  20101005  ...  23.7000  23.08    23.52  10283908\n",
              "2768      0    US1.M     D  20101004  ...  22.7700  22.01    22.76  11287243\n",
              "\n",
              "[2769 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjdiFshSWlId",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0fb254d5-c8bd-4a1d-cf94-4db0c9923a9a"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"4c79765e-3e43-4f5e-bebf-91c990d8a08b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"4c79765e-3e43-4f5e-bebf-91c990d8a08b\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '4c79765e-3e43-4f5e-bebf-91c990d8a08b',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [14.83, 15.02, 14.96, 14.81, 14.66, 15.56, 15.54, 15.29, 15.26, 15.55, 15.27, 15.54, 15.42, 15.72, 16.29, 16.63, 17.415, 17.14, 17.0, 17.835, 17.11, 16.19, 15.39, 15.32, 14.915, 14.4, 14.76, 14.91, 14.63, 14.295, 14.72, 14.95, 15.56, 15.4, 15.38, 16.13, 15.985, 16.155, 16.8, 19.36, 19.27, 19.43, 20.33, 20.46, 20.84, 20.61, 21.29, 21.18, 22.71, 22.53, 22.68, 22.88, 22.78, 23.26, 22.59, 22.09, 22.1, 21.55, 21.5, 22.02, 22.04, 21.91, 21.4, 21.17, 21.235, 21.66, 21.64, 21.29, 21.04, 21.39, 21.47, 21.52, 21.65, 21.3, 21.79, 22.3, 21.91, 22.14, 21.88, 21.725, 21.54, 21.7651, 21.45, 21.92, 21.67, 20.43, 20.49, 21.055, 21.62, 21.15, 20.57, 20.75, 20.95, 21.04, 21.01, 21.27, 21.85, 21.77, 21.56, 21.78, 21.485, 21.695, 21.813, 21.58, 22.47, 22.7, 22.74, 22.79, 23.21, 23.23, 23.44, 23.37, 23.54, 24.09, 24.265, 24.14, 25.06, 24.64, 24.33, 25.08, 24.95, 24.655, 24.575, 24.29, 24.325, 24.76, 25.08, 25.66, 25.5, 25.98, 24.61, 24.67, 24.46, 24.02, 24.2, 24.33, 23.93, 23.86, 23.32, 24.03, 23.63, 23.94, 23.9, 23.705, 23.64, 24.03, 23.83, 23.57, 23.1, 23.3, 24.04, 24.38, 24.35, 24.49, 24.78, 25.32, 24.72, 24.36, 24.06, 24.45, 25.02, 25.34, 24.88, 25.145, 25.21, 24.77, 25.03, 25.135, 25.66, 25.36, 25.94, 25.88, 25.72, 26.285, 25.72, 25.72, 25.73, 25.51, 24.52, 24.705, 24.88, 25.8, 24.75, 24.475, 24.96, 25.085, 25.39, 26.1067, 31.71, 30.82, 29.91, 29.385, 29.76, 30.77, 29.78, 30.02, 30.05, 30.13, 27.8701, 28.21, 29.11, 30.41, 31.045, 30.28, 30.6, 30.8, 32.21, 31.84, 31.89, 31.673, 32.37, 32.43, 34.35, 34.22, 33.74, 34.42, 33.89, 32.55, 32.1, 32.59, 31.95, 33.07, 33.33, 32.26, 33.22, 35.77, 37.04, 37.77, 37.785, 37.18, 37.02, 36.73, 35.54, 34.58, 34.3, 35.72, 33.98, 32.37, 33.08, 32.21, 32.54, 32.89, 32.34, 32.41, 31.84, 33.51, 33.39, 33.38, 32.13, 32.77, 33.06, 33.18, 32.83, 32.87, 33.4, 33.03, 34.72, 34.73, 34.59, 34.89, 34.385, 34.755, 35.69, 35.18, 35.56, 35.535, 35.16, 36.27, 35.74, 36.39, 36.4, 35.98, 35.52, 35.36, 35.79, 36.39, 36.56, 35.97, 36.2, 36.35, 36.27, 36.52, 38.13, 37.56, 38.24, 38.21, 36.03, 35.81, 35.15, 41.87, 40.09, 39.96, 40.55, 39.895, 39.47, 38.78, 38.95, 38.73, 37.94, 39.73, 40.28, 39.48, 39.49, 40.16, 39.35, 39.41, 38.66, 38.95, 37.7, 37.07, 36.99, 36.39, 36.12, 36.44, 36.72, 36.46, 36.89, 36.89, 37.15, 36.52, 37.435, 38.48, 37.89, 37.86, 36.98, 37.44, 39.32, 39.6, 38.88, 38.74, 38.25, 37.56, 38.36, 39.97, 39.75, 39.88, 40.21, 40.08, 40.01, 37.1, 35.56, 34.905, 35.58, 34.83, 34.12, 34.04, 33.48, 33.12, 34.6, 33.98, 33.83, 33.16, 29.95, 29.64, 29.64, 29.41, 30.13, 29.77, 30.03, 31.23, 31.45, 31.33, 30.72, 31.06, 32.19, 31.74, 30.72, 30.5, 30.47, 29.96, 29.415, 29.395, 28.94, 28.75, 28.25, 28.88, 29.06, 29.3, 29.15, 29.795, 30.92, 30.09, 29.0, 29.07, 29.74, 29.0799, 27.86, 27.89, 27.21, 28.27, 28.66, 28.93, 29.3, 28.92, 28.79, 29.23, 29.82, 28.75, 28.85, 28.9, 29.91, 30.37, 30.35, 30.42, 29.24, 29.4, 28.4, 27.43, 26.73, 26.17, 25.57, 25.66, 26.26, 26.48, 25.66, 24.89, 24.2, 24.13, 24.0, 24.76, 24.17, 23.47, 24.88, 25.61, 25.94, 26.24, 27.32, 27.4, 26.62, 27.24, 27.36, 27.26, 27.01, 26.61, 26.24, 25.9, 26.89, 26.29, 25.595, 24.71, 24.43, 24.47, 24.47, 25.34, 26.31, 25.19, 25.71, 25.64, 26.8501, 25.69, 25.57, 25.34, 25.26, 25.85, 24.6, 24.81, 25.85, 25.7, 25.89, 25.8, 25.29, 25.1, 25.21, 25.81, 24.18, 23.8, 23.9999, 22.18, 21.21, 20.97, 20.64, 20.41, 20.81, 20.35, 20.2414, 19.98, 19.71, 19.32, 19.99, 19.5, 17.56, 17.54, 18.15, 18.36, 18.79, 18.95, 18.75, 18.82, 19.68, 21.33, 21.24, 21.42, 21.34, 21.18, 20.2, 20.15, 19.94, 19.89, 20.21, 20.23, 20.49, 20.67, 20.365, 20.805, 21.025, 20.62, 21.04, 20.88, 21.82, 22.02, 22.16, 21.865, 21.81, 21.52, 21.17, 21.47, 21.78, 22.11, 22.565, 21.99, 22.6653, 22.22, 21.49, 21.31, 21.72, 22.17, 21.005, 21.4, 20.76, 20.83, 20.93, 21.15, 21.11, 20.7, 20.5, 20.43, 19.52, 19.49, 19.64, 20.1261, 20.29, 20.34, 20.62, 20.67, 23.02, 23.52, 23.535, 23.32, 23.595, 23.16, 23.97, 23.74, 23.93, 24.2, 23.525, 23.49, 22.69, 23.35, 23.075, 22.98, 22.68, 23.0342, 22.36, 22.11, 21.22, 21.165, 21.1, 22.67, 22.85, 23.52, 23.94, 23.23, 22.94, 23.17, 23.08, 22.4799, 22.26, 22.09, 21.6, 22.11, 22.715, 22.93, 22.72, 22.75, 22.26, 22.66, 22.72, 21.77, 21.81, 21.91, 23.88, 23.8, 24.08, 23.495, 23.61, 23.45, 23.37, 23.19, 23.06, 23.42, 23.02, 22.76, 23.0, 22.83, 23.2, 23.605, 24.34, 29.34, 29.29, 28.66, 28.96, 28.92, 29.17, 29.51, 28.87, 29.21, 29.495, 29.43, 29.27, 29.2, 29.48, 30.01, 29.075, 28.7, 28.82, 29.17, 29.16, 29.69, 29.52, 29.09, 29.4, 28.82, 28.89, 29.63, 29.64, 29.47, 29.28, 28.55, 27.93, 28.18, 28.27, 28.38, 28.41, 29.37, 30.55, 30.41, 30.315, 30.96, 30.9, 31.74, 31.54, 31.77, 30.8, 30.69, 31.76, 33.21, 32.95, 33.2154, 33.57, 33.17, 32.45, 32.35, 32.3, 32.295, 31.81, 32.73, 32.68, 32.57, 32.0, 32.37, 31.98, 31.23, 31.81, 32.6901, 30.725, 29.21, 29.55, 29.52, 29.11, 29.91, 30.16, 29.99, 29.63, 29.71, 29.45, 29.43, 29.89, 29.87, 29.97, 29.96, 30.29, 30.48, 30.8151, 30.85, 35.82, 35.22, 35.82, 36.3, 36.28, 36.52, 36.48, 36.175, 37.52, 37.94, 37.01, 37.465, 40.14, 40.16, 40.7, 40.43, 42.43, 43.07, 43.18, 42.41, 41.89, 42.48, 42.42, 42.2, 42.46, 43.12, 44.19, 44.9, 44.47, 43.2, 43.05, 43.0525, 41.84, 41.385, 41.57, 41.34, 40.5314, 38.36, 37.87, 37.77, 36.85, 36.88, 36.89, 36.86, 36.5, 35.575, 35.27, 35.9, 35.92, 36.63, 36.51, 35.865, 35.58, 35.19, 35.06, 35.57, 36.8, 37.55, 37.235, 37.51, 38.0399, 37.7, 37.36, 36.58, 36.27, 37.04, 36.04, 36.08, 36.6, 36.24, 36.61, 36.25, 36.01, 34.95, 35.21, 35.53, 35.56, 35.215, 34.69, 35.64, 35.48, 36.0, 37.19, 36.16, 36.64, 36.32, 36.17, 38.2, 38.67, 38.56, 39.27, 39.73, 40.01, 39.33, 40.305, 40.23, 40.4, 40.69, 40.48, 39.83, 39.83, 34.04, 33.79, 34.72, 34.09, 33.705, 33.42, 32.75, 35.32, 35.835, 35.87, 35.7, 36.81, 36.45, 35.2711, 35.51, 35.31, 34.87, 35.27, 34.66, 34.82, 35.085, 35.64, 34.86, 34.36, 33.62, 33.61, 33.09, 33.58, 33.59, 33.33, 32.43, 31.7, 32.07, 33.38, 32.79, 33.03, 33.21, 33.23, 32.36, 31.94, 31.31, 31.61, 33.24, 33.62, 34.27, 34.33, 34.28, 34.33, 34.365, 33.03, 33.2, 32.72, 31.56, 31.82, 31.19, 31.3, 31.3, 30.86, 30.08, 30.4, 30.73, 31.21, 31.21, 31.3799, 36.97, 37.75, 37.65, 37.9, 38.68, 39.48, 39.92, 39.59, 40.59, 41.01, 40.54, 40.32, 40.95, 41.55, 41.35, 41.15, 40.73, 40.635, 39.65, 40.33, 40.07, 39.81, 39.67, 40.62, 41.58, 41.46, 42.17, 42.95, 44.07, 44.1, 44.43, 44.39, 43.475, 43.41, 43.77, 44.14, 44.86, 43.97, 43.5, 43.1, 43.87, 44.36, 44.21, 44.31, 44.04, 44.61, 44.59, 44.08, 43.96, 43.68, 43.2, 43.43, 43.38, 42.82, 42.31, 41.04, 40.22, 41.13, 41.095, 40.485, 39.23, 37.88, 38.8, 39.51, 39.64, 40.27, 40.72, 41.63, 40.9, 40.9, 40.385, 39.57, 40.29, 40.19, 39.31, 41.35, 40.61, 39.73, 38.77, 37.87, 37.64, 38.62, 38.61, 38.79, 35.89, 36.9, 36.14, 36.96, 35.81, 34.99, 35.3779, 35.6901, 35.5, 35.51, 36.07, 35.42, 35.02, 34.87, 34.49, 35.87, 35.93, 36.37, 36.76, 37.88, 38.14, 38.44, 38.56, 39.33, 38.97, 38.43, 39.32, 39.09, 40.02, 40.38, 39.645, 40.04, 38.59, 38.53, 38.92, 38.02, 38.62, 39.1, 40.825, 40.41, 47.03, 46.24, 48.9, 50.44, 50.44, 51.17, 51.34, 51.0, 49.71, 49.87, 49.07, 48.66, 47.97, 49.89, 50.34, 50.64, 50.71, 50.465, 50.46, 49.77, 50.26, 50.35, 51.01, 51.46, 51.1, 51.4, 51.68, 51.85, 51.055, 51.33, 50.37, 50.3, 52.51, 52.06, 52.86, 53.42, 53.89, 54.05, 55.99, 57.28, 57.17, 57.86, 58.19, 58.37, 58.86, 59.17, 58.41, 59.28, 58.63, 57.96, 58.6, 59.01, 59.07, 58.71, 56.78, 57.13, 59.21, 61.46, 62.42, 62.64, 62.94, 63.33, 62.98, 64.1, 67.6, 67.37, 66.9, 67.47, 68.9, 68.58, 68.21, 69.05, 69.34, 69.1749, 68.88, 69.97, 69.9401, 71.19, 71.92, 71.66, 72.12, 72.33, 72.81, 72.02, 66.72, 66.69, 66.485, 65.73, 66.2, 68.03, 67.52, 67.44, 67.72, 67.47, 67.08, 69.86, 69.17, 69.9, 70.09, 69.89, 69.86, 70.66, 70.02, 68.86, 68.98, 69.64, 68.95, 68.87, 69.12, 68.96, 69.25, 69.25, 69.71, 68.49, 66.81, 66.94, 68.05, 67.81, 67.33, 68.12, 67.66, 68.21, 69.03, 67.755, 66.52, 63.16, 63.75, 65.29, 66.025, 65.97, 64.73, 63.66, 64.5, 65.31, 65.06, 64.62, 64.79, 65.79, 65.93, 67.02, 67.42, 66.99, 67.04, 66.79, 66.47, 67.66, 67.9, 68.0201, 69.17, 69.17, 68.0, 69.8, 67.96, 68.14, 67.85, 65.45, 64.9, 64.43, 63.65, 63.27, 64.725, 65.35, 65.63, 65.79, 65.05, 64.61, 64.1, 64.12, 63.35, 63.18, 62.34, 63.3, 62.56, 62.99, 63.18, 62.87, 63.47, 63.91, 63.74, 63.57, 63.58, 62.085, 64.14, 63.7, 63.0, 63.49, 63.62, 64.3, 64.64, 64.74, 64.53, 63.67, 63.73, 63.75, 64.87, 66.06, 64.19, 63.89, 65.38, 64.89, 65.53, 66.07, 65.86, 65.505, 64.22, 63.3, 63.18, 62.35, 64.43, 65.59, 66.045, 65.95, 67.81, 67.55, 64.89, 65.15, 65.7, 65.74, 65.27, 65.22, 64.04, 64.6399, 64.03, 63.45, 62.61, 64.08, 62.78, 62.02, 63.15, 62.38, 61.23, 60.62, 61.35, 61.56, 62.33, 62.5, 63.65, 63.25, 63.19, 64.9, 63.51, 63.91, 64.38, 63.31, 62.85, 61.94, 61.46, 61.6, 62.06, 61.59, 61.57, 58.5799, 59.23, 59.89, 59.02, 57.1901, 56.29, 57.24, 57.82, 57.92, 57.45, 57.26, 58.5, 58.95, 58.35, 57.62, 58.38, 57.06, 56.2, 56.58, 56.0, 56.59, 54.99, 56.67, 56.395, 57.98, 57.35, 58.52, 59.61, 58.34, 57.762, 58.17, 58.93, 59.67, 58.82, 59.54, 58.64, 58.93, 60.095, 60.565, 60.63, 60.24, 59.57, 59.57, 59.82, 59.9, 60.42, 60.64, 61.82, 62.42, 61.67, 62.21, 62.27, 62.37, 62.57, 62.59, 62.37, 61.78, 60.71, 60.1, 58.95, 58.06, 57.46, 57.86, 56.48, 59.73, 60.13, 60.2, 58.78, 58.8, 57.63, 58.59, 57.91, 57.79, 58.63, 57.61, 57.57, 57.48, 57.93, 57.46, 56.99, 56.72, 57.11, 56.63, 57.07, 56.71, 57.82, 58.1, 58.81, 59.48, 59.17, 59.5, 60.0899, 58.94, 58.665, 58.01, 58.68, 58.06, 57.99, 58.48, 58.76, 58.14, 58.09, 57.76, 57.97, 57.22, 57.39, 57.28, 57.98, 58.88, 59.23, 59.65, 59.2, 60.02, 59.85, 60.05, 59.88, 59.16, 58.6, 58.36, 58.055, 57.62, 56.79, 56.51, 58.1, 58.07, 57.05, 57.83, 57.84, 57.37, 56.36, 55.96, 55.22, 55.1, 56.665, 57.18, 56.91, 57.42, 58.1, 58.01, 57.58, 58.055, 58.11, 57.52, 57.32, 56.83, 58.67, 57.59, 56.97, 56.89, 58.06, 58.84, 58.39, 58.25, 59.75, 60.6, 60.65, 59.51, 59.31, 58.93, 58.4, 58.09, 58.02, 58.42, 59.06, 58.7, 58.54, 58.89, 58.89, 58.57, 58.21, 58.76, 58.94, 58.13, 58.05, 57.32, 57.32, 57.65, 57.62, 57.85, 57.86, 57.96, 56.24, 53.04, 53.72, 53.45, 53.35, 53.14, 53.38, 53.13, 52.81, 53.56, 52.91, 53.06, 52.55, 51.36, 50.96, 50.9, 53.19, 53.9, 53.39, 54.48, 54.18, 54.41, 55.045, 55.39, 55.56, 56.21, 56.04, 55.81, 55.76, 55.27, 55.84, 55.81, 51.84, 52.18, 53.14, 53.52, 53.4, 53.39, 53.67, 52.7, 52.82, 52.9, 52.85, 52.3, 52.23, 52.15, 51.68, 51.62, 51.4, 51.66, 52.07, 51.78, 52.2, 51.84, 51.55, 52.04, 52.82, 52.44, 53.26, 53.54, 52.93, 52.29, 51.27, 50.82, 50.96, 50.42, 50.48, 51.02, 50.69, 50.699, 46.33, 47.08, 46.18, 45.95, 46.41, 46.14, 46.49, 45.96, 46.12, 46.39, 46.12, 45.45, 44.88, 45.11, 44.54]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4c79765e-3e43-4f5e-bebf-91c990d8a08b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"59e0b4c2-df85-4041-b42b-40e0d3ae7cd7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"59e0b4c2-df85-4041-b42b-40e0d3ae7cd7\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '59e0b4c2-df85-4041-b42b-40e0d3ae7cd7',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('59e0b4c2-df85-4041-b42b-40e0d3ae7cd7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hxgp6en5WlId"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsOy1xB3WlId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb9c47f-9641-4889-823c-f16e8c2c0bfb"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"M\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6951 - accuracy: 0.5040 - val_loss: 0.6822 - val_accuracy: 0.7367\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6392 - accuracy: 0.6376 - val_loss: 0.7615 - val_accuracy: 0.2898\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6238 - accuracy: 0.6631 - val_loss: 0.5655 - val_accuracy: 0.7245\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5969 - accuracy: 0.6933 - val_loss: 0.6343 - val_accuracy: 0.6490\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5691 - accuracy: 0.7208 - val_loss: 0.5232 - val_accuracy: 0.7327\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6570 - accuracy: 0.6148 - val_loss: 0.5507 - val_accuracy: 0.7469\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5827 - accuracy: 0.7114 - val_loss: 0.6161 - val_accuracy: 0.6653\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5591 - accuracy: 0.7087 - val_loss: 0.5718 - val_accuracy: 0.7245\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5429 - accuracy: 0.7383 - val_loss: 0.5657 - val_accuracy: 0.7122\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5462 - accuracy: 0.7228 - val_loss: 0.5218 - val_accuracy: 0.7347\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.740514\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.747106\n",
            "[2]\tvalidation_0-auc:0.755782\n",
            "[3]\tvalidation_0-auc:0.772638\n",
            "[4]\tvalidation_0-auc:0.780594\n",
            "[5]\tvalidation_0-auc:0.781131\n",
            "[6]\tvalidation_0-auc:0.779821\n",
            "[7]\tvalidation_0-auc:0.779725\n",
            "[8]\tvalidation_0-auc:0.777749\n",
            "[9]\tvalidation_0-auc:0.781432\n",
            "[10]\tvalidation_0-auc:0.781528\n",
            "[11]\tvalidation_0-auc:0.781861\n",
            "[12]\tvalidation_0-auc:0.783944\n",
            "[13]\tvalidation_0-auc:0.78417\n",
            "[14]\tvalidation_0-auc:0.783783\n",
            "[15]\tvalidation_0-auc:0.783182\n",
            "[16]\tvalidation_0-auc:0.782259\n",
            "[17]\tvalidation_0-auc:0.77631\n",
            "[18]\tvalidation_0-auc:0.77558\n",
            "[19]\tvalidation_0-auc:0.78549\n",
            "[20]\tvalidation_0-auc:0.784427\n",
            "[21]\tvalidation_0-auc:0.782516\n",
            "[22]\tvalidation_0-auc:0.784116\n",
            "[23]\tvalidation_0-auc:0.781335\n",
            "[24]\tvalidation_0-auc:0.781206\n",
            "[25]\tvalidation_0-auc:0.77995\n",
            "[26]\tvalidation_0-auc:0.77995\n",
            "[27]\tvalidation_0-auc:0.782581\n",
            "[28]\tvalidation_0-auc:0.782055\n",
            "[29]\tvalidation_0-auc:0.781539\n",
            "[30]\tvalidation_0-auc:0.781539\n",
            "[31]\tvalidation_0-auc:0.782044\n",
            "[32]\tvalidation_0-auc:0.782044\n",
            "[33]\tvalidation_0-auc:0.782581\n",
            "[34]\tvalidation_0-auc:0.781679\n",
            "[35]\tvalidation_0-auc:0.782151\n",
            "[36]\tvalidation_0-auc:0.782248\n",
            "[37]\tvalidation_0-auc:0.782162\n",
            "[38]\tvalidation_0-auc:0.782591\n",
            "[39]\tvalidation_0-auc:0.782591\n",
            "[40]\tvalidation_0-auc:0.782065\n",
            "[41]\tvalidation_0-auc:0.782087\n",
            "[42]\tvalidation_0-auc:0.782087\n",
            "[43]\tvalidation_0-auc:0.781786\n",
            "[44]\tvalidation_0-auc:0.781722\n",
            "[45]\tvalidation_0-auc:0.781722\n",
            "[46]\tvalidation_0-auc:0.782312\n",
            "[47]\tvalidation_0-auc:0.782312\n",
            "[48]\tvalidation_0-auc:0.782795\n",
            "[49]\tvalidation_0-auc:0.783246\n",
            "[50]\tvalidation_0-auc:0.780927\n",
            "[51]\tvalidation_0-auc:0.780927\n",
            "[52]\tvalidation_0-auc:0.781035\n",
            "[53]\tvalidation_0-auc:0.780906\n",
            "[54]\tvalidation_0-auc:0.780691\n",
            "[55]\tvalidation_0-auc:0.780777\n",
            "[56]\tvalidation_0-auc:0.781024\n",
            "[57]\tvalidation_0-auc:0.780874\n",
            "[58]\tvalidation_0-auc:0.780616\n",
            "[59]\tvalidation_0-auc:0.780723\n",
            "[60]\tvalidation_0-auc:0.780723\n",
            "[61]\tvalidation_0-auc:0.780723\n",
            "[62]\tvalidation_0-auc:0.780874\n",
            "[63]\tvalidation_0-auc:0.780874\n",
            "[64]\tvalidation_0-auc:0.780981\n",
            "[65]\tvalidation_0-auc:0.781325\n",
            "[66]\tvalidation_0-auc:0.781303\n",
            "[67]\tvalidation_0-auc:0.780895\n",
            "[68]\tvalidation_0-auc:0.780433\n",
            "[69]\tvalidation_0-auc:0.780895\n",
            "Stopping. Best iteration:\n",
            "[19]\tvalidation_0-auc:0.78549\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6869 - accuracy: 0.5367 - val_loss: 0.6270 - val_accuracy: 0.7177\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6644 - accuracy: 0.6054 - val_loss: 0.8859 - val_accuracy: 0.3392\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6410 - accuracy: 0.6527 - val_loss: 0.6239 - val_accuracy: 0.7505\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5829 - accuracy: 0.6932 - val_loss: 0.5366 - val_accuracy: 0.7571\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5589 - accuracy: 0.7213 - val_loss: 0.5402 - val_accuracy: 0.7637\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6449 - accuracy: 0.6273 - val_loss: 0.5682 - val_accuracy: 0.7440\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5466 - accuracy: 0.7111 - val_loss: 0.5681 - val_accuracy: 0.7243\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5328 - accuracy: 0.7220 - val_loss: 0.5276 - val_accuracy: 0.7396\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5304 - accuracy: 0.7467 - val_loss: 0.5528 - val_accuracy: 0.7352\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5276 - accuracy: 0.7426 - val_loss: 0.5368 - val_accuracy: 0.7527\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.688788\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.68594\n",
            "[2]\tvalidation_0-auc:0.698797\n",
            "[3]\tvalidation_0-auc:0.71488\n",
            "[4]\tvalidation_0-auc:0.710649\n",
            "[5]\tvalidation_0-auc:0.728068\n",
            "[6]\tvalidation_0-auc:0.739034\n",
            "[7]\tvalidation_0-auc:0.750012\n",
            "[8]\tvalidation_0-auc:0.746207\n",
            "[9]\tvalidation_0-auc:0.745592\n",
            "[10]\tvalidation_0-auc:0.749114\n",
            "[11]\tvalidation_0-auc:0.751737\n",
            "[12]\tvalidation_0-auc:0.754148\n",
            "[13]\tvalidation_0-auc:0.759501\n",
            "[14]\tvalidation_0-auc:0.761935\n",
            "[15]\tvalidation_0-auc:0.761935\n",
            "[16]\tvalidation_0-auc:0.762006\n",
            "[17]\tvalidation_0-auc:0.761722\n",
            "[18]\tvalidation_0-auc:0.762881\n",
            "[19]\tvalidation_0-auc:0.762881\n",
            "[20]\tvalidation_0-auc:0.764157\n",
            "[21]\tvalidation_0-auc:0.764109\n",
            "[22]\tvalidation_0-auc:0.767111\n",
            "[23]\tvalidation_0-auc:0.767111\n",
            "[24]\tvalidation_0-auc:0.764145\n",
            "[25]\tvalidation_0-auc:0.764145\n",
            "[26]\tvalidation_0-auc:0.764145\n",
            "[27]\tvalidation_0-auc:0.764913\n",
            "[28]\tvalidation_0-auc:0.762372\n",
            "[29]\tvalidation_0-auc:0.762372\n",
            "[30]\tvalidation_0-auc:0.758473\n",
            "[31]\tvalidation_0-auc:0.758756\n",
            "[32]\tvalidation_0-auc:0.763211\n",
            "[33]\tvalidation_0-auc:0.763211\n",
            "[34]\tvalidation_0-auc:0.761037\n",
            "[35]\tvalidation_0-auc:0.761037\n",
            "[36]\tvalidation_0-auc:0.761203\n",
            "[37]\tvalidation_0-auc:0.761203\n",
            "[38]\tvalidation_0-auc:0.761203\n",
            "[39]\tvalidation_0-auc:0.760293\n",
            "[40]\tvalidation_0-auc:0.760943\n",
            "[41]\tvalidation_0-auc:0.758036\n",
            "[42]\tvalidation_0-auc:0.758036\n",
            "[43]\tvalidation_0-auc:0.755199\n",
            "[44]\tvalidation_0-auc:0.755672\n",
            "[45]\tvalidation_0-auc:0.755672\n",
            "[46]\tvalidation_0-auc:0.755672\n",
            "[47]\tvalidation_0-auc:0.755672\n",
            "[48]\tvalidation_0-auc:0.755743\n",
            "[49]\tvalidation_0-auc:0.753982\n",
            "[50]\tvalidation_0-auc:0.756712\n",
            "[51]\tvalidation_0-auc:0.756476\n",
            "[52]\tvalidation_0-auc:0.753663\n",
            "[53]\tvalidation_0-auc:0.756901\n",
            "[54]\tvalidation_0-auc:0.757752\n",
            "[55]\tvalidation_0-auc:0.757846\n",
            "[56]\tvalidation_0-auc:0.756736\n",
            "[57]\tvalidation_0-auc:0.755105\n",
            "[58]\tvalidation_0-auc:0.753616\n",
            "[59]\tvalidation_0-auc:0.752458\n",
            "[60]\tvalidation_0-auc:0.752458\n",
            "[61]\tvalidation_0-auc:0.751453\n",
            "[62]\tvalidation_0-auc:0.749728\n",
            "[63]\tvalidation_0-auc:0.74961\n",
            "[64]\tvalidation_0-auc:0.749634\n",
            "[65]\tvalidation_0-auc:0.750626\n",
            "[66]\tvalidation_0-auc:0.750626\n",
            "[67]\tvalidation_0-auc:0.749374\n",
            "[68]\tvalidation_0-auc:0.747861\n",
            "[69]\tvalidation_0-auc:0.748121\n",
            "[70]\tvalidation_0-auc:0.748334\n",
            "[71]\tvalidation_0-auc:0.747554\n",
            "[72]\tvalidation_0-auc:0.74675\n",
            "Stopping. Best iteration:\n",
            "[22]\tvalidation_0-auc:0.767111\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.7326530612244898 | 0.49107142857142855 | 0.4263565891472868 | 0.45643153526970953 |\n",
            "|     GRU 0.1      | 0.7346938775510204 |  0.496551724137931  | 0.5581395348837209 |  0.5255474452554745 |\n",
            "|   XGBoost 0.1    | 0.7306122448979592 |  0.4915254237288136 | 0.6744186046511628 |  0.5686274509803922 |\n",
            "|    Logreg 0.1    | 0.7346938775510204 | 0.49710982658959535 | 0.6666666666666666 |  0.5695364238410596 |\n",
            "|     SVM 0.1      | 0.7183673469387755 | 0.47305389221556887 | 0.6124031007751938 |  0.5337837837837838 |\n",
            "|  LSTM beta 0.1   | 0.7636761487964989 |  0.5826771653543307 | 0.5736434108527132 |       0.578125      |\n",
            "|   GRU beta 0.1   | 0.7527352297592997 |  0.5555555555555556 | 0.6201550387596899 |  0.5860805860805861 |\n",
            "| XGBoost beta 0.1 | 0.7396061269146609 |  0.5431034482758621 | 0.4883720930232558 |  0.5142857142857143 |\n",
            "| logreg beta 0.1  | 0.7592997811816192 |  0.5725190839694656 | 0.5813953488372093 |  0.576923076923077  |\n",
            "|   svm beta 0.1   | 0.7352297592997812 |       0.53125       | 0.5271317829457365 |  0.529182879377432  |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6886 - accuracy: 0.5161 - val_loss: 0.5395 - val_accuracy: 0.8673\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6474 - accuracy: 0.6208 - val_loss: 0.6852 - val_accuracy: 0.6041\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6362 - accuracy: 0.6383 - val_loss: 0.6322 - val_accuracy: 0.7082\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6227 - accuracy: 0.6430 - val_loss: 0.6492 - val_accuracy: 0.6633\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6199 - accuracy: 0.6564 - val_loss: 0.6412 - val_accuracy: 0.6776\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6723 - accuracy: 0.5631 - val_loss: 0.8448 - val_accuracy: 0.2265\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6119 - accuracy: 0.6584 - val_loss: 0.8124 - val_accuracy: 0.3510\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6001 - accuracy: 0.6591 - val_loss: 0.6300 - val_accuracy: 0.6653\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6039 - accuracy: 0.6577 - val_loss: 0.5652 - val_accuracy: 0.7510\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5850 - accuracy: 0.6718 - val_loss: 0.7018 - val_accuracy: 0.5612\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.715439\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.729864\n",
            "[2]\tvalidation_0-auc:0.735656\n",
            "[3]\tvalidation_0-auc:0.733756\n",
            "[4]\tvalidation_0-auc:0.738643\n",
            "[5]\tvalidation_0-auc:0.733303\n",
            "[6]\tvalidation_0-auc:0.732054\n",
            "[7]\tvalidation_0-auc:0.733629\n",
            "[8]\tvalidation_0-auc:0.742462\n",
            "[9]\tvalidation_0-auc:0.750968\n",
            "[10]\tvalidation_0-auc:0.748561\n",
            "[11]\tvalidation_0-auc:0.751475\n",
            "[12]\tvalidation_0-auc:0.751674\n",
            "[13]\tvalidation_0-auc:0.74981\n",
            "[14]\tvalidation_0-auc:0.749701\n",
            "[15]\tvalidation_0-auc:0.750443\n",
            "[16]\tvalidation_0-auc:0.75057\n",
            "[17]\tvalidation_0-auc:0.748597\n",
            "[18]\tvalidation_0-auc:0.749701\n",
            "[19]\tvalidation_0-auc:0.747638\n",
            "[20]\tvalidation_0-auc:0.748308\n",
            "[21]\tvalidation_0-auc:0.748181\n",
            "[22]\tvalidation_0-auc:0.747566\n",
            "[23]\tvalidation_0-auc:0.747005\n",
            "[24]\tvalidation_0-auc:0.747584\n",
            "[25]\tvalidation_0-auc:0.746643\n",
            "[26]\tvalidation_0-auc:0.748181\n",
            "[27]\tvalidation_0-auc:0.748181\n",
            "[28]\tvalidation_0-auc:0.748398\n",
            "[29]\tvalidation_0-auc:0.748434\n",
            "[30]\tvalidation_0-auc:0.748434\n",
            "[31]\tvalidation_0-auc:0.748561\n",
            "[32]\tvalidation_0-auc:0.749032\n",
            "[33]\tvalidation_0-auc:0.749032\n",
            "[34]\tvalidation_0-auc:0.75124\n",
            "[35]\tvalidation_0-auc:0.75124\n",
            "[36]\tvalidation_0-auc:0.75124\n",
            "[37]\tvalidation_0-auc:0.75124\n",
            "[38]\tvalidation_0-auc:0.751421\n",
            "[39]\tvalidation_0-auc:0.751023\n",
            "[40]\tvalidation_0-auc:0.748652\n",
            "[41]\tvalidation_0-auc:0.748615\n",
            "[42]\tvalidation_0-auc:0.748615\n",
            "[43]\tvalidation_0-auc:0.748615\n",
            "[44]\tvalidation_0-auc:0.748941\n",
            "[45]\tvalidation_0-auc:0.74876\n",
            "[46]\tvalidation_0-auc:0.74876\n",
            "[47]\tvalidation_0-auc:0.748796\n",
            "[48]\tvalidation_0-auc:0.74876\n",
            "[49]\tvalidation_0-auc:0.748796\n",
            "[50]\tvalidation_0-auc:0.748796\n",
            "[51]\tvalidation_0-auc:0.748724\n",
            "[52]\tvalidation_0-auc:0.748833\n",
            "[53]\tvalidation_0-auc:0.748615\n",
            "[54]\tvalidation_0-auc:0.754643\n",
            "[55]\tvalidation_0-auc:0.754643\n",
            "[56]\tvalidation_0-auc:0.754643\n",
            "[57]\tvalidation_0-auc:0.75095\n",
            "[58]\tvalidation_0-auc:0.75095\n",
            "[59]\tvalidation_0-auc:0.751023\n",
            "[60]\tvalidation_0-auc:0.752959\n",
            "[61]\tvalidation_0-auc:0.752923\n",
            "[62]\tvalidation_0-auc:0.752923\n",
            "[63]\tvalidation_0-auc:0.752923\n",
            "[64]\tvalidation_0-auc:0.747837\n",
            "[65]\tvalidation_0-auc:0.740706\n",
            "[66]\tvalidation_0-auc:0.740995\n",
            "[67]\tvalidation_0-auc:0.740054\n",
            "[68]\tvalidation_0-auc:0.739873\n",
            "[69]\tvalidation_0-auc:0.739873\n",
            "[70]\tvalidation_0-auc:0.739873\n",
            "[71]\tvalidation_0-auc:0.740507\n",
            "[72]\tvalidation_0-auc:0.740398\n",
            "[73]\tvalidation_0-auc:0.741158\n",
            "[74]\tvalidation_0-auc:0.74362\n",
            "[75]\tvalidation_0-auc:0.742751\n",
            "[76]\tvalidation_0-auc:0.740724\n",
            "[77]\tvalidation_0-auc:0.740869\n",
            "[78]\tvalidation_0-auc:0.741086\n",
            "[79]\tvalidation_0-auc:0.741448\n",
            "[80]\tvalidation_0-auc:0.741448\n",
            "[81]\tvalidation_0-auc:0.741448\n",
            "[82]\tvalidation_0-auc:0.741448\n",
            "[83]\tvalidation_0-auc:0.741502\n",
            "[84]\tvalidation_0-auc:0.741792\n",
            "[85]\tvalidation_0-auc:0.740127\n",
            "[86]\tvalidation_0-auc:0.741412\n",
            "[87]\tvalidation_0-auc:0.73743\n",
            "[88]\tvalidation_0-auc:0.738045\n",
            "[89]\tvalidation_0-auc:0.738371\n",
            "[90]\tvalidation_0-auc:0.737864\n",
            "[91]\tvalidation_0-auc:0.737213\n",
            "[92]\tvalidation_0-auc:0.736814\n",
            "[93]\tvalidation_0-auc:0.736561\n",
            "[94]\tvalidation_0-auc:0.736778\n",
            "[95]\tvalidation_0-auc:0.736995\n",
            "[96]\tvalidation_0-auc:0.737502\n",
            "[97]\tvalidation_0-auc:0.737538\n",
            "[98]\tvalidation_0-auc:0.737719\n",
            "[99]\tvalidation_0-auc:0.735837\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6535 - accuracy: 0.5930 - val_loss: 0.5823 - val_accuracy: 0.7724\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6002 - accuracy: 0.6905 - val_loss: 0.4550 - val_accuracy: 0.8862\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5681 - accuracy: 0.7193 - val_loss: 0.4296 - val_accuracy: 0.8993\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5360 - accuracy: 0.7412 - val_loss: 0.5628 - val_accuracy: 0.7681\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5275 - accuracy: 0.7481 - val_loss: 0.4044 - val_accuracy: 0.8928\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6252 - accuracy: 0.6383 - val_loss: 0.6505 - val_accuracy: 0.6061\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5274 - accuracy: 0.7515 - val_loss: 0.4910 - val_accuracy: 0.8140\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5047 - accuracy: 0.7584 - val_loss: 0.4203 - val_accuracy: 0.8796\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4917 - accuracy: 0.7721 - val_loss: 0.4986 - val_accuracy: 0.7987\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5013 - accuracy: 0.7618 - val_loss: 0.3947 - val_accuracy: 0.8928\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.759812\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.735165\n",
            "[2]\tvalidation_0-auc:0.719054\n",
            "[3]\tvalidation_0-auc:0.718093\n",
            "[4]\tvalidation_0-auc:0.716071\n",
            "[5]\tvalidation_0-auc:0.717249\n",
            "[6]\tvalidation_0-auc:0.716385\n",
            "[7]\tvalidation_0-auc:0.718838\n",
            "[8]\tvalidation_0-auc:0.718583\n",
            "[9]\tvalidation_0-auc:0.717249\n",
            "[10]\tvalidation_0-auc:0.723155\n",
            "[11]\tvalidation_0-auc:0.72084\n",
            "[12]\tvalidation_0-auc:0.72029\n",
            "[13]\tvalidation_0-auc:0.723077\n",
            "[14]\tvalidation_0-auc:0.723057\n",
            "[15]\tvalidation_0-auc:0.729101\n",
            "[16]\tvalidation_0-auc:0.72863\n",
            "[17]\tvalidation_0-auc:0.727728\n",
            "[18]\tvalidation_0-auc:0.727688\n",
            "[19]\tvalidation_0-auc:0.727492\n",
            "[20]\tvalidation_0-auc:0.727119\n",
            "[21]\tvalidation_0-auc:0.72916\n",
            "[22]\tvalidation_0-auc:0.727845\n",
            "[23]\tvalidation_0-auc:0.72865\n",
            "[24]\tvalidation_0-auc:0.723214\n",
            "[25]\tvalidation_0-auc:0.723175\n",
            "[26]\tvalidation_0-auc:0.719388\n",
            "[27]\tvalidation_0-auc:0.721291\n",
            "[28]\tvalidation_0-auc:0.721762\n",
            "[29]\tvalidation_0-auc:0.721743\n",
            "[30]\tvalidation_0-auc:0.717327\n",
            "[31]\tvalidation_0-auc:0.717288\n",
            "[32]\tvalidation_0-auc:0.717249\n",
            "[33]\tvalidation_0-auc:0.71717\n",
            "[34]\tvalidation_0-auc:0.716228\n",
            "[35]\tvalidation_0-auc:0.716268\n",
            "[36]\tvalidation_0-auc:0.71458\n",
            "[37]\tvalidation_0-auc:0.714305\n",
            "[38]\tvalidation_0-auc:0.714266\n",
            "[39]\tvalidation_0-auc:0.712068\n",
            "[40]\tvalidation_0-auc:0.711951\n",
            "[41]\tvalidation_0-auc:0.713677\n",
            "[42]\tvalidation_0-auc:0.713677\n",
            "[43]\tvalidation_0-auc:0.710361\n",
            "[44]\tvalidation_0-auc:0.707535\n",
            "[45]\tvalidation_0-auc:0.707457\n",
            "[46]\tvalidation_0-auc:0.705926\n",
            "[47]\tvalidation_0-auc:0.704651\n",
            "[48]\tvalidation_0-auc:0.704768\n",
            "[49]\tvalidation_0-auc:0.704768\n",
            "[50]\tvalidation_0-auc:0.702414\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.759812\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.6775510204081633 | 0.23121387283236994 |  0.6153846153846154 | 0.33613445378151263 |\n",
            "|      GRU 0.15     | 0.5612244897959183 | 0.20238095238095238 |  0.7846153846153846 |  0.3217665615141955 |\n",
            "|    XGBoost 0.15   | 0.7285714285714285 |  0.2638888888888889 |  0.5846153846153846 |  0.3636363636363636 |\n",
            "|    Logreg 0.15    | 0.6857142857142857 | 0.24571428571428572 |  0.6615384615384615 |  0.3583333333333334 |\n",
            "|      SVM 0.15     | 0.7306122448979592 | 0.25547445255474455 |  0.5384615384615384 |  0.3465346534653465 |\n",
            "|   LSTM beta 0.15  | 0.8927789934354485 |  0.7352941176470589 | 0.38461538461538464 |  0.5050505050505051 |\n",
            "|   GRU beta 0.15   | 0.8927789934354485 |  0.7105263157894737 |  0.4153846153846154 |  0.5242718446601942 |\n",
            "| XGBoost beta 0.15 | 0.8840262582056893 |  0.6304347826086957 |  0.4461538461538462 |  0.5225225225225225 |\n",
            "|  logreg beta 0.15 | 0.8161925601750547 | 0.39325842696629215 |  0.5384615384615384 |  0.4545454545454546 |\n",
            "|   svm beta 0.15   | 0.8708971553610503 |  0.5555555555555556 | 0.46153846153846156 |  0.504201680672269  |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROgTBAtbWlId",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9e5342c5-5857-4c80-a6e3-c03c1aed0dd7"
      },
      "source": [
        "Result_cross.to_csv('M_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.491071</td>\n",
              "      <td>0.732653</td>\n",
              "      <td>0.456432</td>\n",
              "      <td>0.426357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.496552</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.525547</td>\n",
              "      <td>0.558140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.491525</td>\n",
              "      <td>0.730612</td>\n",
              "      <td>0.568627</td>\n",
              "      <td>0.674419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.497110</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.569536</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.473054</td>\n",
              "      <td>0.718367</td>\n",
              "      <td>0.533784</td>\n",
              "      <td>0.612403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.582677</td>\n",
              "      <td>0.763676</td>\n",
              "      <td>0.578125</td>\n",
              "      <td>0.573643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.752735</td>\n",
              "      <td>0.586081</td>\n",
              "      <td>0.620155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.543103</td>\n",
              "      <td>0.739606</td>\n",
              "      <td>0.514286</td>\n",
              "      <td>0.488372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.572519</td>\n",
              "      <td>0.759300</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.581395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.531250</td>\n",
              "      <td>0.735230</td>\n",
              "      <td>0.529183</td>\n",
              "      <td>0.527132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.231214</td>\n",
              "      <td>0.677551</td>\n",
              "      <td>0.336134</td>\n",
              "      <td>0.615385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.202381</td>\n",
              "      <td>0.561224</td>\n",
              "      <td>0.321767</td>\n",
              "      <td>0.784615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>0.728571</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.584615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.245714</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.358333</td>\n",
              "      <td>0.661538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.255474</td>\n",
              "      <td>0.730612</td>\n",
              "      <td>0.346535</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.735294</td>\n",
              "      <td>0.892779</td>\n",
              "      <td>0.505051</td>\n",
              "      <td>0.384615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.710526</td>\n",
              "      <td>0.892779</td>\n",
              "      <td>0.524272</td>\n",
              "      <td>0.415385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.630435</td>\n",
              "      <td>0.884026</td>\n",
              "      <td>0.522523</td>\n",
              "      <td>0.446154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.393258</td>\n",
              "      <td>0.816193</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.870897</td>\n",
              "      <td>0.504202</td>\n",
              "      <td>0.461538</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1    M  0.491071  0.732653  0.456432  0.426357\n",
              "1            GRU 0.1    M  0.496552  0.734694  0.525547  0.558140\n",
              "2        XGBoost 0.1    M  0.491525  0.730612  0.568627  0.674419\n",
              "3         Logreg 0.1    M  0.497110  0.734694  0.569536  0.666667\n",
              "4            SVM 0.1    M  0.473054  0.718367  0.533784  0.612403\n",
              "5      LSTM beta 0.1    M  0.582677  0.763676  0.578125  0.573643\n",
              "6       GRU beta 0.1    M  0.555556  0.752735  0.586081  0.620155\n",
              "7   XGBoost beta 0.1    M  0.543103  0.739606  0.514286  0.488372\n",
              "8    logreg beta 0.1    M  0.572519  0.759300  0.576923  0.581395\n",
              "9       svm beta 0.1    M  0.531250  0.735230  0.529183  0.527132\n",
              "0          LSTM 0.15    M  0.231214  0.677551  0.336134  0.615385\n",
              "1           GRU 0.15    M  0.202381  0.561224  0.321767  0.784615\n",
              "2       XGBoost 0.15    M  0.263889  0.728571  0.363636  0.584615\n",
              "3        Logreg 0.15    M  0.245714  0.685714  0.358333  0.661538\n",
              "4           SVM 0.15    M  0.255474  0.730612  0.346535  0.538462\n",
              "5     LSTM beta 0.15    M  0.735294  0.892779  0.505051  0.384615\n",
              "6      GRU beta 0.15    M  0.710526  0.892779  0.524272  0.415385\n",
              "7  XGBoost beta 0.15    M  0.630435  0.884026  0.522523  0.446154\n",
              "8   logreg beta 0.15    M  0.393258  0.816193  0.454545  0.538462\n",
              "9      svm beta 0.15    M  0.555556  0.870897  0.504202  0.461538"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPk-zjRGWlId"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_logreg_beta.csv')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeySr84fWlId"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhEkpZOEWlIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "071e4f90-30c4-4c8b-d989-b7851f44b837"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"M\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6920 - accuracy: 0.5161 - val_loss: 0.7052 - val_accuracy: 0.2918\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6527 - accuracy: 0.6087 - val_loss: 0.6474 - val_accuracy: 0.7327\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6134 - accuracy: 0.6785 - val_loss: 0.5988 - val_accuracy: 0.7163\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5965 - accuracy: 0.7067 - val_loss: 0.5818 - val_accuracy: 0.7224\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5633 - accuracy: 0.7215 - val_loss: 0.5953 - val_accuracy: 0.7000\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6648 - accuracy: 0.5839 - val_loss: 0.5832 - val_accuracy: 0.7531\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5763 - accuracy: 0.7027 - val_loss: 0.6770 - val_accuracy: 0.5776\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5653 - accuracy: 0.7154 - val_loss: 0.6039 - val_accuracy: 0.6980\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5516 - accuracy: 0.7248 - val_loss: 0.5027 - val_accuracy: 0.7653\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5387 - accuracy: 0.7349 - val_loss: 0.4854 - val_accuracy: 0.7612\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.740514\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.747106\n",
            "[2]\tvalidation_0-auc:0.755782\n",
            "[3]\tvalidation_0-auc:0.772638\n",
            "[4]\tvalidation_0-auc:0.780594\n",
            "[5]\tvalidation_0-auc:0.781131\n",
            "[6]\tvalidation_0-auc:0.779821\n",
            "[7]\tvalidation_0-auc:0.779725\n",
            "[8]\tvalidation_0-auc:0.777749\n",
            "[9]\tvalidation_0-auc:0.781432\n",
            "[10]\tvalidation_0-auc:0.781528\n",
            "[11]\tvalidation_0-auc:0.781861\n",
            "[12]\tvalidation_0-auc:0.783944\n",
            "[13]\tvalidation_0-auc:0.78417\n",
            "[14]\tvalidation_0-auc:0.783783\n",
            "[15]\tvalidation_0-auc:0.783182\n",
            "[16]\tvalidation_0-auc:0.782259\n",
            "[17]\tvalidation_0-auc:0.77631\n",
            "[18]\tvalidation_0-auc:0.77558\n",
            "[19]\tvalidation_0-auc:0.78549\n",
            "[20]\tvalidation_0-auc:0.784427\n",
            "[21]\tvalidation_0-auc:0.782516\n",
            "[22]\tvalidation_0-auc:0.784116\n",
            "[23]\tvalidation_0-auc:0.781335\n",
            "[24]\tvalidation_0-auc:0.781206\n",
            "[25]\tvalidation_0-auc:0.77995\n",
            "[26]\tvalidation_0-auc:0.77995\n",
            "[27]\tvalidation_0-auc:0.782581\n",
            "[28]\tvalidation_0-auc:0.782055\n",
            "[29]\tvalidation_0-auc:0.781539\n",
            "[30]\tvalidation_0-auc:0.781539\n",
            "[31]\tvalidation_0-auc:0.782044\n",
            "[32]\tvalidation_0-auc:0.782044\n",
            "[33]\tvalidation_0-auc:0.782581\n",
            "[34]\tvalidation_0-auc:0.781679\n",
            "[35]\tvalidation_0-auc:0.782151\n",
            "[36]\tvalidation_0-auc:0.782248\n",
            "[37]\tvalidation_0-auc:0.782162\n",
            "[38]\tvalidation_0-auc:0.782591\n",
            "[39]\tvalidation_0-auc:0.782591\n",
            "[40]\tvalidation_0-auc:0.782065\n",
            "[41]\tvalidation_0-auc:0.782087\n",
            "[42]\tvalidation_0-auc:0.782087\n",
            "[43]\tvalidation_0-auc:0.781786\n",
            "[44]\tvalidation_0-auc:0.781722\n",
            "[45]\tvalidation_0-auc:0.781722\n",
            "[46]\tvalidation_0-auc:0.782312\n",
            "[47]\tvalidation_0-auc:0.782312\n",
            "[48]\tvalidation_0-auc:0.782795\n",
            "[49]\tvalidation_0-auc:0.783246\n",
            "[50]\tvalidation_0-auc:0.780927\n",
            "[51]\tvalidation_0-auc:0.780927\n",
            "[52]\tvalidation_0-auc:0.781035\n",
            "[53]\tvalidation_0-auc:0.780906\n",
            "[54]\tvalidation_0-auc:0.780691\n",
            "[55]\tvalidation_0-auc:0.780777\n",
            "[56]\tvalidation_0-auc:0.781024\n",
            "[57]\tvalidation_0-auc:0.780874\n",
            "[58]\tvalidation_0-auc:0.780616\n",
            "[59]\tvalidation_0-auc:0.780723\n",
            "[60]\tvalidation_0-auc:0.780723\n",
            "[61]\tvalidation_0-auc:0.780723\n",
            "[62]\tvalidation_0-auc:0.780874\n",
            "[63]\tvalidation_0-auc:0.780874\n",
            "[64]\tvalidation_0-auc:0.780981\n",
            "[65]\tvalidation_0-auc:0.781325\n",
            "[66]\tvalidation_0-auc:0.781303\n",
            "[67]\tvalidation_0-auc:0.780895\n",
            "[68]\tvalidation_0-auc:0.780433\n",
            "[69]\tvalidation_0-auc:0.780895\n",
            "Stopping. Best iteration:\n",
            "[19]\tvalidation_0-auc:0.78549\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6843 - accuracy: 0.5498 - val_loss: 0.6481 - val_accuracy: 0.7177\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6641 - accuracy: 0.6033 - val_loss: 0.6272 - val_accuracy: 0.7112\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6165 - accuracy: 0.6644 - val_loss: 0.6045 - val_accuracy: 0.7462\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5682 - accuracy: 0.7090 - val_loss: 0.5338 - val_accuracy: 0.7505\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5506 - accuracy: 0.7186 - val_loss: 0.5349 - val_accuracy: 0.7505\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6393 - accuracy: 0.6369 - val_loss: 0.5500 - val_accuracy: 0.7418\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5620 - accuracy: 0.7090 - val_loss: 0.5869 - val_accuracy: 0.7374\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5417 - accuracy: 0.7289 - val_loss: 0.5038 - val_accuracy: 0.7615\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5383 - accuracy: 0.7275 - val_loss: 0.5166 - val_accuracy: 0.7571\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5262 - accuracy: 0.7385 - val_loss: 0.5192 - val_accuracy: 0.7484\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.688788\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.68594\n",
            "[2]\tvalidation_0-auc:0.698797\n",
            "[3]\tvalidation_0-auc:0.71488\n",
            "[4]\tvalidation_0-auc:0.710649\n",
            "[5]\tvalidation_0-auc:0.728068\n",
            "[6]\tvalidation_0-auc:0.739034\n",
            "[7]\tvalidation_0-auc:0.750012\n",
            "[8]\tvalidation_0-auc:0.746207\n",
            "[9]\tvalidation_0-auc:0.745592\n",
            "[10]\tvalidation_0-auc:0.749114\n",
            "[11]\tvalidation_0-auc:0.751737\n",
            "[12]\tvalidation_0-auc:0.754148\n",
            "[13]\tvalidation_0-auc:0.759501\n",
            "[14]\tvalidation_0-auc:0.761935\n",
            "[15]\tvalidation_0-auc:0.761935\n",
            "[16]\tvalidation_0-auc:0.762006\n",
            "[17]\tvalidation_0-auc:0.761722\n",
            "[18]\tvalidation_0-auc:0.762881\n",
            "[19]\tvalidation_0-auc:0.762881\n",
            "[20]\tvalidation_0-auc:0.764157\n",
            "[21]\tvalidation_0-auc:0.764109\n",
            "[22]\tvalidation_0-auc:0.767111\n",
            "[23]\tvalidation_0-auc:0.767111\n",
            "[24]\tvalidation_0-auc:0.764145\n",
            "[25]\tvalidation_0-auc:0.764145\n",
            "[26]\tvalidation_0-auc:0.764145\n",
            "[27]\tvalidation_0-auc:0.764913\n",
            "[28]\tvalidation_0-auc:0.762372\n",
            "[29]\tvalidation_0-auc:0.762372\n",
            "[30]\tvalidation_0-auc:0.758473\n",
            "[31]\tvalidation_0-auc:0.758756\n",
            "[32]\tvalidation_0-auc:0.763211\n",
            "[33]\tvalidation_0-auc:0.763211\n",
            "[34]\tvalidation_0-auc:0.761037\n",
            "[35]\tvalidation_0-auc:0.761037\n",
            "[36]\tvalidation_0-auc:0.761203\n",
            "[37]\tvalidation_0-auc:0.761203\n",
            "[38]\tvalidation_0-auc:0.761203\n",
            "[39]\tvalidation_0-auc:0.760293\n",
            "[40]\tvalidation_0-auc:0.760943\n",
            "[41]\tvalidation_0-auc:0.758036\n",
            "[42]\tvalidation_0-auc:0.758036\n",
            "[43]\tvalidation_0-auc:0.755199\n",
            "[44]\tvalidation_0-auc:0.755672\n",
            "[45]\tvalidation_0-auc:0.755672\n",
            "[46]\tvalidation_0-auc:0.755672\n",
            "[47]\tvalidation_0-auc:0.755672\n",
            "[48]\tvalidation_0-auc:0.755743\n",
            "[49]\tvalidation_0-auc:0.753982\n",
            "[50]\tvalidation_0-auc:0.756712\n",
            "[51]\tvalidation_0-auc:0.756476\n",
            "[52]\tvalidation_0-auc:0.753663\n",
            "[53]\tvalidation_0-auc:0.756901\n",
            "[54]\tvalidation_0-auc:0.757752\n",
            "[55]\tvalidation_0-auc:0.757846\n",
            "[56]\tvalidation_0-auc:0.756736\n",
            "[57]\tvalidation_0-auc:0.755105\n",
            "[58]\tvalidation_0-auc:0.753616\n",
            "[59]\tvalidation_0-auc:0.752458\n",
            "[60]\tvalidation_0-auc:0.752458\n",
            "[61]\tvalidation_0-auc:0.751453\n",
            "[62]\tvalidation_0-auc:0.749728\n",
            "[63]\tvalidation_0-auc:0.74961\n",
            "[64]\tvalidation_0-auc:0.749634\n",
            "[65]\tvalidation_0-auc:0.750626\n",
            "[66]\tvalidation_0-auc:0.750626\n",
            "[67]\tvalidation_0-auc:0.749374\n",
            "[68]\tvalidation_0-auc:0.747861\n",
            "[69]\tvalidation_0-auc:0.748121\n",
            "[70]\tvalidation_0-auc:0.748334\n",
            "[71]\tvalidation_0-auc:0.747554\n",
            "[72]\tvalidation_0-auc:0.74675\n",
            "Stopping. Best iteration:\n",
            "[22]\tvalidation_0-auc:0.767111\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.1     |        0.7         | 0.45263157894736844 | 0.6666666666666666 |  0.5391849529780565 |\n",
            "|     GRU 0.1      | 0.7612244897959184 |  0.5612244897959183 | 0.4263565891472868 |  0.4845814977973568 |\n",
            "|   XGBoost 0.1    | 0.7306122448979592 |  0.4915254237288136 | 0.6744186046511628 |  0.5686274509803922 |\n",
            "|    Logreg 0.1    | 0.7346938775510204 | 0.49710982658959535 | 0.6666666666666666 |  0.5695364238410596 |\n",
            "|     SVM 0.1      | 0.7183673469387755 | 0.47305389221556887 | 0.6124031007751938 |  0.5337837837837838 |\n",
            "|  LSTM beta 0.1   |  0.75054704595186  |  0.5688073394495413 | 0.4806201550387597 |  0.5210084033613446 |\n",
            "|   GRU beta 0.1   | 0.7483588621444202 |        0.5875       | 0.3643410852713178 | 0.44976076555023925 |\n",
            "| XGBoost beta 0.1 | 0.7396061269146609 |  0.5431034482758621 | 0.4883720930232558 |  0.5142857142857143 |\n",
            "| logreg beta 0.1  | 0.7592997811816192 |  0.5725190839694656 | 0.5813953488372093 |  0.576923076923077  |\n",
            "|   svm beta 0.1   | 0.7352297592997812 |       0.53125       | 0.5271317829457365 |  0.529182879377432  |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6956 - accuracy: 0.5121 - val_loss: 0.7101 - val_accuracy: 0.1367\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6679 - accuracy: 0.5893 - val_loss: 0.6479 - val_accuracy: 0.7837\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6372 - accuracy: 0.6362 - val_loss: 0.5952 - val_accuracy: 0.7469\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6446 - accuracy: 0.6309 - val_loss: 0.6291 - val_accuracy: 0.7367\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6316 - accuracy: 0.6389 - val_loss: 0.6163 - val_accuracy: 0.7143\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6651 - accuracy: 0.5886 - val_loss: 0.6984 - val_accuracy: 0.5408\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6245 - accuracy: 0.6436 - val_loss: 0.6265 - val_accuracy: 0.7020\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6002 - accuracy: 0.6611 - val_loss: 0.6078 - val_accuracy: 0.7041\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5871 - accuracy: 0.6698 - val_loss: 0.6772 - val_accuracy: 0.5939\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5988 - accuracy: 0.6624 - val_loss: 0.6327 - val_accuracy: 0.6633\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.715439\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.729864\n",
            "[2]\tvalidation_0-auc:0.735656\n",
            "[3]\tvalidation_0-auc:0.733756\n",
            "[4]\tvalidation_0-auc:0.738643\n",
            "[5]\tvalidation_0-auc:0.733303\n",
            "[6]\tvalidation_0-auc:0.732054\n",
            "[7]\tvalidation_0-auc:0.733629\n",
            "[8]\tvalidation_0-auc:0.742462\n",
            "[9]\tvalidation_0-auc:0.750968\n",
            "[10]\tvalidation_0-auc:0.748561\n",
            "[11]\tvalidation_0-auc:0.751475\n",
            "[12]\tvalidation_0-auc:0.751674\n",
            "[13]\tvalidation_0-auc:0.74981\n",
            "[14]\tvalidation_0-auc:0.749701\n",
            "[15]\tvalidation_0-auc:0.750443\n",
            "[16]\tvalidation_0-auc:0.75057\n",
            "[17]\tvalidation_0-auc:0.748597\n",
            "[18]\tvalidation_0-auc:0.749701\n",
            "[19]\tvalidation_0-auc:0.747638\n",
            "[20]\tvalidation_0-auc:0.748308\n",
            "[21]\tvalidation_0-auc:0.748181\n",
            "[22]\tvalidation_0-auc:0.747566\n",
            "[23]\tvalidation_0-auc:0.747005\n",
            "[24]\tvalidation_0-auc:0.747584\n",
            "[25]\tvalidation_0-auc:0.746643\n",
            "[26]\tvalidation_0-auc:0.748181\n",
            "[27]\tvalidation_0-auc:0.748181\n",
            "[28]\tvalidation_0-auc:0.748398\n",
            "[29]\tvalidation_0-auc:0.748434\n",
            "[30]\tvalidation_0-auc:0.748434\n",
            "[31]\tvalidation_0-auc:0.748561\n",
            "[32]\tvalidation_0-auc:0.749032\n",
            "[33]\tvalidation_0-auc:0.749032\n",
            "[34]\tvalidation_0-auc:0.75124\n",
            "[35]\tvalidation_0-auc:0.75124\n",
            "[36]\tvalidation_0-auc:0.75124\n",
            "[37]\tvalidation_0-auc:0.75124\n",
            "[38]\tvalidation_0-auc:0.751421\n",
            "[39]\tvalidation_0-auc:0.751023\n",
            "[40]\tvalidation_0-auc:0.748652\n",
            "[41]\tvalidation_0-auc:0.748615\n",
            "[42]\tvalidation_0-auc:0.748615\n",
            "[43]\tvalidation_0-auc:0.748615\n",
            "[44]\tvalidation_0-auc:0.748941\n",
            "[45]\tvalidation_0-auc:0.74876\n",
            "[46]\tvalidation_0-auc:0.74876\n",
            "[47]\tvalidation_0-auc:0.748796\n",
            "[48]\tvalidation_0-auc:0.74876\n",
            "[49]\tvalidation_0-auc:0.748796\n",
            "[50]\tvalidation_0-auc:0.748796\n",
            "[51]\tvalidation_0-auc:0.748724\n",
            "[52]\tvalidation_0-auc:0.748833\n",
            "[53]\tvalidation_0-auc:0.748615\n",
            "[54]\tvalidation_0-auc:0.754643\n",
            "[55]\tvalidation_0-auc:0.754643\n",
            "[56]\tvalidation_0-auc:0.754643\n",
            "[57]\tvalidation_0-auc:0.75095\n",
            "[58]\tvalidation_0-auc:0.75095\n",
            "[59]\tvalidation_0-auc:0.751023\n",
            "[60]\tvalidation_0-auc:0.752959\n",
            "[61]\tvalidation_0-auc:0.752923\n",
            "[62]\tvalidation_0-auc:0.752923\n",
            "[63]\tvalidation_0-auc:0.752923\n",
            "[64]\tvalidation_0-auc:0.747837\n",
            "[65]\tvalidation_0-auc:0.740706\n",
            "[66]\tvalidation_0-auc:0.740995\n",
            "[67]\tvalidation_0-auc:0.740054\n",
            "[68]\tvalidation_0-auc:0.739873\n",
            "[69]\tvalidation_0-auc:0.739873\n",
            "[70]\tvalidation_0-auc:0.739873\n",
            "[71]\tvalidation_0-auc:0.740507\n",
            "[72]\tvalidation_0-auc:0.740398\n",
            "[73]\tvalidation_0-auc:0.741158\n",
            "[74]\tvalidation_0-auc:0.74362\n",
            "[75]\tvalidation_0-auc:0.742751\n",
            "[76]\tvalidation_0-auc:0.740724\n",
            "[77]\tvalidation_0-auc:0.740869\n",
            "[78]\tvalidation_0-auc:0.741086\n",
            "[79]\tvalidation_0-auc:0.741448\n",
            "[80]\tvalidation_0-auc:0.741448\n",
            "[81]\tvalidation_0-auc:0.741448\n",
            "[82]\tvalidation_0-auc:0.741448\n",
            "[83]\tvalidation_0-auc:0.741502\n",
            "[84]\tvalidation_0-auc:0.741792\n",
            "[85]\tvalidation_0-auc:0.740127\n",
            "[86]\tvalidation_0-auc:0.741412\n",
            "[87]\tvalidation_0-auc:0.73743\n",
            "[88]\tvalidation_0-auc:0.738045\n",
            "[89]\tvalidation_0-auc:0.738371\n",
            "[90]\tvalidation_0-auc:0.737864\n",
            "[91]\tvalidation_0-auc:0.737213\n",
            "[92]\tvalidation_0-auc:0.736814\n",
            "[93]\tvalidation_0-auc:0.736561\n",
            "[94]\tvalidation_0-auc:0.736778\n",
            "[95]\tvalidation_0-auc:0.736995\n",
            "[96]\tvalidation_0-auc:0.737502\n",
            "[97]\tvalidation_0-auc:0.737538\n",
            "[98]\tvalidation_0-auc:0.737719\n",
            "[99]\tvalidation_0-auc:0.735837\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6707 - accuracy: 0.5813 - val_loss: 0.5567 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6025 - accuracy: 0.6802 - val_loss: 0.5556 - val_accuracy: 0.7527\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5794 - accuracy: 0.7021 - val_loss: 0.4537 - val_accuracy: 0.8906\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5645 - accuracy: 0.7193 - val_loss: 0.4451 - val_accuracy: 0.8665\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5385 - accuracy: 0.7412 - val_loss: 0.6962 - val_accuracy: 0.5208\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6174 - accuracy: 0.6719 - val_loss: 0.3794 - val_accuracy: 0.8578\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5237 - accuracy: 0.7323 - val_loss: 0.4299 - val_accuracy: 0.8862\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4960 - accuracy: 0.7612 - val_loss: 0.4618 - val_accuracy: 0.8271\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4883 - accuracy: 0.7618 - val_loss: 0.4025 - val_accuracy: 0.8818\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4949 - accuracy: 0.7646 - val_loss: 0.4526 - val_accuracy: 0.8446\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.759812\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.735165\n",
            "[2]\tvalidation_0-auc:0.719054\n",
            "[3]\tvalidation_0-auc:0.718093\n",
            "[4]\tvalidation_0-auc:0.716071\n",
            "[5]\tvalidation_0-auc:0.717249\n",
            "[6]\tvalidation_0-auc:0.716385\n",
            "[7]\tvalidation_0-auc:0.718838\n",
            "[8]\tvalidation_0-auc:0.718583\n",
            "[9]\tvalidation_0-auc:0.717249\n",
            "[10]\tvalidation_0-auc:0.723155\n",
            "[11]\tvalidation_0-auc:0.72084\n",
            "[12]\tvalidation_0-auc:0.72029\n",
            "[13]\tvalidation_0-auc:0.723077\n",
            "[14]\tvalidation_0-auc:0.723057\n",
            "[15]\tvalidation_0-auc:0.729101\n",
            "[16]\tvalidation_0-auc:0.72863\n",
            "[17]\tvalidation_0-auc:0.727728\n",
            "[18]\tvalidation_0-auc:0.727688\n",
            "[19]\tvalidation_0-auc:0.727492\n",
            "[20]\tvalidation_0-auc:0.727119\n",
            "[21]\tvalidation_0-auc:0.72916\n",
            "[22]\tvalidation_0-auc:0.727845\n",
            "[23]\tvalidation_0-auc:0.72865\n",
            "[24]\tvalidation_0-auc:0.723214\n",
            "[25]\tvalidation_0-auc:0.723175\n",
            "[26]\tvalidation_0-auc:0.719388\n",
            "[27]\tvalidation_0-auc:0.721291\n",
            "[28]\tvalidation_0-auc:0.721762\n",
            "[29]\tvalidation_0-auc:0.721743\n",
            "[30]\tvalidation_0-auc:0.717327\n",
            "[31]\tvalidation_0-auc:0.717288\n",
            "[32]\tvalidation_0-auc:0.717249\n",
            "[33]\tvalidation_0-auc:0.71717\n",
            "[34]\tvalidation_0-auc:0.716228\n",
            "[35]\tvalidation_0-auc:0.716268\n",
            "[36]\tvalidation_0-auc:0.71458\n",
            "[37]\tvalidation_0-auc:0.714305\n",
            "[38]\tvalidation_0-auc:0.714266\n",
            "[39]\tvalidation_0-auc:0.712068\n",
            "[40]\tvalidation_0-auc:0.711951\n",
            "[41]\tvalidation_0-auc:0.713677\n",
            "[42]\tvalidation_0-auc:0.713677\n",
            "[43]\tvalidation_0-auc:0.710361\n",
            "[44]\tvalidation_0-auc:0.707535\n",
            "[45]\tvalidation_0-auc:0.707457\n",
            "[46]\tvalidation_0-auc:0.705926\n",
            "[47]\tvalidation_0-auc:0.704651\n",
            "[48]\tvalidation_0-auc:0.704768\n",
            "[49]\tvalidation_0-auc:0.704768\n",
            "[50]\tvalidation_0-auc:0.702414\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.759812\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.7142857142857143 | 0.25165562913907286 |  0.5846153846153846 | 0.35185185185185186 |\n",
            "|      GRU 0.15     | 0.6632653061224489 | 0.23118279569892472 |  0.6615384615384615 |  0.3426294820717131 |\n",
            "|    XGBoost 0.15   | 0.7285714285714285 |  0.2638888888888889 |  0.5846153846153846 |  0.3636363636363636 |\n",
            "|    Logreg 0.15    | 0.6857142857142857 | 0.24571428571428572 |  0.6615384615384615 |  0.3583333333333334 |\n",
            "|      SVM 0.15     | 0.7306122448979592 | 0.25547445255474455 |  0.5384615384615384 |  0.3465346534653465 |\n",
            "|   LSTM beta 0.15  | 0.5207877461706784 |      0.19921875     |  0.7846153846153846 |  0.3177570093457944 |\n",
            "|   GRU beta 0.15   | 0.8446389496717724 |  0.4594594594594595 |  0.5230769230769231 | 0.48920863309352514 |\n",
            "| XGBoost beta 0.15 | 0.8840262582056893 |  0.6304347826086957 |  0.4461538461538462 |  0.5225225225225225 |\n",
            "|  logreg beta 0.15 | 0.8161925601750547 | 0.39325842696629215 |  0.5384615384615384 |  0.4545454545454546 |\n",
            "|   svm beta 0.15   | 0.8708971553610503 |  0.5555555555555556 | 0.46153846153846156 |  0.504201680672269  |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXlr0dWRWlIf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "004d9151-6de0-42be-affc-a62b9375f394"
      },
      "source": [
        "Result_purging.to_csv('M_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.452632</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.539185</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.561224</td>\n",
              "      <td>0.761224</td>\n",
              "      <td>0.484581</td>\n",
              "      <td>0.426357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.491525</td>\n",
              "      <td>0.730612</td>\n",
              "      <td>0.568627</td>\n",
              "      <td>0.674419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.497110</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.569536</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.473054</td>\n",
              "      <td>0.718367</td>\n",
              "      <td>0.533784</td>\n",
              "      <td>0.612403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.568807</td>\n",
              "      <td>0.750547</td>\n",
              "      <td>0.521008</td>\n",
              "      <td>0.480620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.587500</td>\n",
              "      <td>0.748359</td>\n",
              "      <td>0.449761</td>\n",
              "      <td>0.364341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.543103</td>\n",
              "      <td>0.739606</td>\n",
              "      <td>0.514286</td>\n",
              "      <td>0.488372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.572519</td>\n",
              "      <td>0.759300</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.581395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.531250</td>\n",
              "      <td>0.735230</td>\n",
              "      <td>0.529183</td>\n",
              "      <td>0.527132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.251656</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.351852</td>\n",
              "      <td>0.584615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.231183</td>\n",
              "      <td>0.663265</td>\n",
              "      <td>0.342629</td>\n",
              "      <td>0.661538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>0.728571</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.584615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.245714</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.358333</td>\n",
              "      <td>0.661538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.255474</td>\n",
              "      <td>0.730612</td>\n",
              "      <td>0.346535</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.199219</td>\n",
              "      <td>0.520788</td>\n",
              "      <td>0.317757</td>\n",
              "      <td>0.784615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.459459</td>\n",
              "      <td>0.844639</td>\n",
              "      <td>0.489209</td>\n",
              "      <td>0.523077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.630435</td>\n",
              "      <td>0.884026</td>\n",
              "      <td>0.522523</td>\n",
              "      <td>0.446154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.393258</td>\n",
              "      <td>0.816193</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>M</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.870897</td>\n",
              "      <td>0.504202</td>\n",
              "      <td>0.461538</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1    M  0.452632  0.700000  0.539185  0.666667\n",
              "1            GRU 0.1    M  0.561224  0.761224  0.484581  0.426357\n",
              "2        XGBoost 0.1    M  0.491525  0.730612  0.568627  0.674419\n",
              "3         Logreg 0.1    M  0.497110  0.734694  0.569536  0.666667\n",
              "4            SVM 0.1    M  0.473054  0.718367  0.533784  0.612403\n",
              "5      LSTM beta 0.1    M  0.568807  0.750547  0.521008  0.480620\n",
              "6       GRU beta 0.1    M  0.587500  0.748359  0.449761  0.364341\n",
              "7   XGBoost beta 0.1    M  0.543103  0.739606  0.514286  0.488372\n",
              "8    logreg beta 0.1    M  0.572519  0.759300  0.576923  0.581395\n",
              "9       svm beta 0.1    M  0.531250  0.735230  0.529183  0.527132\n",
              "0          LSTM 0.15    M  0.251656  0.714286  0.351852  0.584615\n",
              "1           GRU 0.15    M  0.231183  0.663265  0.342629  0.661538\n",
              "2       XGBoost 0.15    M  0.263889  0.728571  0.363636  0.584615\n",
              "3        Logreg 0.15    M  0.245714  0.685714  0.358333  0.661538\n",
              "4           SVM 0.15    M  0.255474  0.730612  0.346535  0.538462\n",
              "5     LSTM beta 0.15    M  0.199219  0.520788  0.317757  0.784615\n",
              "6      GRU beta 0.15    M  0.459459  0.844639  0.489209  0.523077\n",
              "7  XGBoost beta 0.15    M  0.630435  0.884026  0.522523  0.446154\n",
              "8   logreg beta 0.15    M  0.393258  0.816193  0.454545  0.538462\n",
              "9      svm beta 0.15    M  0.555556  0.870897  0.504202  0.461538"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aItKT2ZuWlIf"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('M_logreg_beta_p.csv')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvssar28WlIf"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFdNi1zkXF5F"
      },
      "source": [
        "## MRO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu1-MDLoXF5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1d0720e6-264b-4edb-9a44-afc66f20b0af"
      },
      "source": [
        "dfs = pd.read_csv(\"MRO.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "dfs"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2768</td>\n",
              "      <td>US1.MRO</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>13.81</td>\n",
              "      <td>14.285</td>\n",
              "      <td>13.790</td>\n",
              "      <td>14.2400</td>\n",
              "      <td>898224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2767</td>\n",
              "      <td>US1.MRO</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>13.62</td>\n",
              "      <td>13.900</td>\n",
              "      <td>13.460</td>\n",
              "      <td>13.6800</td>\n",
              "      <td>814757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.MRO</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>13.76</td>\n",
              "      <td>13.845</td>\n",
              "      <td>13.460</td>\n",
              "      <td>13.7900</td>\n",
              "      <td>733779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.MRO</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>13.99</td>\n",
              "      <td>14.190</td>\n",
              "      <td>13.725</td>\n",
              "      <td>13.7600</td>\n",
              "      <td>1367820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.MRO</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>13.25</td>\n",
              "      <td>13.745</td>\n",
              "      <td>13.245</td>\n",
              "      <td>13.6700</td>\n",
              "      <td>1179235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.MRO</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>34.49</td>\n",
              "      <td>35.530</td>\n",
              "      <td>34.470</td>\n",
              "      <td>35.4701</td>\n",
              "      <td>4593832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.MRO</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>35.28</td>\n",
              "      <td>35.280</td>\n",
              "      <td>34.530</td>\n",
              "      <td>34.8600</td>\n",
              "      <td>4232962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.MRO</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>34.21</td>\n",
              "      <td>35.130</td>\n",
              "      <td>34.080</td>\n",
              "      <td>35.0880</td>\n",
              "      <td>8617466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.MRO</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>33.76</td>\n",
              "      <td>34.500</td>\n",
              "      <td>33.610</td>\n",
              "      <td>34.3200</td>\n",
              "      <td>7447390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.MRO</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>33.59</td>\n",
              "      <td>33.740</td>\n",
              "      <td>33.085</td>\n",
              "      <td>33.3700</td>\n",
              "      <td>4627387</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2769 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...  <HIGH>   <LOW>  <CLOSE>    <VOL>\n",
              "0      2768  US1.MRO     D  20211001  ...  14.285  13.790  14.2400   898224\n",
              "1      2767  US1.MRO     D  20210930  ...  13.900  13.460  13.6800   814757\n",
              "2      2766  US1.MRO     D  20210929  ...  13.845  13.460  13.7900   733779\n",
              "3      2765  US1.MRO     D  20210928  ...  14.190  13.725  13.7600  1367820\n",
              "4      2764  US1.MRO     D  20210927  ...  13.745  13.245  13.6700  1179235\n",
              "...     ...      ...   ...       ...  ...     ...     ...      ...      ...\n",
              "2764      4  US1.MRO     D  20101008  ...  35.530  34.470  35.4701  4593832\n",
              "2765      3  US1.MRO     D  20101007  ...  35.280  34.530  34.8600  4232962\n",
              "2766      2  US1.MRO     D  20101006  ...  35.130  34.080  35.0880  8617466\n",
              "2767      1  US1.MRO     D  20101005  ...  34.500  33.610  34.3200  7447390\n",
              "2768      0  US1.MRO     D  20101004  ...  33.740  33.085  33.3700  4627387\n",
              "\n",
              "[2769 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsO8nD2AXF5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3a8cf689-23b5-43dc-d7a7-0a6fc20dd3a9"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"a8f7d284-fb5c-4c29-8e80-4f7ab669c161\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"a8f7d284-fb5c-4c29-8e80-4f7ab669c161\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'a8f7d284-fb5c-4c29-8e80-4f7ab669c161',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [11.09, 11.42, 11.56, 11.66, 11.3, 11.8, 12.28, 12.2, 11.95, 12.33, 12.22, 13.11, 13.18, 13.04, 13.05, 13.06, 14.17, 12.7, 12.58, 13.01, 12.83, 12.89, 12.43, 12.53, 12.1, 11.77, 11.83, 12.19, 11.94, 11.52, 11.64, 11.75, 12.26, 12.42, 12.63, 12.88, 12.46, 12.02, 12.3, 12.95, 12.73, 12.86, 12.86, 12.05, 12.17, 12.5, 13.28, 13.53, 14.06, 14.14, 13.5, 13.75, 13.76, 13.97, 13.75, 13.675, 13.65, 13.39, 13.22, 13.32, 13.47, 13.92, 13.66, 13.9, 13.54, 13.52, 13.755, 13.65, 13.56, 14.26, 14.215, 14.2, 14.39, 13.9, 13.93, 14.04, 14.11, 13.625, 13.67, 13.34, 12.97, 13.2753, 13.0101, 13.41, 13.45, 13.32, 13.26, 13.18, 13.45, 13.33, 13.14, 13.62, 14.08, 14.32, 14.43, 14.57, 15.43, 15.83, 15.54, 15.47, 15.72, 15.44, 15.47, 15.03, 15.265, 15.27, 15.57, 15.5, 15.765, 15.68, 15.51, 16.53, 17.04, 17.14, 17.37, 17.93, 18.15, 18.68, 18.78, 17.62, 17.74, 17.48, 17.38, 17.76, 17.19, 17.46, 17.26, 17.59, 17.7, 17.13, 16.93, 17.13, 17.385, 16.71, 16.82, 17.05, 17.1167, 16.71, 16.68, 17.68, 17.6, 17.12, 17.39, 17.27, 17.26, 17.33, 16.91, 16.56, 16.27, 16.87, 16.86, 17.26, 17.28, 17.09, 16.595, 16.78, 16.52, 16.67, 16.89, 16.68, 17.09, 17.04, 17.08, 16.92, 15.55, 15.45, 15.27, 15.13, 15.27, 15.905, 15.92, 16.15, 16.0, 15.79, 16.01, 15.92, 15.82, 15.9, 15.791, 15.56, 15.67, 16.09, 15.8, 15.55, 15.83, 15.82, 16.1, 16.355, 16.07, 15.48, 15.625, 15.4, 14.62, 14.71, 14.34, 14.08, 14.3, 14.1665, 12.63, 13.46, 13.61, 13.88, 14.235, 14.73, 15.04, 15.61, 15.52, 15.33, 15.3, 16.05, 16.23, 16.83, 17.55, 16.69, 16.55, 16.49, 16.1, 16.24, 15.51, 16.41, 15.93, 17.14, 17.18, 17.15, 16.68, 16.48, 17.27, 17.84, 18.05, 18.49, 18.32, 18.69, 18.21, 18.6, 18.99, 18.68, 18.16, 18.69, 18.79, 18.36, 19.49, 20.46, 20.665, 20.55, 20.88, 21.17, 20.83, 20.51, 19.96, 21.15, 22.94, 22.88, 23.41, 23.71, 23.995, 23.515, 23.86, 23.28, 22.74, 22.44, 22.5, 22.34, 21.85, 21.55, 21.76, 21.5, 20.88, 20.88, 20.67, 20.55, 20.54, 20.35, 20.53, 20.365, 21.21, 21.07, 21.51, 21.64, 21.77, 20.7, 20.86, 20.61, 20.79, 20.87, 20.21, 19.98, 19.81, 19.32, 19.0, 20.45, 20.09, 20.39, 20.085, 20.15, 20.72, 20.61, 20.54, 20.83, 20.32, 21.12, 21.12, 20.83, 21.24, 21.18, 20.84, 20.46, 20.475, 20.545, 20.37, 20.065, 20.04, 21.48, 21.43, 21.29, 21.98, 21.92, 21.09, 20.705, 21.44, 20.65, 20.865, 20.705, 21.22, 20.74, 20.3, 21.48, 19.93, 21.07, 20.52, 20.33, 19.99, 21.12, 21.48, 21.37, 21.48, 20.99, 21.1, 20.7, 20.59, 21.09, 21.52, 21.44, 21.54, 20.683, 20.72, 21.47, 21.63, 21.545, 21.9, 21.41, 21.65, 21.18, 21.28, 21.42, 21.01, 21.13, 21.265, 20.44, 19.77, 19.72, 19.45, 18.31, 18.21, 18.27, 18.12, 18.46, 17.97, 17.71, 17.975, 18.02, 17.98, 18.21, 18.14, 18.2, 18.16, 17.62, 17.52, 17.06, 16.355, 16.25, 16.8, 15.9, 16.02, 15.56, 16.12, 15.32, 15.83, 16.26, 15.76, 15.85, 16.32, 15.28, 14.855, 15.12, 14.915, 14.88, 14.82, 15.08, 15.215, 14.83, 14.8, 15.025, 15.15, 14.86, 14.51, 14.52, 14.995, 15.335, 15.54, 15.18, 15.3, 15.855, 15.635, 16.34, 16.97, 16.07, 16.28, 15.5, 15.69, 16.33, 17.23, 16.78, 17.52, 18.425, 18.18, 18.065, 18.61, 18.84, 18.685, 19.29, 19.215, 19.27, 18.58, 18.69, 18.835, 18.24, 18.82, 18.61, 17.91, 18.05, 17.96, 17.89, 18.12, 17.67, 17.38, 16.93, 17.07, 17.01, 17.26, 16.91, 17.04, 16.33, 16.025, 15.44, 15.005, 15.13, 15.2, 15.18, 15.15, 15.14, 14.74, 14.45, 14.88, 14.91, 15.08, 14.84, 14.74, 14.56, 14.48, 15.15, 14.88, 14.52, 14.47, 15.0599, 14.52, 14.785, 14.845, 15.54, 15.62, 15.78, 15.75, 16.16, 16.34, 15.58, 15.43, 14.89, 14.21, 14.17, 13.81, 13.55, 13.485, 13.72, 13.63, 13.92, 13.74, 13.76, 13.96, 13.85, 13.76, 13.61, 13.675, 13.61, 13.57, 13.55, 13.89, 13.91, 13.75, 13.595, 13.56, 13.525, 13.73, 13.4, 13.19, 12.79, 12.74, 12.42, 12.0, 12.02, 12.0, 11.93, 11.91, 11.595, 11.49, 11.3, 11.78, 11.73, 11.29, 11.2673, 11.12, 10.9, 10.88, 10.92, 11.05, 10.995, 11.01, 10.89, 10.77, 10.8653, 10.82, 11.19, 11.53, 11.6267, 12.11, 12.06, 12.195, 12.36, 12.345, 12.9, 12.52, 12.04, 11.905, 12.22, 12.34, 12.23, 12.165, 12.34, 11.89, 11.84, 12.08, 12.25, 11.68, 11.59, 11.64, 11.46, 11.44, 11.6, 11.53, 11.46, 11.59, 11.74, 12.2, 11.85, 11.7, 11.445, 11.34, 11.52, 11.54, 11.6, 11.62, 12.07, 12.5, 12.52, 12.38, 12.65, 13.01, 12.62, 12.53, 12.07, 12.205, 12.89, 12.79, 12.71, 13.09, 13.02, 13.12, 13.52, 13.5, 14.53, 14.56, 14.44, 14.53, 14.125, 14.34, 14.43, 14.54, 14.32, 14.49, 14.7, 14.4, 14.58, 14.29, 14.18, 14.85, 14.65, 14.83, 14.88, 14.89, 15.32, 15.6, 15.14, 15.15, 15.09, 15.06, 15.74, 16.02, 15.91, 16.42, 16.61, 16.55, 16.27, 16.31, 15.99, 16.13, 15.81, 15.8, 15.855, 15.84, 15.23, 14.74, 14.61, 14.69, 15.01, 15.05, 15.485, 15.455, 15.55, 15.56, 15.32, 15.845, 16.16, 16.065, 14.865, 16.27, 16.46, 16.17, 16.13, 16.46, 16.01, 15.85, 15.73, 15.98, 15.76, 16.38, 16.24, 16.165, 16.29, 16.405, 16.07, 16.18, 16.05, 15.88, 16.02, 16.52, 17.22, 16.67, 16.58, 16.75, 16.52, 17.41, 17.75, 17.75, 17.7, 17.41, 17.6, 17.27, 17.32, 17.45, 17.45, 17.445, 17.69, 17.48, 17.41, 17.88, 18.18, 18.06, 17.755, 17.32, 17.58, 17.85, 18.14, 18.06, 18.035, 18.085, 18.3, 18.51, 18.58, 18.24, 18.21, 18.8, 18.5, 18.25, 18.37, 18.41, 18.35, 18.39, 18.21, 17.91, 18.09, 14.955, 15.55, 16.2, 16.78, 16.54, 16.49, 15.63, 15.46, 15.56, 15.7, 14.93, 14.89, 15.13, 14.88, 14.25, 14.15, 13.69, 14.16, 12.79, 13.27, 13.19, 13.72, 13.955, 14.24, 14.18, 14.58, 14.63, 14.7, 14.65, 14.19, 14.19, 14.58, 14.77, 14.93, 15.07, 15.72, 15.56, 16.07, 16.125, 15.63, 15.97, 15.81, 15.89, 15.17, 14.05, 14.415, 14.51, 14.87, 14.66, 13.87, 14.08, 14.24, 14.26, 14.015, 14.34, 15.47, 15.67, 16.73, 15.68, 15.63, 15.49, 15.1, 15.02, 15.55, 16.0, 15.96, 16.02, 16.07, 16.43, 15.65, 16.8, 16.6761, 15.69, 15.17, 14.895, 14.7, 14.45, 13.99, 14.42, 14.24, 13.87, 13.42, 13.4154, 12.9001, 12.911, 13.64, 13.35, 13.14, 13.79, 13.575, 14.1, 14.41, 14.85, 14.76, 15.18, 15.21, 15.28, 15.28, 15.59, 14.69, 14.92, 14.74, 14.84, 14.72, 15.68, 15.0, 15.13, 14.7701, 13.6556, 14.6, 15.26, 14.74, 15.075, 14.48, 13.16, 12.66, 13.07, 13.23, 13.28, 13.42, 14.26, 14.54, 14.63, 13.96, 13.33, 13.39, 13.22, 13.07, 12.89, 13.18, 13.53, 13.01, 13.2, 13.1, 12.74, 12.65, 12.85, 12.79, 12.29, 12.33, 12.47, 11.93, 11.44, 12.03, 12.06, 12.16, 12.8, 13.54, 14.09, 14.36, 14.83, 14.44, 13.83, 14.33, 13.86, 14.06, 13.94, 13.35, 13.02, 13.11, 12.93, 13.12, 11.66, 11.77, 11.21, 11.01, 10.63, 10.52, 10.5537, 11.125, 10.54, 10.4, 10.16, 10.29, 10.19, 11.3175, 11.44, 11.62, 11.44, 11.51, 11.3, 11.09, 11.15, 10.6, 10.46, 10.12, 11.03, 11.01, 9.98, 9.1189, 7.97, 8.21, 8.02, 7.385, 7.22, 7.02, 7.48, 6.72, 6.93, 7.4, 7.39, 7.48, 7.0699, 7.13, 7.285, 7.81, 8.4599, 9.17, 9.41, 8.74, 9.73, 9.73, 9.17, 8.76, 8.45, 8.1325, 9.02, 8.79, 7.87, 7.68, 8.14, 9.062, 8.54, 9.21, 9.61, 10.34, 10.67, 11.28, 12.76, 12.83, 12.59, 12.39, 13.03, 12.98, 13.93, 13.96, 12.69, 12.53, 12.5, 12.79, 13.77, 14.451, 13.97, 14.3408, 14.75, 14.5, 14.91, 14.7899, 16.11, 16.56, 16.68, 17.62, 17.52, 17.48, 18.1, 18.5, 17.54, 17.43, 17.65, 17.9, 17.72, 17.87, 17.14, 17.21, 17.305, 18.78, 18.37, 18.28, 19.01, 19.39, 19.67, 19.02, 18.37, 17.56, 17.67, 17.12, 17.58, 18.31, 18.49, 18.02, 18.57, 18.29, 19.28, 19.44, 18.9, 18.58, 18.97, 19.59, 20.17, 19.16, 19.17, 18.04, 16.65, 15.31, 15.41, 14.98, 14.76, 15.74, 15.45, 15.29, 15.66, 16.11, 15.81, 16.43, 16.21, 15.46, 15.42, 15.08, 14.98, 14.93, 16.32, 16.37, 16.86, 16.86, 16.85, 17.28, 16.65, 15.98, 14.65, 14.04, 14.39, 15.745, 16.14, 16.25, 17.48, 17.36, 17.4, 18.02, 19.07, 18.89, 18.99, 18.39, 19.46, 19.76, 20.51, 20.57, 21.03, 21.87, 21.76, 21.19, 20.685, 21.41, 22.11, 22.345, 23.16, 23.24, 23.58, 24.28, 24.41, 25.07, 24.75, 24.46, 24.77, 24.74, 25.7, 25.11, 25.72, 25.79, 26.54, 26.686, 27.74, 26.81, 26.59, 26.61, 26.11, 25.91, 26.52, 26.54, 26.74, 26.45, 26.49, 26.92, 27.04, 26.73, 26.58, 26.96, 26.66, 27.09, 27.08, 27.1, 27.19, 27.23, 27.58, 27.67, 28.1, 28.19, 27.45, 27.33, 27.7, 27.68, 27.61, 28.15, 28.4, 28.41, 29.31, 28.92, 29.81, 29.98, 31.09, 30.77, 31.12, 31.19, 30.72, 30.41, 30.33, 30.55, 30.34, 30.135, 30.98, 30.52, 30.71, 30.69, 29.41, 28.9, 29.19, 28.77, 27.95, 28.16, 27.88, 27.08, 26.65, 26.09, 26.44, 25.91, 26.38, 26.42, 25.89, 25.91, 26.17, 25.68, 26.325, 25.57, 26.015, 25.77, 25.61, 25.8, 25.83, 26.29, 26.77, 27.34, 27.61, 27.74, 27.66, 27.86, 27.99, 29.075, 28.85, 28.54, 28.48, 29.03, 29.03, 29.63, 29.49, 28.78, 28.07, 28.37, 28.3, 27.97, 28.37, 27.99, 28.78, 27.905, 26.62, 25.56, 25.54, 26.98, 27.27, 26.53, 26.82, 26.58, 25.93, 26.6, 25.46, 25.97, 25.9, 26.23, 27.66, 27.73, 27.18, 26.84, 27.29, 28.6, 28.29, 28.42, 28.58, 28.25, 28.28, 28.7, 28.28, 28.19, 27.58, 26.68, 25.32, 24.79, 25.42, 26.24, 26.43, 27.78, 27.24, 29.23, 29.84, 30.53, 29.74, 28.98, 28.93, 32.5, 33.12, 33.58, 33.83, 33.28, 32.78, 32.07, 31.8199, 32.37, 31.95, 32.6499, 33.1201, 33.25, 34.4, 34.34, 33.57, 33.37, 34.56, 35.41, 34.12, 34.37, 34.26, 33.01, 34.5, 34.72, 34.05, 34.87, 34.2501, 33.92, 33.49, 32.75, 32.555, 32.93, 34.23, 34.73, 36.16, 35.84, 36.64, 36.65, 36.9, 37.12, 37.6, 38.605, 38.54, 38.14, 38.69, 38.4, 38.73, 39.23, 39.33, 40.34, 40.13, 39.83, 39.59, 40.02, 40.02, 40.07, 40.22, 40.9, 40.54, 41.28, 41.015, 41.7, 40.93, 40.9, 40.67, 40.38, 39.99, 40.28, 39.92, 39.74, 38.925, 39.03, 39.0, 39.15, 39.03, 39.12, 39.08, 38.08, 38.94, 38.45, 39.25, 38.61, 38.76, 39.65, 39.94, 40.24, 40.52, 40.72, 40.49, 40.2, 39.77, 39.8, 39.53, 40.25, 39.32, 39.69, 39.15, 39.38, 39.59, 39.45, 39.58, 40.32, 39.64, 39.77, 39.91, 39.45, 39.62, 39.71, 39.04, 40.16, 39.74, 39.295, 38.93, 38.89, 39.0, 39.07, 39.16, 38.49, 38.16, 37.8301, 37.3, 37.05, 36.66, 36.82, 36.43, 36.65, 36.65, 36.54, 36.29, 36.27, 36.17, 35.96, 35.39, 35.64, 35.84, 36.23, 36.57, 36.49, 36.29, 35.91, 35.69, 35.51, 36.21, 36.45, 36.53, 36.33, 36.16, 36.9, 36.22, 36.07, 36.37, 36.46, 36.38, 36.56, 36.74, 36.45, 35.895, 35.53, 35.07, 35.4052, 35.48, 35.265, 34.89, 35.6, 35.71, 35.39, 35.41, 35.53, 35.21, 34.49, 34.505, 34.56, 34.24, 34.38, 33.81, 33.31, 33.66, 33.17, 33.07, 32.86, 33.46, 33.94, 34.3, 34.25, 33.86, 33.62, 33.86, 33.53, 33.52, 33.33, 33.78, 34.03, 33.88, 33.26, 33.475, 33.35, 33.53, 33.22, 33.26, 33.29, 32.68, 32.26, 32.59, 31.8, 32.1, 32.33, 32.06, 32.79, 33.03, 33.04, 33.37, 32.9, 33.0, 33.72, 34.39, 34.27, 33.97, 34.17, 34.02, 34.56, 34.105, 34.63, 34.5, 34.39, 34.87, 34.42, 34.52, 34.91, 35.31, 35.19, 35.62, 35.27, 35.25, 35.2, 35.19, 34.91, 34.73, 34.85, 35.59, 35.81, 36.18, 35.69, 36.09, 36.41, 36.57, 36.3, 36.74, 36.23, 36.46, 36.14, 36.23, 36.79, 36.96, 37.6, 37.94, 37.26, 36.93, 36.51, 36.72, 36.42, 36.14, 35.81, 36.28, 36.39, 35.74, 36.46, 35.68, 35.51, 35.28, 35.28, 35.75, 35.98, 36.074, 35.6, 35.6, 34.86]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a8f7d284-fb5c-4c29-8e80-4f7ab669c161');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"d8204db7-c0eb-4121-b92c-aca24bab6760\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"d8204db7-c0eb-4121-b92c-aca24bab6760\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'd8204db7-c0eb-4121-b92c-aca24bab6760',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d8204db7-c0eb-4121-b92c-aca24bab6760');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h42TN8jAXF5M"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKJ-yb6wXF5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a11289ca-4725-4386-a178-f4196dcf2bc3"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"MRO\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6850 - accuracy: 0.5430 - val_loss: 0.8262 - val_accuracy: 0.2102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6308 - accuracy: 0.6631 - val_loss: 0.6113 - val_accuracy: 0.7347\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6138 - accuracy: 0.6805 - val_loss: 0.6257 - val_accuracy: 0.7429\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5819 - accuracy: 0.7275 - val_loss: 0.7716 - val_accuracy: 0.5327\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5402 - accuracy: 0.7369 - val_loss: 0.8533 - val_accuracy: 0.2776\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6339 - accuracy: 0.6342 - val_loss: 0.6148 - val_accuracy: 0.7592\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5380 - accuracy: 0.7349 - val_loss: 0.5665 - val_accuracy: 0.7612\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5168 - accuracy: 0.7483 - val_loss: 0.7915 - val_accuracy: 0.4408\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5217 - accuracy: 0.7369 - val_loss: 0.7049 - val_accuracy: 0.5429\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5014 - accuracy: 0.7497 - val_loss: 0.6577 - val_accuracy: 0.6061\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.764996\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.771995\n",
            "[2]\tvalidation_0-auc:0.780625\n",
            "[3]\tvalidation_0-auc:0.782168\n",
            "[4]\tvalidation_0-auc:0.782795\n",
            "[5]\tvalidation_0-auc:0.783222\n",
            "[6]\tvalidation_0-auc:0.782745\n",
            "[7]\tvalidation_0-auc:0.784125\n",
            "[8]\tvalidation_0-auc:0.784852\n",
            "[9]\tvalidation_0-auc:0.785542\n",
            "[10]\tvalidation_0-auc:0.786872\n",
            "[11]\tvalidation_0-auc:0.786859\n",
            "[12]\tvalidation_0-auc:0.787311\n",
            "[13]\tvalidation_0-auc:0.78839\n",
            "[14]\tvalidation_0-auc:0.789255\n",
            "[15]\tvalidation_0-auc:0.789356\n",
            "[16]\tvalidation_0-auc:0.788352\n",
            "[17]\tvalidation_0-auc:0.788778\n",
            "[18]\tvalidation_0-auc:0.789305\n",
            "[19]\tvalidation_0-auc:0.788289\n",
            "[20]\tvalidation_0-auc:0.788778\n",
            "[21]\tvalidation_0-auc:0.788089\n",
            "[22]\tvalidation_0-auc:0.788465\n",
            "[23]\tvalidation_0-auc:0.788528\n",
            "[24]\tvalidation_0-auc:0.788866\n",
            "[25]\tvalidation_0-auc:0.789017\n",
            "[26]\tvalidation_0-auc:0.789631\n",
            "[27]\tvalidation_0-auc:0.789669\n",
            "[28]\tvalidation_0-auc:0.789669\n",
            "[29]\tvalidation_0-auc:0.789519\n",
            "[30]\tvalidation_0-auc:0.78928\n",
            "[31]\tvalidation_0-auc:0.78928\n",
            "[32]\tvalidation_0-auc:0.789631\n",
            "[33]\tvalidation_0-auc:0.789782\n",
            "[34]\tvalidation_0-auc:0.790409\n",
            "[35]\tvalidation_0-auc:0.790409\n",
            "[36]\tvalidation_0-auc:0.790434\n",
            "[37]\tvalidation_0-auc:0.790434\n",
            "[38]\tvalidation_0-auc:0.790384\n",
            "[39]\tvalidation_0-auc:0.790384\n",
            "[40]\tvalidation_0-auc:0.790384\n",
            "[41]\tvalidation_0-auc:0.790183\n",
            "[42]\tvalidation_0-auc:0.789318\n",
            "[43]\tvalidation_0-auc:0.788904\n",
            "[44]\tvalidation_0-auc:0.788904\n",
            "[45]\tvalidation_0-auc:0.788904\n",
            "[46]\tvalidation_0-auc:0.788904\n",
            "[47]\tvalidation_0-auc:0.788778\n",
            "[48]\tvalidation_0-auc:0.788979\n",
            "[49]\tvalidation_0-auc:0.788979\n",
            "[50]\tvalidation_0-auc:0.789318\n",
            "[51]\tvalidation_0-auc:0.789318\n",
            "[52]\tvalidation_0-auc:0.789318\n",
            "[53]\tvalidation_0-auc:0.789293\n",
            "[54]\tvalidation_0-auc:0.789293\n",
            "[55]\tvalidation_0-auc:0.790974\n",
            "[56]\tvalidation_0-auc:0.791049\n",
            "[57]\tvalidation_0-auc:0.791049\n",
            "[58]\tvalidation_0-auc:0.791224\n",
            "[59]\tvalidation_0-auc:0.791224\n",
            "[60]\tvalidation_0-auc:0.790836\n",
            "[61]\tvalidation_0-auc:0.791613\n",
            "[62]\tvalidation_0-auc:0.791714\n",
            "[63]\tvalidation_0-auc:0.79199\n",
            "[64]\tvalidation_0-auc:0.791613\n",
            "[65]\tvalidation_0-auc:0.791588\n",
            "[66]\tvalidation_0-auc:0.791588\n",
            "[67]\tvalidation_0-auc:0.791011\n",
            "[68]\tvalidation_0-auc:0.790359\n",
            "[69]\tvalidation_0-auc:0.790259\n",
            "[70]\tvalidation_0-auc:0.790259\n",
            "[71]\tvalidation_0-auc:0.790058\n",
            "[72]\tvalidation_0-auc:0.790058\n",
            "[73]\tvalidation_0-auc:0.789983\n",
            "[74]\tvalidation_0-auc:0.790585\n",
            "[75]\tvalidation_0-auc:0.790585\n",
            "[76]\tvalidation_0-auc:0.788766\n",
            "[77]\tvalidation_0-auc:0.789393\n",
            "[78]\tvalidation_0-auc:0.789393\n",
            "[79]\tvalidation_0-auc:0.789318\n",
            "[80]\tvalidation_0-auc:0.78775\n",
            "[81]\tvalidation_0-auc:0.788139\n",
            "[82]\tvalidation_0-auc:0.787863\n",
            "[83]\tvalidation_0-auc:0.787712\n",
            "[84]\tvalidation_0-auc:0.787712\n",
            "[85]\tvalidation_0-auc:0.787712\n",
            "[86]\tvalidation_0-auc:0.786897\n",
            "[87]\tvalidation_0-auc:0.786897\n",
            "[88]\tvalidation_0-auc:0.7877\n",
            "[89]\tvalidation_0-auc:0.7877\n",
            "[90]\tvalidation_0-auc:0.787675\n",
            "[91]\tvalidation_0-auc:0.785919\n",
            "[92]\tvalidation_0-auc:0.785893\n",
            "[93]\tvalidation_0-auc:0.785567\n",
            "[94]\tvalidation_0-auc:0.785756\n",
            "[95]\tvalidation_0-auc:0.784075\n",
            "[96]\tvalidation_0-auc:0.783774\n",
            "[97]\tvalidation_0-auc:0.785128\n",
            "[98]\tvalidation_0-auc:0.78568\n",
            "[99]\tvalidation_0-auc:0.785078\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6825 - accuracy: 0.5491 - val_loss: 0.6756 - val_accuracy: 0.6696\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6484 - accuracy: 0.6088 - val_loss: 0.6730 - val_accuracy: 0.7352\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6079 - accuracy: 0.6760 - val_loss: 0.6442 - val_accuracy: 0.6521\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5606 - accuracy: 0.7069 - val_loss: 0.6157 - val_accuracy: 0.6893\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5488 - accuracy: 0.7289 - val_loss: 0.5509 - val_accuracy: 0.7702\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6672 - accuracy: 0.6033 - val_loss: 0.6150 - val_accuracy: 0.8162\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5754 - accuracy: 0.6911 - val_loss: 0.5685 - val_accuracy: 0.7943\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5477 - accuracy: 0.7268 - val_loss: 0.6001 - val_accuracy: 0.7309\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5306 - accuracy: 0.7509 - val_loss: 0.6470 - val_accuracy: 0.6411\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5202 - accuracy: 0.7385 - val_loss: 0.6583 - val_accuracy: 0.5952\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.706695\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.706695\n",
            "[2]\tvalidation_0-auc:0.764001\n",
            "[3]\tvalidation_0-auc:0.760381\n",
            "[4]\tvalidation_0-auc:0.811626\n",
            "[5]\tvalidation_0-auc:0.81552\n",
            "[6]\tvalidation_0-auc:0.821019\n",
            "[7]\tvalidation_0-auc:0.826395\n",
            "[8]\tvalidation_0-auc:0.825435\n",
            "[9]\tvalidation_0-auc:0.830207\n",
            "[10]\tvalidation_0-auc:0.83997\n",
            "[11]\tvalidation_0-auc:0.827533\n",
            "[12]\tvalidation_0-auc:0.828246\n",
            "[13]\tvalidation_0-auc:0.827067\n",
            "[14]\tvalidation_0-auc:0.829247\n",
            "[15]\tvalidation_0-auc:0.829247\n",
            "[16]\tvalidation_0-auc:0.827738\n",
            "[17]\tvalidation_0-auc:0.824173\n",
            "[18]\tvalidation_0-auc:0.82937\n",
            "[19]\tvalidation_0-auc:0.829233\n",
            "[20]\tvalidation_0-auc:0.828849\n",
            "[21]\tvalidation_0-auc:0.826573\n",
            "[22]\tvalidation_0-auc:0.825311\n",
            "[23]\tvalidation_0-auc:0.825311\n",
            "[24]\tvalidation_0-auc:0.824639\n",
            "[25]\tvalidation_0-auc:0.824173\n",
            "[26]\tvalidation_0-auc:0.826477\n",
            "[27]\tvalidation_0-auc:0.827492\n",
            "[28]\tvalidation_0-auc:0.826614\n",
            "[29]\tvalidation_0-auc:0.827876\n",
            "[30]\tvalidation_0-auc:0.831441\n",
            "[31]\tvalidation_0-auc:0.831441\n",
            "[32]\tvalidation_0-auc:0.831414\n",
            "[33]\tvalidation_0-auc:0.831605\n",
            "[34]\tvalidation_0-auc:0.827615\n",
            "[35]\tvalidation_0-auc:0.827615\n",
            "[36]\tvalidation_0-auc:0.827615\n",
            "[37]\tvalidation_0-auc:0.828026\n",
            "[38]\tvalidation_0-auc:0.824845\n",
            "[39]\tvalidation_0-auc:0.822555\n",
            "[40]\tvalidation_0-auc:0.822089\n",
            "[41]\tvalidation_0-auc:0.823652\n",
            "[42]\tvalidation_0-auc:0.823652\n",
            "[43]\tvalidation_0-auc:0.823364\n",
            "[44]\tvalidation_0-auc:0.822925\n",
            "[45]\tvalidation_0-auc:0.821129\n",
            "[46]\tvalidation_0-auc:0.818894\n",
            "[47]\tvalidation_0-auc:0.818894\n",
            "[48]\tvalidation_0-auc:0.819936\n",
            "[49]\tvalidation_0-auc:0.82165\n",
            "[50]\tvalidation_0-auc:0.82165\n",
            "[51]\tvalidation_0-auc:0.82165\n",
            "[52]\tvalidation_0-auc:0.821239\n",
            "[53]\tvalidation_0-auc:0.821239\n",
            "[54]\tvalidation_0-auc:0.822404\n",
            "[55]\tvalidation_0-auc:0.822404\n",
            "[56]\tvalidation_0-auc:0.821581\n",
            "[57]\tvalidation_0-auc:0.816878\n",
            "[58]\tvalidation_0-auc:0.816713\n",
            "[59]\tvalidation_0-auc:0.816713\n",
            "[60]\tvalidation_0-auc:0.816713\n",
            "Stopping. Best iteration:\n",
            "[10]\tvalidation_0-auc:0.83997\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+---------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |       Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+---------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.27755102040816326 | 0.21923937360178972 | 0.9514563106796117 |  0.3563636363636364 |\n",
            "|     GRU 0.1      |  0.6061224489795919 | 0.32558139534883723 | 0.8155339805825242 | 0.46537396121883656 |\n",
            "|   XGBoost 0.1    |  0.7142857142857143 |  0.4088669950738916 | 0.8058252427184466 |  0.542483660130719  |\n",
            "|    Logreg 0.1    |  0.6183673469387755 |  0.3346456692913386 | 0.8252427184466019 |  0.4761904761904762 |\n",
            "|     SVM 0.1      |  0.6857142857142857 |  0.3813953488372093 | 0.7961165048543689 |  0.5157232704402516 |\n",
            "|  LSTM beta 0.1   |  0.7702407002188184 |  0.4909090909090909 | 0.5242718446601942 |  0.5070422535211268 |\n",
            "|   GRU beta 0.1   |  0.5951859956236324 |  0.3435114503816794 | 0.8737864077669902 |  0.4931506849315068 |\n",
            "| XGBoost beta 0.1 |  0.8358862144420132 |  0.7121212121212122 | 0.4563106796116505 |  0.5562130177514794 |\n",
            "| logreg beta 0.1  |  0.6783369803063457 |  0.397196261682243  | 0.8252427184466019 |  0.5362776025236593 |\n",
            "|   svm beta 0.1   |  0.8183807439824945 |  0.5793650793650794 | 0.7087378640776699 |  0.6375545851528385 |\n",
            "+------------------+---------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 19ms/step - loss: 0.6733 - accuracy: 0.5866 - val_loss: 0.6264 - val_accuracy: 0.7000\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6024 - accuracy: 0.6745 - val_loss: 1.0063 - val_accuracy: 0.2980\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5928 - accuracy: 0.6946 - val_loss: 0.7468 - val_accuracy: 0.4469\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5738 - accuracy: 0.7101 - val_loss: 0.7612 - val_accuracy: 0.5020\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5467 - accuracy: 0.7242 - val_loss: 0.6139 - val_accuracy: 0.7510\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6540 - accuracy: 0.6107 - val_loss: 1.0108 - val_accuracy: 0.2102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5590 - accuracy: 0.7121 - val_loss: 0.6104 - val_accuracy: 0.7245\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5529 - accuracy: 0.7114 - val_loss: 0.5926 - val_accuracy: 0.7367\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5398 - accuracy: 0.7362 - val_loss: 0.6474 - val_accuracy: 0.6551\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5385 - accuracy: 0.7396 - val_loss: 0.6823 - val_accuracy: 0.5898\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.755058\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.755234\n",
            "[2]\tvalidation_0-auc:0.758349\n",
            "[3]\tvalidation_0-auc:0.754449\n",
            "[4]\tvalidation_0-auc:0.754124\n",
            "[5]\tvalidation_0-auc:0.75732\n",
            "[6]\tvalidation_0-auc:0.758471\n",
            "[7]\tvalidation_0-auc:0.75927\n",
            "[8]\tvalidation_0-auc:0.759405\n",
            "[9]\tvalidation_0-auc:0.759405\n",
            "[10]\tvalidation_0-auc:0.758457\n",
            "[11]\tvalidation_0-auc:0.758376\n",
            "[12]\tvalidation_0-auc:0.758349\n",
            "[13]\tvalidation_0-auc:0.758552\n",
            "[14]\tvalidation_0-auc:0.758349\n",
            "[15]\tvalidation_0-auc:0.758295\n",
            "[16]\tvalidation_0-auc:0.758417\n",
            "[17]\tvalidation_0-auc:0.756968\n",
            "[18]\tvalidation_0-auc:0.75713\n",
            "[19]\tvalidation_0-auc:0.757103\n",
            "[20]\tvalidation_0-auc:0.757442\n",
            "[21]\tvalidation_0-auc:0.757618\n",
            "[22]\tvalidation_0-auc:0.757699\n",
            "[23]\tvalidation_0-auc:0.758159\n",
            "[24]\tvalidation_0-auc:0.757577\n",
            "[25]\tvalidation_0-auc:0.758281\n",
            "[26]\tvalidation_0-auc:0.758119\n",
            "[27]\tvalidation_0-auc:0.756805\n",
            "[28]\tvalidation_0-auc:0.756805\n",
            "[29]\tvalidation_0-auc:0.757076\n",
            "[30]\tvalidation_0-auc:0.757238\n",
            "[31]\tvalidation_0-auc:0.756588\n",
            "[32]\tvalidation_0-auc:0.757022\n",
            "[33]\tvalidation_0-auc:0.757293\n",
            "[34]\tvalidation_0-auc:0.757293\n",
            "[35]\tvalidation_0-auc:0.757536\n",
            "[36]\tvalidation_0-auc:0.757401\n",
            "[37]\tvalidation_0-auc:0.756426\n",
            "[38]\tvalidation_0-auc:0.75667\n",
            "[39]\tvalidation_0-auc:0.756683\n",
            "[40]\tvalidation_0-auc:0.756602\n",
            "[41]\tvalidation_0-auc:0.756629\n",
            "[42]\tvalidation_0-auc:0.756629\n",
            "[43]\tvalidation_0-auc:0.756737\n",
            "[44]\tvalidation_0-auc:0.756737\n",
            "[45]\tvalidation_0-auc:0.756764\n",
            "[46]\tvalidation_0-auc:0.756764\n",
            "[47]\tvalidation_0-auc:0.757144\n",
            "[48]\tvalidation_0-auc:0.756683\n",
            "[49]\tvalidation_0-auc:0.755505\n",
            "[50]\tvalidation_0-auc:0.755911\n",
            "[51]\tvalidation_0-auc:0.755911\n",
            "[52]\tvalidation_0-auc:0.755803\n",
            "[53]\tvalidation_0-auc:0.755803\n",
            "[54]\tvalidation_0-auc:0.756439\n",
            "[55]\tvalidation_0-auc:0.757293\n",
            "[56]\tvalidation_0-auc:0.757374\n",
            "[57]\tvalidation_0-auc:0.758335\n",
            "[58]\tvalidation_0-auc:0.758335\n",
            "Stopping. Best iteration:\n",
            "[8]\tvalidation_0-auc:0.759405\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6645 - accuracy: 0.5704 - val_loss: 0.7065 - val_accuracy: 0.3085\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6292 - accuracy: 0.6486 - val_loss: 0.7268 - val_accuracy: 0.4595\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5981 - accuracy: 0.6843 - val_loss: 0.6147 - val_accuracy: 0.8118\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5747 - accuracy: 0.6994 - val_loss: 0.6131 - val_accuracy: 0.7352\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5589 - accuracy: 0.7152 - val_loss: 0.6258 - val_accuracy: 0.7090\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6313 - accuracy: 0.6472 - val_loss: 0.6700 - val_accuracy: 0.6302\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5636 - accuracy: 0.7213 - val_loss: 0.6351 - val_accuracy: 0.6937\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5420 - accuracy: 0.7234 - val_loss: 0.6036 - val_accuracy: 0.7374\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5405 - accuracy: 0.7282 - val_loss: 0.6268 - val_accuracy: 0.6718\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5308 - accuracy: 0.7385 - val_loss: 0.5579 - val_accuracy: 0.7768\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.721951\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.717964\n",
            "[2]\tvalidation_0-auc:0.76186\n",
            "[3]\tvalidation_0-auc:0.770176\n",
            "[4]\tvalidation_0-auc:0.776719\n",
            "[5]\tvalidation_0-auc:0.831118\n",
            "[6]\tvalidation_0-auc:0.830808\n",
            "[7]\tvalidation_0-auc:0.836095\n",
            "[8]\tvalidation_0-auc:0.840896\n",
            "[9]\tvalidation_0-auc:0.839773\n",
            "[10]\tvalidation_0-auc:0.839921\n",
            "[11]\tvalidation_0-auc:0.840334\n",
            "[12]\tvalidation_0-auc:0.840334\n",
            "[13]\tvalidation_0-auc:0.838252\n",
            "[14]\tvalidation_0-auc:0.8371\n",
            "[15]\tvalidation_0-auc:0.835298\n",
            "[16]\tvalidation_0-auc:0.8345\n",
            "[17]\tvalidation_0-auc:0.836657\n",
            "[18]\tvalidation_0-auc:0.839552\n",
            "[19]\tvalidation_0-auc:0.838148\n",
            "[20]\tvalidation_0-auc:0.838267\n",
            "[21]\tvalidation_0-auc:0.840423\n",
            "[22]\tvalidation_0-auc:0.840423\n",
            "[23]\tvalidation_0-auc:0.839478\n",
            "[24]\tvalidation_0-auc:0.839478\n",
            "[25]\tvalidation_0-auc:0.840142\n",
            "[26]\tvalidation_0-auc:0.840142\n",
            "[27]\tvalidation_0-auc:0.839552\n",
            "[28]\tvalidation_0-auc:0.839552\n",
            "[29]\tvalidation_0-auc:0.839581\n",
            "[30]\tvalidation_0-auc:0.839581\n",
            "[31]\tvalidation_0-auc:0.839581\n",
            "[32]\tvalidation_0-auc:0.839581\n",
            "[33]\tvalidation_0-auc:0.839699\n",
            "[34]\tvalidation_0-auc:0.839699\n",
            "[35]\tvalidation_0-auc:0.839699\n",
            "[36]\tvalidation_0-auc:0.842181\n",
            "[37]\tvalidation_0-auc:0.841339\n",
            "[38]\tvalidation_0-auc:0.841339\n",
            "[39]\tvalidation_0-auc:0.8419\n",
            "[40]\tvalidation_0-auc:0.8419\n",
            "[41]\tvalidation_0-auc:0.8419\n",
            "[42]\tvalidation_0-auc:0.839005\n",
            "[43]\tvalidation_0-auc:0.839005\n",
            "[44]\tvalidation_0-auc:0.839094\n",
            "[45]\tvalidation_0-auc:0.839655\n",
            "[46]\tvalidation_0-auc:0.839655\n",
            "[47]\tvalidation_0-auc:0.839301\n",
            "[48]\tvalidation_0-auc:0.84128\n",
            "[49]\tvalidation_0-auc:0.843067\n",
            "[50]\tvalidation_0-auc:0.843067\n",
            "[51]\tvalidation_0-auc:0.840999\n",
            "[52]\tvalidation_0-auc:0.840999\n",
            "[53]\tvalidation_0-auc:0.84097\n",
            "[54]\tvalidation_0-auc:0.842003\n",
            "[55]\tvalidation_0-auc:0.842033\n",
            "[56]\tvalidation_0-auc:0.842033\n",
            "[57]\tvalidation_0-auc:0.842033\n",
            "[58]\tvalidation_0-auc:0.842033\n",
            "[59]\tvalidation_0-auc:0.842033\n",
            "[60]\tvalidation_0-auc:0.841856\n",
            "[61]\tvalidation_0-auc:0.841856\n",
            "[62]\tvalidation_0-auc:0.841915\n",
            "[63]\tvalidation_0-auc:0.841915\n",
            "[64]\tvalidation_0-auc:0.842151\n",
            "[65]\tvalidation_0-auc:0.842151\n",
            "[66]\tvalidation_0-auc:0.842151\n",
            "[67]\tvalidation_0-auc:0.841767\n",
            "[68]\tvalidation_0-auc:0.841501\n",
            "[69]\tvalidation_0-auc:0.841398\n",
            "[70]\tvalidation_0-auc:0.841634\n",
            "[71]\tvalidation_0-auc:0.842284\n",
            "[72]\tvalidation_0-auc:0.842284\n",
            "[73]\tvalidation_0-auc:0.842284\n",
            "[74]\tvalidation_0-auc:0.842934\n",
            "[75]\tvalidation_0-auc:0.842771\n",
            "[76]\tvalidation_0-auc:0.842417\n",
            "[77]\tvalidation_0-auc:0.842476\n",
            "[78]\tvalidation_0-auc:0.842476\n",
            "[79]\tvalidation_0-auc:0.845327\n",
            "[80]\tvalidation_0-auc:0.844544\n",
            "[81]\tvalidation_0-auc:0.844367\n",
            "[82]\tvalidation_0-auc:0.843008\n",
            "[83]\tvalidation_0-auc:0.843008\n",
            "[84]\tvalidation_0-auc:0.842978\n",
            "[85]\tvalidation_0-auc:0.842831\n",
            "[86]\tvalidation_0-auc:0.842949\n",
            "[87]\tvalidation_0-auc:0.842949\n",
            "[88]\tvalidation_0-auc:0.843362\n",
            "[89]\tvalidation_0-auc:0.84351\n",
            "[90]\tvalidation_0-auc:0.84351\n",
            "[91]\tvalidation_0-auc:0.84351\n",
            "[92]\tvalidation_0-auc:0.842712\n",
            "[93]\tvalidation_0-auc:0.842683\n",
            "[94]\tvalidation_0-auc:0.842683\n",
            "[95]\tvalidation_0-auc:0.842506\n",
            "[96]\tvalidation_0-auc:0.843185\n",
            "[97]\tvalidation_0-auc:0.842771\n",
            "[98]\tvalidation_0-auc:0.844308\n",
            "[99]\tvalidation_0-auc:0.844514\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.7510204081632653 |  0.4088050314465409 | 0.6989247311827957 |  0.5158730158730159 |\n",
            "|      GRU 0.15     | 0.5897959183673469 |  0.2923076923076923 | 0.8172043010752689 |  0.4305949008498584 |\n",
            "|    XGBoost 0.15   | 0.6714285714285714 |  0.3411214953271028 | 0.7849462365591398 | 0.47557003257328984 |\n",
            "|    Logreg 0.15    | 0.5551020408163265 | 0.27598566308243727 | 0.8279569892473119 | 0.41397849462365593 |\n",
            "|      SVM 0.15     | 0.6714285714285714 |  0.3380952380952381 | 0.7634408602150538 |  0.4686468646864687 |\n",
            "|   LSTM beta 0.15  | 0.7089715536105032 |  0.3850574712643678 | 0.7204301075268817 |   0.50187265917603  |\n",
            "|   GRU beta 0.15   | 0.7768052516411379 | 0.46938775510204084 | 0.7419354838709677 |  0.5750000000000001 |\n",
            "| XGBoost beta 0.15 | 0.8096280087527352 |  0.5245901639344263 | 0.6881720430107527 |  0.5953488372093024 |\n",
            "|  logreg beta 0.15 | 0.6849015317286652 | 0.37681159420289856 | 0.8387096774193549 |         0.52        |\n",
            "|   svm beta 0.15   | 0.7986870897155361 |  0.5035971223021583 | 0.7526881720430108 |  0.603448275862069  |\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA2z1pK9XF5M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f7bfb328-87e6-44c6-ddc5-f5fe79038dd2"
      },
      "source": [
        "Result_cross.to_csv('MRO_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.219239</td>\n",
              "      <td>0.277551</td>\n",
              "      <td>0.356364</td>\n",
              "      <td>0.951456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.325581</td>\n",
              "      <td>0.606122</td>\n",
              "      <td>0.465374</td>\n",
              "      <td>0.815534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.408867</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.542484</td>\n",
              "      <td>0.805825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.334646</td>\n",
              "      <td>0.618367</td>\n",
              "      <td>0.476190</td>\n",
              "      <td>0.825243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.381395</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.515723</td>\n",
              "      <td>0.796117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.490909</td>\n",
              "      <td>0.770241</td>\n",
              "      <td>0.507042</td>\n",
              "      <td>0.524272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.343511</td>\n",
              "      <td>0.595186</td>\n",
              "      <td>0.493151</td>\n",
              "      <td>0.873786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.712121</td>\n",
              "      <td>0.835886</td>\n",
              "      <td>0.556213</td>\n",
              "      <td>0.456311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.397196</td>\n",
              "      <td>0.678337</td>\n",
              "      <td>0.536278</td>\n",
              "      <td>0.825243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.579365</td>\n",
              "      <td>0.818381</td>\n",
              "      <td>0.637555</td>\n",
              "      <td>0.708738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.408805</td>\n",
              "      <td>0.751020</td>\n",
              "      <td>0.515873</td>\n",
              "      <td>0.698925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.292308</td>\n",
              "      <td>0.589796</td>\n",
              "      <td>0.430595</td>\n",
              "      <td>0.817204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.341121</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.475570</td>\n",
              "      <td>0.784946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.275986</td>\n",
              "      <td>0.555102</td>\n",
              "      <td>0.413978</td>\n",
              "      <td>0.827957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.338095</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.468647</td>\n",
              "      <td>0.763441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.385057</td>\n",
              "      <td>0.708972</td>\n",
              "      <td>0.501873</td>\n",
              "      <td>0.720430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.469388</td>\n",
              "      <td>0.776805</td>\n",
              "      <td>0.575000</td>\n",
              "      <td>0.741935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.524590</td>\n",
              "      <td>0.809628</td>\n",
              "      <td>0.595349</td>\n",
              "      <td>0.688172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.376812</td>\n",
              "      <td>0.684902</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.838710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.503597</td>\n",
              "      <td>0.798687</td>\n",
              "      <td>0.603448</td>\n",
              "      <td>0.752688</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  MRO  0.219239  0.277551  0.356364  0.951456\n",
              "1            GRU 0.1  MRO  0.325581  0.606122  0.465374  0.815534\n",
              "2        XGBoost 0.1  MRO  0.408867  0.714286  0.542484  0.805825\n",
              "3         Logreg 0.1  MRO  0.334646  0.618367  0.476190  0.825243\n",
              "4            SVM 0.1  MRO  0.381395  0.685714  0.515723  0.796117\n",
              "5      LSTM beta 0.1  MRO  0.490909  0.770241  0.507042  0.524272\n",
              "6       GRU beta 0.1  MRO  0.343511  0.595186  0.493151  0.873786\n",
              "7   XGBoost beta 0.1  MRO  0.712121  0.835886  0.556213  0.456311\n",
              "8    logreg beta 0.1  MRO  0.397196  0.678337  0.536278  0.825243\n",
              "9       svm beta 0.1  MRO  0.579365  0.818381  0.637555  0.708738\n",
              "0          LSTM 0.15  MRO  0.408805  0.751020  0.515873  0.698925\n",
              "1           GRU 0.15  MRO  0.292308  0.589796  0.430595  0.817204\n",
              "2       XGBoost 0.15  MRO  0.341121  0.671429  0.475570  0.784946\n",
              "3        Logreg 0.15  MRO  0.275986  0.555102  0.413978  0.827957\n",
              "4           SVM 0.15  MRO  0.338095  0.671429  0.468647  0.763441\n",
              "5     LSTM beta 0.15  MRO  0.385057  0.708972  0.501873  0.720430\n",
              "6      GRU beta 0.15  MRO  0.469388  0.776805  0.575000  0.741935\n",
              "7  XGBoost beta 0.15  MRO  0.524590  0.809628  0.595349  0.688172\n",
              "8   logreg beta 0.15  MRO  0.376812  0.684902  0.520000  0.838710\n",
              "9      svm beta 0.15  MRO  0.503597  0.798687  0.603448  0.752688"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fwkamDvXF5M"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_logreg_beta.csv')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g3klK47XF5M"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6He4d8kXF5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5badd0a-bb6c-4b96-f730-c65c9a2e0e4c"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"MRO\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6948 - accuracy: 0.5336 - val_loss: 0.7395 - val_accuracy: 0.2102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6674 - accuracy: 0.5685 - val_loss: 0.6596 - val_accuracy: 0.6837\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5986 - accuracy: 0.6980 - val_loss: 0.6967 - val_accuracy: 0.5510\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5491 - accuracy: 0.7342 - val_loss: 0.6012 - val_accuracy: 0.7327\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5178 - accuracy: 0.7430 - val_loss: 0.6752 - val_accuracy: 0.6245\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6302 - accuracy: 0.6403 - val_loss: 0.6930 - val_accuracy: 0.5571\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5483 - accuracy: 0.7208 - val_loss: 0.7283 - val_accuracy: 0.4857\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5151 - accuracy: 0.7450 - val_loss: 0.6022 - val_accuracy: 0.7061\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5064 - accuracy: 0.7430 - val_loss: 0.6170 - val_accuracy: 0.6714\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4886 - accuracy: 0.7631 - val_loss: 0.7076 - val_accuracy: 0.5694\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.764996\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.771995\n",
            "[2]\tvalidation_0-auc:0.780625\n",
            "[3]\tvalidation_0-auc:0.782168\n",
            "[4]\tvalidation_0-auc:0.782795\n",
            "[5]\tvalidation_0-auc:0.783222\n",
            "[6]\tvalidation_0-auc:0.782745\n",
            "[7]\tvalidation_0-auc:0.784125\n",
            "[8]\tvalidation_0-auc:0.784852\n",
            "[9]\tvalidation_0-auc:0.785542\n",
            "[10]\tvalidation_0-auc:0.786872\n",
            "[11]\tvalidation_0-auc:0.786859\n",
            "[12]\tvalidation_0-auc:0.787311\n",
            "[13]\tvalidation_0-auc:0.78839\n",
            "[14]\tvalidation_0-auc:0.789255\n",
            "[15]\tvalidation_0-auc:0.789356\n",
            "[16]\tvalidation_0-auc:0.788352\n",
            "[17]\tvalidation_0-auc:0.788778\n",
            "[18]\tvalidation_0-auc:0.789305\n",
            "[19]\tvalidation_0-auc:0.788289\n",
            "[20]\tvalidation_0-auc:0.788778\n",
            "[21]\tvalidation_0-auc:0.788089\n",
            "[22]\tvalidation_0-auc:0.788465\n",
            "[23]\tvalidation_0-auc:0.788528\n",
            "[24]\tvalidation_0-auc:0.788866\n",
            "[25]\tvalidation_0-auc:0.789017\n",
            "[26]\tvalidation_0-auc:0.789631\n",
            "[27]\tvalidation_0-auc:0.789669\n",
            "[28]\tvalidation_0-auc:0.789669\n",
            "[29]\tvalidation_0-auc:0.789519\n",
            "[30]\tvalidation_0-auc:0.78928\n",
            "[31]\tvalidation_0-auc:0.78928\n",
            "[32]\tvalidation_0-auc:0.789631\n",
            "[33]\tvalidation_0-auc:0.789782\n",
            "[34]\tvalidation_0-auc:0.790409\n",
            "[35]\tvalidation_0-auc:0.790409\n",
            "[36]\tvalidation_0-auc:0.790434\n",
            "[37]\tvalidation_0-auc:0.790434\n",
            "[38]\tvalidation_0-auc:0.790384\n",
            "[39]\tvalidation_0-auc:0.790384\n",
            "[40]\tvalidation_0-auc:0.790384\n",
            "[41]\tvalidation_0-auc:0.790183\n",
            "[42]\tvalidation_0-auc:0.789318\n",
            "[43]\tvalidation_0-auc:0.788904\n",
            "[44]\tvalidation_0-auc:0.788904\n",
            "[45]\tvalidation_0-auc:0.788904\n",
            "[46]\tvalidation_0-auc:0.788904\n",
            "[47]\tvalidation_0-auc:0.788778\n",
            "[48]\tvalidation_0-auc:0.788979\n",
            "[49]\tvalidation_0-auc:0.788979\n",
            "[50]\tvalidation_0-auc:0.789318\n",
            "[51]\tvalidation_0-auc:0.789318\n",
            "[52]\tvalidation_0-auc:0.789318\n",
            "[53]\tvalidation_0-auc:0.789293\n",
            "[54]\tvalidation_0-auc:0.789293\n",
            "[55]\tvalidation_0-auc:0.790974\n",
            "[56]\tvalidation_0-auc:0.791049\n",
            "[57]\tvalidation_0-auc:0.791049\n",
            "[58]\tvalidation_0-auc:0.791224\n",
            "[59]\tvalidation_0-auc:0.791224\n",
            "[60]\tvalidation_0-auc:0.790836\n",
            "[61]\tvalidation_0-auc:0.791613\n",
            "[62]\tvalidation_0-auc:0.791714\n",
            "[63]\tvalidation_0-auc:0.79199\n",
            "[64]\tvalidation_0-auc:0.791613\n",
            "[65]\tvalidation_0-auc:0.791588\n",
            "[66]\tvalidation_0-auc:0.791588\n",
            "[67]\tvalidation_0-auc:0.791011\n",
            "[68]\tvalidation_0-auc:0.790359\n",
            "[69]\tvalidation_0-auc:0.790259\n",
            "[70]\tvalidation_0-auc:0.790259\n",
            "[71]\tvalidation_0-auc:0.790058\n",
            "[72]\tvalidation_0-auc:0.790058\n",
            "[73]\tvalidation_0-auc:0.789983\n",
            "[74]\tvalidation_0-auc:0.790585\n",
            "[75]\tvalidation_0-auc:0.790585\n",
            "[76]\tvalidation_0-auc:0.788766\n",
            "[77]\tvalidation_0-auc:0.789393\n",
            "[78]\tvalidation_0-auc:0.789393\n",
            "[79]\tvalidation_0-auc:0.789318\n",
            "[80]\tvalidation_0-auc:0.78775\n",
            "[81]\tvalidation_0-auc:0.788139\n",
            "[82]\tvalidation_0-auc:0.787863\n",
            "[83]\tvalidation_0-auc:0.787712\n",
            "[84]\tvalidation_0-auc:0.787712\n",
            "[85]\tvalidation_0-auc:0.787712\n",
            "[86]\tvalidation_0-auc:0.786897\n",
            "[87]\tvalidation_0-auc:0.786897\n",
            "[88]\tvalidation_0-auc:0.7877\n",
            "[89]\tvalidation_0-auc:0.7877\n",
            "[90]\tvalidation_0-auc:0.787675\n",
            "[91]\tvalidation_0-auc:0.785919\n",
            "[92]\tvalidation_0-auc:0.785893\n",
            "[93]\tvalidation_0-auc:0.785567\n",
            "[94]\tvalidation_0-auc:0.785756\n",
            "[95]\tvalidation_0-auc:0.784075\n",
            "[96]\tvalidation_0-auc:0.783774\n",
            "[97]\tvalidation_0-auc:0.785128\n",
            "[98]\tvalidation_0-auc:0.78568\n",
            "[99]\tvalidation_0-auc:0.785078\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6808 - accuracy: 0.5587 - val_loss: 0.6121 - val_accuracy: 0.7746\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6651 - accuracy: 0.5889 - val_loss: 0.6746 - val_accuracy: 0.6411\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6055 - accuracy: 0.6905 - val_loss: 0.6078 - val_accuracy: 0.8074\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5606 - accuracy: 0.7042 - val_loss: 0.5055 - val_accuracy: 0.8206\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5524 - accuracy: 0.7316 - val_loss: 0.6963 - val_accuracy: 0.5339\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6652 - accuracy: 0.5951 - val_loss: 0.7251 - val_accuracy: 0.3370\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5762 - accuracy: 0.7152 - val_loss: 0.5495 - val_accuracy: 0.8381\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5489 - accuracy: 0.7261 - val_loss: 0.5647 - val_accuracy: 0.7921\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5524 - accuracy: 0.7227 - val_loss: 0.5412 - val_accuracy: 0.7943\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5379 - accuracy: 0.7412 - val_loss: 0.5884 - val_accuracy: 0.7352\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.706695\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.706695\n",
            "[2]\tvalidation_0-auc:0.764001\n",
            "[3]\tvalidation_0-auc:0.760381\n",
            "[4]\tvalidation_0-auc:0.811626\n",
            "[5]\tvalidation_0-auc:0.81552\n",
            "[6]\tvalidation_0-auc:0.821019\n",
            "[7]\tvalidation_0-auc:0.826395\n",
            "[8]\tvalidation_0-auc:0.825435\n",
            "[9]\tvalidation_0-auc:0.830207\n",
            "[10]\tvalidation_0-auc:0.83997\n",
            "[11]\tvalidation_0-auc:0.827533\n",
            "[12]\tvalidation_0-auc:0.828246\n",
            "[13]\tvalidation_0-auc:0.827067\n",
            "[14]\tvalidation_0-auc:0.829247\n",
            "[15]\tvalidation_0-auc:0.829247\n",
            "[16]\tvalidation_0-auc:0.827738\n",
            "[17]\tvalidation_0-auc:0.824173\n",
            "[18]\tvalidation_0-auc:0.82937\n",
            "[19]\tvalidation_0-auc:0.829233\n",
            "[20]\tvalidation_0-auc:0.828849\n",
            "[21]\tvalidation_0-auc:0.826573\n",
            "[22]\tvalidation_0-auc:0.825311\n",
            "[23]\tvalidation_0-auc:0.825311\n",
            "[24]\tvalidation_0-auc:0.824639\n",
            "[25]\tvalidation_0-auc:0.824173\n",
            "[26]\tvalidation_0-auc:0.826477\n",
            "[27]\tvalidation_0-auc:0.827492\n",
            "[28]\tvalidation_0-auc:0.826614\n",
            "[29]\tvalidation_0-auc:0.827876\n",
            "[30]\tvalidation_0-auc:0.831441\n",
            "[31]\tvalidation_0-auc:0.831441\n",
            "[32]\tvalidation_0-auc:0.831414\n",
            "[33]\tvalidation_0-auc:0.831605\n",
            "[34]\tvalidation_0-auc:0.827615\n",
            "[35]\tvalidation_0-auc:0.827615\n",
            "[36]\tvalidation_0-auc:0.827615\n",
            "[37]\tvalidation_0-auc:0.828026\n",
            "[38]\tvalidation_0-auc:0.824845\n",
            "[39]\tvalidation_0-auc:0.822555\n",
            "[40]\tvalidation_0-auc:0.822089\n",
            "[41]\tvalidation_0-auc:0.823652\n",
            "[42]\tvalidation_0-auc:0.823652\n",
            "[43]\tvalidation_0-auc:0.823364\n",
            "[44]\tvalidation_0-auc:0.822925\n",
            "[45]\tvalidation_0-auc:0.821129\n",
            "[46]\tvalidation_0-auc:0.818894\n",
            "[47]\tvalidation_0-auc:0.818894\n",
            "[48]\tvalidation_0-auc:0.819936\n",
            "[49]\tvalidation_0-auc:0.82165\n",
            "[50]\tvalidation_0-auc:0.82165\n",
            "[51]\tvalidation_0-auc:0.82165\n",
            "[52]\tvalidation_0-auc:0.821239\n",
            "[53]\tvalidation_0-auc:0.821239\n",
            "[54]\tvalidation_0-auc:0.822404\n",
            "[55]\tvalidation_0-auc:0.822404\n",
            "[56]\tvalidation_0-auc:0.821581\n",
            "[57]\tvalidation_0-auc:0.816878\n",
            "[58]\tvalidation_0-auc:0.816713\n",
            "[59]\tvalidation_0-auc:0.816713\n",
            "[60]\tvalidation_0-auc:0.816713\n",
            "Stopping. Best iteration:\n",
            "[10]\tvalidation_0-auc:0.83997\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.6244897959183674 |  0.3424124513618677 | 0.8543689320388349 |  0.4888888888888889 |\n",
            "|     GRU 0.1      | 0.5693877551020409 | 0.30985915492957744 | 0.8543689320388349 |  0.4547803617571059 |\n",
            "|   XGBoost 0.1    | 0.7142857142857143 |  0.4088669950738916 | 0.8058252427184466 |  0.542483660130719  |\n",
            "|    Logreg 0.1    | 0.6183673469387755 |  0.3346456692913386 | 0.8252427184466019 |  0.4761904761904762 |\n",
            "|     SVM 0.1      | 0.6857142857142857 |  0.3813953488372093 | 0.7961165048543689 |  0.5157232704402516 |\n",
            "|  LSTM beta 0.1   | 0.5339168490153173 |  0.3116438356164384 | 0.883495145631068  | 0.46075949367088614 |\n",
            "|   GRU beta 0.1   | 0.7352297592997812 |  0.4476744186046512 | 0.7475728155339806 |         0.56        |\n",
            "| XGBoost beta 0.1 | 0.8358862144420132 |  0.7121212121212122 | 0.4563106796116505 |  0.5562130177514794 |\n",
            "| logreg beta 0.1  | 0.6783369803063457 |  0.397196261682243  | 0.8252427184466019 |  0.5362776025236593 |\n",
            "|   svm beta 0.1   | 0.8183807439824945 |  0.5793650793650794 | 0.7087378640776699 |  0.6375545851528385 |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6883 - accuracy: 0.5691 - val_loss: 1.0549 - val_accuracy: 0.1898\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6761 - accuracy: 0.5960 - val_loss: 0.7541 - val_accuracy: 0.1898\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6437 - accuracy: 0.6168 - val_loss: 0.6012 - val_accuracy: 0.7490\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6091 - accuracy: 0.6799 - val_loss: 0.6415 - val_accuracy: 0.6694\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5978 - accuracy: 0.7007 - val_loss: 0.6806 - val_accuracy: 0.6592\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6529 - accuracy: 0.5966 - val_loss: 0.7422 - val_accuracy: 0.4776\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5683 - accuracy: 0.7242 - val_loss: 0.7000 - val_accuracy: 0.5388\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5628 - accuracy: 0.7060 - val_loss: 0.6405 - val_accuracy: 0.6592\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5438 - accuracy: 0.7275 - val_loss: 0.6650 - val_accuracy: 0.6102\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5330 - accuracy: 0.7430 - val_loss: 0.6427 - val_accuracy: 0.6571\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.755058\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.755234\n",
            "[2]\tvalidation_0-auc:0.758349\n",
            "[3]\tvalidation_0-auc:0.754449\n",
            "[4]\tvalidation_0-auc:0.754124\n",
            "[5]\tvalidation_0-auc:0.75732\n",
            "[6]\tvalidation_0-auc:0.758471\n",
            "[7]\tvalidation_0-auc:0.75927\n",
            "[8]\tvalidation_0-auc:0.759405\n",
            "[9]\tvalidation_0-auc:0.759405\n",
            "[10]\tvalidation_0-auc:0.758457\n",
            "[11]\tvalidation_0-auc:0.758376\n",
            "[12]\tvalidation_0-auc:0.758349\n",
            "[13]\tvalidation_0-auc:0.758552\n",
            "[14]\tvalidation_0-auc:0.758349\n",
            "[15]\tvalidation_0-auc:0.758295\n",
            "[16]\tvalidation_0-auc:0.758417\n",
            "[17]\tvalidation_0-auc:0.756968\n",
            "[18]\tvalidation_0-auc:0.75713\n",
            "[19]\tvalidation_0-auc:0.757103\n",
            "[20]\tvalidation_0-auc:0.757442\n",
            "[21]\tvalidation_0-auc:0.757618\n",
            "[22]\tvalidation_0-auc:0.757699\n",
            "[23]\tvalidation_0-auc:0.758159\n",
            "[24]\tvalidation_0-auc:0.757577\n",
            "[25]\tvalidation_0-auc:0.758281\n",
            "[26]\tvalidation_0-auc:0.758119\n",
            "[27]\tvalidation_0-auc:0.756805\n",
            "[28]\tvalidation_0-auc:0.756805\n",
            "[29]\tvalidation_0-auc:0.757076\n",
            "[30]\tvalidation_0-auc:0.757238\n",
            "[31]\tvalidation_0-auc:0.756588\n",
            "[32]\tvalidation_0-auc:0.757022\n",
            "[33]\tvalidation_0-auc:0.757293\n",
            "[34]\tvalidation_0-auc:0.757293\n",
            "[35]\tvalidation_0-auc:0.757536\n",
            "[36]\tvalidation_0-auc:0.757401\n",
            "[37]\tvalidation_0-auc:0.756426\n",
            "[38]\tvalidation_0-auc:0.75667\n",
            "[39]\tvalidation_0-auc:0.756683\n",
            "[40]\tvalidation_0-auc:0.756602\n",
            "[41]\tvalidation_0-auc:0.756629\n",
            "[42]\tvalidation_0-auc:0.756629\n",
            "[43]\tvalidation_0-auc:0.756737\n",
            "[44]\tvalidation_0-auc:0.756737\n",
            "[45]\tvalidation_0-auc:0.756764\n",
            "[46]\tvalidation_0-auc:0.756764\n",
            "[47]\tvalidation_0-auc:0.757144\n",
            "[48]\tvalidation_0-auc:0.756683\n",
            "[49]\tvalidation_0-auc:0.755505\n",
            "[50]\tvalidation_0-auc:0.755911\n",
            "[51]\tvalidation_0-auc:0.755911\n",
            "[52]\tvalidation_0-auc:0.755803\n",
            "[53]\tvalidation_0-auc:0.755803\n",
            "[54]\tvalidation_0-auc:0.756439\n",
            "[55]\tvalidation_0-auc:0.757293\n",
            "[56]\tvalidation_0-auc:0.757374\n",
            "[57]\tvalidation_0-auc:0.758335\n",
            "[58]\tvalidation_0-auc:0.758335\n",
            "Stopping. Best iteration:\n",
            "[8]\tvalidation_0-auc:0.759405\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6882 - accuracy: 0.5312 - val_loss: 0.7286 - val_accuracy: 0.1969\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6436 - accuracy: 0.6493 - val_loss: 0.7519 - val_accuracy: 0.2035\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6121 - accuracy: 0.6637 - val_loss: 0.7090 - val_accuracy: 0.4726\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5880 - accuracy: 0.6911 - val_loss: 0.5531 - val_accuracy: 0.8468\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5557 - accuracy: 0.7207 - val_loss: 0.6698 - val_accuracy: 0.6171\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6325 - accuracy: 0.6232 - val_loss: 0.8977 - val_accuracy: 0.2144\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5588 - accuracy: 0.7021 - val_loss: 0.6614 - val_accuracy: 0.6193\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5342 - accuracy: 0.7358 - val_loss: 0.5573 - val_accuracy: 0.7921\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5379 - accuracy: 0.7440 - val_loss: 0.5213 - val_accuracy: 0.8293\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5287 - accuracy: 0.7529 - val_loss: 0.5268 - val_accuracy: 0.8184\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.721951\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.717964\n",
            "[2]\tvalidation_0-auc:0.76186\n",
            "[3]\tvalidation_0-auc:0.770176\n",
            "[4]\tvalidation_0-auc:0.776719\n",
            "[5]\tvalidation_0-auc:0.831118\n",
            "[6]\tvalidation_0-auc:0.830808\n",
            "[7]\tvalidation_0-auc:0.836095\n",
            "[8]\tvalidation_0-auc:0.840896\n",
            "[9]\tvalidation_0-auc:0.839773\n",
            "[10]\tvalidation_0-auc:0.839921\n",
            "[11]\tvalidation_0-auc:0.840334\n",
            "[12]\tvalidation_0-auc:0.840334\n",
            "[13]\tvalidation_0-auc:0.838252\n",
            "[14]\tvalidation_0-auc:0.8371\n",
            "[15]\tvalidation_0-auc:0.835298\n",
            "[16]\tvalidation_0-auc:0.8345\n",
            "[17]\tvalidation_0-auc:0.836657\n",
            "[18]\tvalidation_0-auc:0.839552\n",
            "[19]\tvalidation_0-auc:0.838148\n",
            "[20]\tvalidation_0-auc:0.838267\n",
            "[21]\tvalidation_0-auc:0.840423\n",
            "[22]\tvalidation_0-auc:0.840423\n",
            "[23]\tvalidation_0-auc:0.839478\n",
            "[24]\tvalidation_0-auc:0.839478\n",
            "[25]\tvalidation_0-auc:0.840142\n",
            "[26]\tvalidation_0-auc:0.840142\n",
            "[27]\tvalidation_0-auc:0.839552\n",
            "[28]\tvalidation_0-auc:0.839552\n",
            "[29]\tvalidation_0-auc:0.839581\n",
            "[30]\tvalidation_0-auc:0.839581\n",
            "[31]\tvalidation_0-auc:0.839581\n",
            "[32]\tvalidation_0-auc:0.839581\n",
            "[33]\tvalidation_0-auc:0.839699\n",
            "[34]\tvalidation_0-auc:0.839699\n",
            "[35]\tvalidation_0-auc:0.839699\n",
            "[36]\tvalidation_0-auc:0.842181\n",
            "[37]\tvalidation_0-auc:0.841339\n",
            "[38]\tvalidation_0-auc:0.841339\n",
            "[39]\tvalidation_0-auc:0.8419\n",
            "[40]\tvalidation_0-auc:0.8419\n",
            "[41]\tvalidation_0-auc:0.8419\n",
            "[42]\tvalidation_0-auc:0.839005\n",
            "[43]\tvalidation_0-auc:0.839005\n",
            "[44]\tvalidation_0-auc:0.839094\n",
            "[45]\tvalidation_0-auc:0.839655\n",
            "[46]\tvalidation_0-auc:0.839655\n",
            "[47]\tvalidation_0-auc:0.839301\n",
            "[48]\tvalidation_0-auc:0.84128\n",
            "[49]\tvalidation_0-auc:0.843067\n",
            "[50]\tvalidation_0-auc:0.843067\n",
            "[51]\tvalidation_0-auc:0.840999\n",
            "[52]\tvalidation_0-auc:0.840999\n",
            "[53]\tvalidation_0-auc:0.84097\n",
            "[54]\tvalidation_0-auc:0.842003\n",
            "[55]\tvalidation_0-auc:0.842033\n",
            "[56]\tvalidation_0-auc:0.842033\n",
            "[57]\tvalidation_0-auc:0.842033\n",
            "[58]\tvalidation_0-auc:0.842033\n",
            "[59]\tvalidation_0-auc:0.842033\n",
            "[60]\tvalidation_0-auc:0.841856\n",
            "[61]\tvalidation_0-auc:0.841856\n",
            "[62]\tvalidation_0-auc:0.841915\n",
            "[63]\tvalidation_0-auc:0.841915\n",
            "[64]\tvalidation_0-auc:0.842151\n",
            "[65]\tvalidation_0-auc:0.842151\n",
            "[66]\tvalidation_0-auc:0.842151\n",
            "[67]\tvalidation_0-auc:0.841767\n",
            "[68]\tvalidation_0-auc:0.841501\n",
            "[69]\tvalidation_0-auc:0.841398\n",
            "[70]\tvalidation_0-auc:0.841634\n",
            "[71]\tvalidation_0-auc:0.842284\n",
            "[72]\tvalidation_0-auc:0.842284\n",
            "[73]\tvalidation_0-auc:0.842284\n",
            "[74]\tvalidation_0-auc:0.842934\n",
            "[75]\tvalidation_0-auc:0.842771\n",
            "[76]\tvalidation_0-auc:0.842417\n",
            "[77]\tvalidation_0-auc:0.842476\n",
            "[78]\tvalidation_0-auc:0.842476\n",
            "[79]\tvalidation_0-auc:0.845327\n",
            "[80]\tvalidation_0-auc:0.844544\n",
            "[81]\tvalidation_0-auc:0.844367\n",
            "[82]\tvalidation_0-auc:0.843008\n",
            "[83]\tvalidation_0-auc:0.843008\n",
            "[84]\tvalidation_0-auc:0.842978\n",
            "[85]\tvalidation_0-auc:0.842831\n",
            "[86]\tvalidation_0-auc:0.842949\n",
            "[87]\tvalidation_0-auc:0.842949\n",
            "[88]\tvalidation_0-auc:0.843362\n",
            "[89]\tvalidation_0-auc:0.84351\n",
            "[90]\tvalidation_0-auc:0.84351\n",
            "[91]\tvalidation_0-auc:0.84351\n",
            "[92]\tvalidation_0-auc:0.842712\n",
            "[93]\tvalidation_0-auc:0.842683\n",
            "[94]\tvalidation_0-auc:0.842683\n",
            "[95]\tvalidation_0-auc:0.842506\n",
            "[96]\tvalidation_0-auc:0.843185\n",
            "[97]\tvalidation_0-auc:0.842771\n",
            "[98]\tvalidation_0-auc:0.844308\n",
            "[99]\tvalidation_0-auc:0.844514\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.6591836734693878 |  0.3287037037037037 | 0.7634408602150538 |  0.459546925566343  |\n",
            "|      GRU 0.15     | 0.6571428571428571 |  0.3287671232876712 | 0.7741935483870968 |  0.4615384615384615 |\n",
            "|    XGBoost 0.15   | 0.6714285714285714 |  0.3411214953271028 | 0.7849462365591398 | 0.47557003257328984 |\n",
            "|    Logreg 0.15    | 0.5551020408163265 | 0.27598566308243727 | 0.8279569892473119 | 0.41397849462365593 |\n",
            "|      SVM 0.15     | 0.6714285714285714 |  0.3380952380952381 | 0.7634408602150538 |  0.4686468646864687 |\n",
            "|   LSTM beta 0.15  | 0.6170678336980306 |  0.3333333333333333 | 0.8817204301075269 |  0.4837758112094395 |\n",
            "|   GRU beta 0.15   | 0.8183807439824945 |  0.5396825396825397 | 0.7311827956989247 |  0.6210045662100456 |\n",
            "| XGBoost beta 0.15 | 0.8096280087527352 |  0.5245901639344263 | 0.6881720430107527 |  0.5953488372093024 |\n",
            "|  logreg beta 0.15 | 0.6849015317286652 | 0.37681159420289856 | 0.8387096774193549 |         0.52        |\n",
            "|   svm beta 0.15   | 0.7986870897155361 |  0.5035971223021583 | 0.7526881720430108 |  0.603448275862069  |\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-20whT5XF5M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "12d87a20-2f3c-4f59-d6c6-32aaaf71bf00"
      },
      "source": [
        "Result_purging.to_csv('MRO_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.342412</td>\n",
              "      <td>0.624490</td>\n",
              "      <td>0.488889</td>\n",
              "      <td>0.854369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.309859</td>\n",
              "      <td>0.569388</td>\n",
              "      <td>0.454780</td>\n",
              "      <td>0.854369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.408867</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.542484</td>\n",
              "      <td>0.805825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.334646</td>\n",
              "      <td>0.618367</td>\n",
              "      <td>0.476190</td>\n",
              "      <td>0.825243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.381395</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.515723</td>\n",
              "      <td>0.796117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.311644</td>\n",
              "      <td>0.533917</td>\n",
              "      <td>0.460759</td>\n",
              "      <td>0.883495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.447674</td>\n",
              "      <td>0.735230</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.747573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.712121</td>\n",
              "      <td>0.835886</td>\n",
              "      <td>0.556213</td>\n",
              "      <td>0.456311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.397196</td>\n",
              "      <td>0.678337</td>\n",
              "      <td>0.536278</td>\n",
              "      <td>0.825243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.579365</td>\n",
              "      <td>0.818381</td>\n",
              "      <td>0.637555</td>\n",
              "      <td>0.708738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.328704</td>\n",
              "      <td>0.659184</td>\n",
              "      <td>0.459547</td>\n",
              "      <td>0.763441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.328767</td>\n",
              "      <td>0.657143</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.774194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.341121</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.475570</td>\n",
              "      <td>0.784946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.275986</td>\n",
              "      <td>0.555102</td>\n",
              "      <td>0.413978</td>\n",
              "      <td>0.827957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.338095</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.468647</td>\n",
              "      <td>0.763441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.617068</td>\n",
              "      <td>0.483776</td>\n",
              "      <td>0.881720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.539683</td>\n",
              "      <td>0.818381</td>\n",
              "      <td>0.621005</td>\n",
              "      <td>0.731183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.524590</td>\n",
              "      <td>0.809628</td>\n",
              "      <td>0.595349</td>\n",
              "      <td>0.688172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.376812</td>\n",
              "      <td>0.684902</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.838710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>MRO</td>\n",
              "      <td>0.503597</td>\n",
              "      <td>0.798687</td>\n",
              "      <td>0.603448</td>\n",
              "      <td>0.752688</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  MRO  0.342412  0.624490  0.488889  0.854369\n",
              "1            GRU 0.1  MRO  0.309859  0.569388  0.454780  0.854369\n",
              "2        XGBoost 0.1  MRO  0.408867  0.714286  0.542484  0.805825\n",
              "3         Logreg 0.1  MRO  0.334646  0.618367  0.476190  0.825243\n",
              "4            SVM 0.1  MRO  0.381395  0.685714  0.515723  0.796117\n",
              "5      LSTM beta 0.1  MRO  0.311644  0.533917  0.460759  0.883495\n",
              "6       GRU beta 0.1  MRO  0.447674  0.735230  0.560000  0.747573\n",
              "7   XGBoost beta 0.1  MRO  0.712121  0.835886  0.556213  0.456311\n",
              "8    logreg beta 0.1  MRO  0.397196  0.678337  0.536278  0.825243\n",
              "9       svm beta 0.1  MRO  0.579365  0.818381  0.637555  0.708738\n",
              "0          LSTM 0.15  MRO  0.328704  0.659184  0.459547  0.763441\n",
              "1           GRU 0.15  MRO  0.328767  0.657143  0.461538  0.774194\n",
              "2       XGBoost 0.15  MRO  0.341121  0.671429  0.475570  0.784946\n",
              "3        Logreg 0.15  MRO  0.275986  0.555102  0.413978  0.827957\n",
              "4           SVM 0.15  MRO  0.338095  0.671429  0.468647  0.763441\n",
              "5     LSTM beta 0.15  MRO  0.333333  0.617068  0.483776  0.881720\n",
              "6      GRU beta 0.15  MRO  0.539683  0.818381  0.621005  0.731183\n",
              "7  XGBoost beta 0.15  MRO  0.524590  0.809628  0.595349  0.688172\n",
              "8   logreg beta 0.15  MRO  0.376812  0.684902  0.520000  0.838710\n",
              "9      svm beta 0.15  MRO  0.503597  0.798687  0.603448  0.752688"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnRmoH_8XF5M"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('MRO_logreg_beta_p.csv')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHOt2ASuXF5M"
      },
      "source": [
        ""
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLmeDk6wXhm1"
      },
      "source": [
        "## NKTR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGNI1SJzXhm7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f6eb49b2-10d1-4ad3-e46d-c654f2a6bec9"
      },
      "source": [
        "dfs = pd.read_csv(\"NKTR.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "dfs"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.NKTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>17.99</td>\n",
              "      <td>18.51</td>\n",
              "      <td>17.950</td>\n",
              "      <td>18.39</td>\n",
              "      <td>34775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.NKTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>18.09</td>\n",
              "      <td>18.28</td>\n",
              "      <td>17.900</td>\n",
              "      <td>17.97</td>\n",
              "      <td>28262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.NKTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>18.43</td>\n",
              "      <td>18.52</td>\n",
              "      <td>17.905</td>\n",
              "      <td>17.92</td>\n",
              "      <td>22410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2763</td>\n",
              "      <td>US1.NKTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>18.66</td>\n",
              "      <td>18.90</td>\n",
              "      <td>18.350</td>\n",
              "      <td>18.40</td>\n",
              "      <td>15735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2762</td>\n",
              "      <td>US1.NKTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>18.59</td>\n",
              "      <td>19.32</td>\n",
              "      <td>18.590</td>\n",
              "      <td>18.84</td>\n",
              "      <td>15873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2762</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.NKTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>14.84</td>\n",
              "      <td>15.83</td>\n",
              "      <td>14.840</td>\n",
              "      <td>15.67</td>\n",
              "      <td>497674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2763</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.NKTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>15.15</td>\n",
              "      <td>15.25</td>\n",
              "      <td>14.800</td>\n",
              "      <td>14.81</td>\n",
              "      <td>236730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.NKTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>14.85</td>\n",
              "      <td>15.15</td>\n",
              "      <td>14.800</td>\n",
              "      <td>15.06</td>\n",
              "      <td>392593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.NKTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>14.75</td>\n",
              "      <td>14.95</td>\n",
              "      <td>14.530</td>\n",
              "      <td>14.93</td>\n",
              "      <td>441063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.NKTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>14.68</td>\n",
              "      <td>14.85</td>\n",
              "      <td>14.350</td>\n",
              "      <td>14.53</td>\n",
              "      <td>419270</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2767 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  <TICKER> <PER>    <DATE>  ...  <HIGH>   <LOW>  <CLOSE>   <VOL>\n",
              "0      2766  US1.NKTR     D  20211001  ...   18.51  17.950    18.39   34775\n",
              "1      2765  US1.NKTR     D  20210930  ...   18.28  17.900    17.97   28262\n",
              "2      2764  US1.NKTR     D  20210929  ...   18.52  17.905    17.92   22410\n",
              "3      2763  US1.NKTR     D  20210928  ...   18.90  18.350    18.40   15735\n",
              "4      2762  US1.NKTR     D  20210927  ...   19.32  18.590    18.84   15873\n",
              "...     ...       ...   ...       ...  ...     ...     ...      ...     ...\n",
              "2762      4  US1.NKTR     D  20101008  ...   15.83  14.840    15.67  497674\n",
              "2763      3  US1.NKTR     D  20101007  ...   15.25  14.800    14.81  236730\n",
              "2764      2  US1.NKTR     D  20101006  ...   15.15  14.800    15.06  392593\n",
              "2765      1  US1.NKTR     D  20101005  ...   14.95  14.530    14.93  441063\n",
              "2766      0  US1.NKTR     D  20101004  ...   14.85  14.350    14.53  419270\n",
              "\n",
              "[2767 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL2xFpRpXhm7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4e881896-96b8-4c62-a650-6989fb96f2a3"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"cb3a097f-d875-4c5e-bbbc-d71af23443a8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"cb3a097f-d875-4c5e-bbbc-d71af23443a8\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'cb3a097f-d875-4c5e-bbbc-d71af23443a8',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [16.18, 18.66, 18.96, 18.2, 17.3, 18.44, 18.21, 17.9, 17.56, 18.81, 18.915, 19.92, 20.13, 20.21, 20.1, 20.7, 21.22, 20.11, 19.95, 20.0, 19.93, 17.86, 17.0, 17.37, 16.96, 16.93, 17.59, 17.37, 17.46, 16.91, 17.31, 17.0, 16.98, 17.02, 17.12, 17.88, 18.1, 18.21, 18.73, 18.84, 18.5784, 20.92, 29.57, 28.8, 28.76, 28.77, 31.1, 31.21, 28.47, 29.32, 29.26, 31.11, 29.99, 32.12, 32.08, 32.34, 31.98, 32.07, 31.98, 32.49, 33.21, 33.7, 33.53, 34.78, 34.83, 34.18, 35.15, 36.27, 36.13, 35.84, 35.62, 34.69, 33.29, 34.62, 33.89, 34.84, 35.39, 35.34, 35.17, 33.7, 33.4, 34.05, 33.63, 33.27, 33.13, 33.0, 32.99, 33.64, 32.92, 34.05, 31.319, 32.11, 32.55, 32.1, 33.8, 33.07, 33.48, 33.42, 32.14, 32.0, 31.35, 33.91, 33.73, 32.61, 34.78, 35.46, 32.99, 32.09, 32.69, 32.22, 32.14, 31.63, 32.02, 32.33, 32.27, 31.78, 31.15, 31.61, 30.9964, 31.68, 31.19, 33.38, 33.47, 35.17, 35.75, 35.82, 35.46, 35.99, 36.3, 35.3, 35.29, 33.69, 33.24, 33.63, 32.69, 31.78, 32.34, 32.39, 32.14, 33.49, 32.61, 34.41, 34.93, 35.56, 36.32, 37.64, 36.21, 35.44, 35.12, 35.81, 36.38, 38.4, 38.295, 38.15, 40.52, 41.03, 41.04, 40.99, 40.05, 40.26, 43.05, 42.81, 41.69, 42.62, 42.25, 42.25, 46.37, 44.91, 43.84, 44.36, 43.75, 42.5, 42.21, 42.33, 43.1, 42.49, 42.99, 43.91, 43.22, 43.94, 44.74, 46.2201, 45.42, 44.62, 44.53, 41.51, 41.01, 41.47, 41.18, 40.7, 36.62, 33.92, 31.57, 34.21, 32.87, 33.63, 33.17, 32.78, 30.43, 30.61, 32.64, 34.27, 35.74, 34.17, 36.51, 37.99, 36.97, 36.43, 36.63, 37.0, 38.04, 38.01, 40.55, 40.4, 39.27, 38.59, 37.89, 39.01, 37.16, 37.35, 38.14, 38.01, 38.22, 38.2, 37.63, 37.35, 36.98, 34.03, 35.33, 37.96, 36.07, 40.0, 39.04, 40.71, 38.67, 37.5, 36.57, 36.79, 36.77, 36.61, 39.495, 39.89, 48.23, 49.56, 50.59, 50.46, 49.06, 48.91, 47.9, 48.63, 51.97, 51.75, 52.38, 52.11, 53.78, 55.33, 56.66, 60.96, 60.91, 59.52, 61.15, 60.86, 59.01, 58.44, 56.68, 58.32, 58.31, 59.25, 61.91, 62.83, 64.41, 66.58, 65.0, 66.0, 67.56, 68.49, 66.5, 66.0, 66.75, 66.11, 65.2265, 63.88, 63.51, 63.09, 61.95, 60.6, 60.03, 59.95, 58.61, 60.39, 59.32, 59.76, 59.37, 55.96, 54.61, 55.39, 53.93, 54.29, 52.71, 52.6, 51.04, 50.52, 50.16, 49.83, 46.46, 49.37, 48.72, 49.09, 47.89, 48.37, 47.8, 47.79, 47.67, 46.77, 47.13, 47.55, 47.66, 47.54, 48.1, 48.83, 48.84, 48.2, 46.24, 49.81, 51.27, 52.85, 55.86, 58.17, 57.35, 58.41, 57.86, 56.4, 53.47, 53.57, 53.02, 54.13, 52.97, 59.96, 55.97, 52.57, 90.33, 80.29, 80.6, 77.83, 79.96, 80.58, 82.8, 81.95, 83.46, 85.3, 79.23, 85.77, 83.4, 80.93, 79.43, 77.29, 77.82, 78.11, 78.76, 77.2, 82.75, 85.31, 84.12, 83.66, 83.66, 84.19, 82.59, 84.51, 85.45, 86.2, 90.13, 92.53, 93.71, 93.49, 100.5, 103.73, 102.77, 102.74, 104.44, 93.86, 101.44, 101.87, 99.36, 98.77, 106.26, 105.03, 104.51, 107.025, 103.0, 104.02, 106.9, 107.2, 105.01, 103.0, 102.0, 104.51, 103.18, 103.06, 108.35, 108.43, 101.31, 98.09, 102.04, 102.83, 84.56, 86.56, 90.06, 90.56, 89.61, 88.2, 86.44, 83.45, 83.45, 82.13, 84.0, 75.67, 78.71, 74.62, 74.65, 76.57, 78.72, 82.43, 88.01, 88.74, 83.62, 82.45, 82.4, 79.02, 78.92, 76.53, 77.78, 76.78, 71.58, 70.17, 69.4, 69.73, 71.99, 69.88, 68.9, 68.03, 57.43, 57.76, 58.34, 57.85, 57.65, 59.71, 60.5, 59.03, 60.0, 58.0, 57.41, 57.92, 57.05, 56.9, 56.91, 55.06, 56.62, 54.89, 55.2, 53.96, 54.83, 49.67, 51.91, 50.72, 53.34, 53.97, 54.15, 52.11, 51.975, 52.3858, 49.75, 49.97, 46.4, 44.99, 44.92, 43.07, 39.56, 37.1, 32.5, 31.75, 30.3, 26.84, 24.27, 23.75, 23.541, 23.37, 24.09, 23.52, 23.61, 23.65, 24.17, 24.15, 24.56, 24.16, 23.68, 24.32, 24.0, 23.67, 23.21, 23.02, 23.07, 23.26, 23.88, 24.81, 24.68, 24.91, 24.49, 24.77, 24.0, 23.68, 23.11, 22.26, 22.93, 22.44, 21.88, 21.38, 21.735, 22.62, 22.05, 22.0, 21.765, 21.88, 21.83, 22.27, 22.485, 21.66, 21.745, 21.63, 21.045, 19.525, 19.2, 19.13, 18.67, 18.68, 18.33, 18.61, 17.79, 18.04, 18.425, 19.52, 19.5, 19.28, 18.26, 17.95, 18.68, 19.63, 20.27, 20.03, 20.045, 20.49, 20.64, 21.83, 22.92, 22.61, 23.1, 23.22, 22.94, 22.24, 22.18, 21.66, 21.61, 20.66, 20.67, 20.81, 20.85, 20.97, 19.515, 19.78, 19.52, 20.06, 19.76, 19.54, 19.34, 19.51, 18.82, 19.735, 19.98, 19.5, 19.16, 18.16, 18.385, 18.2, 18.1, 18.44, 18.26, 18.04, 19.03, 18.66, 18.84, 19.68, 19.66, 20.51, 19.99, 19.88, 19.01, 20.78, 21.5, 21.1, 19.76, 19.28, 18.94, 19.53, 18.9, 19.53, 19.81, 19.71, 19.41, 18.975, 17.54, 17.925, 18.64, 18.58, 18.39, 18.555, 18.77, 18.97, 18.94, 18.79, 19.395, 18.86, 18.37, 19.14, 18.6, 18.585, 18.87, 18.725, 18.22, 18.52, 18.495, 20.12, 21.04, 21.07, 22.36, 22.5589, 23.465, 23.25, 24.2, 23.93, 23.38, 22.43, 22.54, 22.89, 21.84, 22.12, 15.5, 15.71, 15.69, 15.405, 15.695, 15.345, 15.32, 15.25, 14.26, 14.38, 14.63, 14.8, 13.01, 13.08, 13.69, 13.45, 13.2, 13.21, 13.4, 13.09, 12.97, 13.69, 13.375, 13.25, 13.29, 13.7, 13.15, 13.17, 13.45, 12.89, 12.39, 12.63, 12.105, 11.75, 12.37, 12.26, 12.41, 12.13, 12.13, 12.01, 12.24, 12.51, 12.31, 12.93, 12.81, 13.1, 13.51, 13.63, 13.76, 13.19, 13.16, 12.69, 12.28, 12.19, 12.18, 12.66, 12.885, 12.32, 12.34, 12.79, 12.98, 12.9, 12.585, 12.18, 12.23, 12.56, 13.28, 12.95, 12.33, 12.79, 12.59, 12.37, 12.09, 12.28, 12.57, 12.74, 13.48, 13.255, 13.11, 13.5, 13.93, 13.98, 13.91, 13.93, 14.41, 14.41, 13.68, 13.59, 13.07, 12.8, 12.3, 11.85, 12.44, 12.5, 12.4, 12.01, 12.26, 12.61, 12.77, 13.09, 13.34, 13.75, 13.345, 14.07, 15.66, 15.89, 16.1, 16.22, 16.64, 17.21, 16.81, 16.99, 17.06, 16.93, 17.48, 17.18, 16.96, 17.67, 17.34, 18.02, 18.75, 19.51, 19.14, 18.97, 18.99, 19.07, 19.25, 19.09, 18.68, 19.32, 18.66, 19.615, 19.67, 18.94, 18.45, 18.1, 17.86, 17.52, 17.32, 17.43, 17.34, 17.59, 18.14, 17.83, 17.26, 17.72, 17.65, 17.52, 17.63, 17.15, 17.21, 17.04, 17.375, 17.25, 17.58, 17.47, 17.52, 17.18, 17.76, 17.285, 16.1, 15.17, 15.15, 15.31, 15.2, 15.38, 15.285, 14.96, 15.39, 15.455, 15.62, 15.77, 15.69, 15.06, 14.91, 14.29, 14.21, 14.08, 14.39, 14.22, 13.97, 13.755, 13.38, 14.39, 15.54, 15.1, 14.98, 15.21, 14.85, 15.02, 15.05, 15.05, 14.98, 15.8, 16.28, 16.01, 16.02, 16.24, 15.83, 15.95, 15.525, 15.43, 15.17, 14.9, 14.79, 14.53, 14.07, 13.85, 13.39, 13.645, 13.34, 13.91, 13.32, 13.09, 13.3, 13.715, 13.58, 13.38, 13.64, 14.015, 15.5901, 15.97, 15.68, 16.13, 16.25, 16.14, 16.21, 16.22, 15.76, 15.375, 15.56, 15.75, 15.13, 15.18, 15.27, 14.84, 15.3, 15.49, 15.15, 15.31, 14.37, 14.27, 14.44, 13.76, 13.56, 13.345, 12.61, 12.71, 12.61, 12.97, 12.64, 12.6, 12.32, 12.3, 12.77, 13.06, 12.86, 12.035, 11.66, 11.91, 12.06, 11.35, 11.45, 11.15, 11.48, 11.18, 11.27, 11.29, 11.35, 10.99, 11.51, 11.42, 11.34, 11.94, 11.54, 11.53, 11.33, 11.17, 11.355, 11.17, 11.93, 11.68, 12.2, 12.15, 13.33, 13.64, 13.71, 14.42, 15.25, 14.93, 14.76, 14.27, 14.58, 14.03, 14.06, 13.765, 13.19, 14.275, 13.85, 14.73, 14.99, 15.62, 16.03, 16.21, 16.83, 17.16, 17.405, 17.16, 17.07, 16.82, 17.02, 16.52, 15.86, 15.91, 16.13, 15.55, 15.17, 15.45, 15.84, 15.67, 15.31, 15.37, 15.9, 15.21, 15.71, 15.81, 15.66, 15.21, 15.65, 15.06, 14.92, 15.49, 14.945, 14.99, 14.37, 13.725, 13.43, 12.43, 12.89, 13.87, 12.77, 12.89, 12.31, 12.35, 12.5, 12.24, 11.865, 11.91, 12.29, 11.42, 11.34, 11.37, 11.11, 11.11, 11.16, 11.3, 10.86, 10.86, 9.98, 10.21, 11.09, 11.36, 10.78, 11.15, 10.3, 11.41, 11.17, 10.71, 10.96, 10.55, 11.23, 11.94, 13.04, 12.88, 13.045, 13.74, 13.91, 13.76, 13.22, 12.8, 12.53, 12.24, 11.45, 11.34, 11.82, 11.27, 11.12, 11.4, 10.75, 11.04, 10.87, 10.83, 10.08, 9.83, 9.5, 10.27, 10.31, 10.81, 11.09, 11.44, 11.22, 10.99, 10.66, 10.73, 11.46, 11.345, 11.73, 12.3, 12.43, 12.58, 12.62, 12.34, 12.16, 12.88, 12.42, 12.77, 12.72, 12.79, 12.61, 12.455, 12.47, 12.62, 12.285, 12.11, 11.815, 11.395, 11.17, 11.1, 11.67, 11.78, 11.895, 12.12, 12.505, 12.36, 12.97, 13.83, 13.45, 12.57, 12.465, 11.82, 11.8, 11.5, 11.51, 11.19, 11.42, 11.49, 11.65, 11.405, 11.77, 11.685, 11.44, 11.74, 11.78, 11.48, 11.5, 11.51, 11.55, 11.54, 11.87, 11.575, 11.59, 11.53, 11.32, 11.12, 10.93, 10.92, 11.28, 11.47, 11.36, 11.315, 11.41, 11.17, 11.34, 11.47, 9.51, 10.135, 10.34, 10.55, 10.77, 11.06, 10.89, 10.74, 10.96, 11.98, 12.13, 12.1, 11.77, 11.365, 11.21, 11.17, 11.15, 10.72, 10.71, 11.0, 10.81, 11.0, 11.31, 11.09, 10.92, 11.025, 11.51, 11.62, 12.02, 11.885, 12.505, 14.13, 14.06, 13.89, 13.5, 13.05, 12.66, 12.79, 13.02, 13.3, 13.4, 12.95, 13.02, 13.05, 12.98, 13.23, 13.48, 13.585, 13.6, 13.65, 13.6, 13.64, 13.54, 13.69, 13.69, 13.77, 13.85, 13.96, 14.47, 14.13, 14.55, 14.595, 14.63, 15.06, 14.4901, 14.98, 14.75, 14.62, 14.6, 14.51, 14.86, 15.24, 14.6, 15.61, 15.36, 15.68, 15.39, 15.7, 15.19, 14.67, 14.99, 15.17, 15.5, 15.36, 15.58, 15.22, 15.0218, 14.81, 15.36, 15.69, 15.62, 15.21, 14.47, 14.5, 15.3, 15.6, 15.625, 16.31, 15.91, 16.24, 15.96, 15.76, 16.67, 17.05, 16.668, 16.7, 15.93, 14.98, 14.67, 14.315, 13.76, 14.09, 13.75, 13.625, 13.85, 13.97, 13.88, 13.79, 13.35, 13.57, 13.6, 13.81, 13.82, 13.78, 14.02, 13.89, 13.94, 13.485, 13.15, 13.245, 12.66, 12.63, 12.86, 12.4, 13.275, 12.605, 12.53, 12.535, 12.66, 12.68, 12.89, 12.47, 12.45, 12.57, 12.25, 12.07, 12.07, 12.59, 12.71, 12.6, 12.96, 12.71, 12.53, 12.82, 13.36, 13.71, 13.58, 13.53, 13.83, 13.82, 13.98, 13.56, 14.02, 13.9, 13.73, 14.12, 14.48, 14.26, 14.3, 14.28, 14.405, 14.23, 13.44, 13.32, 13.39, 13.595, 13.54, 13.42, 13.51, 13.2597, 12.88, 12.98, 12.64, 12.43, 12.54, 12.07, 11.83, 11.815, 10.55, 11.01, 11.0, 10.91, 10.99, 11.02, 11.01, 11.02, 10.95, 11.11, 11.07, 11.66, 11.745, 12.14, 12.58, 12.29, 12.13, 12.155, 12.46, 13.25, 13.28, 13.47, 12.82, 12.84, 12.99, 12.94, 12.83, 13.18, 13.24, 13.5, 13.89, 13.84, 14.31, 13.89, 12.54, 12.24, 11.185, 11.45, 11.125, 11.01, 11.53, 11.73, 12.05, 11.72, 11.75, 11.69, 11.675, 11.015, 10.94, 11.035, 10.95, 11.03, 11.11, 11.1, 11.21, 10.93, 10.88, 11.24, 11.21, 11.72, 11.47, 11.645, 11.78, 11.48, 11.19, 11.23, 11.88, 11.97, 12.25, 11.05, 10.97, 11.05, 10.89, 10.59, 10.53, 11.02, 11.76, 10.93, 11.15, 11.11, 11.73, 12.57, 12.64, 12.11, 11.76, 12.16, 12.3, 12.71, 12.75, 13.44, 14.5, 14.42, 14.37, 13.17, 13.43, 13.29, 13.36, 13.5, 13.6, 14.03, 14.25, 14.97, 14.78, 14.18, 12.83, 13.07, 13.7, 13.4, 13.63, 13.65, 13.74, 13.53, 13.88, 13.44, 13.62, 13.49, 13.37, 13.27, 12.8, 12.01, 12.43, 12.7, 12.86, 13.61, 13.99, 13.29, 13.37, 12.74, 12.92, 13.99, 13.11, 13.52, 13.04, 13.265, 13.16, 13.13, 12.385, 12.79, 12.41, 12.17, 11.88, 11.68, 11.97, 11.87, 11.34, 11.57, 11.39, 11.74, 11.48, 11.22, 11.19, 10.76, 10.98, 10.84, 10.76, 10.53, 10.75, 10.54, 11.235, 11.38, 11.54, 11.47, 11.655, 11.84, 12.02, 12.52, 12.33, 11.79, 11.84, 11.8, 11.59, 11.075, 11.01, 11.01, 11.36, 11.06, 11.23, 10.89, 10.83, 10.83, 8.96, 9.36, 9.58, 9.51, 9.42, 9.51, 9.6, 10.22, 10.27, 10.38, 10.72, 10.52, 10.49, 10.26]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('cb3a097f-d875-4c5e-bbbc-d71af23443a8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"433a60dd-478d-4aa3-b575-882574bbe634\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"433a60dd-478d-4aa3-b575-882574bbe634\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '433a60dd-478d-4aa3-b575-882574bbe634',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('433a60dd-478d-4aa3-b575-882574bbe634');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2SxfGOyXhm7"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovHB4COzXhm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e73cbc1-6705-4d38-e178-a59fc29be97b"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .2, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"NKTR\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6824 - accuracy: 0.5570 - val_loss: 0.6777 - val_accuracy: 0.5327\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6161 - accuracy: 0.6772 - val_loss: 0.5930 - val_accuracy: 0.6796\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5747 - accuracy: 0.7128 - val_loss: 0.5350 - val_accuracy: 0.7653\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5313 - accuracy: 0.7470 - val_loss: 0.4949 - val_accuracy: 0.7755\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5228 - accuracy: 0.7550 - val_loss: 0.4781 - val_accuracy: 0.7735\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6420 - accuracy: 0.6248 - val_loss: 0.5402 - val_accuracy: 0.7490\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5243 - accuracy: 0.7537 - val_loss: 0.4787 - val_accuracy: 0.7816\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4960 - accuracy: 0.7638 - val_loss: 0.4914 - val_accuracy: 0.7714\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4881 - accuracy: 0.7691 - val_loss: 0.4761 - val_accuracy: 0.7776\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4827 - accuracy: 0.7678 - val_loss: 0.4900 - val_accuracy: 0.7673\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.820074\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.820299\n",
            "[2]\tvalidation_0-auc:0.827489\n",
            "[3]\tvalidation_0-auc:0.835666\n",
            "[4]\tvalidation_0-auc:0.840621\n",
            "[5]\tvalidation_0-auc:0.839868\n",
            "[6]\tvalidation_0-auc:0.838594\n",
            "[7]\tvalidation_0-auc:0.841011\n",
            "[8]\tvalidation_0-auc:0.841817\n",
            "[9]\tvalidation_0-auc:0.842943\n",
            "[10]\tvalidation_0-auc:0.843385\n",
            "[11]\tvalidation_0-auc:0.843515\n",
            "[12]\tvalidation_0-auc:0.844181\n",
            "[13]\tvalidation_0-auc:0.844415\n",
            "[14]\tvalidation_0-auc:0.845169\n",
            "[15]\tvalidation_0-auc:0.845412\n",
            "[16]\tvalidation_0-auc:0.845879\n",
            "[17]\tvalidation_0-auc:0.844961\n",
            "[18]\tvalidation_0-auc:0.844693\n",
            "[19]\tvalidation_0-auc:0.845386\n",
            "[20]\tvalidation_0-auc:0.845282\n",
            "[21]\tvalidation_0-auc:0.845784\n",
            "[22]\tvalidation_0-auc:0.844814\n",
            "[23]\tvalidation_0-auc:0.845619\n",
            "[24]\tvalidation_0-auc:0.845498\n",
            "[25]\tvalidation_0-auc:0.845758\n",
            "[26]\tvalidation_0-auc:0.845992\n",
            "[27]\tvalidation_0-auc:0.846616\n",
            "[28]\tvalidation_0-auc:0.845905\n",
            "[29]\tvalidation_0-auc:0.843757\n",
            "[30]\tvalidation_0-auc:0.842614\n",
            "[31]\tvalidation_0-auc:0.843021\n",
            "[32]\tvalidation_0-auc:0.843393\n",
            "[33]\tvalidation_0-auc:0.843289\n",
            "[34]\tvalidation_0-auc:0.84335\n",
            "[35]\tvalidation_0-auc:0.843532\n",
            "[36]\tvalidation_0-auc:0.843133\n",
            "[37]\tvalidation_0-auc:0.842406\n",
            "[38]\tvalidation_0-auc:0.841912\n",
            "[39]\tvalidation_0-auc:0.841306\n",
            "[40]\tvalidation_0-auc:0.84147\n",
            "[41]\tvalidation_0-auc:0.841531\n",
            "[42]\tvalidation_0-auc:0.842033\n",
            "[43]\tvalidation_0-auc:0.841583\n",
            "[44]\tvalidation_0-auc:0.842172\n",
            "[45]\tvalidation_0-auc:0.842562\n",
            "[46]\tvalidation_0-auc:0.842388\n",
            "[47]\tvalidation_0-auc:0.842302\n",
            "[48]\tvalidation_0-auc:0.841999\n",
            "[49]\tvalidation_0-auc:0.841141\n",
            "[50]\tvalidation_0-auc:0.840933\n",
            "[51]\tvalidation_0-auc:0.840708\n",
            "[52]\tvalidation_0-auc:0.84037\n",
            "[53]\tvalidation_0-auc:0.840301\n",
            "[54]\tvalidation_0-auc:0.840145\n",
            "[55]\tvalidation_0-auc:0.839885\n",
            "[56]\tvalidation_0-auc:0.839816\n",
            "[57]\tvalidation_0-auc:0.839331\n",
            "[58]\tvalidation_0-auc:0.83992\n",
            "[59]\tvalidation_0-auc:0.839746\n",
            "[60]\tvalidation_0-auc:0.839123\n",
            "[61]\tvalidation_0-auc:0.838742\n",
            "[62]\tvalidation_0-auc:0.83862\n",
            "[63]\tvalidation_0-auc:0.838395\n",
            "[64]\tvalidation_0-auc:0.838542\n",
            "[65]\tvalidation_0-auc:0.838525\n",
            "[66]\tvalidation_0-auc:0.837901\n",
            "[67]\tvalidation_0-auc:0.837641\n",
            "[68]\tvalidation_0-auc:0.837226\n",
            "[69]\tvalidation_0-auc:0.837434\n",
            "[70]\tvalidation_0-auc:0.836637\n",
            "[71]\tvalidation_0-auc:0.837087\n",
            "[72]\tvalidation_0-auc:0.836758\n",
            "[73]\tvalidation_0-auc:0.836567\n",
            "[74]\tvalidation_0-auc:0.836359\n",
            "[75]\tvalidation_0-auc:0.835701\n",
            "[76]\tvalidation_0-auc:0.834506\n",
            "[77]\tvalidation_0-auc:0.834523\n",
            "Stopping. Best iteration:\n",
            "[27]\tvalidation_0-auc:0.846616\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6821 - accuracy: 0.5607 - val_loss: 0.6858 - val_accuracy: 0.5580\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6552 - accuracy: 0.6060 - val_loss: 0.6653 - val_accuracy: 0.6214\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6155 - accuracy: 0.6616 - val_loss: 0.5820 - val_accuracy: 0.7112\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5701 - accuracy: 0.7193 - val_loss: 0.5670 - val_accuracy: 0.7352\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5386 - accuracy: 0.7467 - val_loss: 0.5527 - val_accuracy: 0.7199\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6459 - accuracy: 0.6177 - val_loss: 0.5949 - val_accuracy: 0.6718\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5535 - accuracy: 0.7261 - val_loss: 0.5618 - val_accuracy: 0.7199\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5333 - accuracy: 0.7385 - val_loss: 0.5526 - val_accuracy: 0.7177\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5131 - accuracy: 0.7570 - val_loss: 0.5819 - val_accuracy: 0.7155\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5102 - accuracy: 0.7714 - val_loss: 0.5506 - val_accuracy: 0.7396\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.699837\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.699971\n",
            "[2]\tvalidation_0-auc:0.724162\n",
            "[3]\tvalidation_0-auc:0.725356\n",
            "[4]\tvalidation_0-auc:0.725573\n",
            "[5]\tvalidation_0-auc:0.727766\n",
            "[6]\tvalidation_0-auc:0.733368\n",
            "[7]\tvalidation_0-auc:0.74103\n",
            "[8]\tvalidation_0-auc:0.743677\n",
            "[9]\tvalidation_0-auc:0.743656\n",
            "[10]\tvalidation_0-auc:0.748785\n",
            "[11]\tvalidation_0-auc:0.751493\n",
            "[12]\tvalidation_0-auc:0.747745\n",
            "[13]\tvalidation_0-auc:0.74827\n",
            "[14]\tvalidation_0-auc:0.746385\n",
            "[15]\tvalidation_0-auc:0.746303\n",
            "[16]\tvalidation_0-auc:0.745211\n",
            "[17]\tvalidation_0-auc:0.747971\n",
            "[18]\tvalidation_0-auc:0.747992\n",
            "[19]\tvalidation_0-auc:0.751617\n",
            "[20]\tvalidation_0-auc:0.747724\n",
            "[21]\tvalidation_0-auc:0.748115\n",
            "[22]\tvalidation_0-auc:0.75069\n",
            "[23]\tvalidation_0-auc:0.752647\n",
            "[24]\tvalidation_0-auc:0.756663\n",
            "[25]\tvalidation_0-auc:0.756714\n",
            "[26]\tvalidation_0-auc:0.75622\n",
            "[27]\tvalidation_0-auc:0.755108\n",
            "[28]\tvalidation_0-auc:0.756941\n",
            "[29]\tvalidation_0-auc:0.758259\n",
            "[30]\tvalidation_0-auc:0.757126\n",
            "[31]\tvalidation_0-auc:0.756539\n",
            "[32]\tvalidation_0-auc:0.756169\n",
            "[33]\tvalidation_0-auc:0.756086\n",
            "[34]\tvalidation_0-auc:0.756035\n",
            "[35]\tvalidation_0-auc:0.758239\n",
            "[36]\tvalidation_0-auc:0.758053\n",
            "[37]\tvalidation_0-auc:0.758578\n",
            "[38]\tvalidation_0-auc:0.759155\n",
            "[39]\tvalidation_0-auc:0.758723\n",
            "[40]\tvalidation_0-auc:0.761915\n",
            "[41]\tvalidation_0-auc:0.762502\n",
            "[42]\tvalidation_0-auc:0.762749\n",
            "[43]\tvalidation_0-auc:0.762111\n",
            "[44]\tvalidation_0-auc:0.762646\n",
            "[45]\tvalidation_0-auc:0.761791\n",
            "[46]\tvalidation_0-auc:0.76348\n",
            "[47]\tvalidation_0-auc:0.765437\n",
            "[48]\tvalidation_0-auc:0.764181\n",
            "[49]\tvalidation_0-auc:0.763604\n",
            "[50]\tvalidation_0-auc:0.762986\n",
            "[51]\tvalidation_0-auc:0.76313\n",
            "[52]\tvalidation_0-auc:0.763449\n",
            "[53]\tvalidation_0-auc:0.764356\n",
            "[54]\tvalidation_0-auc:0.764624\n",
            "[55]\tvalidation_0-auc:0.764294\n",
            "[56]\tvalidation_0-auc:0.765715\n",
            "[57]\tvalidation_0-auc:0.765736\n",
            "[58]\tvalidation_0-auc:0.765715\n",
            "[59]\tvalidation_0-auc:0.764912\n",
            "[60]\tvalidation_0-auc:0.766415\n",
            "[61]\tvalidation_0-auc:0.766354\n",
            "[62]\tvalidation_0-auc:0.767651\n",
            "[63]\tvalidation_0-auc:0.76831\n",
            "[64]\tvalidation_0-auc:0.768599\n",
            "[65]\tvalidation_0-auc:0.770061\n",
            "[66]\tvalidation_0-auc:0.770514\n",
            "[67]\tvalidation_0-auc:0.76969\n",
            "[68]\tvalidation_0-auc:0.769917\n",
            "[69]\tvalidation_0-auc:0.770391\n",
            "[70]\tvalidation_0-auc:0.770329\n",
            "[71]\tvalidation_0-auc:0.770308\n",
            "[72]\tvalidation_0-auc:0.770452\n",
            "[73]\tvalidation_0-auc:0.770452\n",
            "[74]\tvalidation_0-auc:0.769567\n",
            "[75]\tvalidation_0-auc:0.768619\n",
            "[76]\tvalidation_0-auc:0.768146\n",
            "[77]\tvalidation_0-auc:0.768104\n",
            "[78]\tvalidation_0-auc:0.768599\n",
            "[79]\tvalidation_0-auc:0.770308\n",
            "[80]\tvalidation_0-auc:0.76864\n",
            "[81]\tvalidation_0-auc:0.768588\n",
            "[82]\tvalidation_0-auc:0.768259\n",
            "[83]\tvalidation_0-auc:0.768279\n",
            "[84]\tvalidation_0-auc:0.769227\n",
            "[85]\tvalidation_0-auc:0.768835\n",
            "[86]\tvalidation_0-auc:0.769701\n",
            "[87]\tvalidation_0-auc:0.770483\n",
            "[88]\tvalidation_0-auc:0.770772\n",
            "[89]\tvalidation_0-auc:0.77106\n",
            "[90]\tvalidation_0-auc:0.771719\n",
            "[91]\tvalidation_0-auc:0.771348\n",
            "[92]\tvalidation_0-auc:0.77141\n",
            "[93]\tvalidation_0-auc:0.771101\n",
            "[94]\tvalidation_0-auc:0.771122\n",
            "[95]\tvalidation_0-auc:0.771019\n",
            "[96]\tvalidation_0-auc:0.77141\n",
            "[97]\tvalidation_0-auc:0.772049\n",
            "[98]\tvalidation_0-auc:0.772543\n",
            "[99]\tvalidation_0-auc:0.772512\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.773469387755102  | 0.7193877551020408 | 0.7157360406091371 | 0.717557251908397  |\n",
            "|     GRU 0.1      | 0.7673469387755102 | 0.6751054852320675 | 0.8121827411167513 | 0.7373271889400922 |\n",
            "|   XGBoost 0.1    | 0.7816326530612245 | 0.7586206896551724 | 0.6700507614213198 | 0.7115902964959568 |\n",
            "|    Logreg 0.1    | 0.7877551020408163 | 0.7961783439490446 | 0.6345177664974619 | 0.7062146892655368 |\n",
            "|     SVM 0.1      | 0.7755102040816326 | 0.7636363636363637 | 0.6395939086294417 | 0.696132596685083  |\n",
            "|  LSTM beta 0.1   | 0.7199124726477024 | 0.6754385964912281 | 0.4583333333333333 | 0.5460992907801417 |\n",
            "|   GRU beta 0.1   | 0.7396061269146609 | 0.6467065868263473 | 0.6428571428571429 | 0.6447761194029851 |\n",
            "| XGBoost beta 0.1 | 0.7286652078774617 |       0.625        | 0.6547619047619048 | 0.6395348837209303 |\n",
            "| logreg beta 0.1  | 0.7199124726477024 | 0.644927536231884  | 0.5297619047619048 | 0.5816993464052287 |\n",
            "|   svm beta 0.1   | 0.7111597374179431 | 0.618421052631579  | 0.5595238095238095 |       0.5875       |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6927 - accuracy: 0.5389 - val_loss: 0.7016 - val_accuracy: 0.3184\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6381 - accuracy: 0.6463 - val_loss: 0.5936 - val_accuracy: 0.7510\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5953 - accuracy: 0.6899 - val_loss: 0.5915 - val_accuracy: 0.6796\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5870 - accuracy: 0.6926 - val_loss: 0.5606 - val_accuracy: 0.7510\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5857 - accuracy: 0.6879 - val_loss: 0.5626 - val_accuracy: 0.7163\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6390 - accuracy: 0.6201 - val_loss: 0.5433 - val_accuracy: 0.7531\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5747 - accuracy: 0.6980 - val_loss: 0.5865 - val_accuracy: 0.6959\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5713 - accuracy: 0.7034 - val_loss: 0.5252 - val_accuracy: 0.7571\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5620 - accuracy: 0.7007 - val_loss: 0.5381 - val_accuracy: 0.7429\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5564 - accuracy: 0.7168 - val_loss: 0.5740 - val_accuracy: 0.7143\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.786342\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.797189\n",
            "[2]\tvalidation_0-auc:0.793366\n",
            "[3]\tvalidation_0-auc:0.795845\n",
            "[4]\tvalidation_0-auc:0.795322\n",
            "[5]\tvalidation_0-auc:0.797643\n",
            "[6]\tvalidation_0-auc:0.796833\n",
            "[7]\tvalidation_0-auc:0.796764\n",
            "[8]\tvalidation_0-auc:0.798364\n",
            "[9]\tvalidation_0-auc:0.794768\n",
            "[10]\tvalidation_0-auc:0.796349\n",
            "[11]\tvalidation_0-auc:0.797139\n",
            "[12]\tvalidation_0-auc:0.799036\n",
            "[13]\tvalidation_0-auc:0.799105\n",
            "[14]\tvalidation_0-auc:0.798038\n",
            "[15]\tvalidation_0-auc:0.797771\n",
            "[16]\tvalidation_0-auc:0.797643\n",
            "[17]\tvalidation_0-auc:0.796537\n",
            "[18]\tvalidation_0-auc:0.796645\n",
            "[19]\tvalidation_0-auc:0.796744\n",
            "[20]\tvalidation_0-auc:0.79626\n",
            "[21]\tvalidation_0-auc:0.796063\n",
            "[22]\tvalidation_0-auc:0.796112\n",
            "[23]\tvalidation_0-auc:0.795658\n",
            "[24]\tvalidation_0-auc:0.795835\n",
            "[25]\tvalidation_0-auc:0.79546\n",
            "[26]\tvalidation_0-auc:0.794186\n",
            "[27]\tvalidation_0-auc:0.79466\n",
            "[28]\tvalidation_0-auc:0.794601\n",
            "[29]\tvalidation_0-auc:0.794136\n",
            "[30]\tvalidation_0-auc:0.795401\n",
            "[31]\tvalidation_0-auc:0.795006\n",
            "[32]\tvalidation_0-auc:0.79461\n",
            "[33]\tvalidation_0-auc:0.794037\n",
            "[34]\tvalidation_0-auc:0.793741\n",
            "[35]\tvalidation_0-auc:0.793563\n",
            "[36]\tvalidation_0-auc:0.793415\n",
            "[37]\tvalidation_0-auc:0.793079\n",
            "[38]\tvalidation_0-auc:0.792704\n",
            "[39]\tvalidation_0-auc:0.792684\n",
            "[40]\tvalidation_0-auc:0.79142\n",
            "[41]\tvalidation_0-auc:0.791025\n",
            "[42]\tvalidation_0-auc:0.791805\n",
            "[43]\tvalidation_0-auc:0.791706\n",
            "[44]\tvalidation_0-auc:0.791686\n",
            "[45]\tvalidation_0-auc:0.792368\n",
            "[46]\tvalidation_0-auc:0.791736\n",
            "[47]\tvalidation_0-auc:0.791341\n",
            "[48]\tvalidation_0-auc:0.791025\n",
            "[49]\tvalidation_0-auc:0.790955\n",
            "[50]\tvalidation_0-auc:0.790955\n",
            "[51]\tvalidation_0-auc:0.790501\n",
            "[52]\tvalidation_0-auc:0.789553\n",
            "[53]\tvalidation_0-auc:0.789533\n",
            "[54]\tvalidation_0-auc:0.789335\n",
            "[55]\tvalidation_0-auc:0.789335\n",
            "[56]\tvalidation_0-auc:0.789049\n",
            "[57]\tvalidation_0-auc:0.789424\n",
            "[58]\tvalidation_0-auc:0.789088\n",
            "[59]\tvalidation_0-auc:0.788219\n",
            "[60]\tvalidation_0-auc:0.788753\n",
            "[61]\tvalidation_0-auc:0.78814\n",
            "[62]\tvalidation_0-auc:0.788476\n",
            "[63]\tvalidation_0-auc:0.788101\n",
            "Stopping. Best iteration:\n",
            "[13]\tvalidation_0-auc:0.799105\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6493 - accuracy: 0.6012 - val_loss: 0.6604 - val_accuracy: 0.6586\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6023 - accuracy: 0.6767 - val_loss: 0.6494 - val_accuracy: 0.6521\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5471 - accuracy: 0.7213 - val_loss: 0.6049 - val_accuracy: 0.7068\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5375 - accuracy: 0.7440 - val_loss: 0.6135 - val_accuracy: 0.6761\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5321 - accuracy: 0.7406 - val_loss: 0.7089 - val_accuracy: 0.6346\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6227 - accuracy: 0.6452 - val_loss: 0.6328 - val_accuracy: 0.6565\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5314 - accuracy: 0.7392 - val_loss: 0.6462 - val_accuracy: 0.6674\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5245 - accuracy: 0.7358 - val_loss: 0.7094 - val_accuracy: 0.6368\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5213 - accuracy: 0.7454 - val_loss: 0.6978 - val_accuracy: 0.6324\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5168 - accuracy: 0.7358 - val_loss: 0.6071 - val_accuracy: 0.6893\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.697168\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.689734\n",
            "[2]\tvalidation_0-auc:0.695652\n",
            "[3]\tvalidation_0-auc:0.709177\n",
            "[4]\tvalidation_0-auc:0.708306\n",
            "[5]\tvalidation_0-auc:0.707063\n",
            "[6]\tvalidation_0-auc:0.709674\n",
            "[7]\tvalidation_0-auc:0.713179\n",
            "[8]\tvalidation_0-auc:0.72259\n",
            "[9]\tvalidation_0-auc:0.727202\n",
            "[10]\tvalidation_0-auc:0.728942\n",
            "[11]\tvalidation_0-auc:0.729464\n",
            "[12]\tvalidation_0-auc:0.729054\n",
            "[13]\tvalidation_0-auc:0.731465\n",
            "[14]\tvalidation_0-auc:0.730595\n",
            "[15]\tvalidation_0-auc:0.730123\n",
            "[16]\tvalidation_0-auc:0.72888\n",
            "[17]\tvalidation_0-auc:0.729588\n",
            "[18]\tvalidation_0-auc:0.731192\n",
            "[19]\tvalidation_0-auc:0.729812\n",
            "[20]\tvalidation_0-auc:0.728967\n",
            "[21]\tvalidation_0-auc:0.730123\n",
            "[22]\tvalidation_0-auc:0.733044\n",
            "[23]\tvalidation_0-auc:0.732746\n",
            "[24]\tvalidation_0-auc:0.732447\n",
            "[25]\tvalidation_0-auc:0.732162\n",
            "[26]\tvalidation_0-auc:0.734063\n",
            "[27]\tvalidation_0-auc:0.734474\n",
            "[28]\tvalidation_0-auc:0.733753\n",
            "[29]\tvalidation_0-auc:0.732833\n",
            "[30]\tvalidation_0-auc:0.733181\n",
            "[31]\tvalidation_0-auc:0.733653\n",
            "[32]\tvalidation_0-auc:0.734126\n",
            "[33]\tvalidation_0-auc:0.73517\n",
            "[34]\tvalidation_0-auc:0.735791\n",
            "[35]\tvalidation_0-auc:0.735294\n",
            "[36]\tvalidation_0-auc:0.736972\n",
            "[37]\tvalidation_0-auc:0.736624\n",
            "[38]\tvalidation_0-auc:0.735605\n",
            "[39]\tvalidation_0-auc:0.736376\n",
            "[40]\tvalidation_0-auc:0.735729\n",
            "[41]\tvalidation_0-auc:0.73461\n",
            "[42]\tvalidation_0-auc:0.734834\n",
            "[43]\tvalidation_0-auc:0.735456\n",
            "[44]\tvalidation_0-auc:0.736649\n",
            "[45]\tvalidation_0-auc:0.736226\n",
            "[46]\tvalidation_0-auc:0.737022\n",
            "[47]\tvalidation_0-auc:0.736624\n",
            "[48]\tvalidation_0-auc:0.737644\n",
            "[49]\tvalidation_0-auc:0.736947\n",
            "[50]\tvalidation_0-auc:0.73788\n",
            "[51]\tvalidation_0-auc:0.73972\n",
            "[52]\tvalidation_0-auc:0.740117\n",
            "[53]\tvalidation_0-auc:0.739844\n",
            "[54]\tvalidation_0-auc:0.738825\n",
            "[55]\tvalidation_0-auc:0.738999\n",
            "[56]\tvalidation_0-auc:0.740615\n",
            "[57]\tvalidation_0-auc:0.740739\n",
            "[58]\tvalidation_0-auc:0.740565\n",
            "[59]\tvalidation_0-auc:0.740664\n",
            "[60]\tvalidation_0-auc:0.739943\n",
            "[61]\tvalidation_0-auc:0.738054\n",
            "[62]\tvalidation_0-auc:0.738327\n",
            "[63]\tvalidation_0-auc:0.738476\n",
            "[64]\tvalidation_0-auc:0.738626\n",
            "[65]\tvalidation_0-auc:0.739123\n",
            "[66]\tvalidation_0-auc:0.738551\n",
            "[67]\tvalidation_0-auc:0.738775\n",
            "[68]\tvalidation_0-auc:0.737979\n",
            "[69]\tvalidation_0-auc:0.739794\n",
            "[70]\tvalidation_0-auc:0.740291\n",
            "[71]\tvalidation_0-auc:0.741684\n",
            "[72]\tvalidation_0-auc:0.742628\n",
            "[73]\tvalidation_0-auc:0.741485\n",
            "[74]\tvalidation_0-auc:0.741907\n",
            "[75]\tvalidation_0-auc:0.739869\n",
            "[76]\tvalidation_0-auc:0.739446\n",
            "[77]\tvalidation_0-auc:0.739123\n",
            "[78]\tvalidation_0-auc:0.740341\n",
            "[79]\tvalidation_0-auc:0.740739\n",
            "[80]\tvalidation_0-auc:0.74049\n",
            "[81]\tvalidation_0-auc:0.740963\n",
            "[82]\tvalidation_0-auc:0.741261\n",
            "[83]\tvalidation_0-auc:0.740739\n",
            "[84]\tvalidation_0-auc:0.740813\n",
            "[85]\tvalidation_0-auc:0.742032\n",
            "[86]\tvalidation_0-auc:0.741584\n",
            "[87]\tvalidation_0-auc:0.740938\n",
            "[88]\tvalidation_0-auc:0.741087\n",
            "[89]\tvalidation_0-auc:0.741162\n",
            "[90]\tvalidation_0-auc:0.740167\n",
            "[91]\tvalidation_0-auc:0.740863\n",
            "[92]\tvalidation_0-auc:0.740789\n",
            "[93]\tvalidation_0-auc:0.740615\n",
            "[94]\tvalidation_0-auc:0.739297\n",
            "[95]\tvalidation_0-auc:0.738029\n",
            "[96]\tvalidation_0-auc:0.738327\n",
            "[97]\tvalidation_0-auc:0.738302\n",
            "[98]\tvalidation_0-auc:0.738278\n",
            "[99]\tvalidation_0-auc:0.738029\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|     LSTM 0.2     | 0.7163265306122449 |  0.5211267605633803 |        0.75        | 0.6149584487534626 |\n",
            "|     GRU 0.2      | 0.7142857142857143 |  0.5173913043478261 | 0.8040540540540541 | 0.6296296296296297 |\n",
            "|   XGBoost 0.2    | 0.7551020408163265 |  0.5760869565217391 | 0.7162162162162162 | 0.6385542168674698 |\n",
            "|    Logreg 0.2    | 0.736734693877551  |  0.5458937198067633 | 0.7635135135135135 | 0.6366197183098592 |\n",
            "|     SVM 0.2      | 0.753061224489796  |  0.5685279187817259 | 0.7567567567567568 | 0.6492753623188405 |\n",
            "|  LSTM beta 0.2   | 0.6345733041575492 | 0.38571428571428573 | 0.680672268907563  | 0.4924012158054712 |\n",
            "|   GRU beta 0.2   | 0.6892778993435449 | 0.43352601156069365 | 0.6302521008403361 | 0.5136986301369864 |\n",
            "| XGBoost beta 0.2 | 0.649890590809628  | 0.40375586854460094 | 0.7226890756302521 | 0.5180722891566265 |\n",
            "| logreg beta 0.2  | 0.6892778993435449 |  0.4327485380116959 | 0.6218487394957983 | 0.5103448275862069 |\n",
            "|   svm beta 0.2   | 0.6389496717724289 | 0.38613861386138615 | 0.6554621848739496 | 0.4859813084112149 |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6653 - accuracy: 0.5738 - val_loss: 0.5993 - val_accuracy: 0.7469\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6102 - accuracy: 0.6779 - val_loss: 0.5565 - val_accuracy: 0.7408\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5926 - accuracy: 0.6886 - val_loss: 0.5522 - val_accuracy: 0.7571\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5711 - accuracy: 0.6926 - val_loss: 0.5343 - val_accuracy: 0.7776\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5482 - accuracy: 0.7188 - val_loss: 0.5419 - val_accuracy: 0.7551\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 18ms/step - loss: 0.6342 - accuracy: 0.6329 - val_loss: 0.5684 - val_accuracy: 0.7143\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5438 - accuracy: 0.7289 - val_loss: 0.5330 - val_accuracy: 0.7408\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5382 - accuracy: 0.7242 - val_loss: 0.5278 - val_accuracy: 0.7388\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5318 - accuracy: 0.7342 - val_loss: 0.6054 - val_accuracy: 0.6694\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5209 - accuracy: 0.7537 - val_loss: 0.5048 - val_accuracy: 0.7612\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.808397\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.80781\n",
            "[2]\tvalidation_0-auc:0.808262\n",
            "[3]\tvalidation_0-auc:0.810076\n",
            "[4]\tvalidation_0-auc:0.816005\n",
            "[5]\tvalidation_0-auc:0.818541\n",
            "[6]\tvalidation_0-auc:0.81874\n",
            "[7]\tvalidation_0-auc:0.821447\n",
            "[8]\tvalidation_0-auc:0.821492\n",
            "[9]\tvalidation_0-auc:0.820752\n",
            "[10]\tvalidation_0-auc:0.822115\n",
            "[11]\tvalidation_0-auc:0.822702\n",
            "[12]\tvalidation_0-auc:0.822973\n",
            "[13]\tvalidation_0-auc:0.823947\n",
            "[14]\tvalidation_0-auc:0.825364\n",
            "[15]\tvalidation_0-auc:0.825815\n",
            "[16]\tvalidation_0-auc:0.826835\n",
            "[17]\tvalidation_0-auc:0.826799\n",
            "[18]\tvalidation_0-auc:0.827034\n",
            "[19]\tvalidation_0-auc:0.827828\n",
            "[20]\tvalidation_0-auc:0.828415\n",
            "[21]\tvalidation_0-auc:0.828397\n",
            "[22]\tvalidation_0-auc:0.829119\n",
            "[23]\tvalidation_0-auc:0.828965\n",
            "[24]\tvalidation_0-auc:0.829904\n",
            "[25]\tvalidation_0-auc:0.829046\n",
            "[26]\tvalidation_0-auc:0.82929\n",
            "[27]\tvalidation_0-auc:0.827503\n",
            "[28]\tvalidation_0-auc:0.828234\n",
            "[29]\tvalidation_0-auc:0.828126\n",
            "[30]\tvalidation_0-auc:0.827954\n",
            "[31]\tvalidation_0-auc:0.827223\n",
            "[32]\tvalidation_0-auc:0.827566\n",
            "[33]\tvalidation_0-auc:0.827684\n",
            "[34]\tvalidation_0-auc:0.827729\n",
            "[35]\tvalidation_0-auc:0.827458\n",
            "[36]\tvalidation_0-auc:0.826664\n",
            "[37]\tvalidation_0-auc:0.825978\n",
            "[38]\tvalidation_0-auc:0.826501\n",
            "[39]\tvalidation_0-auc:0.825743\n",
            "[40]\tvalidation_0-auc:0.825689\n",
            "[41]\tvalidation_0-auc:0.826014\n",
            "[42]\tvalidation_0-auc:0.825382\n",
            "[43]\tvalidation_0-auc:0.824588\n",
            "[44]\tvalidation_0-auc:0.824768\n",
            "[45]\tvalidation_0-auc:0.824732\n",
            "[46]\tvalidation_0-auc:0.824155\n",
            "[47]\tvalidation_0-auc:0.823451\n",
            "[48]\tvalidation_0-auc:0.823487\n",
            "[49]\tvalidation_0-auc:0.822973\n",
            "[50]\tvalidation_0-auc:0.823063\n",
            "[51]\tvalidation_0-auc:0.822991\n",
            "[52]\tvalidation_0-auc:0.822359\n",
            "[53]\tvalidation_0-auc:0.823162\n",
            "[54]\tvalidation_0-auc:0.823397\n",
            "[55]\tvalidation_0-auc:0.822945\n",
            "[56]\tvalidation_0-auc:0.823649\n",
            "[57]\tvalidation_0-auc:0.822512\n",
            "[58]\tvalidation_0-auc:0.822684\n",
            "[59]\tvalidation_0-auc:0.8229\n",
            "[60]\tvalidation_0-auc:0.823063\n",
            "[61]\tvalidation_0-auc:0.822828\n",
            "[62]\tvalidation_0-auc:0.822467\n",
            "[63]\tvalidation_0-auc:0.822404\n",
            "[64]\tvalidation_0-auc:0.822945\n",
            "[65]\tvalidation_0-auc:0.823126\n",
            "[66]\tvalidation_0-auc:0.823027\n",
            "[67]\tvalidation_0-auc:0.823171\n",
            "[68]\tvalidation_0-auc:0.822846\n",
            "[69]\tvalidation_0-auc:0.823009\n",
            "[70]\tvalidation_0-auc:0.82263\n",
            "[71]\tvalidation_0-auc:0.823207\n",
            "[72]\tvalidation_0-auc:0.823009\n",
            "[73]\tvalidation_0-auc:0.823424\n",
            "[74]\tvalidation_0-auc:0.823478\n",
            "Stopping. Best iteration:\n",
            "[24]\tvalidation_0-auc:0.829904\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6834 - accuracy: 0.5525 - val_loss: 0.7191 - val_accuracy: 0.5624\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6779 - accuracy: 0.5827 - val_loss: 0.6469 - val_accuracy: 0.6871\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6072 - accuracy: 0.6815 - val_loss: 0.5650 - val_accuracy: 0.7330\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5461 - accuracy: 0.7316 - val_loss: 0.6816 - val_accuracy: 0.6258\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5118 - accuracy: 0.7461 - val_loss: 0.6224 - val_accuracy: 0.7046\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6341 - accuracy: 0.6266 - val_loss: 0.6132 - val_accuracy: 0.6565\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5339 - accuracy: 0.7426 - val_loss: 0.6459 - val_accuracy: 0.6630\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5211 - accuracy: 0.7481 - val_loss: 0.6694 - val_accuracy: 0.6630\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5108 - accuracy: 0.7522 - val_loss: 0.6196 - val_accuracy: 0.6674\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5003 - accuracy: 0.7509 - val_loss: 0.6863 - val_accuracy: 0.6499\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.688019\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.722208\n",
            "[2]\tvalidation_0-auc:0.712346\n",
            "[3]\tvalidation_0-auc:0.693705\n",
            "[4]\tvalidation_0-auc:0.702353\n",
            "[5]\tvalidation_0-auc:0.707076\n",
            "[6]\tvalidation_0-auc:0.711744\n",
            "[7]\tvalidation_0-auc:0.697652\n",
            "[8]\tvalidation_0-auc:0.70654\n",
            "[9]\tvalidation_0-auc:0.708486\n",
            "[10]\tvalidation_0-auc:0.71122\n",
            "[11]\tvalidation_0-auc:0.717277\n",
            "[12]\tvalidation_0-auc:0.713395\n",
            "[13]\tvalidation_0-auc:0.71017\n",
            "[14]\tvalidation_0-auc:0.709995\n",
            "[15]\tvalidation_0-auc:0.707459\n",
            "[16]\tvalidation_0-auc:0.708388\n",
            "[17]\tvalidation_0-auc:0.707655\n",
            "[18]\tvalidation_0-auc:0.711602\n",
            "[19]\tvalidation_0-auc:0.712554\n",
            "[20]\tvalidation_0-auc:0.711898\n",
            "[21]\tvalidation_0-auc:0.711657\n",
            "[22]\tvalidation_0-auc:0.712991\n",
            "[23]\tvalidation_0-auc:0.713833\n",
            "[24]\tvalidation_0-auc:0.71403\n",
            "[25]\tvalidation_0-auc:0.715855\n",
            "[26]\tvalidation_0-auc:0.71544\n",
            "[27]\tvalidation_0-auc:0.717386\n",
            "[28]\tvalidation_0-auc:0.717474\n",
            "[29]\tvalidation_0-auc:0.718611\n",
            "[30]\tvalidation_0-auc:0.719267\n",
            "[31]\tvalidation_0-auc:0.719868\n",
            "[32]\tvalidation_0-auc:0.719704\n",
            "[33]\tvalidation_0-auc:0.71895\n",
            "[34]\tvalidation_0-auc:0.721049\n",
            "[35]\tvalidation_0-auc:0.722885\n",
            "[36]\tvalidation_0-auc:0.722361\n",
            "[37]\tvalidation_0-auc:0.725225\n",
            "[38]\tvalidation_0-auc:0.72516\n",
            "[39]\tvalidation_0-auc:0.723126\n",
            "[40]\tvalidation_0-auc:0.725925\n",
            "[41]\tvalidation_0-auc:0.72575\n",
            "[42]\tvalidation_0-auc:0.726504\n",
            "[43]\tvalidation_0-auc:0.726297\n",
            "[44]\tvalidation_0-auc:0.726319\n",
            "[45]\tvalidation_0-auc:0.725182\n",
            "[46]\tvalidation_0-auc:0.72516\n",
            "[47]\tvalidation_0-auc:0.725739\n",
            "[48]\tvalidation_0-auc:0.725433\n",
            "[49]\tvalidation_0-auc:0.725739\n",
            "[50]\tvalidation_0-auc:0.727204\n",
            "[51]\tvalidation_0-auc:0.727816\n",
            "[52]\tvalidation_0-auc:0.728735\n",
            "[53]\tvalidation_0-auc:0.728604\n",
            "[54]\tvalidation_0-auc:0.729773\n",
            "[55]\tvalidation_0-auc:0.72962\n",
            "[56]\tvalidation_0-auc:0.729314\n",
            "[57]\tvalidation_0-auc:0.729686\n",
            "[58]\tvalidation_0-auc:0.730408\n",
            "[59]\tvalidation_0-auc:0.730484\n",
            "[60]\tvalidation_0-auc:0.730375\n",
            "[61]\tvalidation_0-auc:0.729981\n",
            "[62]\tvalidation_0-auc:0.727795\n",
            "[63]\tvalidation_0-auc:0.72751\n",
            "[64]\tvalidation_0-auc:0.728604\n",
            "[65]\tvalidation_0-auc:0.729019\n",
            "[66]\tvalidation_0-auc:0.729675\n",
            "[67]\tvalidation_0-auc:0.730112\n",
            "[68]\tvalidation_0-auc:0.730069\n",
            "[69]\tvalidation_0-auc:0.730987\n",
            "[70]\tvalidation_0-auc:0.731206\n",
            "[71]\tvalidation_0-auc:0.732059\n",
            "[72]\tvalidation_0-auc:0.733567\n",
            "[73]\tvalidation_0-auc:0.732999\n",
            "[74]\tvalidation_0-auc:0.732999\n",
            "[75]\tvalidation_0-auc:0.733611\n",
            "[76]\tvalidation_0-auc:0.734923\n",
            "[77]\tvalidation_0-auc:0.734857\n",
            "[78]\tvalidation_0-auc:0.734639\n",
            "[79]\tvalidation_0-auc:0.734267\n",
            "[80]\tvalidation_0-auc:0.735295\n",
            "[81]\tvalidation_0-auc:0.735273\n",
            "[82]\tvalidation_0-auc:0.735732\n",
            "[83]\tvalidation_0-auc:0.735667\n",
            "[84]\tvalidation_0-auc:0.734879\n",
            "[85]\tvalidation_0-auc:0.734201\n",
            "[86]\tvalidation_0-auc:0.733677\n",
            "[87]\tvalidation_0-auc:0.733436\n",
            "[88]\tvalidation_0-auc:0.733589\n",
            "[89]\tvalidation_0-auc:0.733655\n",
            "[90]\tvalidation_0-auc:0.732999\n",
            "[91]\tvalidation_0-auc:0.733196\n",
            "[92]\tvalidation_0-auc:0.733611\n",
            "[93]\tvalidation_0-auc:0.732102\n",
            "[94]\tvalidation_0-auc:0.732299\n",
            "[95]\tvalidation_0-auc:0.732255\n",
            "[96]\tvalidation_0-auc:0.731523\n",
            "[97]\tvalidation_0-auc:0.732441\n",
            "[98]\tvalidation_0-auc:0.732179\n",
            "[99]\tvalidation_0-auc:0.730561\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.7551020408163265 |  0.6476683937823834 | 0.7062146892655368 | 0.6756756756756755 |\n",
            "|      GRU 0.15     | 0.7612244897959184 |         0.65        | 0.7344632768361582 | 0.6896551724137931 |\n",
            "|    XGBoost 0.15   | 0.7714285714285715 |  0.6701570680628273 | 0.7231638418079096 | 0.6956521739130435 |\n",
            "|    Logreg 0.15    | 0.7551020408163265 |  0.6338028169014085 | 0.7627118644067796 | 0.6923076923076923 |\n",
            "|      SVM 0.15     | 0.773469387755102  |        0.665        | 0.751412429378531  | 0.7055702917771882 |\n",
            "|   LSTM beta 0.15  | 0.7045951859956237 |  0.5333333333333333 | 0.7027027027027027 | 0.6064139941690962 |\n",
            "|   GRU beta 0.15   | 0.649890590809628  | 0.47391304347826085 | 0.7364864864864865 | 0.5767195767195767 |\n",
            "| XGBoost beta 0.15 | 0.6630196936542669 |  0.4861111111111111 | 0.7094594594594594 | 0.5769230769230769 |\n",
            "|  logreg beta 0.15 | 0.6958424507658644 |  0.5243243243243243 | 0.6554054054054054 | 0.5825825825825827 |\n",
            "|   svm beta 0.15   | 0.6783369803063457 |  0.5024875621890548 | 0.6824324324324325 | 0.5787965616045846 |\n",
            "+-------------------+--------------------+---------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6vXIbh7Xhm7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a6e3658a-9fde-4d00-bcdf-422ae19885a5"
      },
      "source": [
        "Result_cross.to_csv('NKTR_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.719388</td>\n",
              "      <td>0.773469</td>\n",
              "      <td>0.717557</td>\n",
              "      <td>0.715736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.675105</td>\n",
              "      <td>0.767347</td>\n",
              "      <td>0.737327</td>\n",
              "      <td>0.812183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.758621</td>\n",
              "      <td>0.781633</td>\n",
              "      <td>0.711590</td>\n",
              "      <td>0.670051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.796178</td>\n",
              "      <td>0.787755</td>\n",
              "      <td>0.706215</td>\n",
              "      <td>0.634518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.763636</td>\n",
              "      <td>0.775510</td>\n",
              "      <td>0.696133</td>\n",
              "      <td>0.639594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.675439</td>\n",
              "      <td>0.719912</td>\n",
              "      <td>0.546099</td>\n",
              "      <td>0.458333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.646707</td>\n",
              "      <td>0.739606</td>\n",
              "      <td>0.644776</td>\n",
              "      <td>0.642857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.728665</td>\n",
              "      <td>0.639535</td>\n",
              "      <td>0.654762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.644928</td>\n",
              "      <td>0.719912</td>\n",
              "      <td>0.581699</td>\n",
              "      <td>0.529762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.618421</td>\n",
              "      <td>0.711160</td>\n",
              "      <td>0.587500</td>\n",
              "      <td>0.559524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.521127</td>\n",
              "      <td>0.716327</td>\n",
              "      <td>0.614958</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.517391</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.629630</td>\n",
              "      <td>0.804054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.576087</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.638554</td>\n",
              "      <td>0.716216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.545894</td>\n",
              "      <td>0.736735</td>\n",
              "      <td>0.636620</td>\n",
              "      <td>0.763514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.568528</td>\n",
              "      <td>0.753061</td>\n",
              "      <td>0.649275</td>\n",
              "      <td>0.756757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.385714</td>\n",
              "      <td>0.634573</td>\n",
              "      <td>0.492401</td>\n",
              "      <td>0.680672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.433526</td>\n",
              "      <td>0.689278</td>\n",
              "      <td>0.513699</td>\n",
              "      <td>0.630252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.403756</td>\n",
              "      <td>0.649891</td>\n",
              "      <td>0.518072</td>\n",
              "      <td>0.722689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.432749</td>\n",
              "      <td>0.689278</td>\n",
              "      <td>0.510345</td>\n",
              "      <td>0.621849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.386139</td>\n",
              "      <td>0.638950</td>\n",
              "      <td>0.485981</td>\n",
              "      <td>0.655462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.647668</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.675676</td>\n",
              "      <td>0.706215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.761224</td>\n",
              "      <td>0.689655</td>\n",
              "      <td>0.734463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.670157</td>\n",
              "      <td>0.771429</td>\n",
              "      <td>0.695652</td>\n",
              "      <td>0.723164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.633803</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.762712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.665000</td>\n",
              "      <td>0.773469</td>\n",
              "      <td>0.705570</td>\n",
              "      <td>0.751412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.704595</td>\n",
              "      <td>0.606414</td>\n",
              "      <td>0.702703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.473913</td>\n",
              "      <td>0.649891</td>\n",
              "      <td>0.576720</td>\n",
              "      <td>0.736486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.486111</td>\n",
              "      <td>0.663020</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.709459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.524324</td>\n",
              "      <td>0.695842</td>\n",
              "      <td>0.582583</td>\n",
              "      <td>0.655405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.502488</td>\n",
              "      <td>0.678337</td>\n",
              "      <td>0.578797</td>\n",
              "      <td>0.682432</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  NKTR  0.719388  0.773469  0.717557  0.715736\n",
              "1            GRU 0.1  NKTR  0.675105  0.767347  0.737327  0.812183\n",
              "2        XGBoost 0.1  NKTR  0.758621  0.781633  0.711590  0.670051\n",
              "3         Logreg 0.1  NKTR  0.796178  0.787755  0.706215  0.634518\n",
              "4            SVM 0.1  NKTR  0.763636  0.775510  0.696133  0.639594\n",
              "5      LSTM beta 0.1  NKTR  0.675439  0.719912  0.546099  0.458333\n",
              "6       GRU beta 0.1  NKTR  0.646707  0.739606  0.644776  0.642857\n",
              "7   XGBoost beta 0.1  NKTR  0.625000  0.728665  0.639535  0.654762\n",
              "8    logreg beta 0.1  NKTR  0.644928  0.719912  0.581699  0.529762\n",
              "9       svm beta 0.1  NKTR  0.618421  0.711160  0.587500  0.559524\n",
              "0           LSTM 0.2  NKTR  0.521127  0.716327  0.614958  0.750000\n",
              "1            GRU 0.2  NKTR  0.517391  0.714286  0.629630  0.804054\n",
              "2        XGBoost 0.2  NKTR  0.576087  0.755102  0.638554  0.716216\n",
              "3         Logreg 0.2  NKTR  0.545894  0.736735  0.636620  0.763514\n",
              "4            SVM 0.2  NKTR  0.568528  0.753061  0.649275  0.756757\n",
              "5      LSTM beta 0.2  NKTR  0.385714  0.634573  0.492401  0.680672\n",
              "6       GRU beta 0.2  NKTR  0.433526  0.689278  0.513699  0.630252\n",
              "7   XGBoost beta 0.2  NKTR  0.403756  0.649891  0.518072  0.722689\n",
              "8    logreg beta 0.2  NKTR  0.432749  0.689278  0.510345  0.621849\n",
              "9       svm beta 0.2  NKTR  0.386139  0.638950  0.485981  0.655462\n",
              "0          LSTM 0.15  NKTR  0.647668  0.755102  0.675676  0.706215\n",
              "1           GRU 0.15  NKTR  0.650000  0.761224  0.689655  0.734463\n",
              "2       XGBoost 0.15  NKTR  0.670157  0.771429  0.695652  0.723164\n",
              "3        Logreg 0.15  NKTR  0.633803  0.755102  0.692308  0.762712\n",
              "4           SVM 0.15  NKTR  0.665000  0.773469  0.705570  0.751412\n",
              "5     LSTM beta 0.15  NKTR  0.533333  0.704595  0.606414  0.702703\n",
              "6      GRU beta 0.15  NKTR  0.473913  0.649891  0.576720  0.736486\n",
              "7  XGBoost beta 0.15  NKTR  0.486111  0.663020  0.576923  0.709459\n",
              "8   logreg beta 0.15  NKTR  0.524324  0.695842  0.582583  0.655405\n",
              "9      svm beta 0.15  NKTR  0.502488  0.678337  0.578797  0.682432"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nyXK2FzXhm7"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_logreg_beta.csv')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdr43dCDXhm7"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4bGvgBuXhm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f01719f-7517-4868-9236-34648f98973d"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"NKTR\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6870 - accuracy: 0.5483 - val_loss: 0.6636 - val_accuracy: 0.6041\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6231 - accuracy: 0.6624 - val_loss: 0.5894 - val_accuracy: 0.7286\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5580 - accuracy: 0.7302 - val_loss: 0.5166 - val_accuracy: 0.7653\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5140 - accuracy: 0.7584 - val_loss: 0.4839 - val_accuracy: 0.7735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5217 - accuracy: 0.7584 - val_loss: 0.4849 - val_accuracy: 0.7633\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6212 - accuracy: 0.6604 - val_loss: 0.5433 - val_accuracy: 0.7551\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5380 - accuracy: 0.7483 - val_loss: 0.4937 - val_accuracy: 0.7755\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5059 - accuracy: 0.7644 - val_loss: 0.4875 - val_accuracy: 0.7776\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5092 - accuracy: 0.7651 - val_loss: 0.4758 - val_accuracy: 0.7735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5034 - accuracy: 0.7544 - val_loss: 0.4754 - val_accuracy: 0.7714\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.820074\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.820299\n",
            "[2]\tvalidation_0-auc:0.827489\n",
            "[3]\tvalidation_0-auc:0.835666\n",
            "[4]\tvalidation_0-auc:0.840621\n",
            "[5]\tvalidation_0-auc:0.839868\n",
            "[6]\tvalidation_0-auc:0.838594\n",
            "[7]\tvalidation_0-auc:0.841011\n",
            "[8]\tvalidation_0-auc:0.841817\n",
            "[9]\tvalidation_0-auc:0.842943\n",
            "[10]\tvalidation_0-auc:0.843385\n",
            "[11]\tvalidation_0-auc:0.843515\n",
            "[12]\tvalidation_0-auc:0.844181\n",
            "[13]\tvalidation_0-auc:0.844415\n",
            "[14]\tvalidation_0-auc:0.845169\n",
            "[15]\tvalidation_0-auc:0.845412\n",
            "[16]\tvalidation_0-auc:0.845879\n",
            "[17]\tvalidation_0-auc:0.844961\n",
            "[18]\tvalidation_0-auc:0.844693\n",
            "[19]\tvalidation_0-auc:0.845386\n",
            "[20]\tvalidation_0-auc:0.845282\n",
            "[21]\tvalidation_0-auc:0.845784\n",
            "[22]\tvalidation_0-auc:0.844814\n",
            "[23]\tvalidation_0-auc:0.845619\n",
            "[24]\tvalidation_0-auc:0.845498\n",
            "[25]\tvalidation_0-auc:0.845758\n",
            "[26]\tvalidation_0-auc:0.845992\n",
            "[27]\tvalidation_0-auc:0.846616\n",
            "[28]\tvalidation_0-auc:0.845905\n",
            "[29]\tvalidation_0-auc:0.843757\n",
            "[30]\tvalidation_0-auc:0.842614\n",
            "[31]\tvalidation_0-auc:0.843021\n",
            "[32]\tvalidation_0-auc:0.843393\n",
            "[33]\tvalidation_0-auc:0.843289\n",
            "[34]\tvalidation_0-auc:0.84335\n",
            "[35]\tvalidation_0-auc:0.843532\n",
            "[36]\tvalidation_0-auc:0.843133\n",
            "[37]\tvalidation_0-auc:0.842406\n",
            "[38]\tvalidation_0-auc:0.841912\n",
            "[39]\tvalidation_0-auc:0.841306\n",
            "[40]\tvalidation_0-auc:0.84147\n",
            "[41]\tvalidation_0-auc:0.841531\n",
            "[42]\tvalidation_0-auc:0.842033\n",
            "[43]\tvalidation_0-auc:0.841583\n",
            "[44]\tvalidation_0-auc:0.842172\n",
            "[45]\tvalidation_0-auc:0.842562\n",
            "[46]\tvalidation_0-auc:0.842388\n",
            "[47]\tvalidation_0-auc:0.842302\n",
            "[48]\tvalidation_0-auc:0.841999\n",
            "[49]\tvalidation_0-auc:0.841141\n",
            "[50]\tvalidation_0-auc:0.840933\n",
            "[51]\tvalidation_0-auc:0.840708\n",
            "[52]\tvalidation_0-auc:0.84037\n",
            "[53]\tvalidation_0-auc:0.840301\n",
            "[54]\tvalidation_0-auc:0.840145\n",
            "[55]\tvalidation_0-auc:0.839885\n",
            "[56]\tvalidation_0-auc:0.839816\n",
            "[57]\tvalidation_0-auc:0.839331\n",
            "[58]\tvalidation_0-auc:0.83992\n",
            "[59]\tvalidation_0-auc:0.839746\n",
            "[60]\tvalidation_0-auc:0.839123\n",
            "[61]\tvalidation_0-auc:0.838742\n",
            "[62]\tvalidation_0-auc:0.83862\n",
            "[63]\tvalidation_0-auc:0.838395\n",
            "[64]\tvalidation_0-auc:0.838542\n",
            "[65]\tvalidation_0-auc:0.838525\n",
            "[66]\tvalidation_0-auc:0.837901\n",
            "[67]\tvalidation_0-auc:0.837641\n",
            "[68]\tvalidation_0-auc:0.837226\n",
            "[69]\tvalidation_0-auc:0.837434\n",
            "[70]\tvalidation_0-auc:0.836637\n",
            "[71]\tvalidation_0-auc:0.837087\n",
            "[72]\tvalidation_0-auc:0.836758\n",
            "[73]\tvalidation_0-auc:0.836567\n",
            "[74]\tvalidation_0-auc:0.836359\n",
            "[75]\tvalidation_0-auc:0.835701\n",
            "[76]\tvalidation_0-auc:0.834506\n",
            "[77]\tvalidation_0-auc:0.834523\n",
            "Stopping. Best iteration:\n",
            "[27]\tvalidation_0-auc:0.846616\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6824 - accuracy: 0.5710 - val_loss: 0.6853 - val_accuracy: 0.5886\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6812 - accuracy: 0.5697 - val_loss: 0.6718 - val_accuracy: 0.6061\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6555 - accuracy: 0.6239 - val_loss: 0.6347 - val_accuracy: 0.6674\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5977 - accuracy: 0.6898 - val_loss: 0.5495 - val_accuracy: 0.7265\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5508 - accuracy: 0.7008 - val_loss: 0.5717 - val_accuracy: 0.7155\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.6479 - accuracy: 0.6259 - val_loss: 0.6387 - val_accuracy: 0.6193\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5577 - accuracy: 0.7035 - val_loss: 0.5417 - val_accuracy: 0.7177\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5282 - accuracy: 0.7461 - val_loss: 0.5395 - val_accuracy: 0.7374\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5076 - accuracy: 0.7618 - val_loss: 0.5224 - val_accuracy: 0.7637\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5140 - accuracy: 0.7474 - val_loss: 0.5359 - val_accuracy: 0.7309\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.699837\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.699971\n",
            "[2]\tvalidation_0-auc:0.724162\n",
            "[3]\tvalidation_0-auc:0.725356\n",
            "[4]\tvalidation_0-auc:0.725573\n",
            "[5]\tvalidation_0-auc:0.727766\n",
            "[6]\tvalidation_0-auc:0.733368\n",
            "[7]\tvalidation_0-auc:0.74103\n",
            "[8]\tvalidation_0-auc:0.743677\n",
            "[9]\tvalidation_0-auc:0.743656\n",
            "[10]\tvalidation_0-auc:0.748785\n",
            "[11]\tvalidation_0-auc:0.751493\n",
            "[12]\tvalidation_0-auc:0.747745\n",
            "[13]\tvalidation_0-auc:0.74827\n",
            "[14]\tvalidation_0-auc:0.746385\n",
            "[15]\tvalidation_0-auc:0.746303\n",
            "[16]\tvalidation_0-auc:0.745211\n",
            "[17]\tvalidation_0-auc:0.747971\n",
            "[18]\tvalidation_0-auc:0.747992\n",
            "[19]\tvalidation_0-auc:0.751617\n",
            "[20]\tvalidation_0-auc:0.747724\n",
            "[21]\tvalidation_0-auc:0.748115\n",
            "[22]\tvalidation_0-auc:0.75069\n",
            "[23]\tvalidation_0-auc:0.752647\n",
            "[24]\tvalidation_0-auc:0.756663\n",
            "[25]\tvalidation_0-auc:0.756714\n",
            "[26]\tvalidation_0-auc:0.75622\n",
            "[27]\tvalidation_0-auc:0.755108\n",
            "[28]\tvalidation_0-auc:0.756941\n",
            "[29]\tvalidation_0-auc:0.758259\n",
            "[30]\tvalidation_0-auc:0.757126\n",
            "[31]\tvalidation_0-auc:0.756539\n",
            "[32]\tvalidation_0-auc:0.756169\n",
            "[33]\tvalidation_0-auc:0.756086\n",
            "[34]\tvalidation_0-auc:0.756035\n",
            "[35]\tvalidation_0-auc:0.758239\n",
            "[36]\tvalidation_0-auc:0.758053\n",
            "[37]\tvalidation_0-auc:0.758578\n",
            "[38]\tvalidation_0-auc:0.759155\n",
            "[39]\tvalidation_0-auc:0.758723\n",
            "[40]\tvalidation_0-auc:0.761915\n",
            "[41]\tvalidation_0-auc:0.762502\n",
            "[42]\tvalidation_0-auc:0.762749\n",
            "[43]\tvalidation_0-auc:0.762111\n",
            "[44]\tvalidation_0-auc:0.762646\n",
            "[45]\tvalidation_0-auc:0.761791\n",
            "[46]\tvalidation_0-auc:0.76348\n",
            "[47]\tvalidation_0-auc:0.765437\n",
            "[48]\tvalidation_0-auc:0.764181\n",
            "[49]\tvalidation_0-auc:0.763604\n",
            "[50]\tvalidation_0-auc:0.762986\n",
            "[51]\tvalidation_0-auc:0.76313\n",
            "[52]\tvalidation_0-auc:0.763449\n",
            "[53]\tvalidation_0-auc:0.764356\n",
            "[54]\tvalidation_0-auc:0.764624\n",
            "[55]\tvalidation_0-auc:0.764294\n",
            "[56]\tvalidation_0-auc:0.765715\n",
            "[57]\tvalidation_0-auc:0.765736\n",
            "[58]\tvalidation_0-auc:0.765715\n",
            "[59]\tvalidation_0-auc:0.764912\n",
            "[60]\tvalidation_0-auc:0.766415\n",
            "[61]\tvalidation_0-auc:0.766354\n",
            "[62]\tvalidation_0-auc:0.767651\n",
            "[63]\tvalidation_0-auc:0.76831\n",
            "[64]\tvalidation_0-auc:0.768599\n",
            "[65]\tvalidation_0-auc:0.770061\n",
            "[66]\tvalidation_0-auc:0.770514\n",
            "[67]\tvalidation_0-auc:0.76969\n",
            "[68]\tvalidation_0-auc:0.769917\n",
            "[69]\tvalidation_0-auc:0.770391\n",
            "[70]\tvalidation_0-auc:0.770329\n",
            "[71]\tvalidation_0-auc:0.770308\n",
            "[72]\tvalidation_0-auc:0.770452\n",
            "[73]\tvalidation_0-auc:0.770452\n",
            "[74]\tvalidation_0-auc:0.769567\n",
            "[75]\tvalidation_0-auc:0.768619\n",
            "[76]\tvalidation_0-auc:0.768146\n",
            "[77]\tvalidation_0-auc:0.768104\n",
            "[78]\tvalidation_0-auc:0.768599\n",
            "[79]\tvalidation_0-auc:0.770308\n",
            "[80]\tvalidation_0-auc:0.76864\n",
            "[81]\tvalidation_0-auc:0.768588\n",
            "[82]\tvalidation_0-auc:0.768259\n",
            "[83]\tvalidation_0-auc:0.768279\n",
            "[84]\tvalidation_0-auc:0.769227\n",
            "[85]\tvalidation_0-auc:0.768835\n",
            "[86]\tvalidation_0-auc:0.769701\n",
            "[87]\tvalidation_0-auc:0.770483\n",
            "[88]\tvalidation_0-auc:0.770772\n",
            "[89]\tvalidation_0-auc:0.77106\n",
            "[90]\tvalidation_0-auc:0.771719\n",
            "[91]\tvalidation_0-auc:0.771348\n",
            "[92]\tvalidation_0-auc:0.77141\n",
            "[93]\tvalidation_0-auc:0.771101\n",
            "[94]\tvalidation_0-auc:0.771122\n",
            "[95]\tvalidation_0-auc:0.771019\n",
            "[96]\tvalidation_0-auc:0.77141\n",
            "[97]\tvalidation_0-auc:0.772049\n",
            "[98]\tvalidation_0-auc:0.772543\n",
            "[99]\tvalidation_0-auc:0.772512\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.763265306122449  | 0.6901408450704225 | 0.7461928934010152 | 0.7170731707317074 |\n",
            "|     GRU 0.1      | 0.7714285714285715 | 0.7272727272727273 | 0.6903553299492385 | 0.7083333333333331 |\n",
            "|   XGBoost 0.1    | 0.7816326530612245 | 0.7586206896551724 | 0.6700507614213198 | 0.7115902964959568 |\n",
            "|    Logreg 0.1    | 0.7877551020408163 | 0.7961783439490446 | 0.6345177664974619 | 0.7062146892655368 |\n",
            "|     SVM 0.1      | 0.7755102040816326 | 0.7636363636363637 | 0.6395939086294417 | 0.696132596685083  |\n",
            "|  LSTM beta 0.1   | 0.7155361050328227 | 0.6130952380952381 | 0.6130952380952381 | 0.6130952380952381 |\n",
            "|   GRU beta 0.1   | 0.7308533916849015 | 0.6470588235294118 | 0.5892857142857143 | 0.6168224299065421 |\n",
            "| XGBoost beta 0.1 | 0.7286652078774617 |       0.625        | 0.6547619047619048 | 0.6395348837209303 |\n",
            "| logreg beta 0.1  | 0.7199124726477024 | 0.644927536231884  | 0.5297619047619048 | 0.5816993464052287 |\n",
            "|   svm beta 0.1   | 0.7111597374179431 | 0.618421052631579  | 0.5595238095238095 |       0.5875       |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6752 - accuracy: 0.5685 - val_loss: 0.8413 - val_accuracy: 0.4082\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6199 - accuracy: 0.6564 - val_loss: 0.6053 - val_accuracy: 0.7510\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6032 - accuracy: 0.6785 - val_loss: 0.6040 - val_accuracy: 0.6633\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5870 - accuracy: 0.7034 - val_loss: 0.5934 - val_accuracy: 0.6857\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5889 - accuracy: 0.7054 - val_loss: 0.6343 - val_accuracy: 0.6306\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6517 - accuracy: 0.6054 - val_loss: 0.6146 - val_accuracy: 0.6571\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5854 - accuracy: 0.7000 - val_loss: 0.6178 - val_accuracy: 0.6612\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5549 - accuracy: 0.7081 - val_loss: 0.5683 - val_accuracy: 0.7204\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5639 - accuracy: 0.7013 - val_loss: 0.6013 - val_accuracy: 0.6796\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5677 - accuracy: 0.7141 - val_loss: 0.5800 - val_accuracy: 0.7102\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.786342\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.797189\n",
            "[2]\tvalidation_0-auc:0.793366\n",
            "[3]\tvalidation_0-auc:0.795845\n",
            "[4]\tvalidation_0-auc:0.795322\n",
            "[5]\tvalidation_0-auc:0.797643\n",
            "[6]\tvalidation_0-auc:0.796833\n",
            "[7]\tvalidation_0-auc:0.796764\n",
            "[8]\tvalidation_0-auc:0.798364\n",
            "[9]\tvalidation_0-auc:0.794768\n",
            "[10]\tvalidation_0-auc:0.796349\n",
            "[11]\tvalidation_0-auc:0.797139\n",
            "[12]\tvalidation_0-auc:0.799036\n",
            "[13]\tvalidation_0-auc:0.799105\n",
            "[14]\tvalidation_0-auc:0.798038\n",
            "[15]\tvalidation_0-auc:0.797771\n",
            "[16]\tvalidation_0-auc:0.797643\n",
            "[17]\tvalidation_0-auc:0.796537\n",
            "[18]\tvalidation_0-auc:0.796645\n",
            "[19]\tvalidation_0-auc:0.796744\n",
            "[20]\tvalidation_0-auc:0.79626\n",
            "[21]\tvalidation_0-auc:0.796063\n",
            "[22]\tvalidation_0-auc:0.796112\n",
            "[23]\tvalidation_0-auc:0.795658\n",
            "[24]\tvalidation_0-auc:0.795835\n",
            "[25]\tvalidation_0-auc:0.79546\n",
            "[26]\tvalidation_0-auc:0.794186\n",
            "[27]\tvalidation_0-auc:0.79466\n",
            "[28]\tvalidation_0-auc:0.794601\n",
            "[29]\tvalidation_0-auc:0.794136\n",
            "[30]\tvalidation_0-auc:0.795401\n",
            "[31]\tvalidation_0-auc:0.795006\n",
            "[32]\tvalidation_0-auc:0.79461\n",
            "[33]\tvalidation_0-auc:0.794037\n",
            "[34]\tvalidation_0-auc:0.793741\n",
            "[35]\tvalidation_0-auc:0.793563\n",
            "[36]\tvalidation_0-auc:0.793415\n",
            "[37]\tvalidation_0-auc:0.793079\n",
            "[38]\tvalidation_0-auc:0.792704\n",
            "[39]\tvalidation_0-auc:0.792684\n",
            "[40]\tvalidation_0-auc:0.79142\n",
            "[41]\tvalidation_0-auc:0.791025\n",
            "[42]\tvalidation_0-auc:0.791805\n",
            "[43]\tvalidation_0-auc:0.791706\n",
            "[44]\tvalidation_0-auc:0.791686\n",
            "[45]\tvalidation_0-auc:0.792368\n",
            "[46]\tvalidation_0-auc:0.791736\n",
            "[47]\tvalidation_0-auc:0.791341\n",
            "[48]\tvalidation_0-auc:0.791025\n",
            "[49]\tvalidation_0-auc:0.790955\n",
            "[50]\tvalidation_0-auc:0.790955\n",
            "[51]\tvalidation_0-auc:0.790501\n",
            "[52]\tvalidation_0-auc:0.789553\n",
            "[53]\tvalidation_0-auc:0.789533\n",
            "[54]\tvalidation_0-auc:0.789335\n",
            "[55]\tvalidation_0-auc:0.789335\n",
            "[56]\tvalidation_0-auc:0.789049\n",
            "[57]\tvalidation_0-auc:0.789424\n",
            "[58]\tvalidation_0-auc:0.789088\n",
            "[59]\tvalidation_0-auc:0.788219\n",
            "[60]\tvalidation_0-auc:0.788753\n",
            "[61]\tvalidation_0-auc:0.78814\n",
            "[62]\tvalidation_0-auc:0.788476\n",
            "[63]\tvalidation_0-auc:0.788101\n",
            "Stopping. Best iteration:\n",
            "[13]\tvalidation_0-auc:0.799105\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6617 - accuracy: 0.5875 - val_loss: 0.6295 - val_accuracy: 0.7462\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6079 - accuracy: 0.6555 - val_loss: 0.5884 - val_accuracy: 0.7396\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5795 - accuracy: 0.7042 - val_loss: 0.7090 - val_accuracy: 0.6127\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5451 - accuracy: 0.7152 - val_loss: 0.6262 - val_accuracy: 0.6608\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5312 - accuracy: 0.7502 - val_loss: 0.6173 - val_accuracy: 0.6718\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6234 - accuracy: 0.6644 - val_loss: 0.6093 - val_accuracy: 0.6696\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5262 - accuracy: 0.7234 - val_loss: 0.6121 - val_accuracy: 0.6980\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5360 - accuracy: 0.7296 - val_loss: 0.6510 - val_accuracy: 0.6674\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5146 - accuracy: 0.7447 - val_loss: 0.7266 - val_accuracy: 0.6105\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5254 - accuracy: 0.7330 - val_loss: 0.5924 - val_accuracy: 0.6958\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.697168\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.689734\n",
            "[2]\tvalidation_0-auc:0.695652\n",
            "[3]\tvalidation_0-auc:0.709177\n",
            "[4]\tvalidation_0-auc:0.708306\n",
            "[5]\tvalidation_0-auc:0.707063\n",
            "[6]\tvalidation_0-auc:0.709674\n",
            "[7]\tvalidation_0-auc:0.713179\n",
            "[8]\tvalidation_0-auc:0.72259\n",
            "[9]\tvalidation_0-auc:0.727202\n",
            "[10]\tvalidation_0-auc:0.728942\n",
            "[11]\tvalidation_0-auc:0.729464\n",
            "[12]\tvalidation_0-auc:0.729054\n",
            "[13]\tvalidation_0-auc:0.731465\n",
            "[14]\tvalidation_0-auc:0.730595\n",
            "[15]\tvalidation_0-auc:0.730123\n",
            "[16]\tvalidation_0-auc:0.72888\n",
            "[17]\tvalidation_0-auc:0.729588\n",
            "[18]\tvalidation_0-auc:0.731192\n",
            "[19]\tvalidation_0-auc:0.729812\n",
            "[20]\tvalidation_0-auc:0.728967\n",
            "[21]\tvalidation_0-auc:0.730123\n",
            "[22]\tvalidation_0-auc:0.733044\n",
            "[23]\tvalidation_0-auc:0.732746\n",
            "[24]\tvalidation_0-auc:0.732447\n",
            "[25]\tvalidation_0-auc:0.732162\n",
            "[26]\tvalidation_0-auc:0.734063\n",
            "[27]\tvalidation_0-auc:0.734474\n",
            "[28]\tvalidation_0-auc:0.733753\n",
            "[29]\tvalidation_0-auc:0.732833\n",
            "[30]\tvalidation_0-auc:0.733181\n",
            "[31]\tvalidation_0-auc:0.733653\n",
            "[32]\tvalidation_0-auc:0.734126\n",
            "[33]\tvalidation_0-auc:0.73517\n",
            "[34]\tvalidation_0-auc:0.735791\n",
            "[35]\tvalidation_0-auc:0.735294\n",
            "[36]\tvalidation_0-auc:0.736972\n",
            "[37]\tvalidation_0-auc:0.736624\n",
            "[38]\tvalidation_0-auc:0.735605\n",
            "[39]\tvalidation_0-auc:0.736376\n",
            "[40]\tvalidation_0-auc:0.735729\n",
            "[41]\tvalidation_0-auc:0.73461\n",
            "[42]\tvalidation_0-auc:0.734834\n",
            "[43]\tvalidation_0-auc:0.735456\n",
            "[44]\tvalidation_0-auc:0.736649\n",
            "[45]\tvalidation_0-auc:0.736226\n",
            "[46]\tvalidation_0-auc:0.737022\n",
            "[47]\tvalidation_0-auc:0.736624\n",
            "[48]\tvalidation_0-auc:0.737644\n",
            "[49]\tvalidation_0-auc:0.736947\n",
            "[50]\tvalidation_0-auc:0.73788\n",
            "[51]\tvalidation_0-auc:0.73972\n",
            "[52]\tvalidation_0-auc:0.740117\n",
            "[53]\tvalidation_0-auc:0.739844\n",
            "[54]\tvalidation_0-auc:0.738825\n",
            "[55]\tvalidation_0-auc:0.738999\n",
            "[56]\tvalidation_0-auc:0.740615\n",
            "[57]\tvalidation_0-auc:0.740739\n",
            "[58]\tvalidation_0-auc:0.740565\n",
            "[59]\tvalidation_0-auc:0.740664\n",
            "[60]\tvalidation_0-auc:0.739943\n",
            "[61]\tvalidation_0-auc:0.738054\n",
            "[62]\tvalidation_0-auc:0.738327\n",
            "[63]\tvalidation_0-auc:0.738476\n",
            "[64]\tvalidation_0-auc:0.738626\n",
            "[65]\tvalidation_0-auc:0.739123\n",
            "[66]\tvalidation_0-auc:0.738551\n",
            "[67]\tvalidation_0-auc:0.738775\n",
            "[68]\tvalidation_0-auc:0.737979\n",
            "[69]\tvalidation_0-auc:0.739794\n",
            "[70]\tvalidation_0-auc:0.740291\n",
            "[71]\tvalidation_0-auc:0.741684\n",
            "[72]\tvalidation_0-auc:0.742628\n",
            "[73]\tvalidation_0-auc:0.741485\n",
            "[74]\tvalidation_0-auc:0.741907\n",
            "[75]\tvalidation_0-auc:0.739869\n",
            "[76]\tvalidation_0-auc:0.739446\n",
            "[77]\tvalidation_0-auc:0.739123\n",
            "[78]\tvalidation_0-auc:0.740341\n",
            "[79]\tvalidation_0-auc:0.740739\n",
            "[80]\tvalidation_0-auc:0.74049\n",
            "[81]\tvalidation_0-auc:0.740963\n",
            "[82]\tvalidation_0-auc:0.741261\n",
            "[83]\tvalidation_0-auc:0.740739\n",
            "[84]\tvalidation_0-auc:0.740813\n",
            "[85]\tvalidation_0-auc:0.742032\n",
            "[86]\tvalidation_0-auc:0.741584\n",
            "[87]\tvalidation_0-auc:0.740938\n",
            "[88]\tvalidation_0-auc:0.741087\n",
            "[89]\tvalidation_0-auc:0.741162\n",
            "[90]\tvalidation_0-auc:0.740167\n",
            "[91]\tvalidation_0-auc:0.740863\n",
            "[92]\tvalidation_0-auc:0.740789\n",
            "[93]\tvalidation_0-auc:0.740615\n",
            "[94]\tvalidation_0-auc:0.739297\n",
            "[95]\tvalidation_0-auc:0.738029\n",
            "[96]\tvalidation_0-auc:0.738327\n",
            "[97]\tvalidation_0-auc:0.738302\n",
            "[98]\tvalidation_0-auc:0.738278\n",
            "[99]\tvalidation_0-auc:0.738029\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "|     LSTM 0.2     | 0.6306122448979592 | 0.44086021505376344 | 0.831081081081081  | 0.5761124121779859 |\n",
            "|     GRU 0.2      | 0.710204081632653  |  0.5128205128205128 | 0.8108108108108109 | 0.6282722513089004 |\n",
            "|   XGBoost 0.2    | 0.7551020408163265 |  0.5760869565217391 | 0.7162162162162162 | 0.6385542168674698 |\n",
            "|    Logreg 0.2    | 0.736734693877551  |  0.5458937198067633 | 0.7635135135135135 | 0.6366197183098592 |\n",
            "|     SVM 0.2      | 0.753061224489796  |  0.5685279187817259 | 0.7567567567567568 | 0.6492753623188405 |\n",
            "|  LSTM beta 0.2   | 0.6717724288840262 |  0.4205128205128205 | 0.6890756302521008 | 0.5222929936305732 |\n",
            "|   GRU beta 0.2   | 0.6958424507658644 | 0.44047619047619047 | 0.6218487394957983 | 0.5156794425087108 |\n",
            "| XGBoost beta 0.2 | 0.649890590809628  | 0.40375586854460094 | 0.7226890756302521 | 0.5180722891566265 |\n",
            "| logreg beta 0.2  | 0.6892778993435449 |  0.4327485380116959 | 0.6218487394957983 | 0.5103448275862069 |\n",
            "|   svm beta 0.2   | 0.6389496717724289 | 0.38613861386138615 | 0.6554621848739496 | 0.4859813084112149 |\n",
            "+------------------+--------------------+---------------------+--------------------+--------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6575 - accuracy: 0.5973 - val_loss: 0.6928 - val_accuracy: 0.5286\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5983 - accuracy: 0.6872 - val_loss: 0.5617 - val_accuracy: 0.7551\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6005 - accuracy: 0.6899 - val_loss: 0.5506 - val_accuracy: 0.7510\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5710 - accuracy: 0.7195 - val_loss: 0.5610 - val_accuracy: 0.7184\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5640 - accuracy: 0.7295 - val_loss: 0.5343 - val_accuracy: 0.7551\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6500 - accuracy: 0.5960 - val_loss: 0.6376 - val_accuracy: 0.6408\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5513 - accuracy: 0.7242 - val_loss: 0.5114 - val_accuracy: 0.7633\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5390 - accuracy: 0.7329 - val_loss: 0.5502 - val_accuracy: 0.7102\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5139 - accuracy: 0.7396 - val_loss: 0.5090 - val_accuracy: 0.7449\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5234 - accuracy: 0.7497 - val_loss: 0.4971 - val_accuracy: 0.7755\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.808397\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.80781\n",
            "[2]\tvalidation_0-auc:0.808262\n",
            "[3]\tvalidation_0-auc:0.810076\n",
            "[4]\tvalidation_0-auc:0.816005\n",
            "[5]\tvalidation_0-auc:0.818541\n",
            "[6]\tvalidation_0-auc:0.81874\n",
            "[7]\tvalidation_0-auc:0.821447\n",
            "[8]\tvalidation_0-auc:0.821492\n",
            "[9]\tvalidation_0-auc:0.820752\n",
            "[10]\tvalidation_0-auc:0.822115\n",
            "[11]\tvalidation_0-auc:0.822702\n",
            "[12]\tvalidation_0-auc:0.822973\n",
            "[13]\tvalidation_0-auc:0.823947\n",
            "[14]\tvalidation_0-auc:0.825364\n",
            "[15]\tvalidation_0-auc:0.825815\n",
            "[16]\tvalidation_0-auc:0.826835\n",
            "[17]\tvalidation_0-auc:0.826799\n",
            "[18]\tvalidation_0-auc:0.827034\n",
            "[19]\tvalidation_0-auc:0.827828\n",
            "[20]\tvalidation_0-auc:0.828415\n",
            "[21]\tvalidation_0-auc:0.828397\n",
            "[22]\tvalidation_0-auc:0.829119\n",
            "[23]\tvalidation_0-auc:0.828965\n",
            "[24]\tvalidation_0-auc:0.829904\n",
            "[25]\tvalidation_0-auc:0.829046\n",
            "[26]\tvalidation_0-auc:0.82929\n",
            "[27]\tvalidation_0-auc:0.827503\n",
            "[28]\tvalidation_0-auc:0.828234\n",
            "[29]\tvalidation_0-auc:0.828126\n",
            "[30]\tvalidation_0-auc:0.827954\n",
            "[31]\tvalidation_0-auc:0.827223\n",
            "[32]\tvalidation_0-auc:0.827566\n",
            "[33]\tvalidation_0-auc:0.827684\n",
            "[34]\tvalidation_0-auc:0.827729\n",
            "[35]\tvalidation_0-auc:0.827458\n",
            "[36]\tvalidation_0-auc:0.826664\n",
            "[37]\tvalidation_0-auc:0.825978\n",
            "[38]\tvalidation_0-auc:0.826501\n",
            "[39]\tvalidation_0-auc:0.825743\n",
            "[40]\tvalidation_0-auc:0.825689\n",
            "[41]\tvalidation_0-auc:0.826014\n",
            "[42]\tvalidation_0-auc:0.825382\n",
            "[43]\tvalidation_0-auc:0.824588\n",
            "[44]\tvalidation_0-auc:0.824768\n",
            "[45]\tvalidation_0-auc:0.824732\n",
            "[46]\tvalidation_0-auc:0.824155\n",
            "[47]\tvalidation_0-auc:0.823451\n",
            "[48]\tvalidation_0-auc:0.823487\n",
            "[49]\tvalidation_0-auc:0.822973\n",
            "[50]\tvalidation_0-auc:0.823063\n",
            "[51]\tvalidation_0-auc:0.822991\n",
            "[52]\tvalidation_0-auc:0.822359\n",
            "[53]\tvalidation_0-auc:0.823162\n",
            "[54]\tvalidation_0-auc:0.823397\n",
            "[55]\tvalidation_0-auc:0.822945\n",
            "[56]\tvalidation_0-auc:0.823649\n",
            "[57]\tvalidation_0-auc:0.822512\n",
            "[58]\tvalidation_0-auc:0.822684\n",
            "[59]\tvalidation_0-auc:0.8229\n",
            "[60]\tvalidation_0-auc:0.823063\n",
            "[61]\tvalidation_0-auc:0.822828\n",
            "[62]\tvalidation_0-auc:0.822467\n",
            "[63]\tvalidation_0-auc:0.822404\n",
            "[64]\tvalidation_0-auc:0.822945\n",
            "[65]\tvalidation_0-auc:0.823126\n",
            "[66]\tvalidation_0-auc:0.823027\n",
            "[67]\tvalidation_0-auc:0.823171\n",
            "[68]\tvalidation_0-auc:0.822846\n",
            "[69]\tvalidation_0-auc:0.823009\n",
            "[70]\tvalidation_0-auc:0.82263\n",
            "[71]\tvalidation_0-auc:0.823207\n",
            "[72]\tvalidation_0-auc:0.823009\n",
            "[73]\tvalidation_0-auc:0.823424\n",
            "[74]\tvalidation_0-auc:0.823478\n",
            "Stopping. Best iteration:\n",
            "[24]\tvalidation_0-auc:0.829904\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6844 - accuracy: 0.5477 - val_loss: 0.7292 - val_accuracy: 0.3589\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6705 - accuracy: 0.5834 - val_loss: 0.7014 - val_accuracy: 0.5952\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6163 - accuracy: 0.6637 - val_loss: 0.6114 - val_accuracy: 0.6827\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5447 - accuracy: 0.7310 - val_loss: 0.5991 - val_accuracy: 0.6937\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5185 - accuracy: 0.7358 - val_loss: 0.5691 - val_accuracy: 0.7287\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6189 - accuracy: 0.6472 - val_loss: 0.6199 - val_accuracy: 0.6630\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5245 - accuracy: 0.7289 - val_loss: 0.5737 - val_accuracy: 0.7024\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5269 - accuracy: 0.7323 - val_loss: 0.5791 - val_accuracy: 0.6915\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5180 - accuracy: 0.7515 - val_loss: 0.6077 - val_accuracy: 0.6958\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5055 - accuracy: 0.7488 - val_loss: 0.5937 - val_accuracy: 0.7133\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.688019\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.722208\n",
            "[2]\tvalidation_0-auc:0.712346\n",
            "[3]\tvalidation_0-auc:0.693705\n",
            "[4]\tvalidation_0-auc:0.702353\n",
            "[5]\tvalidation_0-auc:0.707076\n",
            "[6]\tvalidation_0-auc:0.711744\n",
            "[7]\tvalidation_0-auc:0.697652\n",
            "[8]\tvalidation_0-auc:0.70654\n",
            "[9]\tvalidation_0-auc:0.708486\n",
            "[10]\tvalidation_0-auc:0.71122\n",
            "[11]\tvalidation_0-auc:0.717277\n",
            "[12]\tvalidation_0-auc:0.713395\n",
            "[13]\tvalidation_0-auc:0.71017\n",
            "[14]\tvalidation_0-auc:0.709995\n",
            "[15]\tvalidation_0-auc:0.707459\n",
            "[16]\tvalidation_0-auc:0.708388\n",
            "[17]\tvalidation_0-auc:0.707655\n",
            "[18]\tvalidation_0-auc:0.711602\n",
            "[19]\tvalidation_0-auc:0.712554\n",
            "[20]\tvalidation_0-auc:0.711898\n",
            "[21]\tvalidation_0-auc:0.711657\n",
            "[22]\tvalidation_0-auc:0.712991\n",
            "[23]\tvalidation_0-auc:0.713833\n",
            "[24]\tvalidation_0-auc:0.71403\n",
            "[25]\tvalidation_0-auc:0.715855\n",
            "[26]\tvalidation_0-auc:0.71544\n",
            "[27]\tvalidation_0-auc:0.717386\n",
            "[28]\tvalidation_0-auc:0.717474\n",
            "[29]\tvalidation_0-auc:0.718611\n",
            "[30]\tvalidation_0-auc:0.719267\n",
            "[31]\tvalidation_0-auc:0.719868\n",
            "[32]\tvalidation_0-auc:0.719704\n",
            "[33]\tvalidation_0-auc:0.71895\n",
            "[34]\tvalidation_0-auc:0.721049\n",
            "[35]\tvalidation_0-auc:0.722885\n",
            "[36]\tvalidation_0-auc:0.722361\n",
            "[37]\tvalidation_0-auc:0.725225\n",
            "[38]\tvalidation_0-auc:0.72516\n",
            "[39]\tvalidation_0-auc:0.723126\n",
            "[40]\tvalidation_0-auc:0.725925\n",
            "[41]\tvalidation_0-auc:0.72575\n",
            "[42]\tvalidation_0-auc:0.726504\n",
            "[43]\tvalidation_0-auc:0.726297\n",
            "[44]\tvalidation_0-auc:0.726319\n",
            "[45]\tvalidation_0-auc:0.725182\n",
            "[46]\tvalidation_0-auc:0.72516\n",
            "[47]\tvalidation_0-auc:0.725739\n",
            "[48]\tvalidation_0-auc:0.725433\n",
            "[49]\tvalidation_0-auc:0.725739\n",
            "[50]\tvalidation_0-auc:0.727204\n",
            "[51]\tvalidation_0-auc:0.727816\n",
            "[52]\tvalidation_0-auc:0.728735\n",
            "[53]\tvalidation_0-auc:0.728604\n",
            "[54]\tvalidation_0-auc:0.729773\n",
            "[55]\tvalidation_0-auc:0.72962\n",
            "[56]\tvalidation_0-auc:0.729314\n",
            "[57]\tvalidation_0-auc:0.729686\n",
            "[58]\tvalidation_0-auc:0.730408\n",
            "[59]\tvalidation_0-auc:0.730484\n",
            "[60]\tvalidation_0-auc:0.730375\n",
            "[61]\tvalidation_0-auc:0.729981\n",
            "[62]\tvalidation_0-auc:0.727795\n",
            "[63]\tvalidation_0-auc:0.72751\n",
            "[64]\tvalidation_0-auc:0.728604\n",
            "[65]\tvalidation_0-auc:0.729019\n",
            "[66]\tvalidation_0-auc:0.729675\n",
            "[67]\tvalidation_0-auc:0.730112\n",
            "[68]\tvalidation_0-auc:0.730069\n",
            "[69]\tvalidation_0-auc:0.730987\n",
            "[70]\tvalidation_0-auc:0.731206\n",
            "[71]\tvalidation_0-auc:0.732059\n",
            "[72]\tvalidation_0-auc:0.733567\n",
            "[73]\tvalidation_0-auc:0.732999\n",
            "[74]\tvalidation_0-auc:0.732999\n",
            "[75]\tvalidation_0-auc:0.733611\n",
            "[76]\tvalidation_0-auc:0.734923\n",
            "[77]\tvalidation_0-auc:0.734857\n",
            "[78]\tvalidation_0-auc:0.734639\n",
            "[79]\tvalidation_0-auc:0.734267\n",
            "[80]\tvalidation_0-auc:0.735295\n",
            "[81]\tvalidation_0-auc:0.735273\n",
            "[82]\tvalidation_0-auc:0.735732\n",
            "[83]\tvalidation_0-auc:0.735667\n",
            "[84]\tvalidation_0-auc:0.734879\n",
            "[85]\tvalidation_0-auc:0.734201\n",
            "[86]\tvalidation_0-auc:0.733677\n",
            "[87]\tvalidation_0-auc:0.733436\n",
            "[88]\tvalidation_0-auc:0.733589\n",
            "[89]\tvalidation_0-auc:0.733655\n",
            "[90]\tvalidation_0-auc:0.732999\n",
            "[91]\tvalidation_0-auc:0.733196\n",
            "[92]\tvalidation_0-auc:0.733611\n",
            "[93]\tvalidation_0-auc:0.732102\n",
            "[94]\tvalidation_0-auc:0.732299\n",
            "[95]\tvalidation_0-auc:0.732255\n",
            "[96]\tvalidation_0-auc:0.731523\n",
            "[97]\tvalidation_0-auc:0.732441\n",
            "[98]\tvalidation_0-auc:0.732179\n",
            "[99]\tvalidation_0-auc:0.730561\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.7551020408163265 | 0.6492146596858639 | 0.7005649717514124 | 0.6739130434782609 |\n",
            "|      GRU 0.15     | 0.7755102040816326 | 0.6830601092896175 | 0.7062146892655368 | 0.6944444444444444 |\n",
            "|    XGBoost 0.15   | 0.7714285714285715 | 0.6701570680628273 | 0.7231638418079096 | 0.6956521739130435 |\n",
            "|    Logreg 0.15    | 0.7551020408163265 | 0.6338028169014085 | 0.7627118644067796 | 0.6923076923076923 |\n",
            "|      SVM 0.15     | 0.773469387755102  |       0.665        | 0.751412429378531  | 0.7055702917771882 |\n",
            "|   LSTM beta 0.15  | 0.7286652078774617 | 0.5652173913043478 | 0.7027027027027027 | 0.6265060240963856 |\n",
            "|   GRU beta 0.15   | 0.7133479212253829 | 0.5435897435897435 | 0.7162162162162162 | 0.6180758017492711 |\n",
            "| XGBoost beta 0.15 | 0.6630196936542669 | 0.4861111111111111 | 0.7094594594594594 | 0.5769230769230769 |\n",
            "|  logreg beta 0.15 | 0.6958424507658644 | 0.5243243243243243 | 0.6554054054054054 | 0.5825825825825827 |\n",
            "|   svm beta 0.15   | 0.6783369803063457 | 0.5024875621890548 | 0.6824324324324325 | 0.5787965616045846 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGQlms8BXhm8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9683fa6e-d2e4-4cfd-f49d-af24229cd84f"
      },
      "source": [
        "Result_purging.to_csv('NKTR_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.690141</td>\n",
              "      <td>0.763265</td>\n",
              "      <td>0.717073</td>\n",
              "      <td>0.746193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.771429</td>\n",
              "      <td>0.708333</td>\n",
              "      <td>0.690355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.758621</td>\n",
              "      <td>0.781633</td>\n",
              "      <td>0.711590</td>\n",
              "      <td>0.670051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.796178</td>\n",
              "      <td>0.787755</td>\n",
              "      <td>0.706215</td>\n",
              "      <td>0.634518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.763636</td>\n",
              "      <td>0.775510</td>\n",
              "      <td>0.696133</td>\n",
              "      <td>0.639594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.613095</td>\n",
              "      <td>0.715536</td>\n",
              "      <td>0.613095</td>\n",
              "      <td>0.613095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.730853</td>\n",
              "      <td>0.616822</td>\n",
              "      <td>0.589286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.728665</td>\n",
              "      <td>0.639535</td>\n",
              "      <td>0.654762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.644928</td>\n",
              "      <td>0.719912</td>\n",
              "      <td>0.581699</td>\n",
              "      <td>0.529762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.618421</td>\n",
              "      <td>0.711160</td>\n",
              "      <td>0.587500</td>\n",
              "      <td>0.559524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.440860</td>\n",
              "      <td>0.630612</td>\n",
              "      <td>0.576112</td>\n",
              "      <td>0.831081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.512821</td>\n",
              "      <td>0.710204</td>\n",
              "      <td>0.628272</td>\n",
              "      <td>0.810811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.576087</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.638554</td>\n",
              "      <td>0.716216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.545894</td>\n",
              "      <td>0.736735</td>\n",
              "      <td>0.636620</td>\n",
              "      <td>0.763514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.568528</td>\n",
              "      <td>0.753061</td>\n",
              "      <td>0.649275</td>\n",
              "      <td>0.756757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.420513</td>\n",
              "      <td>0.671772</td>\n",
              "      <td>0.522293</td>\n",
              "      <td>0.689076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.440476</td>\n",
              "      <td>0.695842</td>\n",
              "      <td>0.515679</td>\n",
              "      <td>0.621849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.403756</td>\n",
              "      <td>0.649891</td>\n",
              "      <td>0.518072</td>\n",
              "      <td>0.722689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.432749</td>\n",
              "      <td>0.689278</td>\n",
              "      <td>0.510345</td>\n",
              "      <td>0.621849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.386139</td>\n",
              "      <td>0.638950</td>\n",
              "      <td>0.485981</td>\n",
              "      <td>0.655462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.649215</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.673913</td>\n",
              "      <td>0.700565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.683060</td>\n",
              "      <td>0.775510</td>\n",
              "      <td>0.694444</td>\n",
              "      <td>0.706215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.670157</td>\n",
              "      <td>0.771429</td>\n",
              "      <td>0.695652</td>\n",
              "      <td>0.723164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.633803</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.762712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.665000</td>\n",
              "      <td>0.773469</td>\n",
              "      <td>0.705570</td>\n",
              "      <td>0.751412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.565217</td>\n",
              "      <td>0.728665</td>\n",
              "      <td>0.626506</td>\n",
              "      <td>0.702703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.543590</td>\n",
              "      <td>0.713348</td>\n",
              "      <td>0.618076</td>\n",
              "      <td>0.716216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.486111</td>\n",
              "      <td>0.663020</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.709459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.524324</td>\n",
              "      <td>0.695842</td>\n",
              "      <td>0.582583</td>\n",
              "      <td>0.655405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>NKTR</td>\n",
              "      <td>0.502488</td>\n",
              "      <td>0.678337</td>\n",
              "      <td>0.578797</td>\n",
              "      <td>0.682432</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  NKTR  0.690141  0.763265  0.717073  0.746193\n",
              "1            GRU 0.1  NKTR  0.727273  0.771429  0.708333  0.690355\n",
              "2        XGBoost 0.1  NKTR  0.758621  0.781633  0.711590  0.670051\n",
              "3         Logreg 0.1  NKTR  0.796178  0.787755  0.706215  0.634518\n",
              "4            SVM 0.1  NKTR  0.763636  0.775510  0.696133  0.639594\n",
              "5      LSTM beta 0.1  NKTR  0.613095  0.715536  0.613095  0.613095\n",
              "6       GRU beta 0.1  NKTR  0.647059  0.730853  0.616822  0.589286\n",
              "7   XGBoost beta 0.1  NKTR  0.625000  0.728665  0.639535  0.654762\n",
              "8    logreg beta 0.1  NKTR  0.644928  0.719912  0.581699  0.529762\n",
              "9       svm beta 0.1  NKTR  0.618421  0.711160  0.587500  0.559524\n",
              "0           LSTM 0.2  NKTR  0.440860  0.630612  0.576112  0.831081\n",
              "1            GRU 0.2  NKTR  0.512821  0.710204  0.628272  0.810811\n",
              "2        XGBoost 0.2  NKTR  0.576087  0.755102  0.638554  0.716216\n",
              "3         Logreg 0.2  NKTR  0.545894  0.736735  0.636620  0.763514\n",
              "4            SVM 0.2  NKTR  0.568528  0.753061  0.649275  0.756757\n",
              "5      LSTM beta 0.2  NKTR  0.420513  0.671772  0.522293  0.689076\n",
              "6       GRU beta 0.2  NKTR  0.440476  0.695842  0.515679  0.621849\n",
              "7   XGBoost beta 0.2  NKTR  0.403756  0.649891  0.518072  0.722689\n",
              "8    logreg beta 0.2  NKTR  0.432749  0.689278  0.510345  0.621849\n",
              "9       svm beta 0.2  NKTR  0.386139  0.638950  0.485981  0.655462\n",
              "0          LSTM 0.15  NKTR  0.649215  0.755102  0.673913  0.700565\n",
              "1           GRU 0.15  NKTR  0.683060  0.775510  0.694444  0.706215\n",
              "2       XGBoost 0.15  NKTR  0.670157  0.771429  0.695652  0.723164\n",
              "3        Logreg 0.15  NKTR  0.633803  0.755102  0.692308  0.762712\n",
              "4           SVM 0.15  NKTR  0.665000  0.773469  0.705570  0.751412\n",
              "5     LSTM beta 0.15  NKTR  0.565217  0.728665  0.626506  0.702703\n",
              "6      GRU beta 0.15  NKTR  0.543590  0.713348  0.618076  0.716216\n",
              "7  XGBoost beta 0.15  NKTR  0.486111  0.663020  0.576923  0.709459\n",
              "8   logreg beta 0.15  NKTR  0.524324  0.695842  0.582583  0.655405\n",
              "9      svm beta 0.15  NKTR  0.502488  0.678337  0.578797  0.682432"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ryC_d4SXhm8"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NKTR_logreg_beta_p.csv')"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXdmJJvjXhm8"
      },
      "source": [
        ""
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmO44zEWX4gq"
      },
      "source": [
        "## NVDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-EAGlliX4gw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "bc15293b-c464-4050-ac4f-3a5b8c31ce5b"
      },
      "source": [
        "dfs = pd.read_csv(\"NVDA.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "dfs"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.NVDA</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>207.70</td>\n",
              "      <td>208.485</td>\n",
              "      <td>202.04</td>\n",
              "      <td>207.23</td>\n",
              "      <td>657406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.NVDA</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>207.89</td>\n",
              "      <td>210.650</td>\n",
              "      <td>206.89</td>\n",
              "      <td>207.12</td>\n",
              "      <td>537697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.NVDA</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>209.33</td>\n",
              "      <td>210.160</td>\n",
              "      <td>204.68</td>\n",
              "      <td>205.22</td>\n",
              "      <td>606902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2763</td>\n",
              "      <td>US1.NVDA</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>212.01</td>\n",
              "      <td>214.030</td>\n",
              "      <td>206.51</td>\n",
              "      <td>207.02</td>\n",
              "      <td>835341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2762</td>\n",
              "      <td>US1.NVDA</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>217.16</td>\n",
              "      <td>217.750</td>\n",
              "      <td>213.28</td>\n",
              "      <td>216.57</td>\n",
              "      <td>625939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2762</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.NVDA</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>10.65</td>\n",
              "      <td>10.950</td>\n",
              "      <td>10.51</td>\n",
              "      <td>10.86</td>\n",
              "      <td>16998198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2763</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.NVDA</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>10.82</td>\n",
              "      <td>10.840</td>\n",
              "      <td>10.38</td>\n",
              "      <td>10.70</td>\n",
              "      <td>18732301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.NVDA</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>11.32</td>\n",
              "      <td>11.370</td>\n",
              "      <td>10.67</td>\n",
              "      <td>10.78</td>\n",
              "      <td>25753399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.NVDA</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>11.48</td>\n",
              "      <td>11.500</td>\n",
              "      <td>11.29</td>\n",
              "      <td>11.32</td>\n",
              "      <td>18266877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.NVDA</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>11.24</td>\n",
              "      <td>11.430</td>\n",
              "      <td>11.01</td>\n",
              "      <td>11.23</td>\n",
              "      <td>18184874</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2767 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  <TICKER> <PER>    <DATE>  ...   <HIGH>   <LOW>  <CLOSE>     <VOL>\n",
              "0      2766  US1.NVDA     D  20211001  ...  208.485  202.04   207.23    657406\n",
              "1      2765  US1.NVDA     D  20210930  ...  210.650  206.89   207.12    537697\n",
              "2      2764  US1.NVDA     D  20210929  ...  210.160  204.68   205.22    606902\n",
              "3      2763  US1.NVDA     D  20210928  ...  214.030  206.51   207.02    835341\n",
              "4      2762  US1.NVDA     D  20210927  ...  217.750  213.28   216.57    625939\n",
              "...     ...       ...   ...       ...  ...      ...     ...      ...       ...\n",
              "2762      4  US1.NVDA     D  20101008  ...   10.950   10.51    10.86  16998198\n",
              "2763      3  US1.NVDA     D  20101007  ...   10.840   10.38    10.70  18732301\n",
              "2764      2  US1.NVDA     D  20101006  ...   11.370   10.67    10.78  25753399\n",
              "2765      1  US1.NVDA     D  20101005  ...   11.500   11.29    11.32  18266877\n",
              "2766      0  US1.NVDA     D  20101004  ...   11.430   11.01    11.23  18184874\n",
              "\n",
              "[2767 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkX_oC6eX4gx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4f9205ed-c1fc-43e5-f191-c036feaf4475"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"00cd1d2d-13d8-4391-aafb-279f178593e7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"00cd1d2d-13d8-4391-aafb-279f178593e7\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '00cd1d2d-13d8-4391-aafb-279f178593e7',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [182.04, 181.32, 173.12, 173.94, 174.07, 171.79, 177.37, 178.27, 172.56, 174.84, 172.62, 176.91, 179.99, 181.02, 180.2, 181.92, 184.26, 184.41, 183.24, 180.44, 178.62, 179.76, 168.76, 164.16, 167.51, 166.99, 161.21, 161.81, 165.46, 162.52, 171.46, 171.24, 167.84, 170.77, 159.49, 148.7975, 150.07, 156.08, 151.45, 154.18, 158.31, 153.97, 152.34, 150.73, 161.17, 164.92, 168.72, 175.46, 174.82, 175.04, 173.32, 178.7, 175.71, 171.31, 168.44, 170.18, 169.69, 167.07, 167.31, 167.6, 166.28, 160.06, 157.32, 157.26, 160.33, 162.4963, 162.2201, 166.2, 164.16, 163.23, 159.25, 151.48, 152.66, 151.76, 154.1, 153.11, 152.89, 145.02, 144.58, 148.32, 146.165, 150.76, 148.45, 145.5, 143.79, 141.33, 143.02, 133.78, 135.46, 139.11, 140.31, 143.3, 145.13, 147.37, 152.2, 155.06, 151.67, 156.535, 160.24, 159.58, 162.03, 158.4, 168.84, 170.2, 173.88, 173.2, 179.88, 183.0, 183.19, 180.47, 181.09, 179.36, 178.09, 186.84, 191.17, 190.67, 188.45, 186.3, 187.27, 188.28, 184.7, 190.0, 191.51, 192.149, 189.28, 191.78, 190.95, 188.285, 188.62, 183.0, 182.31, 179.58, 177.22, 176.59, 176.95, 173.78, 177.5, 183.9, 174.4, 175.74, 168.97, 169.82, 165.56, 168.62, 162.51, 161.13, 150.67, 149.25, 152.04, 156.51, 156.8, 156.44, 154.29, 155.4, 157.14, 158.69, 159.22, 155.81, 158.57, 156.63, 157.3, 154.53, 152.87, 151.14, 146.47, 148.22, 147.42, 153.0, 149.95, 149.19, 144.73, 143.75, 137.46, 131.55, 137.96, 160.15, 157.83, 149.3, 148.84, 156.93, 151.72, 148.84, 149.86, 150.38, 148.8, 145.17, 142.69, 139.82, 143.4, 136.19, 127.99, 136.22, 133.5, 133.65, 131.62, 133.12, 126.8898, 129.58, 135.16, 138.49, 146.94, 143.61, 146.42, 148.88, 148.85, 148.21, 151.86, 147.6, 158.3, 157.16, 170.05, 163.67, 157.37, 160.08, 153.73, 153.05, 145.16, 144.71, 149.08, 144.7, 164.38, 202.49, 197.2, 199.33, 189.44, 205.6103, 205.98, 213.78, 211.06, 211.77, 214.87, 218.06, 210.88, 203.01, 185.61, 198.29, 207.89, 199.32, 221.05, 231.14, 229.12, 239.57, 243.06, 245.59, 235.38, 246.57, 235.12, 245.71, 265.53, 265.75, 269.86, 279.28, 286.73, 286.52, 289.37, 281.12, 267.4, 266.92, 268.4, 265.71, 263.44, 266.27, 271.89, 271.015, 273.93, 276.45, 271.34, 268.2, 272.8, 274.74, 271.88, 272.75, 278.44, 283.7, 280.71, 277.8, 278.49, 274.4, 275.89, 272.28, 266.87, 262.83, 253.32, 247.85, 244.9, 257.53, 259.02, 261.46, 256.12, 254.79, 256.45, 258.4, 256.91, 254.09, 252.12, 250.62, 246.57, 244.83, 244.14, 251.99, 254.88, 251.89, 248.7, 249.38, 250.89, 252.01, 251.69, 253.68, 248.2, 249.32, 251.23, 247.47, 253.25, 249.25, 247.32, 242.74, 237.1, 242.24, 236.9, 240.86, 235.59, 241.97, 239.16, 250.76, 257.19, 262.29, 260.17, 265.08, 265.38, 266.9, 262.37, 262.73, 260.6, 262.17, 262.9, 265.27, 265.091, 264.85, 257.59, 252.22, 252.99, 248.59, 249.29, 247.74, 247.57, 242.7, 244.26, 245.91, 247.71, 246.1, 245.51, 255.37, 254.53, 260.24, 255.87, 250.3969, 248.69, 239.08, 232.99, 226.31, 227.1, 224.97, 226.34, 225.15, 216.66, 221.24, 223.88, 228.71, 229.05, 236.36, 237.55, 231.47, 231.49, 234.65, 226.24, 227.91, 215.41, 214.17, 221.36, 226.18, 225.35, 221.05, 232.31, 221.31, 225.51, 244.39, 232.925, 241.85, 248.54, 249.58, 240.99, 250.48, 249.35, 248.74, 247.71, 249.76, 245.33, 241.17, 241.84, 242.16, 235.6399, 236.54, 232.18, 242.01, 246.06, 246.5, 245.93, 242.15, 241.51, 249.07, 243.84, 246.5, 241.42, 232.69, 228.02, 231.46, 217.73, 228.81, 225.56, 213.67, 233.51, 240.48, 245.83, 242.7, 246.78, 243.27, 236.34, 235.84, 238.94, 233.67, 230.08, 224.44, 224.7, 220.17, 222.69, 224.05, 223.66, 221.95, 221.98, 215.28, 213.59, 212.5, 199.3, 193.55, 197.34, 197.1199, 197.46, 195.26, 195.88, 196.7501, 196.1, 197.96, 191.6, 186.47, 186.22, 190.81, 194.66, 191.48, 191.98, 189.246, 187.7, 186.6577, 197.68, 200.6, 196.37, 210.71, 214.13, 216.94, 214.93, 216.01, 214.03, 211.31, 211.59, 209.98, 214.18, 212.58, 216.14, 205.1592, 209.1, 212.01, 209.62, 208.68, 205.911, 207.2, 206.75, 203.76, 201.75, 195.59, 193.64, 198.67, 196.62, 196.86, 197.79, 197.54, 197.72, 197.91, 194.59, 191.03, 190.93, 188.94, 185.38, 181.3, 180.72, 180.83, 179.36, 178.93, 178.67, 175.66, 175.73, 171.96, 170.95, 178.97, 180.76, 185.83, 187.36, 187.59, 180.1, 169.39, 170.35, 169.61, 168.93, 163.68, 166.59, 165.8, 165.92, 170.39, 169.44, 165.65, 164.7, 164.96, 163.81, 165.2, 165.78, 162.5, 159.14, 161.58, 161.47, 165.13, 166.96, 168.45, 155.85, 164.76, 172.1, 170.28, 172.32, 167.163, 166.48, 164.27, 164.4826, 162.51, 164.4, 161.74, 167.21, 165.35, 166.17, 168.03, 167.51, 165.1, 165.99, 164.26, 164.935, 160.63, 162.54, 155.88, 153.71, 146.76, 143.49, 143.03, 139.2, 144.57, 146.71, 151.75, 146.6, 152.14, 153.84, 158.37, 159.47, 157.1, 157.33, 151.5, 152.37, 151.72, 151.39, 149.97, 149.59, 159.94, 149.12, 147.35, 148.01, 143.54, 144.35, 144.35, 144.87, 141.83, 138.26, 138.57, 137.02, 138.9, 136.01, 133.06, 127.78, 136.84, 134.31, 127.89, 126.43, 121.3, 102.89, 102.77, 103.86, 103.85, 104.25, 103.48, 106.65, 104.3, 105.63, 104.02, 104.73, 102.95, 101.68, 101.25, 99.68, 99.28, 99.23, 95.49, 97.29, 98.12, 97.77, 100.34, 100.76, 100.01, 100.78, 108.38, 108.93, 109.41, 107.32, 107.7, 108.24, 107.47, 107.1, 108.07, 105.91, 109.45, 106.1, 103.81, 102.55, 101.75, 101.85, 99.1, 98.54, 98.55, 98.73, 97.66, 98.43, 99.0, 102.78, 101.48, 104.41, 101.3925, 100.48, 110.76, 111.07, 107.23, 107.24, 108.98, 108.78, 108.37, 113.63, 116.45, 118.63, 119.13, 117.31, 114.38, 115.39, 113.95, 109.17, 110.01, 111.77, 109.65, 107.79, 107.31, 105.09, 104.01, 105.15, 102.96, 101.09, 103.43, 103.45, 105.16, 106.47, 107.29, 103.13, 101.74, 104.39, 102.0, 106.74, 111.41, 109.25, 117.32, 109.78, 107.11, 105.83, 105.15, 101.65, 100.4, 98.72, 96.46, 91.17, 89.59, 91.82, 93.47, 95.07, 93.39, 91.88, 88.45, 87.64, 92.21, 93.25, 94.11, 94.05, 93.96, 93.65, 92.98, 93.36, 92.39, 91.64, 86.19, 83.66, 87.97, 67.77, 69.95, 71.16, 71.26, 67.57, 67.97, 68.76, 69.07, 71.16, 70.56, 70.68, 72.16, 71.87, 70.71, 67.54, 67.73, 66.48, 66.6, 65.61, 65.99, 65.35, 66.43, 66.12, 67.0911, 66.84, 67.34, 68.23, 68.3, 68.4595, 68.51, 67.408, 66.78, 66.54, 64.34, 64.97, 65.02, 64.85, 63.1, 63.67, 62.85, 62.69, 60.4, 59.8864, 60.75, 59.53, 62.64, 62.2, 63.12, 62.53, 63.14, 61.34, 61.64, 61.99, 62.03, 61.53, 61.94, 62.91, 62.52, 62.25, 62.1, 61.13, 62.6, 62.975, 63.03, 59.73, 58.51, 58.89, 58.74, 58.2, 57.23, 56.18, 56.04, 56.73, 57.1, 56.16, 56.06, 56.63, 55.68, 54.67, 53.22, 54.22, 53.51, 52.98, 52.69, 53.32, 52.78, 52.8, 52.0211, 50.85, 48.88, 47.66, 47.35, 46.64, 47.01, 46.62, 45.89, 45.24, 45.72, 48.49, 47.23, 47.26, 47.55, 46.73, 47.55, 47.36, 46.88, 46.8, 46.2, 47.38, 46.18, 46.34, 46.24, 46.48, 47.14, 46.81, 46.73, 45.9, 45.65, 45.175, 45.35, 44.4, 44.33, 43.56, 43.36, 42.28, 42.19, 40.97, 35.56, 36.06, 35.96, 35.28, 35.33, 34.96, 34.76, 35.57, 35.99, 35.53, 36.2, 37.3, 36.48, 36.45, 36.28, 36.41, 36.45, 36.31, 36.97, 37.13, 36.84, 36.74, 35.84, 35.87, 35.66, 35.43, 35.8, 35.75, 35.82, 36.15, 35.62, 35.76, 35.39, 34.84, 34.48, 34.43, 33.85, 33.905, 33.82, 32.82, 33.11, 32.14, 32.295, 32.22, 31.68, 31.73, 31.75, 32.34, 32.55, 32.65, 32.95, 32.75, 31.36, 31.68, 31.88, 31.815, 31.6, 31.52, 30.44, 30.04, 27.679, 26.99, 25.71, 25.3, 25.45, 25.49, 25.21, 26.43, 28.21, 28.19, 28.06, 29.29, 29.29, 28.05, 28.36, 28.7, 28.41, 28.45, 27.79, 27.47, 27.33, 27.11, 28.67, 29.27, 30.18, 29.68, 29.63, 30.27, 31.53, 32.88, 32.37, 32.96, 33.39, 33.675, 33.14, 33.17, 33.06, 32.93, 32.895, 32.16, 32.66, 33.16, 32.97, 32.575, 32.48, 32.99, 32.71, 33.55, 33.12, 33.74, 32.435, 32.5, 32.75, 31.71, 31.39, 31.14, 31.16, 30.91, 31.39, 31.12, 31.03, 30.41, 30.4, 29.81, 30.4, 30.51, 30.81, 31.41, 31.54, 27.7, 28.03, 28.49, 28.7, 28.37, 27.68, 28.67, 28.44, 28.47, 28.59, 28.39, 27.41, 27.77, 27.82, 27.86, 27.43, 27.37, 26.41, 26.345, 26.07, 26.18, 26.02, 25.75, 25.41, 24.8, 24.17, 24.64, 23.71, 23.3, 23.61, 23.44, 23.0, 22.9, 23.53, 23.3, 23.31, 23.11, 22.94, 22.69, 22.65, 22.58, 22.23, 22.69, 21.75, 22.275, 22.22, 21.565, 22.47, 22.73, 22.64, 21.82, 20.28, 20.71, 21.48, 22.155, 22.99, 23.08, 23.39, 23.545, 23.52, 23.72, 23.66, 23.78, 22.98, 20.48, 20.58, 20.3708, 20.29, 19.95, 20.01, 19.98, 19.735, 19.32, 19.42, 19.65, 19.41, 19.79, 19.7, 20.08, 20.18, 19.75, 19.89, 19.88, 19.75, 19.41, 19.65, 19.785, 20.17, 20.41, 20.4, 20.12, 20.12, 20.74, 21.17, 21.01, 21.215, 21.77, 21.86, 21.93, 21.58, 21.325, 21.065, 21.1, 21.7, 21.455, 21.85, 21.75, 22.265, 22.09, 21.7, 21.93, 22.38, 22.14, 22.15, 21.84, 20.73, 20.855, 20.9, 21.04, 21.035, 21.27, 21.295, 21.29, 20.95, 20.83, 20.63, 20.83, 22.5, 22.09, 22.035, 22.615, 22.75, 22.19, 22.14, 22.3, 22.1975, 22.03, 22.23, 22.31, 22.055, 22.09, 22.195, 22.5, 22.62, 22.37, 22.55, 22.76, 22.52, 22.03, 21.86, 21.67, 21.06, 21.01, 20.93, 21.47, 21.38, 20.97, 21.03, 22.395, 22.71, 23.49, 23.21, 22.88, 23.25, 22.97, 22.7, 22.66, 22.875, 22.99, 22.61, 22.55, 22.865, 22.445, 22.18, 22.6, 22.0501, 22.2, 22.14, 22.31, 22.16, 22.33, 22.18, 22.115, 22.38, 22.305, 22.3, 20.81, 20.94, 20.39, 20.4, 20.49, 20.17, 20.105, 19.615, 19.21, 19.78, 19.31, 19.59, 20.61, 20.72, 20.64, 20.3, 20.02, 19.96, 19.6, 19.74, 19.66, 19.69, 19.94, 19.86, 19.14, 19.18, 19.8, 20.125, 20.05, 20.36, 20.565, 20.59, 20.6363, 20.64, 20.78, 20.42, 20.22, 20.15, 19.34, 19.56, 19.635, 20.27, 20.265, 20.735, 20.805, 21.07, 20.96, 21.13, 20.61, 20.58, 20.9672, 20.91, 20.57, 20.58, 20.46, 20.35, 20.005, 20.17, 19.7, 19.785, 19.56, 19.64, 19.77, 20.015, 19.79, 20.21, 20.13, 20.14, 19.88, 19.53, 18.7, 18.81, 18.93, 18.49, 18.475, 18.285, 17.895, 18.325, 17.57, 17.42, 17.45, 17.435, 17.18, 16.785, 16.85, 17.92, 18.26, 17.925, 18.12, 18.23, 18.19, 18.27, 18.455, 18.515, 18.54, 18.51, 18.92, 18.805, 18.91, 19.06, 19.445, 19.15, 19.135, 18.86, 19.13, 19.41, 19.61, 19.53, 19.79, 19.96, 20.03, 19.68, 19.49, 19.44, 19.39, 19.235, 19.455, 19.1, 19.085, 19.06, 19.25, 19.37, 19.295, 19.04, 18.795, 19.0, 18.895, 18.905, 18.99, 17.46, 17.65, 17.67, 17.64, 17.68, 17.5, 18.08, 17.78, 17.715, 17.79, 18.1, 18.09, 18.47, 18.55, 18.44, 19.3, 19.35, 19.38, 19.29, 19.05, 19.005, 19.12, 18.545, 18.7, 18.8738, 18.685, 18.74, 18.53, 18.39, 18.37, 18.6, 18.43, 18.71, 18.925, 19.13, 19.585, 19.6, 19.49, 19.53, 19.52, 19.41, 19.145, 19.05, 19.03, 18.95, 18.88, 18.86, 18.94, 19.005, 18.945, 18.98, 18.82, 18.48, 18.32, 18.25, 18.24, 18.54, 17.98, 18.0, 18.11, 18.27, 18.565, 18.04, 18.495, 18.27, 18.26, 18.62, 18.42, 18.57, 18.46, 18.68, 18.65, 18.725, 19.25, 19.09, 18.88, 18.71, 18.56, 18.48, 18.445, 18.32, 18.12, 18.385, 18.84, 18.86, 18.21, 18.15, 18.72, 18.53, 18.65, 17.92, 17.89, 17.795, 18.03, 18.45, 18.46, 18.51, 18.575, 18.555, 18.23, 17.81, 17.8, 17.74, 18.315, 18.28, 18.085, 18.355, 18.39, 18.64, 18.48, 18.29, 18.375, 18.49, 18.71, 18.745, 18.88, 18.64, 18.77, 18.145, 17.895, 17.915, 17.35, 16.82, 16.245, 15.91, 15.86, 15.64, 15.44, 15.58, 15.49, 15.695, 15.71, 15.46, 15.6, 15.46, 15.57, 15.97, 16.03, 16.045, 15.99, 16.06, 16.01, 15.84, 15.36, 15.73, 15.755, 16.37, 16.13, 15.87, 15.655, 15.86, 16.01, 15.96, 15.76, 15.67, 15.8232, 15.79, 15.69, 15.38, 15.31, 15.115, 15.025, 15.02, 15.105, 15.42, 15.56, 15.21, 15.46, 15.71, 15.96, 15.73, 15.75, 15.6022, 15.7, 15.63, 15.5, 15.19, 15.34, 15.21, 15.44, 15.78, 16.17, 16.22, 16.15, 15.72, 15.69, 15.561, 14.545, 14.89, 14.79, 14.81, 15.25, 15.18, 15.22, 15.225, 15.21, 15.235, 15.38, 15.51, 15.78, 15.85]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('00cd1d2d-13d8-4391-aafb-279f178593e7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"e6ab3729-1e29-4b13-a08c-61ae7e0b8972\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"e6ab3729-1e29-4b13-a08c-61ae7e0b8972\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'e6ab3729-1e29-4b13-a08c-61ae7e0b8972',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e6ab3729-1e29-4b13-a08c-61ae7e0b8972');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V3uEQQwX4gx"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJJWuxWFX4gx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59625dbe-0aac-45f4-d9cb-8b6253b7f1ed"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .2, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"NVDA\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6014 - accuracy: 0.7195 - val_loss: 0.4851 - val_accuracy: 0.8122\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5460 - accuracy: 0.7396 - val_loss: 0.4376 - val_accuracy: 0.8327\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5096 - accuracy: 0.7758 - val_loss: 0.4309 - val_accuracy: 0.8184\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4935 - accuracy: 0.7812 - val_loss: 0.5178 - val_accuracy: 0.7694\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4941 - accuracy: 0.7832 - val_loss: 0.4486 - val_accuracy: 0.8388\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.5831 - accuracy: 0.7228 - val_loss: 0.4851 - val_accuracy: 0.8122\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4794 - accuracy: 0.7926 - val_loss: 0.4257 - val_accuracy: 0.8347\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4482 - accuracy: 0.8087 - val_loss: 0.4356 - val_accuracy: 0.8449\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4473 - accuracy: 0.8047 - val_loss: 0.4374 - val_accuracy: 0.8429\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4432 - accuracy: 0.8181 - val_loss: 0.4231 - val_accuracy: 0.8469\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.711165\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.710782\n",
            "[2]\tvalidation_0-auc:0.724806\n",
            "[3]\tvalidation_0-auc:0.726417\n",
            "[4]\tvalidation_0-auc:0.729353\n",
            "[5]\tvalidation_0-auc:0.735703\n",
            "[6]\tvalidation_0-auc:0.731702\n",
            "[7]\tvalidation_0-auc:0.732357\n",
            "[8]\tvalidation_0-auc:0.735498\n",
            "[9]\tvalidation_0-auc:0.740469\n",
            "[10]\tvalidation_0-auc:0.733177\n",
            "[11]\tvalidation_0-auc:0.733832\n",
            "[12]\tvalidation_0-auc:0.738871\n",
            "[13]\tvalidation_0-auc:0.732699\n",
            "[14]\tvalidation_0-auc:0.732494\n",
            "[15]\tvalidation_0-auc:0.732221\n",
            "[16]\tvalidation_0-auc:0.736072\n",
            "[17]\tvalidation_0-auc:0.735198\n",
            "[18]\tvalidation_0-auc:0.731224\n",
            "[19]\tvalidation_0-auc:0.730309\n",
            "[20]\tvalidation_0-auc:0.730528\n",
            "[21]\tvalidation_0-auc:0.730459\n",
            "[22]\tvalidation_0-auc:0.729613\n",
            "[23]\tvalidation_0-auc:0.731552\n",
            "[24]\tvalidation_0-auc:0.730541\n",
            "[25]\tvalidation_0-auc:0.730446\n",
            "[26]\tvalidation_0-auc:0.72583\n",
            "[27]\tvalidation_0-auc:0.725858\n",
            "[28]\tvalidation_0-auc:0.722307\n",
            "[29]\tvalidation_0-auc:0.721911\n",
            "[30]\tvalidation_0-auc:0.72314\n",
            "[31]\tvalidation_0-auc:0.724943\n",
            "[32]\tvalidation_0-auc:0.724751\n",
            "[33]\tvalidation_0-auc:0.724424\n",
            "[34]\tvalidation_0-auc:0.724751\n",
            "[35]\tvalidation_0-auc:0.72467\n",
            "[36]\tvalidation_0-auc:0.725421\n",
            "[37]\tvalidation_0-auc:0.72527\n",
            "[38]\tvalidation_0-auc:0.725107\n",
            "[39]\tvalidation_0-auc:0.721119\n",
            "[40]\tvalidation_0-auc:0.721228\n",
            "[41]\tvalidation_0-auc:0.721228\n",
            "[42]\tvalidation_0-auc:0.721952\n",
            "[43]\tvalidation_0-auc:0.722853\n",
            "[44]\tvalidation_0-auc:0.723058\n",
            "[45]\tvalidation_0-auc:0.722498\n",
            "[46]\tvalidation_0-auc:0.722744\n",
            "[47]\tvalidation_0-auc:0.722758\n",
            "[48]\tvalidation_0-auc:0.722758\n",
            "[49]\tvalidation_0-auc:0.723782\n",
            "[50]\tvalidation_0-auc:0.722881\n",
            "[51]\tvalidation_0-auc:0.722881\n",
            "[52]\tvalidation_0-auc:0.722881\n",
            "[53]\tvalidation_0-auc:0.723768\n",
            "[54]\tvalidation_0-auc:0.722034\n",
            "[55]\tvalidation_0-auc:0.722908\n",
            "[56]\tvalidation_0-auc:0.722143\n",
            "[57]\tvalidation_0-auc:0.722908\n",
            "[58]\tvalidation_0-auc:0.723318\n",
            "[59]\tvalidation_0-auc:0.722785\n",
            "Stopping. Best iteration:\n",
            "[9]\tvalidation_0-auc:0.740469\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6123 - accuracy: 0.7117 - val_loss: 0.5036 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5953 - accuracy: 0.7145 - val_loss: 0.4568 - val_accuracy: 0.8556\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5779 - accuracy: 0.7165 - val_loss: 0.4353 - val_accuracy: 0.8556\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5351 - accuracy: 0.7406 - val_loss: 0.4070 - val_accuracy: 0.8796\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5017 - accuracy: 0.7639 - val_loss: 0.3720 - val_accuracy: 0.8775\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 13ms/step - loss: 0.5914 - accuracy: 0.7165 - val_loss: 0.4277 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5174 - accuracy: 0.7563 - val_loss: 0.3956 - val_accuracy: 0.8709\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4937 - accuracy: 0.7701 - val_loss: 0.3811 - val_accuracy: 0.9015\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4557 - accuracy: 0.8016 - val_loss: 0.3893 - val_accuracy: 0.8884\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4723 - accuracy: 0.7927 - val_loss: 0.4074 - val_accuracy: 0.8972\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.789816\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.738065\n",
            "[2]\tvalidation_0-auc:0.720763\n",
            "[3]\tvalidation_0-auc:0.720646\n",
            "[4]\tvalidation_0-auc:0.740739\n",
            "[5]\tvalidation_0-auc:0.741417\n",
            "[6]\tvalidation_0-auc:0.770984\n",
            "[7]\tvalidation_0-auc:0.748392\n",
            "[8]\tvalidation_0-auc:0.746764\n",
            "[9]\tvalidation_0-auc:0.739518\n",
            "[10]\tvalidation_0-auc:0.725548\n",
            "[11]\tvalidation_0-auc:0.740952\n",
            "[12]\tvalidation_0-auc:0.74752\n",
            "[13]\tvalidation_0-auc:0.739382\n",
            "[14]\tvalidation_0-auc:0.735701\n",
            "[15]\tvalidation_0-auc:0.73357\n",
            "[16]\tvalidation_0-auc:0.747888\n",
            "[17]\tvalidation_0-auc:0.732717\n",
            "[18]\tvalidation_0-auc:0.737096\n",
            "[19]\tvalidation_0-auc:0.737406\n",
            "[20]\tvalidation_0-auc:0.746086\n",
            "[21]\tvalidation_0-auc:0.75124\n",
            "[22]\tvalidation_0-auc:0.750368\n",
            "[23]\tvalidation_0-auc:0.756413\n",
            "[24]\tvalidation_0-auc:0.755987\n",
            "[25]\tvalidation_0-auc:0.756336\n",
            "[26]\tvalidation_0-auc:0.759668\n",
            "[27]\tvalidation_0-auc:0.759668\n",
            "[28]\tvalidation_0-auc:0.75777\n",
            "[29]\tvalidation_0-auc:0.75777\n",
            "[30]\tvalidation_0-auc:0.760405\n",
            "[31]\tvalidation_0-auc:0.754069\n",
            "[32]\tvalidation_0-auc:0.75279\n",
            "[33]\tvalidation_0-auc:0.757886\n",
            "[34]\tvalidation_0-auc:0.757886\n",
            "[35]\tvalidation_0-auc:0.757886\n",
            "[36]\tvalidation_0-auc:0.757498\n",
            "[37]\tvalidation_0-auc:0.756336\n",
            "[38]\tvalidation_0-auc:0.757808\n",
            "[39]\tvalidation_0-auc:0.758312\n",
            "[40]\tvalidation_0-auc:0.752887\n",
            "[41]\tvalidation_0-auc:0.749884\n",
            "[42]\tvalidation_0-auc:0.756646\n",
            "[43]\tvalidation_0-auc:0.755754\n",
            "[44]\tvalidation_0-auc:0.755754\n",
            "[45]\tvalidation_0-auc:0.762652\n",
            "[46]\tvalidation_0-auc:0.762652\n",
            "[47]\tvalidation_0-auc:0.768348\n",
            "[48]\tvalidation_0-auc:0.767244\n",
            "[49]\tvalidation_0-auc:0.766895\n",
            "[50]\tvalidation_0-auc:0.766314\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.789816\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.8387755102040816 |        0.76        | 0.20652173913043478 |  0.3247863247863248 |\n",
            "|     GRU 0.1      | 0.8469387755102041 | 0.7428571428571429 |  0.2826086956521739 |  0.4094488188976378 |\n",
            "|   XGBoost 0.1    | 0.8306122448979592 | 0.5714285714285714 |  0.391304347826087  |  0.4645161290322581 |\n",
            "|    Logreg 0.1    | 0.8448979591836735 |        0.7         | 0.30434782608695654 | 0.42424242424242425 |\n",
            "|     SVM 0.1      | 0.8448979591836735 | 0.6904761904761905 | 0.31521739130434784 | 0.43283582089552236 |\n",
            "|  LSTM beta 0.1   | 0.8774617067833698 | 0.9166666666666666 | 0.16666666666666666 | 0.28205128205128205 |\n",
            "|   GRU beta 0.1   | 0.8971553610503282 | 0.7435897435897436 |  0.4393939393939394 |  0.5523809523809524 |\n",
            "| XGBoost beta 0.1 | 0.838074398249453  |        0.3         | 0.09090909090909091 | 0.13953488372093023 |\n",
            "| logreg beta 0.1  | 0.9015317286652079 | 0.7837837837837838 |  0.4393939393939394 |  0.5631067961165048 |\n",
            "|   svm beta 0.1   | 0.8862144420131292 | 0.6666666666666666 | 0.42424242424242425 |  0.5185185185185185 |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.5190 - accuracy: 0.7926 - val_loss: 0.4249 - val_accuracy: 0.8653\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4798 - accuracy: 0.8000 - val_loss: 0.3531 - val_accuracy: 0.8694\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4570 - accuracy: 0.8101 - val_loss: 0.3710 - val_accuracy: 0.8653\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4476 - accuracy: 0.8121 - val_loss: 0.3643 - val_accuracy: 0.8816\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4454 - accuracy: 0.8208 - val_loss: 0.3673 - val_accuracy: 0.8714\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.5279 - accuracy: 0.7933 - val_loss: 0.4223 - val_accuracy: 0.8653\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4683 - accuracy: 0.8074 - val_loss: 0.3874 - val_accuracy: 0.8735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4426 - accuracy: 0.8235 - val_loss: 0.3639 - val_accuracy: 0.8735\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4245 - accuracy: 0.8336 - val_loss: 0.3686 - val_accuracy: 0.8735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4219 - accuracy: 0.8322 - val_loss: 0.3590 - val_accuracy: 0.8673\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.710674\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.730685\n",
            "[2]\tvalidation_0-auc:0.730685\n",
            "[3]\tvalidation_0-auc:0.748785\n",
            "[4]\tvalidation_0-auc:0.74925\n",
            "[5]\tvalidation_0-auc:0.756754\n",
            "[6]\tvalidation_0-auc:0.748571\n",
            "[7]\tvalidation_0-auc:0.75209\n",
            "[8]\tvalidation_0-auc:0.752484\n",
            "[9]\tvalidation_0-auc:0.750607\n",
            "[10]\tvalidation_0-auc:0.749428\n",
            "[11]\tvalidation_0-auc:0.736081\n",
            "[12]\tvalidation_0-auc:0.743729\n",
            "[13]\tvalidation_0-auc:0.744425\n",
            "[14]\tvalidation_0-auc:0.742353\n",
            "[15]\tvalidation_0-auc:0.743175\n",
            "[16]\tvalidation_0-auc:0.745497\n",
            "[17]\tvalidation_0-auc:0.745265\n",
            "[18]\tvalidation_0-auc:0.746498\n",
            "[19]\tvalidation_0-auc:0.746373\n",
            "[20]\tvalidation_0-auc:0.745426\n",
            "[21]\tvalidation_0-auc:0.744836\n",
            "[22]\tvalidation_0-auc:0.747088\n",
            "[23]\tvalidation_0-auc:0.747016\n",
            "[24]\tvalidation_0-auc:0.745748\n",
            "[25]\tvalidation_0-auc:0.745247\n",
            "[26]\tvalidation_0-auc:0.74539\n",
            "[27]\tvalidation_0-auc:0.740066\n",
            "[28]\tvalidation_0-auc:0.73869\n",
            "[29]\tvalidation_0-auc:0.738618\n",
            "[30]\tvalidation_0-auc:0.736224\n",
            "[31]\tvalidation_0-auc:0.735903\n",
            "[32]\tvalidation_0-auc:0.736046\n",
            "[33]\tvalidation_0-auc:0.736189\n",
            "[34]\tvalidation_0-auc:0.731418\n",
            "[35]\tvalidation_0-auc:0.726629\n",
            "[36]\tvalidation_0-auc:0.726004\n",
            "[37]\tvalidation_0-auc:0.729506\n",
            "[38]\tvalidation_0-auc:0.728613\n",
            "[39]\tvalidation_0-auc:0.72704\n",
            "[40]\tvalidation_0-auc:0.726719\n",
            "[41]\tvalidation_0-auc:0.723485\n",
            "[42]\tvalidation_0-auc:0.723378\n",
            "[43]\tvalidation_0-auc:0.724092\n",
            "[44]\tvalidation_0-auc:0.723842\n",
            "[45]\tvalidation_0-auc:0.719054\n",
            "[46]\tvalidation_0-auc:0.717982\n",
            "[47]\tvalidation_0-auc:0.716785\n",
            "[48]\tvalidation_0-auc:0.716356\n",
            "[49]\tvalidation_0-auc:0.715266\n",
            "[50]\tvalidation_0-auc:0.715552\n",
            "[51]\tvalidation_0-auc:0.714623\n",
            "[52]\tvalidation_0-auc:0.713336\n",
            "[53]\tvalidation_0-auc:0.713211\n",
            "[54]\tvalidation_0-auc:0.71364\n",
            "[55]\tvalidation_0-auc:0.713908\n",
            "Stopping. Best iteration:\n",
            "[5]\tvalidation_0-auc:0.756754\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.5366 - accuracy: 0.7893 - val_loss: 0.4115 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5079 - accuracy: 0.7927 - val_loss: 0.3710 - val_accuracy: 0.8556\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4671 - accuracy: 0.8037 - val_loss: 0.3669 - val_accuracy: 0.8556\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.4387 - accuracy: 0.8140 - val_loss: 0.3424 - val_accuracy: 0.8556\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4431 - accuracy: 0.8181 - val_loss: 0.3483 - val_accuracy: 0.8556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5156 - accuracy: 0.7920 - val_loss: 0.3779 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4444 - accuracy: 0.8202 - val_loss: 0.3459 - val_accuracy: 0.8731\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4267 - accuracy: 0.8250 - val_loss: 0.3456 - val_accuracy: 0.8731\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4185 - accuracy: 0.8257 - val_loss: 0.3817 - val_accuracy: 0.8972\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4139 - accuracy: 0.8284 - val_loss: 0.3618 - val_accuracy: 0.8709\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.84831\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.850868\n",
            "[2]\tvalidation_0-auc:0.84862\n",
            "[3]\tvalidation_0-auc:0.844397\n",
            "[4]\tvalidation_0-auc:0.882624\n",
            "[5]\tvalidation_0-auc:0.898958\n",
            "[6]\tvalidation_0-auc:0.890801\n",
            "[7]\tvalidation_0-auc:0.890839\n",
            "[8]\tvalidation_0-auc:0.889793\n",
            "[9]\tvalidation_0-auc:0.895451\n",
            "[10]\tvalidation_0-auc:0.898744\n",
            "[11]\tvalidation_0-auc:0.897563\n",
            "[12]\tvalidation_0-auc:0.897834\n",
            "[13]\tvalidation_0-auc:0.89795\n",
            "[14]\tvalidation_0-auc:0.897291\n",
            "[15]\tvalidation_0-auc:0.896691\n",
            "[16]\tvalidation_0-auc:0.895199\n",
            "[17]\tvalidation_0-auc:0.892041\n",
            "[18]\tvalidation_0-auc:0.890471\n",
            "[19]\tvalidation_0-auc:0.892409\n",
            "[20]\tvalidation_0-auc:0.890626\n",
            "[21]\tvalidation_0-auc:0.890859\n",
            "[22]\tvalidation_0-auc:0.88896\n",
            "[23]\tvalidation_0-auc:0.890452\n",
            "[24]\tvalidation_0-auc:0.890801\n",
            "[25]\tvalidation_0-auc:0.891188\n",
            "[26]\tvalidation_0-auc:0.892506\n",
            "[27]\tvalidation_0-auc:0.892506\n",
            "[28]\tvalidation_0-auc:0.890917\n",
            "[29]\tvalidation_0-auc:0.891692\n",
            "[30]\tvalidation_0-auc:0.886867\n",
            "[31]\tvalidation_0-auc:0.887487\n",
            "[32]\tvalidation_0-auc:0.887022\n",
            "[33]\tvalidation_0-auc:0.886635\n",
            "[34]\tvalidation_0-auc:0.886538\n",
            "[35]\tvalidation_0-auc:0.881016\n",
            "[36]\tvalidation_0-auc:0.881016\n",
            "[37]\tvalidation_0-auc:0.879757\n",
            "[38]\tvalidation_0-auc:0.879485\n",
            "[39]\tvalidation_0-auc:0.880144\n",
            "[40]\tvalidation_0-auc:0.879292\n",
            "[41]\tvalidation_0-auc:0.878827\n",
            "[42]\tvalidation_0-auc:0.878517\n",
            "[43]\tvalidation_0-auc:0.879001\n",
            "[44]\tvalidation_0-auc:0.880745\n",
            "[45]\tvalidation_0-auc:0.881132\n",
            "[46]\tvalidation_0-auc:0.878265\n",
            "[47]\tvalidation_0-auc:0.877528\n",
            "[48]\tvalidation_0-auc:0.877257\n",
            "[49]\tvalidation_0-auc:0.877257\n",
            "[50]\tvalidation_0-auc:0.876928\n",
            "[51]\tvalidation_0-auc:0.876192\n",
            "[52]\tvalidation_0-auc:0.873363\n",
            "[53]\tvalidation_0-auc:0.875029\n",
            "[54]\tvalidation_0-auc:0.874525\n",
            "[55]\tvalidation_0-auc:0.872704\n",
            "Stopping. Best iteration:\n",
            "[5]\tvalidation_0-auc:0.898958\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+----------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall        |       F1 score      |\n",
            "+------------------+--------------------+--------------------+----------------------+---------------------+\n",
            "|     LSTM 0.2     | 0.8714285714285714 |        1.0         | 0.045454545454545456 | 0.08695652173913045 |\n",
            "|     GRU 0.2      | 0.8673469387755102 |        1.0         | 0.015151515151515152 | 0.02985074626865672 |\n",
            "|   XGBoost 0.2    | 0.8734693877551021 | 0.6666666666666666 | 0.12121212121212122  | 0.20512820512820512 |\n",
            "|    Logreg 0.2    | 0.8693877551020408 |        1.0         | 0.030303030303030304 | 0.05882352941176471 |\n",
            "|     SVM 0.2      | 0.8693877551020408 |        1.0         | 0.030303030303030304 | 0.05882352941176471 |\n",
            "|  LSTM beta 0.2   | 0.8555798687089715 |        0.0         |         0.0          |         0.0         |\n",
            "|   GRU beta 0.2   | 0.8708971553610503 | 0.8888888888888888 | 0.12121212121212122  | 0.21333333333333335 |\n",
            "| XGBoost beta 0.2 | 0.9037199124726477 |        1.0         |  0.3333333333333333  |         0.5         |\n",
            "| logreg beta 0.2  | 0.8687089715536105 |        1.0         | 0.09090909090909091  | 0.16666666666666669 |\n",
            "|   svm beta 0.2   | 0.8708971553610503 |        1.0         | 0.10606060606060606  | 0.19178082191780824 |\n",
            "+------------------+--------------------+--------------------+----------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.5984 - accuracy: 0.7416 - val_loss: 0.4432 - val_accuracy: 0.8653\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5413 - accuracy: 0.7591 - val_loss: 0.3632 - val_accuracy: 0.8796\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5212 - accuracy: 0.7725 - val_loss: 0.4036 - val_accuracy: 0.8796\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4897 - accuracy: 0.7893 - val_loss: 0.5849 - val_accuracy: 0.6490\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4975 - accuracy: 0.7597 - val_loss: 0.4383 - val_accuracy: 0.8224\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.5583 - accuracy: 0.7483 - val_loss: 0.3625 - val_accuracy: 0.8653\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4808 - accuracy: 0.7953 - val_loss: 0.5019 - val_accuracy: 0.8245\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4691 - accuracy: 0.8027 - val_loss: 0.4216 - val_accuracy: 0.8694\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4624 - accuracy: 0.8000 - val_loss: 0.3764 - val_accuracy: 0.8776\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4496 - accuracy: 0.8107 - val_loss: 0.3810 - val_accuracy: 0.8755\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.709602\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.720662\n",
            "[2]\tvalidation_0-auc:0.721573\n",
            "[3]\tvalidation_0-auc:0.723896\n",
            "[4]\tvalidation_0-auc:0.725254\n",
            "[5]\tvalidation_0-auc:0.729703\n",
            "[6]\tvalidation_0-auc:0.733097\n",
            "[7]\tvalidation_0-auc:0.741048\n",
            "[8]\tvalidation_0-auc:0.735617\n",
            "[9]\tvalidation_0-auc:0.737207\n",
            "[10]\tvalidation_0-auc:0.732937\n",
            "[11]\tvalidation_0-auc:0.732365\n",
            "[12]\tvalidation_0-auc:0.730793\n",
            "[13]\tvalidation_0-auc:0.734884\n",
            "[14]\tvalidation_0-auc:0.733919\n",
            "[15]\tvalidation_0-auc:0.733223\n",
            "[16]\tvalidation_0-auc:0.735277\n",
            "[17]\tvalidation_0-auc:0.734706\n",
            "[18]\tvalidation_0-auc:0.733187\n",
            "[19]\tvalidation_0-auc:0.733508\n",
            "[20]\tvalidation_0-auc:0.739065\n",
            "[21]\tvalidation_0-auc:0.737171\n",
            "[22]\tvalidation_0-auc:0.738958\n",
            "[23]\tvalidation_0-auc:0.738761\n",
            "[24]\tvalidation_0-auc:0.740548\n",
            "[25]\tvalidation_0-auc:0.740798\n",
            "[26]\tvalidation_0-auc:0.73953\n",
            "[27]\tvalidation_0-auc:0.737171\n",
            "[28]\tvalidation_0-auc:0.738315\n",
            "[29]\tvalidation_0-auc:0.737457\n",
            "[30]\tvalidation_0-auc:0.735706\n",
            "[31]\tvalidation_0-auc:0.737886\n",
            "[32]\tvalidation_0-auc:0.736582\n",
            "[33]\tvalidation_0-auc:0.737278\n",
            "[34]\tvalidation_0-auc:0.736742\n",
            "[35]\tvalidation_0-auc:0.734455\n",
            "[36]\tvalidation_0-auc:0.734527\n",
            "[37]\tvalidation_0-auc:0.735706\n",
            "[38]\tvalidation_0-auc:0.735938\n",
            "[39]\tvalidation_0-auc:0.733919\n",
            "[40]\tvalidation_0-auc:0.735206\n",
            "[41]\tvalidation_0-auc:0.73542\n",
            "[42]\tvalidation_0-auc:0.733401\n",
            "[43]\tvalidation_0-auc:0.730864\n",
            "[44]\tvalidation_0-auc:0.733187\n",
            "[45]\tvalidation_0-auc:0.732937\n",
            "[46]\tvalidation_0-auc:0.730149\n",
            "[47]\tvalidation_0-auc:0.732222\n",
            "[48]\tvalidation_0-auc:0.732847\n",
            "[49]\tvalidation_0-auc:0.733097\n",
            "[50]\tvalidation_0-auc:0.731096\n",
            "[51]\tvalidation_0-auc:0.731525\n",
            "[52]\tvalidation_0-auc:0.728774\n",
            "[53]\tvalidation_0-auc:0.730382\n",
            "[54]\tvalidation_0-auc:0.730596\n",
            "[55]\tvalidation_0-auc:0.727987\n",
            "[56]\tvalidation_0-auc:0.726844\n",
            "[57]\tvalidation_0-auc:0.723878\n",
            "Stopping. Best iteration:\n",
            "[7]\tvalidation_0-auc:0.741048\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.5858 - accuracy: 0.7316 - val_loss: 0.4369 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5807 - accuracy: 0.7364 - val_loss: 0.4431 - val_accuracy: 0.8556\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5500 - accuracy: 0.7399 - val_loss: 0.3993 - val_accuracy: 0.8556\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 13ms/step - loss: 0.5138 - accuracy: 0.7639 - val_loss: 0.4115 - val_accuracy: 0.8556\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4888 - accuracy: 0.7817 - val_loss: 0.3743 - val_accuracy: 0.8687\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5699 - accuracy: 0.7454 - val_loss: 0.4739 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5048 - accuracy: 0.7728 - val_loss: 0.3980 - val_accuracy: 0.8665\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4894 - accuracy: 0.7893 - val_loss: 0.3944 - val_accuracy: 0.9103\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4774 - accuracy: 0.7852 - val_loss: 0.3904 - val_accuracy: 0.9212\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4524 - accuracy: 0.7962 - val_loss: 0.3353 - val_accuracy: 0.8643\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.828489\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.821902\n",
            "[2]\tvalidation_0-auc:0.810722\n",
            "[3]\tvalidation_0-auc:0.830776\n",
            "[4]\tvalidation_0-auc:0.835019\n",
            "[5]\tvalidation_0-auc:0.836143\n",
            "[6]\tvalidation_0-auc:0.831841\n",
            "[7]\tvalidation_0-auc:0.824091\n",
            "[8]\tvalidation_0-auc:0.831551\n",
            "[9]\tvalidation_0-auc:0.832558\n",
            "[10]\tvalidation_0-auc:0.832461\n",
            "[11]\tvalidation_0-auc:0.83159\n",
            "[12]\tvalidation_0-auc:0.843292\n",
            "[13]\tvalidation_0-auc:0.845656\n",
            "[14]\tvalidation_0-auc:0.846896\n",
            "[15]\tvalidation_0-auc:0.848524\n",
            "[16]\tvalidation_0-auc:0.849841\n",
            "[17]\tvalidation_0-auc:0.849376\n",
            "[18]\tvalidation_0-auc:0.85143\n",
            "[19]\tvalidation_0-auc:0.850655\n",
            "[20]\tvalidation_0-auc:0.851042\n",
            "[21]\tvalidation_0-auc:0.851585\n",
            "[22]\tvalidation_0-auc:0.850074\n",
            "[23]\tvalidation_0-auc:0.853232\n",
            "[24]\tvalidation_0-auc:0.853348\n",
            "[25]\tvalidation_0-auc:0.854084\n",
            "[26]\tvalidation_0-auc:0.853387\n",
            "[27]\tvalidation_0-auc:0.859664\n",
            "[28]\tvalidation_0-auc:0.858076\n",
            "[29]\tvalidation_0-auc:0.857456\n",
            "[30]\tvalidation_0-auc:0.858192\n",
            "[31]\tvalidation_0-auc:0.858231\n",
            "[32]\tvalidation_0-auc:0.859781\n",
            "[33]\tvalidation_0-auc:0.862493\n",
            "[34]\tvalidation_0-auc:0.862067\n",
            "[35]\tvalidation_0-auc:0.861641\n",
            "[36]\tvalidation_0-auc:0.862028\n",
            "[37]\tvalidation_0-auc:0.861951\n",
            "[38]\tvalidation_0-auc:0.860749\n",
            "[39]\tvalidation_0-auc:0.861311\n",
            "[40]\tvalidation_0-auc:0.860808\n",
            "[41]\tvalidation_0-auc:0.860575\n",
            "[42]\tvalidation_0-auc:0.860265\n",
            "[43]\tvalidation_0-auc:0.861544\n",
            "[44]\tvalidation_0-auc:0.861699\n",
            "[45]\tvalidation_0-auc:0.861583\n",
            "[46]\tvalidation_0-auc:0.864063\n",
            "[47]\tvalidation_0-auc:0.866543\n",
            "[48]\tvalidation_0-auc:0.866349\n",
            "[49]\tvalidation_0-auc:0.867046\n",
            "[50]\tvalidation_0-auc:0.866775\n",
            "[51]\tvalidation_0-auc:0.868984\n",
            "[52]\tvalidation_0-auc:0.868984\n",
            "[53]\tvalidation_0-auc:0.866891\n",
            "[54]\tvalidation_0-auc:0.868015\n",
            "[55]\tvalidation_0-auc:0.867938\n",
            "[56]\tvalidation_0-auc:0.867938\n",
            "[57]\tvalidation_0-auc:0.86693\n",
            "[58]\tvalidation_0-auc:0.86662\n",
            "[59]\tvalidation_0-auc:0.867046\n",
            "[60]\tvalidation_0-auc:0.866795\n",
            "[61]\tvalidation_0-auc:0.863074\n",
            "[62]\tvalidation_0-auc:0.862609\n",
            "[63]\tvalidation_0-auc:0.862454\n",
            "[64]\tvalidation_0-auc:0.862454\n",
            "[65]\tvalidation_0-auc:0.863772\n",
            "[66]\tvalidation_0-auc:0.863501\n",
            "[67]\tvalidation_0-auc:0.862513\n",
            "[68]\tvalidation_0-auc:0.862513\n",
            "[69]\tvalidation_0-auc:0.862048\n",
            "[70]\tvalidation_0-auc:0.861815\n",
            "[71]\tvalidation_0-auc:0.86228\n",
            "[72]\tvalidation_0-auc:0.86228\n",
            "[73]\tvalidation_0-auc:0.861137\n",
            "[74]\tvalidation_0-auc:0.861873\n",
            "[75]\tvalidation_0-auc:0.86197\n",
            "[76]\tvalidation_0-auc:0.863365\n",
            "[77]\tvalidation_0-auc:0.865496\n",
            "[78]\tvalidation_0-auc:0.86445\n",
            "[79]\tvalidation_0-auc:0.863753\n",
            "[80]\tvalidation_0-auc:0.863753\n",
            "[81]\tvalidation_0-auc:0.862513\n",
            "[82]\tvalidation_0-auc:0.86197\n",
            "[83]\tvalidation_0-auc:0.862086\n",
            "[84]\tvalidation_0-auc:0.862474\n",
            "[85]\tvalidation_0-auc:0.862823\n",
            "[86]\tvalidation_0-auc:0.860033\n",
            "[87]\tvalidation_0-auc:0.859723\n",
            "[88]\tvalidation_0-auc:0.859529\n",
            "[89]\tvalidation_0-auc:0.857882\n",
            "[90]\tvalidation_0-auc:0.857436\n",
            "[91]\tvalidation_0-auc:0.857204\n",
            "[92]\tvalidation_0-auc:0.856971\n",
            "[93]\tvalidation_0-auc:0.856855\n",
            "[94]\tvalidation_0-auc:0.856157\n",
            "[95]\tvalidation_0-auc:0.854917\n",
            "[96]\tvalidation_0-auc:0.854801\n",
            "[97]\tvalidation_0-auc:0.854801\n",
            "[98]\tvalidation_0-auc:0.853871\n",
            "[99]\tvalidation_0-auc:0.854181\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.8224489795918367 | 0.38461538461538464 |  0.5303030303030303 | 0.44585987261146504 |\n",
            "|      GRU 0.15     | 0.8755102040816326 |  0.6190476190476191 | 0.19696969696969696 | 0.29885057471264365 |\n",
            "|    XGBoost 0.15   | 0.8653061224489796 |         0.5         |  0.3181818181818182 |  0.3888888888888889 |\n",
            "|    Logreg 0.15    | 0.8918367346938776 |  0.782608695652174  |  0.2727272727272727 | 0.40449438202247184 |\n",
            "|      SVM 0.15     | 0.8816326530612245 |  0.7222222222222222 | 0.19696969696969696 | 0.30952380952380953 |\n",
            "|   LSTM beta 0.15  | 0.8687089715536105 |        0.875        | 0.10606060606060606 |  0.1891891891891892 |\n",
            "|   GRU beta 0.15   | 0.8643326039387309 |         1.0         | 0.06060606060606061 |  0.1142857142857143 |\n",
            "| XGBoost beta 0.15 | 0.9059080962800875 |  0.7804878048780488 | 0.48484848484848486 |  0.5981308411214953 |\n",
            "|  logreg beta 0.15 | 0.8971553610503282 |  0.9130434782608695 |  0.3181818181818182 | 0.47191011235955055 |\n",
            "|   svm beta 0.15   | 0.8796498905908097 |         1.0         | 0.16666666666666666 |  0.2857142857142857 |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXKfwY7wX4gx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d1384e11-857c-479b-8fbc-d8320c6440e8"
      },
      "source": [
        "Result_cross.to_csv('NVDA_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.760000</td>\n",
              "      <td>0.838776</td>\n",
              "      <td>0.324786</td>\n",
              "      <td>0.206522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.742857</td>\n",
              "      <td>0.846939</td>\n",
              "      <td>0.409449</td>\n",
              "      <td>0.282609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.830612</td>\n",
              "      <td>0.464516</td>\n",
              "      <td>0.391304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.844898</td>\n",
              "      <td>0.424242</td>\n",
              "      <td>0.304348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.844898</td>\n",
              "      <td>0.432836</td>\n",
              "      <td>0.315217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.877462</td>\n",
              "      <td>0.282051</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.743590</td>\n",
              "      <td>0.897155</td>\n",
              "      <td>0.552381</td>\n",
              "      <td>0.439394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.838074</td>\n",
              "      <td>0.139535</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.783784</td>\n",
              "      <td>0.901532</td>\n",
              "      <td>0.563107</td>\n",
              "      <td>0.439394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.886214</td>\n",
              "      <td>0.518519</td>\n",
              "      <td>0.424242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.871429</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.045455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.867347</td>\n",
              "      <td>0.029851</td>\n",
              "      <td>0.015152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.205128</td>\n",
              "      <td>0.121212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.869388</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.030303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.869388</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.030303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.855580</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.870897</td>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.121212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.903720</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.868709</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.870897</td>\n",
              "      <td>0.191781</td>\n",
              "      <td>0.106061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.384615</td>\n",
              "      <td>0.822449</td>\n",
              "      <td>0.445860</td>\n",
              "      <td>0.530303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.619048</td>\n",
              "      <td>0.875510</td>\n",
              "      <td>0.298851</td>\n",
              "      <td>0.196970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.865306</td>\n",
              "      <td>0.388889</td>\n",
              "      <td>0.318182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.891837</td>\n",
              "      <td>0.404494</td>\n",
              "      <td>0.272727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>0.881633</td>\n",
              "      <td>0.309524</td>\n",
              "      <td>0.196970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.868709</td>\n",
              "      <td>0.189189</td>\n",
              "      <td>0.106061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.864333</td>\n",
              "      <td>0.114286</td>\n",
              "      <td>0.060606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.780488</td>\n",
              "      <td>0.905908</td>\n",
              "      <td>0.598131</td>\n",
              "      <td>0.484848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.913043</td>\n",
              "      <td>0.897155</td>\n",
              "      <td>0.471910</td>\n",
              "      <td>0.318182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.879650</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  NVDA  0.760000  0.838776  0.324786  0.206522\n",
              "1            GRU 0.1  NVDA  0.742857  0.846939  0.409449  0.282609\n",
              "2        XGBoost 0.1  NVDA  0.571429  0.830612  0.464516  0.391304\n",
              "3         Logreg 0.1  NVDA  0.700000  0.844898  0.424242  0.304348\n",
              "4            SVM 0.1  NVDA  0.690476  0.844898  0.432836  0.315217\n",
              "5      LSTM beta 0.1  NVDA  0.916667  0.877462  0.282051  0.166667\n",
              "6       GRU beta 0.1  NVDA  0.743590  0.897155  0.552381  0.439394\n",
              "7   XGBoost beta 0.1  NVDA  0.300000  0.838074  0.139535  0.090909\n",
              "8    logreg beta 0.1  NVDA  0.783784  0.901532  0.563107  0.439394\n",
              "9       svm beta 0.1  NVDA  0.666667  0.886214  0.518519  0.424242\n",
              "0           LSTM 0.2  NVDA  1.000000  0.871429  0.086957  0.045455\n",
              "1            GRU 0.2  NVDA  1.000000  0.867347  0.029851  0.015152\n",
              "2        XGBoost 0.2  NVDA  0.666667  0.873469  0.205128  0.121212\n",
              "3         Logreg 0.2  NVDA  1.000000  0.869388  0.058824  0.030303\n",
              "4            SVM 0.2  NVDA  1.000000  0.869388  0.058824  0.030303\n",
              "5      LSTM beta 0.2  NVDA  0.000000  0.855580  0.000000  0.000000\n",
              "6       GRU beta 0.2  NVDA  0.888889  0.870897  0.213333  0.121212\n",
              "7   XGBoost beta 0.2  NVDA  1.000000  0.903720  0.500000  0.333333\n",
              "8    logreg beta 0.2  NVDA  1.000000  0.868709  0.166667  0.090909\n",
              "9       svm beta 0.2  NVDA  1.000000  0.870897  0.191781  0.106061\n",
              "0          LSTM 0.15  NVDA  0.384615  0.822449  0.445860  0.530303\n",
              "1           GRU 0.15  NVDA  0.619048  0.875510  0.298851  0.196970\n",
              "2       XGBoost 0.15  NVDA  0.500000  0.865306  0.388889  0.318182\n",
              "3        Logreg 0.15  NVDA  0.782609  0.891837  0.404494  0.272727\n",
              "4           SVM 0.15  NVDA  0.722222  0.881633  0.309524  0.196970\n",
              "5     LSTM beta 0.15  NVDA  0.875000  0.868709  0.189189  0.106061\n",
              "6      GRU beta 0.15  NVDA  1.000000  0.864333  0.114286  0.060606\n",
              "7  XGBoost beta 0.15  NVDA  0.780488  0.905908  0.598131  0.484848\n",
              "8   logreg beta 0.15  NVDA  0.913043  0.897155  0.471910  0.318182\n",
              "9      svm beta 0.15  NVDA  1.000000  0.879650  0.285714  0.166667"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MQu2phiX4gx"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_logreg_beta.csv')"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN8EgkdNX4gx"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKxRkDlpX4gx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0954877b-eb21-453c-825e-c51f86b38160"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"NVDA\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6129 - accuracy: 0.7161 - val_loss: 0.5083 - val_accuracy: 0.8122\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6001 - accuracy: 0.7208 - val_loss: 0.5182 - val_accuracy: 0.8122\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5861 - accuracy: 0.7168 - val_loss: 0.5237 - val_accuracy: 0.8122\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5210 - accuracy: 0.7550 - val_loss: 0.5804 - val_accuracy: 0.6510\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5080 - accuracy: 0.7705 - val_loss: 0.4577 - val_accuracy: 0.8204\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.5840 - accuracy: 0.7248 - val_loss: 0.5098 - val_accuracy: 0.8204\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4940 - accuracy: 0.7812 - val_loss: 0.5140 - val_accuracy: 0.8347\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4523 - accuracy: 0.8148 - val_loss: 0.4689 - val_accuracy: 0.8327\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4402 - accuracy: 0.8047 - val_loss: 0.4591 - val_accuracy: 0.8327\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4371 - accuracy: 0.8040 - val_loss: 0.4502 - val_accuracy: 0.8388\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.711165\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.710782\n",
            "[2]\tvalidation_0-auc:0.724806\n",
            "[3]\tvalidation_0-auc:0.726417\n",
            "[4]\tvalidation_0-auc:0.729353\n",
            "[5]\tvalidation_0-auc:0.735703\n",
            "[6]\tvalidation_0-auc:0.731702\n",
            "[7]\tvalidation_0-auc:0.732357\n",
            "[8]\tvalidation_0-auc:0.735498\n",
            "[9]\tvalidation_0-auc:0.740469\n",
            "[10]\tvalidation_0-auc:0.733177\n",
            "[11]\tvalidation_0-auc:0.733832\n",
            "[12]\tvalidation_0-auc:0.738871\n",
            "[13]\tvalidation_0-auc:0.732699\n",
            "[14]\tvalidation_0-auc:0.732494\n",
            "[15]\tvalidation_0-auc:0.732221\n",
            "[16]\tvalidation_0-auc:0.736072\n",
            "[17]\tvalidation_0-auc:0.735198\n",
            "[18]\tvalidation_0-auc:0.731224\n",
            "[19]\tvalidation_0-auc:0.730309\n",
            "[20]\tvalidation_0-auc:0.730528\n",
            "[21]\tvalidation_0-auc:0.730459\n",
            "[22]\tvalidation_0-auc:0.729613\n",
            "[23]\tvalidation_0-auc:0.731552\n",
            "[24]\tvalidation_0-auc:0.730541\n",
            "[25]\tvalidation_0-auc:0.730446\n",
            "[26]\tvalidation_0-auc:0.72583\n",
            "[27]\tvalidation_0-auc:0.725858\n",
            "[28]\tvalidation_0-auc:0.722307\n",
            "[29]\tvalidation_0-auc:0.721911\n",
            "[30]\tvalidation_0-auc:0.72314\n",
            "[31]\tvalidation_0-auc:0.724943\n",
            "[32]\tvalidation_0-auc:0.724751\n",
            "[33]\tvalidation_0-auc:0.724424\n",
            "[34]\tvalidation_0-auc:0.724751\n",
            "[35]\tvalidation_0-auc:0.72467\n",
            "[36]\tvalidation_0-auc:0.725421\n",
            "[37]\tvalidation_0-auc:0.72527\n",
            "[38]\tvalidation_0-auc:0.725107\n",
            "[39]\tvalidation_0-auc:0.721119\n",
            "[40]\tvalidation_0-auc:0.721228\n",
            "[41]\tvalidation_0-auc:0.721228\n",
            "[42]\tvalidation_0-auc:0.721952\n",
            "[43]\tvalidation_0-auc:0.722853\n",
            "[44]\tvalidation_0-auc:0.723058\n",
            "[45]\tvalidation_0-auc:0.722498\n",
            "[46]\tvalidation_0-auc:0.722744\n",
            "[47]\tvalidation_0-auc:0.722758\n",
            "[48]\tvalidation_0-auc:0.722758\n",
            "[49]\tvalidation_0-auc:0.723782\n",
            "[50]\tvalidation_0-auc:0.722881\n",
            "[51]\tvalidation_0-auc:0.722881\n",
            "[52]\tvalidation_0-auc:0.722881\n",
            "[53]\tvalidation_0-auc:0.723768\n",
            "[54]\tvalidation_0-auc:0.722034\n",
            "[55]\tvalidation_0-auc:0.722908\n",
            "[56]\tvalidation_0-auc:0.722143\n",
            "[57]\tvalidation_0-auc:0.722908\n",
            "[58]\tvalidation_0-auc:0.723318\n",
            "[59]\tvalidation_0-auc:0.722785\n",
            "Stopping. Best iteration:\n",
            "[9]\tvalidation_0-auc:0.740469\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6156 - accuracy: 0.7104 - val_loss: 0.4857 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5991 - accuracy: 0.7145 - val_loss: 0.4392 - val_accuracy: 0.8556\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5673 - accuracy: 0.7097 - val_loss: 0.4341 - val_accuracy: 0.8556\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5183 - accuracy: 0.7419 - val_loss: 0.4814 - val_accuracy: 0.9037\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4956 - accuracy: 0.7763 - val_loss: 0.4270 - val_accuracy: 0.8972\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6002 - accuracy: 0.7179 - val_loss: 0.4660 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5101 - accuracy: 0.7563 - val_loss: 0.3908 - val_accuracy: 0.9190\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4794 - accuracy: 0.7797 - val_loss: 0.3869 - val_accuracy: 0.9015\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4805 - accuracy: 0.7968 - val_loss: 0.3791 - val_accuracy: 0.9125\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4743 - accuracy: 0.7920 - val_loss: 0.4626 - val_accuracy: 0.8753\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.789816\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.738065\n",
            "[2]\tvalidation_0-auc:0.720763\n",
            "[3]\tvalidation_0-auc:0.720646\n",
            "[4]\tvalidation_0-auc:0.740739\n",
            "[5]\tvalidation_0-auc:0.741417\n",
            "[6]\tvalidation_0-auc:0.770984\n",
            "[7]\tvalidation_0-auc:0.748392\n",
            "[8]\tvalidation_0-auc:0.746764\n",
            "[9]\tvalidation_0-auc:0.739518\n",
            "[10]\tvalidation_0-auc:0.725548\n",
            "[11]\tvalidation_0-auc:0.740952\n",
            "[12]\tvalidation_0-auc:0.74752\n",
            "[13]\tvalidation_0-auc:0.739382\n",
            "[14]\tvalidation_0-auc:0.735701\n",
            "[15]\tvalidation_0-auc:0.73357\n",
            "[16]\tvalidation_0-auc:0.747888\n",
            "[17]\tvalidation_0-auc:0.732717\n",
            "[18]\tvalidation_0-auc:0.737096\n",
            "[19]\tvalidation_0-auc:0.737406\n",
            "[20]\tvalidation_0-auc:0.746086\n",
            "[21]\tvalidation_0-auc:0.75124\n",
            "[22]\tvalidation_0-auc:0.750368\n",
            "[23]\tvalidation_0-auc:0.756413\n",
            "[24]\tvalidation_0-auc:0.755987\n",
            "[25]\tvalidation_0-auc:0.756336\n",
            "[26]\tvalidation_0-auc:0.759668\n",
            "[27]\tvalidation_0-auc:0.759668\n",
            "[28]\tvalidation_0-auc:0.75777\n",
            "[29]\tvalidation_0-auc:0.75777\n",
            "[30]\tvalidation_0-auc:0.760405\n",
            "[31]\tvalidation_0-auc:0.754069\n",
            "[32]\tvalidation_0-auc:0.75279\n",
            "[33]\tvalidation_0-auc:0.757886\n",
            "[34]\tvalidation_0-auc:0.757886\n",
            "[35]\tvalidation_0-auc:0.757886\n",
            "[36]\tvalidation_0-auc:0.757498\n",
            "[37]\tvalidation_0-auc:0.756336\n",
            "[38]\tvalidation_0-auc:0.757808\n",
            "[39]\tvalidation_0-auc:0.758312\n",
            "[40]\tvalidation_0-auc:0.752887\n",
            "[41]\tvalidation_0-auc:0.749884\n",
            "[42]\tvalidation_0-auc:0.756646\n",
            "[43]\tvalidation_0-auc:0.755754\n",
            "[44]\tvalidation_0-auc:0.755754\n",
            "[45]\tvalidation_0-auc:0.762652\n",
            "[46]\tvalidation_0-auc:0.762652\n",
            "[47]\tvalidation_0-auc:0.768348\n",
            "[48]\tvalidation_0-auc:0.767244\n",
            "[49]\tvalidation_0-auc:0.766895\n",
            "[50]\tvalidation_0-auc:0.766314\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.789816\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.8204081632653061 | 0.8333333333333334 | 0.05434782608695652 |  0.1020408163265306 |\n",
            "|     GRU 0.1      | 0.8387755102040816 | 0.6101694915254238 |  0.391304347826087  |  0.4768211920529801 |\n",
            "|   XGBoost 0.1    | 0.8306122448979592 | 0.5714285714285714 |  0.391304347826087  |  0.4645161290322581 |\n",
            "|    Logreg 0.1    | 0.8448979591836735 |        0.7         | 0.30434782608695654 | 0.42424242424242425 |\n",
            "|     SVM 0.1      | 0.8448979591836735 | 0.6904761904761905 | 0.31521739130434784 | 0.43283582089552236 |\n",
            "|  LSTM beta 0.1   | 0.8971553610503282 | 0.6862745098039216 |  0.5303030303030303 |  0.5982905982905983 |\n",
            "|   GRU beta 0.1   |  0.87527352297593  | 0.5692307692307692 |  0.5606060606060606 |  0.564885496183206  |\n",
            "| XGBoost beta 0.1 | 0.838074398249453  |        0.3         | 0.09090909090909091 | 0.13953488372093023 |\n",
            "| logreg beta 0.1  | 0.9015317286652079 | 0.7837837837837838 |  0.4393939393939394 |  0.5631067961165048 |\n",
            "|   svm beta 0.1   | 0.8862144420131292 | 0.6666666666666666 | 0.42424242424242425 |  0.5185185185185185 |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.5254 - accuracy: 0.7919 - val_loss: 0.4216 - val_accuracy: 0.8653\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5146 - accuracy: 0.7940 - val_loss: 0.3922 - val_accuracy: 0.8653\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4880 - accuracy: 0.7946 - val_loss: 0.4106 - val_accuracy: 0.8653\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4649 - accuracy: 0.7933 - val_loss: 0.3881 - val_accuracy: 0.8653\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4469 - accuracy: 0.8101 - val_loss: 0.3651 - val_accuracy: 0.8653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.5058 - accuracy: 0.7953 - val_loss: 0.4055 - val_accuracy: 0.8653\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4499 - accuracy: 0.8148 - val_loss: 0.3764 - val_accuracy: 0.8776\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4305 - accuracy: 0.8302 - val_loss: 0.3496 - val_accuracy: 0.8694\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4328 - accuracy: 0.8369 - val_loss: 0.3538 - val_accuracy: 0.8714\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4303 - accuracy: 0.8322 - val_loss: 0.3754 - val_accuracy: 0.8735\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.710674\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.730685\n",
            "[2]\tvalidation_0-auc:0.730685\n",
            "[3]\tvalidation_0-auc:0.748785\n",
            "[4]\tvalidation_0-auc:0.74925\n",
            "[5]\tvalidation_0-auc:0.756754\n",
            "[6]\tvalidation_0-auc:0.748571\n",
            "[7]\tvalidation_0-auc:0.75209\n",
            "[8]\tvalidation_0-auc:0.752484\n",
            "[9]\tvalidation_0-auc:0.750607\n",
            "[10]\tvalidation_0-auc:0.749428\n",
            "[11]\tvalidation_0-auc:0.736081\n",
            "[12]\tvalidation_0-auc:0.743729\n",
            "[13]\tvalidation_0-auc:0.744425\n",
            "[14]\tvalidation_0-auc:0.742353\n",
            "[15]\tvalidation_0-auc:0.743175\n",
            "[16]\tvalidation_0-auc:0.745497\n",
            "[17]\tvalidation_0-auc:0.745265\n",
            "[18]\tvalidation_0-auc:0.746498\n",
            "[19]\tvalidation_0-auc:0.746373\n",
            "[20]\tvalidation_0-auc:0.745426\n",
            "[21]\tvalidation_0-auc:0.744836\n",
            "[22]\tvalidation_0-auc:0.747088\n",
            "[23]\tvalidation_0-auc:0.747016\n",
            "[24]\tvalidation_0-auc:0.745748\n",
            "[25]\tvalidation_0-auc:0.745247\n",
            "[26]\tvalidation_0-auc:0.74539\n",
            "[27]\tvalidation_0-auc:0.740066\n",
            "[28]\tvalidation_0-auc:0.73869\n",
            "[29]\tvalidation_0-auc:0.738618\n",
            "[30]\tvalidation_0-auc:0.736224\n",
            "[31]\tvalidation_0-auc:0.735903\n",
            "[32]\tvalidation_0-auc:0.736046\n",
            "[33]\tvalidation_0-auc:0.736189\n",
            "[34]\tvalidation_0-auc:0.731418\n",
            "[35]\tvalidation_0-auc:0.726629\n",
            "[36]\tvalidation_0-auc:0.726004\n",
            "[37]\tvalidation_0-auc:0.729506\n",
            "[38]\tvalidation_0-auc:0.728613\n",
            "[39]\tvalidation_0-auc:0.72704\n",
            "[40]\tvalidation_0-auc:0.726719\n",
            "[41]\tvalidation_0-auc:0.723485\n",
            "[42]\tvalidation_0-auc:0.723378\n",
            "[43]\tvalidation_0-auc:0.724092\n",
            "[44]\tvalidation_0-auc:0.723842\n",
            "[45]\tvalidation_0-auc:0.719054\n",
            "[46]\tvalidation_0-auc:0.717982\n",
            "[47]\tvalidation_0-auc:0.716785\n",
            "[48]\tvalidation_0-auc:0.716356\n",
            "[49]\tvalidation_0-auc:0.715266\n",
            "[50]\tvalidation_0-auc:0.715552\n",
            "[51]\tvalidation_0-auc:0.714623\n",
            "[52]\tvalidation_0-auc:0.713336\n",
            "[53]\tvalidation_0-auc:0.713211\n",
            "[54]\tvalidation_0-auc:0.71364\n",
            "[55]\tvalidation_0-auc:0.713908\n",
            "Stopping. Best iteration:\n",
            "[5]\tvalidation_0-auc:0.756754\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.5432 - accuracy: 0.7859 - val_loss: 0.4136 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5217 - accuracy: 0.7900 - val_loss: 0.4417 - val_accuracy: 0.8556\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4978 - accuracy: 0.7886 - val_loss: 0.3819 - val_accuracy: 0.8556\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4758 - accuracy: 0.7927 - val_loss: 0.3621 - val_accuracy: 0.8556\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4488 - accuracy: 0.7955 - val_loss: 0.3503 - val_accuracy: 0.8556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.4922 - accuracy: 0.8010 - val_loss: 0.3454 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4312 - accuracy: 0.8325 - val_loss: 0.3421 - val_accuracy: 0.8556\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4351 - accuracy: 0.8291 - val_loss: 0.3397 - val_accuracy: 0.8775\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4073 - accuracy: 0.8339 - val_loss: 0.3123 - val_accuracy: 0.8578\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4042 - accuracy: 0.8312 - val_loss: 0.3726 - val_accuracy: 0.9409\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.84831\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.850868\n",
            "[2]\tvalidation_0-auc:0.84862\n",
            "[3]\tvalidation_0-auc:0.844397\n",
            "[4]\tvalidation_0-auc:0.882624\n",
            "[5]\tvalidation_0-auc:0.898958\n",
            "[6]\tvalidation_0-auc:0.890801\n",
            "[7]\tvalidation_0-auc:0.890839\n",
            "[8]\tvalidation_0-auc:0.889793\n",
            "[9]\tvalidation_0-auc:0.895451\n",
            "[10]\tvalidation_0-auc:0.898744\n",
            "[11]\tvalidation_0-auc:0.897563\n",
            "[12]\tvalidation_0-auc:0.897834\n",
            "[13]\tvalidation_0-auc:0.89795\n",
            "[14]\tvalidation_0-auc:0.897291\n",
            "[15]\tvalidation_0-auc:0.896691\n",
            "[16]\tvalidation_0-auc:0.895199\n",
            "[17]\tvalidation_0-auc:0.892041\n",
            "[18]\tvalidation_0-auc:0.890471\n",
            "[19]\tvalidation_0-auc:0.892409\n",
            "[20]\tvalidation_0-auc:0.890626\n",
            "[21]\tvalidation_0-auc:0.890859\n",
            "[22]\tvalidation_0-auc:0.88896\n",
            "[23]\tvalidation_0-auc:0.890452\n",
            "[24]\tvalidation_0-auc:0.890801\n",
            "[25]\tvalidation_0-auc:0.891188\n",
            "[26]\tvalidation_0-auc:0.892506\n",
            "[27]\tvalidation_0-auc:0.892506\n",
            "[28]\tvalidation_0-auc:0.890917\n",
            "[29]\tvalidation_0-auc:0.891692\n",
            "[30]\tvalidation_0-auc:0.886867\n",
            "[31]\tvalidation_0-auc:0.887487\n",
            "[32]\tvalidation_0-auc:0.887022\n",
            "[33]\tvalidation_0-auc:0.886635\n",
            "[34]\tvalidation_0-auc:0.886538\n",
            "[35]\tvalidation_0-auc:0.881016\n",
            "[36]\tvalidation_0-auc:0.881016\n",
            "[37]\tvalidation_0-auc:0.879757\n",
            "[38]\tvalidation_0-auc:0.879485\n",
            "[39]\tvalidation_0-auc:0.880144\n",
            "[40]\tvalidation_0-auc:0.879292\n",
            "[41]\tvalidation_0-auc:0.878827\n",
            "[42]\tvalidation_0-auc:0.878517\n",
            "[43]\tvalidation_0-auc:0.879001\n",
            "[44]\tvalidation_0-auc:0.880745\n",
            "[45]\tvalidation_0-auc:0.881132\n",
            "[46]\tvalidation_0-auc:0.878265\n",
            "[47]\tvalidation_0-auc:0.877528\n",
            "[48]\tvalidation_0-auc:0.877257\n",
            "[49]\tvalidation_0-auc:0.877257\n",
            "[50]\tvalidation_0-auc:0.876928\n",
            "[51]\tvalidation_0-auc:0.876192\n",
            "[52]\tvalidation_0-auc:0.873363\n",
            "[53]\tvalidation_0-auc:0.875029\n",
            "[54]\tvalidation_0-auc:0.874525\n",
            "[55]\tvalidation_0-auc:0.872704\n",
            "Stopping. Best iteration:\n",
            "[5]\tvalidation_0-auc:0.898958\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+----------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall        |       F1 score      |\n",
            "+------------------+--------------------+--------------------+----------------------+---------------------+\n",
            "|     LSTM 0.2     | 0.8653061224489796 |        0.0         |         0.0          |         0.0         |\n",
            "|     GRU 0.2      | 0.8734693877551021 | 0.8333333333333334 | 0.07575757575757576  |  0.1388888888888889 |\n",
            "|   XGBoost 0.2    | 0.8734693877551021 | 0.6666666666666666 | 0.12121212121212122  | 0.20512820512820512 |\n",
            "|    Logreg 0.2    | 0.8693877551020408 |        1.0         | 0.030303030303030304 | 0.05882352941176471 |\n",
            "|     SVM 0.2      | 0.8693877551020408 |        1.0         | 0.030303030303030304 | 0.05882352941176471 |\n",
            "|  LSTM beta 0.2   | 0.8555798687089715 |        0.0         |         0.0          |         0.0         |\n",
            "|   GRU beta 0.2   | 0.9409190371991247 | 0.9148936170212766 |  0.6515151515151515  |  0.7610619469026548 |\n",
            "| XGBoost beta 0.2 | 0.9037199124726477 |        1.0         |  0.3333333333333333  |         0.5         |\n",
            "| logreg beta 0.2  | 0.8687089715536105 |        1.0         | 0.09090909090909091  | 0.16666666666666669 |\n",
            "|   svm beta 0.2   | 0.8708971553610503 |        1.0         | 0.10606060606060606  | 0.19178082191780824 |\n",
            "+------------------+--------------------+--------------------+----------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.5831 - accuracy: 0.7403 - val_loss: 0.4773 - val_accuracy: 0.8653\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5556 - accuracy: 0.7463 - val_loss: 0.3639 - val_accuracy: 0.8714\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5312 - accuracy: 0.7591 - val_loss: 0.4769 - val_accuracy: 0.8694\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5032 - accuracy: 0.7718 - val_loss: 0.3785 - val_accuracy: 0.8714\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4830 - accuracy: 0.7779 - val_loss: 0.4235 - val_accuracy: 0.8612\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.5828 - accuracy: 0.7356 - val_loss: 0.4199 - val_accuracy: 0.8653\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5075 - accuracy: 0.7705 - val_loss: 0.3558 - val_accuracy: 0.8694\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4613 - accuracy: 0.7913 - val_loss: 0.3975 - val_accuracy: 0.8612\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4620 - accuracy: 0.8060 - val_loss: 0.3889 - val_accuracy: 0.8694\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4666 - accuracy: 0.8040 - val_loss: 0.4386 - val_accuracy: 0.8592\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.709602\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.720662\n",
            "[2]\tvalidation_0-auc:0.721573\n",
            "[3]\tvalidation_0-auc:0.723896\n",
            "[4]\tvalidation_0-auc:0.725254\n",
            "[5]\tvalidation_0-auc:0.729703\n",
            "[6]\tvalidation_0-auc:0.733097\n",
            "[7]\tvalidation_0-auc:0.741048\n",
            "[8]\tvalidation_0-auc:0.735617\n",
            "[9]\tvalidation_0-auc:0.737207\n",
            "[10]\tvalidation_0-auc:0.732937\n",
            "[11]\tvalidation_0-auc:0.732365\n",
            "[12]\tvalidation_0-auc:0.730793\n",
            "[13]\tvalidation_0-auc:0.734884\n",
            "[14]\tvalidation_0-auc:0.733919\n",
            "[15]\tvalidation_0-auc:0.733223\n",
            "[16]\tvalidation_0-auc:0.735277\n",
            "[17]\tvalidation_0-auc:0.734706\n",
            "[18]\tvalidation_0-auc:0.733187\n",
            "[19]\tvalidation_0-auc:0.733508\n",
            "[20]\tvalidation_0-auc:0.739065\n",
            "[21]\tvalidation_0-auc:0.737171\n",
            "[22]\tvalidation_0-auc:0.738958\n",
            "[23]\tvalidation_0-auc:0.738761\n",
            "[24]\tvalidation_0-auc:0.740548\n",
            "[25]\tvalidation_0-auc:0.740798\n",
            "[26]\tvalidation_0-auc:0.73953\n",
            "[27]\tvalidation_0-auc:0.737171\n",
            "[28]\tvalidation_0-auc:0.738315\n",
            "[29]\tvalidation_0-auc:0.737457\n",
            "[30]\tvalidation_0-auc:0.735706\n",
            "[31]\tvalidation_0-auc:0.737886\n",
            "[32]\tvalidation_0-auc:0.736582\n",
            "[33]\tvalidation_0-auc:0.737278\n",
            "[34]\tvalidation_0-auc:0.736742\n",
            "[35]\tvalidation_0-auc:0.734455\n",
            "[36]\tvalidation_0-auc:0.734527\n",
            "[37]\tvalidation_0-auc:0.735706\n",
            "[38]\tvalidation_0-auc:0.735938\n",
            "[39]\tvalidation_0-auc:0.733919\n",
            "[40]\tvalidation_0-auc:0.735206\n",
            "[41]\tvalidation_0-auc:0.73542\n",
            "[42]\tvalidation_0-auc:0.733401\n",
            "[43]\tvalidation_0-auc:0.730864\n",
            "[44]\tvalidation_0-auc:0.733187\n",
            "[45]\tvalidation_0-auc:0.732937\n",
            "[46]\tvalidation_0-auc:0.730149\n",
            "[47]\tvalidation_0-auc:0.732222\n",
            "[48]\tvalidation_0-auc:0.732847\n",
            "[49]\tvalidation_0-auc:0.733097\n",
            "[50]\tvalidation_0-auc:0.731096\n",
            "[51]\tvalidation_0-auc:0.731525\n",
            "[52]\tvalidation_0-auc:0.728774\n",
            "[53]\tvalidation_0-auc:0.730382\n",
            "[54]\tvalidation_0-auc:0.730596\n",
            "[55]\tvalidation_0-auc:0.727987\n",
            "[56]\tvalidation_0-auc:0.726844\n",
            "[57]\tvalidation_0-auc:0.723878\n",
            "Stopping. Best iteration:\n",
            "[7]\tvalidation_0-auc:0.741048\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.5896 - accuracy: 0.7303 - val_loss: 0.5245 - val_accuracy: 0.8556\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5817 - accuracy: 0.7364 - val_loss: 0.4910 - val_accuracy: 0.8556\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5692 - accuracy: 0.7364 - val_loss: 0.3984 - val_accuracy: 0.8556\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5325 - accuracy: 0.7454 - val_loss: 0.4011 - val_accuracy: 0.8556\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5026 - accuracy: 0.7660 - val_loss: 0.3843 - val_accuracy: 0.8621\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5683 - accuracy: 0.7502 - val_loss: 0.4595 - val_accuracy: 0.8578\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4880 - accuracy: 0.7804 - val_loss: 0.5338 - val_accuracy: 0.8534\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4715 - accuracy: 0.7865 - val_loss: 0.4267 - val_accuracy: 0.8972\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4555 - accuracy: 0.8003 - val_loss: 0.3282 - val_accuracy: 0.8687\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4707 - accuracy: 0.7852 - val_loss: 0.3526 - val_accuracy: 0.8709\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.828489\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.821902\n",
            "[2]\tvalidation_0-auc:0.810722\n",
            "[3]\tvalidation_0-auc:0.830776\n",
            "[4]\tvalidation_0-auc:0.835019\n",
            "[5]\tvalidation_0-auc:0.836143\n",
            "[6]\tvalidation_0-auc:0.831841\n",
            "[7]\tvalidation_0-auc:0.824091\n",
            "[8]\tvalidation_0-auc:0.831551\n",
            "[9]\tvalidation_0-auc:0.832558\n",
            "[10]\tvalidation_0-auc:0.832461\n",
            "[11]\tvalidation_0-auc:0.83159\n",
            "[12]\tvalidation_0-auc:0.843292\n",
            "[13]\tvalidation_0-auc:0.845656\n",
            "[14]\tvalidation_0-auc:0.846896\n",
            "[15]\tvalidation_0-auc:0.848524\n",
            "[16]\tvalidation_0-auc:0.849841\n",
            "[17]\tvalidation_0-auc:0.849376\n",
            "[18]\tvalidation_0-auc:0.85143\n",
            "[19]\tvalidation_0-auc:0.850655\n",
            "[20]\tvalidation_0-auc:0.851042\n",
            "[21]\tvalidation_0-auc:0.851585\n",
            "[22]\tvalidation_0-auc:0.850074\n",
            "[23]\tvalidation_0-auc:0.853232\n",
            "[24]\tvalidation_0-auc:0.853348\n",
            "[25]\tvalidation_0-auc:0.854084\n",
            "[26]\tvalidation_0-auc:0.853387\n",
            "[27]\tvalidation_0-auc:0.859664\n",
            "[28]\tvalidation_0-auc:0.858076\n",
            "[29]\tvalidation_0-auc:0.857456\n",
            "[30]\tvalidation_0-auc:0.858192\n",
            "[31]\tvalidation_0-auc:0.858231\n",
            "[32]\tvalidation_0-auc:0.859781\n",
            "[33]\tvalidation_0-auc:0.862493\n",
            "[34]\tvalidation_0-auc:0.862067\n",
            "[35]\tvalidation_0-auc:0.861641\n",
            "[36]\tvalidation_0-auc:0.862028\n",
            "[37]\tvalidation_0-auc:0.861951\n",
            "[38]\tvalidation_0-auc:0.860749\n",
            "[39]\tvalidation_0-auc:0.861311\n",
            "[40]\tvalidation_0-auc:0.860808\n",
            "[41]\tvalidation_0-auc:0.860575\n",
            "[42]\tvalidation_0-auc:0.860265\n",
            "[43]\tvalidation_0-auc:0.861544\n",
            "[44]\tvalidation_0-auc:0.861699\n",
            "[45]\tvalidation_0-auc:0.861583\n",
            "[46]\tvalidation_0-auc:0.864063\n",
            "[47]\tvalidation_0-auc:0.866543\n",
            "[48]\tvalidation_0-auc:0.866349\n",
            "[49]\tvalidation_0-auc:0.867046\n",
            "[50]\tvalidation_0-auc:0.866775\n",
            "[51]\tvalidation_0-auc:0.868984\n",
            "[52]\tvalidation_0-auc:0.868984\n",
            "[53]\tvalidation_0-auc:0.866891\n",
            "[54]\tvalidation_0-auc:0.868015\n",
            "[55]\tvalidation_0-auc:0.867938\n",
            "[56]\tvalidation_0-auc:0.867938\n",
            "[57]\tvalidation_0-auc:0.86693\n",
            "[58]\tvalidation_0-auc:0.86662\n",
            "[59]\tvalidation_0-auc:0.867046\n",
            "[60]\tvalidation_0-auc:0.866795\n",
            "[61]\tvalidation_0-auc:0.863074\n",
            "[62]\tvalidation_0-auc:0.862609\n",
            "[63]\tvalidation_0-auc:0.862454\n",
            "[64]\tvalidation_0-auc:0.862454\n",
            "[65]\tvalidation_0-auc:0.863772\n",
            "[66]\tvalidation_0-auc:0.863501\n",
            "[67]\tvalidation_0-auc:0.862513\n",
            "[68]\tvalidation_0-auc:0.862513\n",
            "[69]\tvalidation_0-auc:0.862048\n",
            "[70]\tvalidation_0-auc:0.861815\n",
            "[71]\tvalidation_0-auc:0.86228\n",
            "[72]\tvalidation_0-auc:0.86228\n",
            "[73]\tvalidation_0-auc:0.861137\n",
            "[74]\tvalidation_0-auc:0.861873\n",
            "[75]\tvalidation_0-auc:0.86197\n",
            "[76]\tvalidation_0-auc:0.863365\n",
            "[77]\tvalidation_0-auc:0.865496\n",
            "[78]\tvalidation_0-auc:0.86445\n",
            "[79]\tvalidation_0-auc:0.863753\n",
            "[80]\tvalidation_0-auc:0.863753\n",
            "[81]\tvalidation_0-auc:0.862513\n",
            "[82]\tvalidation_0-auc:0.86197\n",
            "[83]\tvalidation_0-auc:0.862086\n",
            "[84]\tvalidation_0-auc:0.862474\n",
            "[85]\tvalidation_0-auc:0.862823\n",
            "[86]\tvalidation_0-auc:0.860033\n",
            "[87]\tvalidation_0-auc:0.859723\n",
            "[88]\tvalidation_0-auc:0.859529\n",
            "[89]\tvalidation_0-auc:0.857882\n",
            "[90]\tvalidation_0-auc:0.857436\n",
            "[91]\tvalidation_0-auc:0.857204\n",
            "[92]\tvalidation_0-auc:0.856971\n",
            "[93]\tvalidation_0-auc:0.856855\n",
            "[94]\tvalidation_0-auc:0.856157\n",
            "[95]\tvalidation_0-auc:0.854917\n",
            "[96]\tvalidation_0-auc:0.854801\n",
            "[97]\tvalidation_0-auc:0.854801\n",
            "[98]\tvalidation_0-auc:0.853871\n",
            "[99]\tvalidation_0-auc:0.854181\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.8612244897959184 | 0.47058823529411764 | 0.24242424242424243 |         0.32        |\n",
            "|      GRU 0.15     | 0.8591836734693877 |  0.4716981132075472 |  0.3787878787878788 | 0.42016806722689076 |\n",
            "|    XGBoost 0.15   | 0.8653061224489796 |         0.5         |  0.3181818181818182 |  0.3888888888888889 |\n",
            "|    Logreg 0.15    | 0.8918367346938776 |  0.782608695652174  |  0.2727272727272727 | 0.40449438202247184 |\n",
            "|      SVM 0.15     | 0.8816326530612245 |  0.7222222222222222 | 0.19696969696969696 | 0.30952380952380953 |\n",
            "|   LSTM beta 0.15  | 0.862144420131291  |         0.8         | 0.06060606060606061 | 0.11267605633802819 |\n",
            "|   GRU beta 0.15   | 0.8708971553610503 |  0.8888888888888888 | 0.12121212121212122 | 0.21333333333333335 |\n",
            "| XGBoost beta 0.15 | 0.9059080962800875 |  0.7804878048780488 | 0.48484848484848486 |  0.5981308411214953 |\n",
            "|  logreg beta 0.15 | 0.8971553610503282 |  0.9130434782608695 |  0.3181818181818182 | 0.47191011235955055 |\n",
            "|   svm beta 0.15   | 0.8796498905908097 |         1.0         | 0.16666666666666666 |  0.2857142857142857 |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9JnnvtXX4gx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f93cc316-cfa2-435a-aa84-1eb93dca4825"
      },
      "source": [
        "Result_purging.to_csv('NVDA_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.820408</td>\n",
              "      <td>0.102041</td>\n",
              "      <td>0.054348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.610169</td>\n",
              "      <td>0.838776</td>\n",
              "      <td>0.476821</td>\n",
              "      <td>0.391304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.830612</td>\n",
              "      <td>0.464516</td>\n",
              "      <td>0.391304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.844898</td>\n",
              "      <td>0.424242</td>\n",
              "      <td>0.304348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.844898</td>\n",
              "      <td>0.432836</td>\n",
              "      <td>0.315217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.686275</td>\n",
              "      <td>0.897155</td>\n",
              "      <td>0.598291</td>\n",
              "      <td>0.530303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.569231</td>\n",
              "      <td>0.875274</td>\n",
              "      <td>0.564885</td>\n",
              "      <td>0.560606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.838074</td>\n",
              "      <td>0.139535</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.783784</td>\n",
              "      <td>0.901532</td>\n",
              "      <td>0.563107</td>\n",
              "      <td>0.439394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.886214</td>\n",
              "      <td>0.518519</td>\n",
              "      <td>0.424242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.865306</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.138889</td>\n",
              "      <td>0.075758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.873469</td>\n",
              "      <td>0.205128</td>\n",
              "      <td>0.121212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.869388</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.030303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.869388</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.030303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.855580</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.914894</td>\n",
              "      <td>0.940919</td>\n",
              "      <td>0.761062</td>\n",
              "      <td>0.651515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.903720</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.868709</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.870897</td>\n",
              "      <td>0.191781</td>\n",
              "      <td>0.106061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.861224</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.242424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.471698</td>\n",
              "      <td>0.859184</td>\n",
              "      <td>0.420168</td>\n",
              "      <td>0.378788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.865306</td>\n",
              "      <td>0.388889</td>\n",
              "      <td>0.318182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.891837</td>\n",
              "      <td>0.404494</td>\n",
              "      <td>0.272727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>0.881633</td>\n",
              "      <td>0.309524</td>\n",
              "      <td>0.196970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.862144</td>\n",
              "      <td>0.112676</td>\n",
              "      <td>0.060606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.870897</td>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.121212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.780488</td>\n",
              "      <td>0.905908</td>\n",
              "      <td>0.598131</td>\n",
              "      <td>0.484848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>0.913043</td>\n",
              "      <td>0.897155</td>\n",
              "      <td>0.471910</td>\n",
              "      <td>0.318182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>NVDA</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.879650</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  NVDA  0.833333  0.820408  0.102041  0.054348\n",
              "1            GRU 0.1  NVDA  0.610169  0.838776  0.476821  0.391304\n",
              "2        XGBoost 0.1  NVDA  0.571429  0.830612  0.464516  0.391304\n",
              "3         Logreg 0.1  NVDA  0.700000  0.844898  0.424242  0.304348\n",
              "4            SVM 0.1  NVDA  0.690476  0.844898  0.432836  0.315217\n",
              "5      LSTM beta 0.1  NVDA  0.686275  0.897155  0.598291  0.530303\n",
              "6       GRU beta 0.1  NVDA  0.569231  0.875274  0.564885  0.560606\n",
              "7   XGBoost beta 0.1  NVDA  0.300000  0.838074  0.139535  0.090909\n",
              "8    logreg beta 0.1  NVDA  0.783784  0.901532  0.563107  0.439394\n",
              "9       svm beta 0.1  NVDA  0.666667  0.886214  0.518519  0.424242\n",
              "0           LSTM 0.2  NVDA  0.000000  0.865306  0.000000  0.000000\n",
              "1            GRU 0.2  NVDA  0.833333  0.873469  0.138889  0.075758\n",
              "2        XGBoost 0.2  NVDA  0.666667  0.873469  0.205128  0.121212\n",
              "3         Logreg 0.2  NVDA  1.000000  0.869388  0.058824  0.030303\n",
              "4            SVM 0.2  NVDA  1.000000  0.869388  0.058824  0.030303\n",
              "5      LSTM beta 0.2  NVDA  0.000000  0.855580  0.000000  0.000000\n",
              "6       GRU beta 0.2  NVDA  0.914894  0.940919  0.761062  0.651515\n",
              "7   XGBoost beta 0.2  NVDA  1.000000  0.903720  0.500000  0.333333\n",
              "8    logreg beta 0.2  NVDA  1.000000  0.868709  0.166667  0.090909\n",
              "9       svm beta 0.2  NVDA  1.000000  0.870897  0.191781  0.106061\n",
              "0          LSTM 0.15  NVDA  0.470588  0.861224  0.320000  0.242424\n",
              "1           GRU 0.15  NVDA  0.471698  0.859184  0.420168  0.378788\n",
              "2       XGBoost 0.15  NVDA  0.500000  0.865306  0.388889  0.318182\n",
              "3        Logreg 0.15  NVDA  0.782609  0.891837  0.404494  0.272727\n",
              "4           SVM 0.15  NVDA  0.722222  0.881633  0.309524  0.196970\n",
              "5     LSTM beta 0.15  NVDA  0.800000  0.862144  0.112676  0.060606\n",
              "6      GRU beta 0.15  NVDA  0.888889  0.870897  0.213333  0.121212\n",
              "7  XGBoost beta 0.15  NVDA  0.780488  0.905908  0.598131  0.484848\n",
              "8   logreg beta 0.15  NVDA  0.913043  0.897155  0.471910  0.318182\n",
              "9      svm beta 0.15  NVDA  1.000000  0.879650  0.285714  0.166667"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGqhawMTX4gx"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('NVDA_logreg_beta_p.csv')"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXs7iH8WX4gy"
      },
      "source": [
        ""
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqUeeZsxYLzW"
      },
      "source": [
        "## RHT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bnaIwqDYLzd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "outputId": "fbd9f489-1769-44c0-8402-a035f96b756b"
      },
      "source": [
        "dfs = pd.read_csv(\"RHT.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "dfs"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2203</td>\n",
              "      <td>US1.RHT</td>\n",
              "      <td>D</td>\n",
              "      <td>20190708</td>\n",
              "      <td>0</td>\n",
              "      <td>187.70</td>\n",
              "      <td>187.8100</td>\n",
              "      <td>187.6000</td>\n",
              "      <td>187.70</td>\n",
              "      <td>523715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2202</td>\n",
              "      <td>US1.RHT</td>\n",
              "      <td>D</td>\n",
              "      <td>20190705</td>\n",
              "      <td>0</td>\n",
              "      <td>187.60</td>\n",
              "      <td>187.9300</td>\n",
              "      <td>187.5100</td>\n",
              "      <td>187.75</td>\n",
              "      <td>671573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2201</td>\n",
              "      <td>US1.RHT</td>\n",
              "      <td>D</td>\n",
              "      <td>20190703</td>\n",
              "      <td>0</td>\n",
              "      <td>187.65</td>\n",
              "      <td>187.9500</td>\n",
              "      <td>187.4800</td>\n",
              "      <td>187.70</td>\n",
              "      <td>1956137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2200</td>\n",
              "      <td>US1.RHT</td>\n",
              "      <td>D</td>\n",
              "      <td>20190702</td>\n",
              "      <td>0</td>\n",
              "      <td>188.40</td>\n",
              "      <td>188.4000</td>\n",
              "      <td>187.9000</td>\n",
              "      <td>188.17</td>\n",
              "      <td>1213064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2199</td>\n",
              "      <td>US1.RHT</td>\n",
              "      <td>D</td>\n",
              "      <td>20190701</td>\n",
              "      <td>0</td>\n",
              "      <td>187.95</td>\n",
              "      <td>188.4194</td>\n",
              "      <td>187.5536</td>\n",
              "      <td>188.40</td>\n",
              "      <td>1278873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2199</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.RHT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>37.99</td>\n",
              "      <td>38.8500</td>\n",
              "      <td>37.9500</td>\n",
              "      <td>38.70</td>\n",
              "      <td>2255539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2200</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.RHT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>38.60</td>\n",
              "      <td>38.7499</td>\n",
              "      <td>37.5100</td>\n",
              "      <td>38.19</td>\n",
              "      <td>5042146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2201</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.RHT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>41.40</td>\n",
              "      <td>41.5000</td>\n",
              "      <td>37.3300</td>\n",
              "      <td>38.37</td>\n",
              "      <td>9840635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2202</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.RHT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>40.94</td>\n",
              "      <td>41.6050</td>\n",
              "      <td>40.8900</td>\n",
              "      <td>41.48</td>\n",
              "      <td>3305240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2203</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.RHT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>41.04</td>\n",
              "      <td>41.2000</td>\n",
              "      <td>40.2400</td>\n",
              "      <td>40.63</td>\n",
              "      <td>2037558</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2204 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...    <HIGH>     <LOW>  <CLOSE>    <VOL>\n",
              "0      2203  US1.RHT     D  20190708  ...  187.8100  187.6000   187.70   523715\n",
              "1      2202  US1.RHT     D  20190705  ...  187.9300  187.5100   187.75   671573\n",
              "2      2201  US1.RHT     D  20190703  ...  187.9500  187.4800   187.70  1956137\n",
              "3      2200  US1.RHT     D  20190702  ...  188.4000  187.9000   188.17  1213064\n",
              "4      2199  US1.RHT     D  20190701  ...  188.4194  187.5536   188.40  1278873\n",
              "...     ...      ...   ...       ...  ...       ...       ...      ...      ...\n",
              "2199      4  US1.RHT     D  20101008  ...   38.8500   37.9500    38.70  2255539\n",
              "2200      3  US1.RHT     D  20101007  ...   38.7499   37.5100    38.19  5042146\n",
              "2201      2  US1.RHT     D  20101006  ...   41.5000   37.3300    38.37  9840635\n",
              "2202      1  US1.RHT     D  20101005  ...   41.6050   40.8900    41.48  3305240\n",
              "2203      0  US1.RHT     D  20101004  ...   41.2000   40.2400    40.63  2037558\n",
              "\n",
              "[2204 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf5QQ1zRYLzd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2aaddaed-4cfe-4166-fc36-1d2f4ecbeec7"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 1500)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"74e4c7f3-5085-43bb-b4b7-b1dc368d2c79\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"74e4c7f3-5085-43bb-b4b7-b1dc368d2c79\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '74e4c7f3-5085-43bb-b4b7-b1dc368d2c79',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [97.42, 97.25, 96.56, 94.93, 95.54, 94.83, 95.76, 95.52, 96.81, 95.99, 98.2, 99.37, 98.89, 98.52, 90.0, 90.33, 87.85, 87.92, 88.93, 89.29, 88.76, 88.82, 91.94, 91.93, 91.66, 91.78, 91.52, 90.69, 89.56, 89.42, 88.55, 88.77, 87.99, 87.63, 87.9, 86.17, 85.51, 85.6, 89.61, 89.67, 88.22, 88.56, 89.15, 88.58, 88.18, 88.62, 87.89, 87.23, 88.12, 88.3, 88.08, 88.17, 87.74, 87.59, 87.45, 87.04, 87.42, 87.26, 86.74, 86.26, 85.93, 85.42, 85.16, 85.04, 84.905, 85.01, 84.67, 85.62, 86.49, 86.51, 85.93, 86.74, 86.495, 82.32, 82.96, 82.76, 82.55, 81.29, 84.51, 83.64, 84.09, 83.28, 82.57, 82.87, 82.23, 81.57, 82.045, 81.95, 82.04, 82.05, 82.41, 83.21, 82.8, 83.77, 84.83, 84.61, 84.7, 83.76, 82.59, 81.53, 81.06, 80.09, 79.78, 79.25, 79.3, 78.66, 78.86, 78.23, 78.11, 77.7, 76.06, 75.89, 75.5, 75.7, 75.23, 76.43, 75.42, 74.08, 74.19, 73.92, 73.6, 72.77, 73.36, 73.04, 72.33, 71.66, 72.13, 73.8, 73.11, 73.11, 70.12, 69.71, 69.65, 70.0, 70.19, 71.0099, 68.7, 79.79, 79.39, 80.49, 79.96, 80.41, 79.08, 80.55, 78.39, 78.65, 78.88, 78.91, 77.95, 77.71, 76.07, 75.19, 79.11, 80.01, 78.38, 78.81, 78.74, 79.13, 79.65, 79.04, 79.8, 78.64, 76.81, 75.505, 77.04, 76.94, 76.78, 76.68, 76.66, 74.44, 74.75, 74.77, 76.67, 77.46, 77.11, 76.91, 77.03, 77.36, 78.21, 76.96, 77.57, 78.26, 77.5, 76.2, 77.21, 76.84, 77.18, 76.76, 77.99, 77.795, 78.58, 79.44, 79.66, 80.19, 80.85, 80.17, 80.555, 79.12, 78.74, 79.16, 79.95, 77.06, 75.8, 75.96, 73.8, 73.67, 73.75, 72.43, 72.96, 72.26, 72.59, 73.16, 73.36, 73.74, 72.93, 72.97, 72.94, 73.66, 74.35, 74.9, 74.73, 74.89, 74.09, 74.22, 73.72, 73.17, 73.585, 73.98, 73.98, 74.42, 74.37, 75.175, 75.24, 75.83, 74.61, 74.57, 72.905, 74.81, 75.29, 74.43, 73.65, 75.06, 73.55, 73.46, 72.31, 73.52, 72.35, 72.8, 72.82, 73.48, 73.67, 73.91, 72.63, 72.7, 71.42, 72.19, 71.26, 71.98, 72.6, 72.95, 71.5, 70.14, 73.63, 78.37, 79.73, 80.38, 78.87, 77.28, 76.97, 77.01, 77.17, 76.91, 76.93, 78.23, 79.07, 78.88, 78.14, 78.19, 78.23, 77.41, 77.46, 76.53, 76.23, 76.7, 76.41, 74.81, 73.17, 72.43, 73.0, 72.44, 72.93, 72.07, 71.91, 72.15, 73.1, 71.74, 71.73, 71.58, 71.45, 72.35, 74.32, 73.35, 73.64, 74.42, 74.47, 74.45, 74.83, 76.14, 76.05, 75.04, 75.44, 74.65, 74.44, 74.05, 72.62, 72.82, 73.8, 73.63, 74.76, 74.05, 75.26, 75.5, 74.5, 74.04, 73.53, 73.48, 73.25, 72.55, 75.71, 74.88, 74.0736, 72.73, 72.86, 71.55, 72.11, 71.8, 70.07, 69.88, 68.27, 66.94, 67.57, 68.66, 68.26, 68.39, 65.36, 67.98, 67.67, 65.35, 65.3, 66.09, 65.89, 65.35, 66.3, 63.84, 64.1, 61.96, 63.54, 61.62, 60.96, 63.14, 68.6, 67.69, 67.82, 70.43, 70.01, 67.38, 67.99, 70.96, 71.75, 72.45, 70.3, 70.48, 71.6, 73.6, 76.53, 76.61, 78.66, 78.27, 79.17, 79.49, 80.61, 81.93, 81.7, 82.8, 83.65, 83.54, 82.35, 81.74, 82.18, 80.93, 80.44, 81.39, 78.85, 78.75, 78.06, 77.49, 78.58, 79.62, 79.06, 81.02, 80.52, 81.9, 80.32, 81.64, 82.24, 81.42, 82.5, 82.52, 82.05, 82.75, 82.21, 81.33, 80.52, 80.28, 79.09, 77.84, 79.82, 81.45, 80.81, 80.28, 81.03, 81.6, 82.58, 80.84, 80.13, 79.09, 78.87, 79.07, 77.88, 77.78, 77.43, 76.9701, 76.83, 78.26, 78.91, 77.62, 77.71, 75.115, 74.73, 75.97, 76.51, 75.16, 74.17, 72.77, 74.12, 73.62, 73.19, 71.86, 70.49, 70.74, 72.91, 73.15, 73.27, 72.7, 72.74, 71.07, 71.38, 70.99, 71.75, 69.79, 70.53, 70.36, 69.67, 71.03, 68.67, 69.65, 70.28, 70.06, 72.22, 73.83, 74.185, 73.035, 68.97, 69.03, 72.34, 75.15, 77.5, 78.67, 78.77, 78.33, 76.98, 77.05, 77.44, 79.16, 77.62, 77.65, 80.14, 79.05, 78.28, 79.1, 79.46, 79.27, 78.72, 77.15, 78.97, 79.35, 79.4, 78.3, 79.21, 79.06, 80.57, 80.11, 80.69, 79.57, 78.54, 77.26, 76.02, 76.86, 76.6, 76.86, 77.55, 75.92, 75.47, 77.46, 79.13, 78.53, 79.61, 79.47, 79.31, 78.5, 78.26, 78.51, 78.66, 78.21, 78.56, 78.41, 77.37, 78.26, 77.92, 77.06, 77.84, 77.51, 76.73, 77.23, 77.78, 78.44, 77.3, 78.82, 78.85, 78.48, 78.14, 78.47, 77.77, 78.01, 76.59, 76.16, 76.39, 75.99, 74.78, 74.76, 75.01, 74.76, 75.34, 75.25, 76.43, 76.59, 76.59, 76.85, 76.88, 75.765, 75.22, 74.45, 73.92, 74.81, 75.9, 74.91, 75.06, 75.77, 75.46, 75.03, 75.36, 75.37, 75.09, 75.39, 75.75, 76.71, 76.52, 75.3674, 68.45, 69.4, 69.46, 69.49, 69.16, 67.07, 67.1, 67.28, 66.16, 66.34, 65.13, 65.14, 66.85, 66.6, 67.97, 67.21, 68.48, 69.65, 69.12, 69.4, 69.76, 68.91, 68.55, 70.45, 69.48, 69.22, 68.62, 68.48, 67.97, 67.36, 65.74, 64.9, 64.98, 65.29, 64.8, 64.97, 64.23, 63.78, 65.74, 64.69, 65.08, 66.26, 66.07, 65.925, 64.78, 65.15, 65.2, 64.37, 66.36, 67.45, 67.62, 68.73, 69.01, 68.76, 67.5, 68.11, 68.99, 69.13, 69.47, 70.28, 71.1, 70.92, 70.43, 68.9, 68.02, 61.49, 59.49, 58.23, 58.98, 57.89, 58.43, 58.71, 59.08, 60.08, 61.68, 61.68, 62.13, 61.42, 61.4801, 62.15, 62.07, 61.99, 62.33, 62.23, 62.79, 62.93, 63.26, 63.0, 62.38, 61.11, 61.39, 61.32, 61.41, 61.28, 60.74, 59.58, 59.15, 59.16, 58.92, 58.26, 56.85, 57.36, 56.17, 56.02, 55.49, 54.8, 57.12, 56.32, 55.66, 54.92, 54.86, 54.32, 54.24, 55.49, 58.07, 58.56, 56.68, 58.14, 58.08, 57.38, 56.48, 56.14, 55.49, 55.43, 55.88, 56.91, 56.33, 56.69, 57.94, 60.68, 61.07, 60.72, 59.695, 60.54, 61.69, 61.3, 60.81, 61.38, 61.07, 61.21, 61.51, 61.29, 60.91, 60.645, 61.06, 62.03, 61.35, 62.12, 61.57, 61.67, 62.54, 60.95, 59.84, 59.84, 59.77, 59.26, 59.17, 58.805, 58.39, 58.29, 58.45, 58.62, 58.4, 58.12, 58.32, 56.15, 56.71, 56.7, 56.39, 55.88, 55.67, 54.29, 54.84, 53.94, 54.84, 54.38, 55.21, 54.74, 54.78, 54.92, 54.47, 55.72, 56.1, 56.11, 56.175, 55.26, 55.165, 55.34, 55.42, 54.755, 55.004, 55.0499, 55.11, 53.09, 52.4, 52.13, 51.93, 50.5, 51.48, 51.35, 51.41, 50.84, 50.31, 49.71, 49.99, 50.05, 50.11, 50.53, 50.0, 50.8, 50.68, 50.46, 50.1, 49.72, 50.1, 50.005, 49.49, 49.39, 49.3, 50.0, 48.72, 48.34, 48.75, 48.87, 49.43, 48.56, 48.77, 48.645, 48.55, 48.18, 48.37, 49.63, 50.04, 50.89, 50.92, 50.56, 50.62, 50.11, 49.82, 49.34, 50.01, 52.08, 50.0, 49.85, 50.45, 51.34, 53.11, 53.71, 52.98, 52.23, 56.11, 55.98, 57.32, 57.09, 57.42, 58.46, 58.25, 58.8, 58.62, 57.62, 57.41, 59.09, 58.67, 59.5, 60.52, 60.87, 60.55, 60.5, 58.27, 58.98, 59.94, 59.75, 58.97, 58.88, 58.68, 59.29, 58.64, 58.09, 58.31, 58.43, 57.68, 57.48, 56.67, 56.99, 55.54, 54.7, 54.85, 54.69, 56.5, 57.13, 55.57, 57.23, 55.82, 56.74, 58.12, 58.92, 59.15, 58.6, 59.07, 59.2, 59.37, 57.55, 56.81, 57.3, 56.99, 57.42, 56.33, 55.76, 55.84, 56.04, 56.08, 56.25, 55.88, 55.98, 56.08, 56.08, 48.99, 48.92, 48.21, 47.53, 46.78, 46.36, 46.16, 46.69, 47.47, 46.77, 46.65, 46.67, 45.9, 46.18, 46.85, 46.89, 46.55, 46.86, 47.11, 47.09, 46.53, 46.43, 46.55, 47.1, 46.32, 46.37, 44.59, 44.26, 43.31, 43.21, 44.08, 42.93, 43.08, 43.19, 43.27, 43.42, 44.08, 43.6, 43.41, 43.76, 42.36, 42.92, 42.58, 43.29, 43.53, 43.6, 42.61, 42.86, 42.82, 42.865, 43.09, 43.3, 45.45, 45.06, 44.91, 45.27, 45.73, 46.15, 45.92, 46.6, 46.88, 46.74, 52.94, 53.23, 53.6, 53.57, 52.57, 53.02, 52.43, 52.85, 53.48, 52.82, 52.2, 51.06, 50.85, 51.14, 50.84, 50.51, 50.83, 50.32, 50.165, 51.7, 51.58, 51.97, 51.11, 51.33, 51.25, 51.51, 51.97, 53.11, 53.53, 52.3, 52.47, 52.055, 51.26, 51.67, 52.23, 52.46, 52.49, 51.78, 51.08, 50.78, 50.83, 51.77, 50.53, 48.78]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('74e4c7f3-5085-43bb-b4b7-b1dc368d2c79');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"d25d98f4-3415-43bb-b04a-2d13425d3c52\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"d25d98f4-3415-43bb-b04a-2d13425d3c52\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'd25d98f4-3415-43bb-b04a-2d13425d3c52',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d25d98f4-3415-43bb-b04a-2d13425d3c52');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv1Kx3N5YLzd"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hkY2zf-YLzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d9e93e5-bc56-4c82-e36b-257300e67045"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .2, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1500, test_end=1900)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"RHT\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 17ms/step - loss: 0.5319 - accuracy: 0.7882 - val_loss: 0.6570 - val_accuracy: 0.6487\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5230 - accuracy: 0.7924 - val_loss: 0.7062 - val_accuracy: 0.6487\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5181 - accuracy: 0.7924 - val_loss: 0.7226 - val_accuracy: 0.6487\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5162 - accuracy: 0.7924 - val_loss: 0.6692 - val_accuracy: 0.6487\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5071 - accuracy: 0.7832 - val_loss: 0.6826 - val_accuracy: 0.6487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 15ms/step - loss: 0.5316 - accuracy: 0.7924 - val_loss: 0.6738 - val_accuracy: 0.6487\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4651 - accuracy: 0.8059 - val_loss: 0.5676 - val_accuracy: 0.6846\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4485 - accuracy: 0.8160 - val_loss: 0.5401 - val_accuracy: 0.6872\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4300 - accuracy: 0.8134 - val_loss: 0.5111 - val_accuracy: 0.7410\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4155 - accuracy: 0.8328 - val_loss: 0.5118 - val_accuracy: 0.7308\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.792634\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.826303\n",
            "[2]\tvalidation_0-auc:0.840469\n",
            "[3]\tvalidation_0-auc:0.839099\n",
            "[4]\tvalidation_0-auc:0.842229\n",
            "[5]\tvalidation_0-auc:0.839791\n",
            "[6]\tvalidation_0-auc:0.841508\n",
            "[7]\tvalidation_0-auc:0.848086\n",
            "[8]\tvalidation_0-auc:0.846672\n",
            "[9]\tvalidation_0-auc:0.847754\n",
            "[10]\tvalidation_0-auc:0.848821\n",
            "[11]\tvalidation_0-auc:0.848778\n",
            "[12]\tvalidation_0-auc:0.845691\n",
            "[13]\tvalidation_0-auc:0.846412\n",
            "[14]\tvalidation_0-auc:0.844335\n",
            "[15]\tvalidation_0-auc:0.845619\n",
            "[16]\tvalidation_0-auc:0.843369\n",
            "[17]\tvalidation_0-auc:0.841724\n",
            "[18]\tvalidation_0-auc:0.838666\n",
            "[19]\tvalidation_0-auc:0.837699\n",
            "[20]\tvalidation_0-auc:0.836733\n",
            "[21]\tvalidation_0-auc:0.837483\n",
            "[22]\tvalidation_0-auc:0.83604\n",
            "[23]\tvalidation_0-auc:0.835839\n",
            "[24]\tvalidation_0-auc:0.834396\n",
            "[25]\tvalidation_0-auc:0.834165\n",
            "[26]\tvalidation_0-auc:0.832405\n",
            "[27]\tvalidation_0-auc:0.831309\n",
            "[28]\tvalidation_0-auc:0.829059\n",
            "[29]\tvalidation_0-auc:0.827039\n",
            "[30]\tvalidation_0-auc:0.827443\n",
            "[31]\tvalidation_0-auc:0.825683\n",
            "[32]\tvalidation_0-auc:0.824067\n",
            "[33]\tvalidation_0-auc:0.823144\n",
            "[34]\tvalidation_0-auc:0.822942\n",
            "[35]\tvalidation_0-auc:0.823\n",
            "[36]\tvalidation_0-auc:0.81974\n",
            "[37]\tvalidation_0-auc:0.819971\n",
            "[38]\tvalidation_0-auc:0.820548\n",
            "[39]\tvalidation_0-auc:0.81948\n",
            "[40]\tvalidation_0-auc:0.818369\n",
            "[41]\tvalidation_0-auc:0.818225\n",
            "[42]\tvalidation_0-auc:0.818874\n",
            "[43]\tvalidation_0-auc:0.818167\n",
            "[44]\tvalidation_0-auc:0.817186\n",
            "[45]\tvalidation_0-auc:0.816956\n",
            "[46]\tvalidation_0-auc:0.817071\n",
            "[47]\tvalidation_0-auc:0.81684\n",
            "[48]\tvalidation_0-auc:0.816725\n",
            "[49]\tvalidation_0-auc:0.815816\n",
            "[50]\tvalidation_0-auc:0.816682\n",
            "[51]\tvalidation_0-auc:0.816364\n",
            "[52]\tvalidation_0-auc:0.816018\n",
            "[53]\tvalidation_0-auc:0.815326\n",
            "[54]\tvalidation_0-auc:0.814691\n",
            "[55]\tvalidation_0-auc:0.814662\n",
            "[56]\tvalidation_0-auc:0.81472\n",
            "[57]\tvalidation_0-auc:0.813739\n",
            "[58]\tvalidation_0-auc:0.813104\n",
            "[59]\tvalidation_0-auc:0.810825\n",
            "[60]\tvalidation_0-auc:0.808776\n",
            "Stopping. Best iteration:\n",
            "[10]\tvalidation_0-auc:0.848821\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 3s 18ms/step - loss: 0.5401 - accuracy: 0.7848 - val_loss: 0.7160 - val_accuracy: 0.6471\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.5261 - accuracy: 0.7865 - val_loss: 0.7418 - val_accuracy: 0.6471\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.5169 - accuracy: 0.7865 - val_loss: 0.7115 - val_accuracy: 0.6471\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4955 - accuracy: 0.7865 - val_loss: 0.6207 - val_accuracy: 0.6471\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4628 - accuracy: 0.7891 - val_loss: 0.5590 - val_accuracy: 0.6471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 2s 14ms/step - loss: 0.5377 - accuracy: 0.7796 - val_loss: 0.6324 - val_accuracy: 0.6471\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4818 - accuracy: 0.7857 - val_loss: 0.5821 - val_accuracy: 0.6387\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.4531 - accuracy: 0.8003 - val_loss: 0.5456 - val_accuracy: 0.7143\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.4357 - accuracy: 0.8116 - val_loss: 0.5374 - val_accuracy: 0.7311\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.3996 - accuracy: 0.8315 - val_loss: 0.5161 - val_accuracy: 0.7535\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.719697\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.743403\n",
            "[2]\tvalidation_0-auc:0.774153\n",
            "[3]\tvalidation_0-auc:0.772195\n",
            "[4]\tvalidation_0-auc:0.759191\n",
            "[5]\tvalidation_0-auc:0.772933\n",
            "[6]\tvalidation_0-auc:0.774222\n",
            "[7]\tvalidation_0-auc:0.787673\n",
            "[8]\tvalidation_0-auc:0.791538\n",
            "[9]\tvalidation_0-auc:0.791933\n",
            "[10]\tvalidation_0-auc:0.794596\n",
            "[11]\tvalidation_0-auc:0.796657\n",
            "[12]\tvalidation_0-auc:0.796537\n",
            "[13]\tvalidation_0-auc:0.797499\n",
            "[14]\tvalidation_0-auc:0.80346\n",
            "[15]\tvalidation_0-auc:0.807428\n",
            "[16]\tvalidation_0-auc:0.806707\n",
            "[17]\tvalidation_0-auc:0.813681\n",
            "[18]\tvalidation_0-auc:0.814231\n",
            "[19]\tvalidation_0-auc:0.814815\n",
            "[20]\tvalidation_0-auc:0.816395\n",
            "[21]\tvalidation_0-auc:0.814317\n",
            "[22]\tvalidation_0-auc:0.815416\n",
            "[23]\tvalidation_0-auc:0.816997\n",
            "[24]\tvalidation_0-auc:0.81545\n",
            "[25]\tvalidation_0-auc:0.815708\n",
            "[26]\tvalidation_0-auc:0.816086\n",
            "[27]\tvalidation_0-auc:0.816258\n",
            "[28]\tvalidation_0-auc:0.815296\n",
            "[29]\tvalidation_0-auc:0.814987\n",
            "[30]\tvalidation_0-auc:0.815502\n",
            "[31]\tvalidation_0-auc:0.814523\n",
            "[32]\tvalidation_0-auc:0.815571\n",
            "[33]\tvalidation_0-auc:0.817478\n",
            "[34]\tvalidation_0-auc:0.816808\n",
            "[35]\tvalidation_0-auc:0.815502\n",
            "[36]\tvalidation_0-auc:0.815485\n",
            "[37]\tvalidation_0-auc:0.813784\n",
            "[38]\tvalidation_0-auc:0.816034\n",
            "[39]\tvalidation_0-auc:0.817323\n",
            "[40]\tvalidation_0-auc:0.815983\n",
            "[41]\tvalidation_0-auc:0.816704\n",
            "[42]\tvalidation_0-auc:0.815931\n",
            "[43]\tvalidation_0-auc:0.81789\n",
            "[44]\tvalidation_0-auc:0.817374\n",
            "[45]\tvalidation_0-auc:0.817873\n",
            "[46]\tvalidation_0-auc:0.818938\n",
            "[47]\tvalidation_0-auc:0.820089\n",
            "[48]\tvalidation_0-auc:0.821291\n",
            "[49]\tvalidation_0-auc:0.82191\n",
            "[50]\tvalidation_0-auc:0.822253\n",
            "[51]\tvalidation_0-auc:0.820638\n",
            "[52]\tvalidation_0-auc:0.821016\n",
            "[53]\tvalidation_0-auc:0.82203\n",
            "[54]\tvalidation_0-auc:0.821205\n",
            "[55]\tvalidation_0-auc:0.821137\n",
            "[56]\tvalidation_0-auc:0.820879\n",
            "[57]\tvalidation_0-auc:0.820432\n",
            "[58]\tvalidation_0-auc:0.820123\n",
            "[59]\tvalidation_0-auc:0.818457\n",
            "[60]\tvalidation_0-auc:0.819281\n",
            "[61]\tvalidation_0-auc:0.819178\n",
            "[62]\tvalidation_0-auc:0.820071\n",
            "[63]\tvalidation_0-auc:0.820484\n",
            "[64]\tvalidation_0-auc:0.820587\n",
            "[65]\tvalidation_0-auc:0.820381\n",
            "[66]\tvalidation_0-auc:0.819058\n",
            "[67]\tvalidation_0-auc:0.818989\n",
            "[68]\tvalidation_0-auc:0.818199\n",
            "[69]\tvalidation_0-auc:0.818216\n",
            "[70]\tvalidation_0-auc:0.818491\n",
            "[71]\tvalidation_0-auc:0.817374\n",
            "[72]\tvalidation_0-auc:0.817958\n",
            "[73]\tvalidation_0-auc:0.816928\n",
            "[74]\tvalidation_0-auc:0.816722\n",
            "[75]\tvalidation_0-auc:0.816653\n",
            "[76]\tvalidation_0-auc:0.816223\n",
            "[77]\tvalidation_0-auc:0.817289\n",
            "[78]\tvalidation_0-auc:0.817014\n",
            "[79]\tvalidation_0-auc:0.816808\n",
            "[80]\tvalidation_0-auc:0.81722\n",
            "[81]\tvalidation_0-auc:0.816636\n",
            "[82]\tvalidation_0-auc:0.816258\n",
            "[83]\tvalidation_0-auc:0.815674\n",
            "[84]\tvalidation_0-auc:0.815502\n",
            "[85]\tvalidation_0-auc:0.815914\n",
            "[86]\tvalidation_0-auc:0.81588\n",
            "[87]\tvalidation_0-auc:0.815399\n",
            "[88]\tvalidation_0-auc:0.815536\n",
            "[89]\tvalidation_0-auc:0.815296\n",
            "[90]\tvalidation_0-auc:0.815468\n",
            "[91]\tvalidation_0-auc:0.815433\n",
            "[92]\tvalidation_0-auc:0.81399\n",
            "[93]\tvalidation_0-auc:0.813956\n",
            "[94]\tvalidation_0-auc:0.81399\n",
            "[95]\tvalidation_0-auc:0.813784\n",
            "[96]\tvalidation_0-auc:0.815227\n",
            "[97]\tvalidation_0-auc:0.814746\n",
            "[98]\tvalidation_0-auc:0.814334\n",
            "[99]\tvalidation_0-auc:0.815365\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.6487179487179487 |        0.0         |         0.0         |         0.0         |\n",
            "|     GRU 0.1      | 0.7307692307692307 | 0.8636363636363636 |  0.2773722627737226 |  0.4198895027624309 |\n",
            "|   XGBoost 0.1    | 0.764102564102564  | 0.8688524590163934 | 0.38686131386861317 |  0.5353535353535354 |\n",
            "|    Logreg 0.1    | 0.7153846153846154 | 0.8611111111111112 | 0.22627737226277372 |  0.3583815028901734 |\n",
            "|     SVM 0.1      | 0.7256410256410256 |       0.8125       |  0.2846715328467153 | 0.42162162162162165 |\n",
            "|  LSTM beta 0.1   | 0.6470588235294118 |        0.0         |         0.0         |         0.0         |\n",
            "|   GRU beta 0.1   | 0.7535014005602241 |       0.7375       | 0.46825396825396826 |  0.5728155339805825 |\n",
            "| XGBoost beta 0.1 | 0.7703081232492998 | 0.7075471698113207 |  0.5952380952380952 |  0.646551724137931  |\n",
            "| logreg beta 0.1  | 0.7254901960784313 |        0.78        | 0.30952380952380953 |  0.4431818181818182 |\n",
            "|   svm beta 0.1   | 0.7450980392156863 | 0.6666666666666666 |  0.5555555555555556 |  0.606060606060606  |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 16ms/step - loss: 0.3884 - accuracy: 0.8866 - val_loss: 1.7800 - val_accuracy: 0.2308\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3502 - accuracy: 0.8933 - val_loss: 1.6112 - val_accuracy: 0.2308\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3513 - accuracy: 0.8933 - val_loss: 1.6039 - val_accuracy: 0.2308\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3534 - accuracy: 0.8933 - val_loss: 1.6610 - val_accuracy: 0.2308\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3457 - accuracy: 0.8933 - val_loss: 1.6875 - val_accuracy: 0.2308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 15ms/step - loss: 0.3789 - accuracy: 0.8908 - val_loss: 1.8925 - val_accuracy: 0.2308\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3494 - accuracy: 0.8933 - val_loss: 2.1167 - val_accuracy: 0.2308\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3197 - accuracy: 0.8966 - val_loss: 1.8612 - val_accuracy: 0.2436\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3097 - accuracy: 0.8916 - val_loss: 2.1948 - val_accuracy: 0.2359\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3098 - accuracy: 0.8882 - val_loss: 1.7529 - val_accuracy: 0.2513\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.614796\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.613796\n",
            "[2]\tvalidation_0-auc:0.613407\n",
            "[3]\tvalidation_0-auc:0.62937\n",
            "[4]\tvalidation_0-auc:0.684204\n",
            "[5]\tvalidation_0-auc:0.686296\n",
            "[6]\tvalidation_0-auc:0.702074\n",
            "[7]\tvalidation_0-auc:0.70063\n",
            "[8]\tvalidation_0-auc:0.700741\n",
            "[9]\tvalidation_0-auc:0.700444\n",
            "[10]\tvalidation_0-auc:0.700907\n",
            "[11]\tvalidation_0-auc:0.699352\n",
            "[12]\tvalidation_0-auc:0.710741\n",
            "[13]\tvalidation_0-auc:0.704648\n",
            "[14]\tvalidation_0-auc:0.705056\n",
            "[15]\tvalidation_0-auc:0.707056\n",
            "[16]\tvalidation_0-auc:0.709537\n",
            "[17]\tvalidation_0-auc:0.710722\n",
            "[18]\tvalidation_0-auc:0.704278\n",
            "[19]\tvalidation_0-auc:0.708407\n",
            "[20]\tvalidation_0-auc:0.710981\n",
            "[21]\tvalidation_0-auc:0.711926\n",
            "[22]\tvalidation_0-auc:0.715556\n",
            "[23]\tvalidation_0-auc:0.711796\n",
            "[24]\tvalidation_0-auc:0.715944\n",
            "[25]\tvalidation_0-auc:0.71863\n",
            "[26]\tvalidation_0-auc:0.715889\n",
            "[27]\tvalidation_0-auc:0.713333\n",
            "[28]\tvalidation_0-auc:0.711981\n",
            "[29]\tvalidation_0-auc:0.709815\n",
            "[30]\tvalidation_0-auc:0.708926\n",
            "[31]\tvalidation_0-auc:0.705778\n",
            "[32]\tvalidation_0-auc:0.705926\n",
            "[33]\tvalidation_0-auc:0.703593\n",
            "[34]\tvalidation_0-auc:0.700926\n",
            "[35]\tvalidation_0-auc:0.699907\n",
            "[36]\tvalidation_0-auc:0.695982\n",
            "[37]\tvalidation_0-auc:0.694574\n",
            "[38]\tvalidation_0-auc:0.690833\n",
            "[39]\tvalidation_0-auc:0.689241\n",
            "[40]\tvalidation_0-auc:0.688352\n",
            "[41]\tvalidation_0-auc:0.688463\n",
            "[42]\tvalidation_0-auc:0.687556\n",
            "[43]\tvalidation_0-auc:0.687074\n",
            "[44]\tvalidation_0-auc:0.688296\n",
            "[45]\tvalidation_0-auc:0.687\n",
            "[46]\tvalidation_0-auc:0.687426\n",
            "[47]\tvalidation_0-auc:0.686389\n",
            "[48]\tvalidation_0-auc:0.685167\n",
            "[49]\tvalidation_0-auc:0.683278\n",
            "[50]\tvalidation_0-auc:0.681574\n",
            "[51]\tvalidation_0-auc:0.681426\n",
            "[52]\tvalidation_0-auc:0.68163\n",
            "[53]\tvalidation_0-auc:0.682259\n",
            "[54]\tvalidation_0-auc:0.681889\n",
            "[55]\tvalidation_0-auc:0.68063\n",
            "[56]\tvalidation_0-auc:0.681593\n",
            "[57]\tvalidation_0-auc:0.681074\n",
            "[58]\tvalidation_0-auc:0.681074\n",
            "[59]\tvalidation_0-auc:0.680333\n",
            "[60]\tvalidation_0-auc:0.682222\n",
            "[61]\tvalidation_0-auc:0.680741\n",
            "[62]\tvalidation_0-auc:0.681593\n",
            "[63]\tvalidation_0-auc:0.681407\n",
            "[64]\tvalidation_0-auc:0.679259\n",
            "[65]\tvalidation_0-auc:0.679741\n",
            "[66]\tvalidation_0-auc:0.677889\n",
            "[67]\tvalidation_0-auc:0.679556\n",
            "[68]\tvalidation_0-auc:0.678926\n",
            "[69]\tvalidation_0-auc:0.679333\n",
            "[70]\tvalidation_0-auc:0.679482\n",
            "[71]\tvalidation_0-auc:0.678296\n",
            "[72]\tvalidation_0-auc:0.68\n",
            "[73]\tvalidation_0-auc:0.679407\n",
            "[74]\tvalidation_0-auc:0.677741\n",
            "[75]\tvalidation_0-auc:0.677481\n",
            "Stopping. Best iteration:\n",
            "[25]\tvalidation_0-auc:0.71863\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 3s 17ms/step - loss: 0.3779 - accuracy: 0.8859 - val_loss: 1.7956 - val_accuracy: 0.2521\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.3666 - accuracy: 0.8902 - val_loss: 1.7311 - val_accuracy: 0.2521\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.3633 - accuracy: 0.8902 - val_loss: 1.7047 - val_accuracy: 0.2521\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.3631 - accuracy: 0.8902 - val_loss: 1.7143 - val_accuracy: 0.2521\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.3484 - accuracy: 0.8902 - val_loss: 1.5341 - val_accuracy: 0.2521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 3s 16ms/step - loss: 0.3895 - accuracy: 0.8859 - val_loss: 1.7398 - val_accuracy: 0.2521\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.3063 - accuracy: 0.8946 - val_loss: 2.1204 - val_accuracy: 0.2521\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.2872 - accuracy: 0.9092 - val_loss: 1.7708 - val_accuracy: 0.2661\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.2909 - accuracy: 0.9032 - val_loss: 1.9339 - val_accuracy: 0.2577\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.2839 - accuracy: 0.9041 - val_loss: 2.0649 - val_accuracy: 0.2717\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.670454\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.664836\n",
            "[2]\tvalidation_0-auc:0.64124\n",
            "[3]\tvalidation_0-auc:0.633999\n",
            "[4]\tvalidation_0-auc:0.639638\n",
            "[5]\tvalidation_0-auc:0.634623\n",
            "[6]\tvalidation_0-auc:0.636871\n",
            "[7]\tvalidation_0-auc:0.636829\n",
            "[8]\tvalidation_0-auc:0.63633\n",
            "[9]\tvalidation_0-auc:0.63094\n",
            "[10]\tvalidation_0-auc:0.635976\n",
            "[11]\tvalidation_0-auc:0.631273\n",
            "[12]\tvalidation_0-auc:0.637828\n",
            "[13]\tvalidation_0-auc:0.636704\n",
            "[14]\tvalidation_0-auc:0.637474\n",
            "[15]\tvalidation_0-auc:0.638951\n",
            "[16]\tvalidation_0-auc:0.639243\n",
            "[17]\tvalidation_0-auc:0.651352\n",
            "[18]\tvalidation_0-auc:0.65901\n",
            "[19]\tvalidation_0-auc:0.666146\n",
            "[20]\tvalidation_0-auc:0.674241\n",
            "[21]\tvalidation_0-auc:0.680275\n",
            "[22]\tvalidation_0-auc:0.686517\n",
            "[23]\tvalidation_0-auc:0.69149\n",
            "[24]\tvalidation_0-auc:0.687765\n",
            "[25]\tvalidation_0-auc:0.696005\n",
            "[26]\tvalidation_0-auc:0.691677\n",
            "[27]\tvalidation_0-auc:0.687682\n",
            "[28]\tvalidation_0-auc:0.694008\n",
            "[29]\tvalidation_0-auc:0.689305\n",
            "[30]\tvalidation_0-auc:0.694965\n",
            "[31]\tvalidation_0-auc:0.691968\n",
            "[32]\tvalidation_0-auc:0.695839\n",
            "[33]\tvalidation_0-auc:0.697628\n",
            "[34]\tvalidation_0-auc:0.693154\n",
            "[35]\tvalidation_0-auc:0.6964\n",
            "[36]\tvalidation_0-auc:0.701456\n",
            "[37]\tvalidation_0-auc:0.702809\n",
            "[38]\tvalidation_0-auc:0.698564\n",
            "[39]\tvalidation_0-auc:0.702226\n",
            "[40]\tvalidation_0-auc:0.708489\n",
            "[41]\tvalidation_0-auc:0.713005\n",
            "[42]\tvalidation_0-auc:0.713587\n",
            "[43]\tvalidation_0-auc:0.710633\n",
            "[44]\tvalidation_0-auc:0.711715\n",
            "[45]\tvalidation_0-auc:0.707241\n",
            "[46]\tvalidation_0-auc:0.710737\n",
            "[47]\tvalidation_0-auc:0.708864\n",
            "[48]\tvalidation_0-auc:0.702518\n",
            "[49]\tvalidation_0-auc:0.701685\n",
            "[50]\tvalidation_0-auc:0.700895\n",
            "[51]\tvalidation_0-auc:0.701977\n",
            "[52]\tvalidation_0-auc:0.705327\n",
            "[53]\tvalidation_0-auc:0.703288\n",
            "[54]\tvalidation_0-auc:0.705868\n",
            "[55]\tvalidation_0-auc:0.706076\n",
            "[56]\tvalidation_0-auc:0.703953\n",
            "[57]\tvalidation_0-auc:0.700749\n",
            "[58]\tvalidation_0-auc:0.70516\n",
            "[59]\tvalidation_0-auc:0.705077\n",
            "[60]\tvalidation_0-auc:0.707241\n",
            "[61]\tvalidation_0-auc:0.708739\n",
            "[62]\tvalidation_0-auc:0.709571\n",
            "[63]\tvalidation_0-auc:0.709238\n",
            "[64]\tvalidation_0-auc:0.707407\n",
            "[65]\tvalidation_0-auc:0.704869\n",
            "[66]\tvalidation_0-auc:0.703912\n",
            "[67]\tvalidation_0-auc:0.704203\n",
            "[68]\tvalidation_0-auc:0.705368\n",
            "[69]\tvalidation_0-auc:0.704661\n",
            "[70]\tvalidation_0-auc:0.699709\n",
            "[71]\tvalidation_0-auc:0.701956\n",
            "[72]\tvalidation_0-auc:0.701956\n",
            "[73]\tvalidation_0-auc:0.700208\n",
            "[74]\tvalidation_0-auc:0.700083\n",
            "[75]\tvalidation_0-auc:0.703163\n",
            "[76]\tvalidation_0-auc:0.708406\n",
            "[77]\tvalidation_0-auc:0.704494\n",
            "[78]\tvalidation_0-auc:0.70258\n",
            "[79]\tvalidation_0-auc:0.706409\n",
            "[80]\tvalidation_0-auc:0.710071\n",
            "[81]\tvalidation_0-auc:0.708281\n",
            "[82]\tvalidation_0-auc:0.707158\n",
            "[83]\tvalidation_0-auc:0.706617\n",
            "[84]\tvalidation_0-auc:0.705452\n",
            "[85]\tvalidation_0-auc:0.705035\n",
            "[86]\tvalidation_0-auc:0.701914\n",
            "[87]\tvalidation_0-auc:0.700208\n",
            "[88]\tvalidation_0-auc:0.699459\n",
            "[89]\tvalidation_0-auc:0.702871\n",
            "[90]\tvalidation_0-auc:0.701706\n",
            "[91]\tvalidation_0-auc:0.701165\n",
            "[92]\tvalidation_0-auc:0.701873\n",
            "Stopping. Best iteration:\n",
            "[42]\tvalidation_0-auc:0.713587\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+---------------------+--------------------+----------------------+----------------------+\n",
            "|      Model       |       Accuracy      |     Precision      |        Recall        |       F1 score       |\n",
            "+------------------+---------------------+--------------------+----------------------+----------------------+\n",
            "|     LSTM 0.2     | 0.23076923076923078 |        0.0         |         0.0          |         0.0          |\n",
            "|     GRU 0.2      |  0.2512820512820513 |        1.0         | 0.02666666666666667  | 0.05194805194805195  |\n",
            "|   XGBoost 0.2    |  0.2923076923076923 | 0.9615384615384616 | 0.08333333333333333  | 0.15337423312883433  |\n",
            "|    Logreg 0.2    | 0.24102564102564103 |        1.0         | 0.013333333333333334 | 0.02631578947368421  |\n",
            "|     SVM 0.2      |  0.258974358974359  |        1.0         | 0.03666666666666667  |  0.0707395498392283  |\n",
            "|  LSTM beta 0.2   | 0.25210084033613445 |        0.0         |         0.0          |         0.0          |\n",
            "|   GRU beta 0.2   | 0.27170868347338933 |        1.0         | 0.026217228464419477 | 0.051094890510948905 |\n",
            "| XGBoost beta 0.2 | 0.29971988795518206 | 0.8695652173913043 |  0.0749063670411985  | 0.13793103448275865  |\n",
            "| logreg beta 0.2  |  0.2689075630252101 |        1.0         | 0.02247191011235955  | 0.04395604395604395  |\n",
            "|   svm beta 0.2   |  0.2689075630252101 |        1.0         | 0.02247191011235955  | 0.04395604395604395  |\n",
            "+------------------+---------------------+--------------------+----------------------+----------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 16ms/step - loss: 0.5065 - accuracy: 0.8092 - val_loss: 0.7686 - val_accuracy: 0.6128\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4911 - accuracy: 0.8151 - val_loss: 0.7539 - val_accuracy: 0.6128\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4768 - accuracy: 0.8151 - val_loss: 0.8258 - val_accuracy: 0.6128\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4504 - accuracy: 0.8109 - val_loss: 0.6194 - val_accuracy: 0.6128\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4259 - accuracy: 0.8286 - val_loss: 0.6290 - val_accuracy: 0.6385\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 15ms/step - loss: 0.5155 - accuracy: 0.8118 - val_loss: 0.8285 - val_accuracy: 0.6128\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4642 - accuracy: 0.8168 - val_loss: 0.7093 - val_accuracy: 0.6154\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4227 - accuracy: 0.8336 - val_loss: 0.6394 - val_accuracy: 0.6487\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4057 - accuracy: 0.8269 - val_loss: 0.5978 - val_accuracy: 0.7077\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3990 - accuracy: 0.8462 - val_loss: 0.6231 - val_accuracy: 0.6846\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.75045\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.749799\n",
            "[2]\tvalidation_0-auc:0.760633\n",
            "[3]\tvalidation_0-auc:0.77144\n",
            "[4]\tvalidation_0-auc:0.767491\n",
            "[5]\tvalidation_0-auc:0.775596\n",
            "[6]\tvalidation_0-auc:0.777356\n",
            "[7]\tvalidation_0-auc:0.772271\n",
            "[8]\tvalidation_0-auc:0.77931\n",
            "[9]\tvalidation_0-auc:0.783452\n",
            "[10]\tvalidation_0-auc:0.783036\n",
            "[11]\tvalidation_0-auc:0.780944\n",
            "[12]\tvalidation_0-auc:0.780016\n",
            "[13]\tvalidation_0-auc:0.778561\n",
            "[14]\tvalidation_0-auc:0.773352\n",
            "[15]\tvalidation_0-auc:0.771094\n",
            "[16]\tvalidation_0-auc:0.772396\n",
            "[17]\tvalidation_0-auc:0.767505\n",
            "[18]\tvalidation_0-auc:0.767201\n",
            "[19]\tvalidation_0-auc:0.764901\n",
            "[20]\tvalidation_0-auc:0.767187\n",
            "[21]\tvalidation_0-auc:0.764277\n",
            "[22]\tvalidation_0-auc:0.762684\n",
            "[23]\tvalidation_0-auc:0.764277\n",
            "[24]\tvalidation_0-auc:0.76267\n",
            "[25]\tvalidation_0-auc:0.758486\n",
            "[26]\tvalidation_0-auc:0.757461\n",
            "[27]\tvalidation_0-auc:0.757211\n",
            "[28]\tvalidation_0-auc:0.755812\n",
            "[29]\tvalidation_0-auc:0.755563\n",
            "[30]\tvalidation_0-auc:0.755105\n",
            "[31]\tvalidation_0-auc:0.753789\n",
            "[32]\tvalidation_0-auc:0.751046\n",
            "[33]\tvalidation_0-auc:0.749938\n",
            "[34]\tvalidation_0-auc:0.748358\n",
            "[35]\tvalidation_0-auc:0.746585\n",
            "[36]\tvalidation_0-auc:0.745546\n",
            "[37]\tvalidation_0-auc:0.744077\n",
            "[38]\tvalidation_0-auc:0.743897\n",
            "[39]\tvalidation_0-auc:0.745172\n",
            "[40]\tvalidation_0-auc:0.743357\n",
            "[41]\tvalidation_0-auc:0.743855\n",
            "[42]\tvalidation_0-auc:0.744784\n",
            "[43]\tvalidation_0-auc:0.745532\n",
            "[44]\tvalidation_0-auc:0.745338\n",
            "[45]\tvalidation_0-auc:0.742955\n",
            "[46]\tvalidation_0-auc:0.742359\n",
            "[47]\tvalidation_0-auc:0.741666\n",
            "[48]\tvalidation_0-auc:0.741057\n",
            "[49]\tvalidation_0-auc:0.739921\n",
            "[50]\tvalidation_0-auc:0.739616\n",
            "[51]\tvalidation_0-auc:0.7383\n",
            "[52]\tvalidation_0-auc:0.738688\n",
            "[53]\tvalidation_0-auc:0.738133\n",
            "[54]\tvalidation_0-auc:0.736443\n",
            "[55]\tvalidation_0-auc:0.737579\n",
            "[56]\tvalidation_0-auc:0.737164\n",
            "[57]\tvalidation_0-auc:0.737219\n",
            "[58]\tvalidation_0-auc:0.738078\n",
            "[59]\tvalidation_0-auc:0.738535\n",
            "Stopping. Best iteration:\n",
            "[9]\tvalidation_0-auc:0.783452\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 3s 16ms/step - loss: 0.5108 - accuracy: 0.8047 - val_loss: 0.7212 - val_accuracy: 0.6078\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4964 - accuracy: 0.8099 - val_loss: 0.7988 - val_accuracy: 0.6078\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4856 - accuracy: 0.8099 - val_loss: 0.8197 - val_accuracy: 0.6078\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4607 - accuracy: 0.8099 - val_loss: 0.7544 - val_accuracy: 0.6078\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4306 - accuracy: 0.8055 - val_loss: 0.6084 - val_accuracy: 0.6078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 2s 14ms/step - loss: 0.5168 - accuracy: 0.8055 - val_loss: 0.7404 - val_accuracy: 0.6078\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.4417 - accuracy: 0.8142 - val_loss: 0.5734 - val_accuracy: 0.6751\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.4053 - accuracy: 0.8228 - val_loss: 0.5454 - val_accuracy: 0.7255\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.3828 - accuracy: 0.8332 - val_loss: 0.6614 - val_accuracy: 0.6667\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.3864 - accuracy: 0.8228 - val_loss: 0.5607 - val_accuracy: 0.7143\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.759414\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.772992\n",
            "[2]\tvalidation_0-auc:0.769108\n",
            "[3]\tvalidation_0-auc:0.773173\n",
            "[4]\tvalidation_0-auc:0.779526\n",
            "[5]\tvalidation_0-auc:0.789121\n",
            "[6]\tvalidation_0-auc:0.790076\n",
            "[7]\tvalidation_0-auc:0.788891\n",
            "[8]\tvalidation_0-auc:0.791656\n",
            "[9]\tvalidation_0-auc:0.792594\n",
            "[10]\tvalidation_0-auc:0.793845\n",
            "[11]\tvalidation_0-auc:0.791754\n",
            "[12]\tvalidation_0-auc:0.794289\n",
            "[13]\tvalidation_0-auc:0.796215\n",
            "[14]\tvalidation_0-auc:0.800592\n",
            "[15]\tvalidation_0-auc:0.80102\n",
            "[16]\tvalidation_0-auc:0.795935\n",
            "[17]\tvalidation_0-auc:0.794124\n",
            "[18]\tvalidation_0-auc:0.794207\n",
            "[19]\tvalidation_0-auc:0.794009\n",
            "[20]\tvalidation_0-auc:0.796725\n",
            "[21]\tvalidation_0-auc:0.798535\n",
            "[22]\tvalidation_0-auc:0.79972\n",
            "[23]\tvalidation_0-auc:0.800872\n",
            "[24]\tvalidation_0-auc:0.800625\n",
            "[25]\tvalidation_0-auc:0.801086\n",
            "[26]\tvalidation_0-auc:0.801662\n",
            "[27]\tvalidation_0-auc:0.800675\n",
            "[28]\tvalidation_0-auc:0.8013\n",
            "[29]\tvalidation_0-auc:0.803242\n",
            "[30]\tvalidation_0-auc:0.800642\n",
            "[31]\tvalidation_0-auc:0.80209\n",
            "[32]\tvalidation_0-auc:0.802123\n",
            "[33]\tvalidation_0-auc:0.801827\n",
            "[34]\tvalidation_0-auc:0.803176\n",
            "[35]\tvalidation_0-auc:0.803111\n",
            "[36]\tvalidation_0-auc:0.804427\n",
            "[37]\tvalidation_0-auc:0.803127\n",
            "[38]\tvalidation_0-auc:0.80293\n",
            "[39]\tvalidation_0-auc:0.802419\n",
            "[40]\tvalidation_0-auc:0.802172\n",
            "[41]\tvalidation_0-auc:0.801745\n",
            "[42]\tvalidation_0-auc:0.801909\n",
            "[43]\tvalidation_0-auc:0.801843\n",
            "[44]\tvalidation_0-auc:0.801777\n",
            "[45]\tvalidation_0-auc:0.802337\n",
            "[46]\tvalidation_0-auc:0.803522\n",
            "[47]\tvalidation_0-auc:0.804378\n",
            "[48]\tvalidation_0-auc:0.805283\n",
            "[49]\tvalidation_0-auc:0.801926\n",
            "[50]\tvalidation_0-auc:0.802814\n",
            "[51]\tvalidation_0-auc:0.802386\n",
            "[52]\tvalidation_0-auc:0.801531\n",
            "[53]\tvalidation_0-auc:0.79944\n",
            "[54]\tvalidation_0-auc:0.799078\n",
            "[55]\tvalidation_0-auc:0.799243\n",
            "[56]\tvalidation_0-auc:0.79865\n",
            "[57]\tvalidation_0-auc:0.798453\n",
            "[58]\tvalidation_0-auc:0.79865\n",
            "[59]\tvalidation_0-auc:0.798288\n",
            "[60]\tvalidation_0-auc:0.798058\n",
            "[61]\tvalidation_0-auc:0.797893\n",
            "[62]\tvalidation_0-auc:0.797696\n",
            "[63]\tvalidation_0-auc:0.797795\n",
            "[64]\tvalidation_0-auc:0.797005\n",
            "[65]\tvalidation_0-auc:0.796643\n",
            "[66]\tvalidation_0-auc:0.796215\n",
            "[67]\tvalidation_0-auc:0.795359\n",
            "[68]\tvalidation_0-auc:0.796017\n",
            "[69]\tvalidation_0-auc:0.796313\n",
            "[70]\tvalidation_0-auc:0.794964\n",
            "[71]\tvalidation_0-auc:0.795145\n",
            "[72]\tvalidation_0-auc:0.795704\n",
            "[73]\tvalidation_0-auc:0.795112\n",
            "[74]\tvalidation_0-auc:0.795013\n",
            "[75]\tvalidation_0-auc:0.794421\n",
            "[76]\tvalidation_0-auc:0.793828\n",
            "[77]\tvalidation_0-auc:0.794618\n",
            "[78]\tvalidation_0-auc:0.795079\n",
            "[79]\tvalidation_0-auc:0.794519\n",
            "[80]\tvalidation_0-auc:0.794059\n",
            "[81]\tvalidation_0-auc:0.793861\n",
            "[82]\tvalidation_0-auc:0.792972\n",
            "[83]\tvalidation_0-auc:0.791936\n",
            "[84]\tvalidation_0-auc:0.792001\n",
            "[85]\tvalidation_0-auc:0.790191\n",
            "[86]\tvalidation_0-auc:0.791277\n",
            "[87]\tvalidation_0-auc:0.791146\n",
            "[88]\tvalidation_0-auc:0.790092\n",
            "[89]\tvalidation_0-auc:0.79131\n",
            "[90]\tvalidation_0-auc:0.79108\n",
            "[91]\tvalidation_0-auc:0.791804\n",
            "[92]\tvalidation_0-auc:0.790783\n",
            "[93]\tvalidation_0-auc:0.790685\n",
            "[94]\tvalidation_0-auc:0.79029\n",
            "[95]\tvalidation_0-auc:0.789533\n",
            "[96]\tvalidation_0-auc:0.78894\n",
            "[97]\tvalidation_0-auc:0.787031\n",
            "[98]\tvalidation_0-auc:0.786669\n",
            "Stopping. Best iteration:\n",
            "[48]\tvalidation_0-auc:0.805283\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.6384615384615384 | 0.7272727272727273 | 0.10596026490066225 | 0.18497109826589594 |\n",
            "|      GRU 0.15     | 0.6846153846153846 | 0.7916666666666666 | 0.25165562913907286 | 0.38190954773869346 |\n",
            "|    XGBoost 0.15   | 0.6974358974358974 | 0.8235294117647058 |  0.2781456953642384 |  0.4158415841584159 |\n",
            "|    Logreg 0.15    | 0.6487179487179487 | 0.8181818181818182 | 0.11920529801324503 | 0.20809248554913296 |\n",
            "|      SVM 0.15     | 0.6743589743589744 | 0.8157894736842105 |  0.2052980132450331 |  0.328042328042328  |\n",
            "|   LSTM beta 0.15  | 0.6078431372549019 |        0.0         |         0.0         |         0.0         |\n",
            "|   GRU beta 0.15   | 0.7142857142857143 | 0.8275862068965517 | 0.34285714285714286 | 0.48484848484848486 |\n",
            "| XGBoost beta 0.15 | 0.7170868347338936 | 0.696969696969697  |  0.4928571428571429 |  0.5774058577405858 |\n",
            "|  logreg beta 0.15 | 0.6862745098039216 | 0.868421052631579  |  0.2357142857142857 |  0.3707865168539326 |\n",
            "|   svm beta 0.15   | 0.7226890756302521 | 0.7204301075268817 |  0.4785714285714286 |  0.5751072961373391 |\n",
            "+-------------------+--------------------+--------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dbz23v6pYLzd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "outputId": "cd542e0b-bae7-48c7-d1f9-40f9ebb544d4"
      },
      "source": [
        "Result_cross.to_csv('RHT_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.648718</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.863636</td>\n",
              "      <td>0.730769</td>\n",
              "      <td>0.419890</td>\n",
              "      <td>0.277372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.868852</td>\n",
              "      <td>0.764103</td>\n",
              "      <td>0.535354</td>\n",
              "      <td>0.386861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.861111</td>\n",
              "      <td>0.715385</td>\n",
              "      <td>0.358382</td>\n",
              "      <td>0.226277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.725641</td>\n",
              "      <td>0.421622</td>\n",
              "      <td>0.284672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.737500</td>\n",
              "      <td>0.753501</td>\n",
              "      <td>0.572816</td>\n",
              "      <td>0.468254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.707547</td>\n",
              "      <td>0.770308</td>\n",
              "      <td>0.646552</td>\n",
              "      <td>0.595238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.780000</td>\n",
              "      <td>0.725490</td>\n",
              "      <td>0.443182</td>\n",
              "      <td>0.309524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.745098</td>\n",
              "      <td>0.606061</td>\n",
              "      <td>0.555556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.251282</td>\n",
              "      <td>0.051948</td>\n",
              "      <td>0.026667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.961538</td>\n",
              "      <td>0.292308</td>\n",
              "      <td>0.153374</td>\n",
              "      <td>0.083333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.241026</td>\n",
              "      <td>0.026316</td>\n",
              "      <td>0.013333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.258974</td>\n",
              "      <td>0.070740</td>\n",
              "      <td>0.036667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.252101</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.271709</td>\n",
              "      <td>0.051095</td>\n",
              "      <td>0.026217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.299720</td>\n",
              "      <td>0.137931</td>\n",
              "      <td>0.074906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.268908</td>\n",
              "      <td>0.043956</td>\n",
              "      <td>0.022472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.268908</td>\n",
              "      <td>0.043956</td>\n",
              "      <td>0.022472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.638462</td>\n",
              "      <td>0.184971</td>\n",
              "      <td>0.105960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.684615</td>\n",
              "      <td>0.381910</td>\n",
              "      <td>0.251656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.697436</td>\n",
              "      <td>0.415842</td>\n",
              "      <td>0.278146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.648718</td>\n",
              "      <td>0.208092</td>\n",
              "      <td>0.119205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.815789</td>\n",
              "      <td>0.674359</td>\n",
              "      <td>0.328042</td>\n",
              "      <td>0.205298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.607843</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.827586</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.484848</td>\n",
              "      <td>0.342857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.696970</td>\n",
              "      <td>0.717087</td>\n",
              "      <td>0.577406</td>\n",
              "      <td>0.492857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.868421</td>\n",
              "      <td>0.686275</td>\n",
              "      <td>0.370787</td>\n",
              "      <td>0.235714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.720430</td>\n",
              "      <td>0.722689</td>\n",
              "      <td>0.575107</td>\n",
              "      <td>0.478571</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  RHT  0.000000  0.648718  0.000000  0.000000\n",
              "1            GRU 0.1  RHT  0.863636  0.730769  0.419890  0.277372\n",
              "2        XGBoost 0.1  RHT  0.868852  0.764103  0.535354  0.386861\n",
              "3         Logreg 0.1  RHT  0.861111  0.715385  0.358382  0.226277\n",
              "4            SVM 0.1  RHT  0.812500  0.725641  0.421622  0.284672\n",
              "5      LSTM beta 0.1  RHT  0.000000  0.647059  0.000000  0.000000\n",
              "6       GRU beta 0.1  RHT  0.737500  0.753501  0.572816  0.468254\n",
              "7   XGBoost beta 0.1  RHT  0.707547  0.770308  0.646552  0.595238\n",
              "8    logreg beta 0.1  RHT  0.780000  0.725490  0.443182  0.309524\n",
              "9       svm beta 0.1  RHT  0.666667  0.745098  0.606061  0.555556\n",
              "0           LSTM 0.2  RHT  0.000000  0.230769  0.000000  0.000000\n",
              "1            GRU 0.2  RHT  1.000000  0.251282  0.051948  0.026667\n",
              "2        XGBoost 0.2  RHT  0.961538  0.292308  0.153374  0.083333\n",
              "3         Logreg 0.2  RHT  1.000000  0.241026  0.026316  0.013333\n",
              "4            SVM 0.2  RHT  1.000000  0.258974  0.070740  0.036667\n",
              "5      LSTM beta 0.2  RHT  0.000000  0.252101  0.000000  0.000000\n",
              "6       GRU beta 0.2  RHT  1.000000  0.271709  0.051095  0.026217\n",
              "7   XGBoost beta 0.2  RHT  0.869565  0.299720  0.137931  0.074906\n",
              "8    logreg beta 0.2  RHT  1.000000  0.268908  0.043956  0.022472\n",
              "9       svm beta 0.2  RHT  1.000000  0.268908  0.043956  0.022472\n",
              "0          LSTM 0.15  RHT  0.727273  0.638462  0.184971  0.105960\n",
              "1           GRU 0.15  RHT  0.791667  0.684615  0.381910  0.251656\n",
              "2       XGBoost 0.15  RHT  0.823529  0.697436  0.415842  0.278146\n",
              "3        Logreg 0.15  RHT  0.818182  0.648718  0.208092  0.119205\n",
              "4           SVM 0.15  RHT  0.815789  0.674359  0.328042  0.205298\n",
              "5     LSTM beta 0.15  RHT  0.000000  0.607843  0.000000  0.000000\n",
              "6      GRU beta 0.15  RHT  0.827586  0.714286  0.484848  0.342857\n",
              "7  XGBoost beta 0.15  RHT  0.696970  0.717087  0.577406  0.492857\n",
              "8   logreg beta 0.15  RHT  0.868421  0.686275  0.370787  0.235714\n",
              "9      svm beta 0.15  RHT  0.720430  0.722689  0.575107  0.478571"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT7obJAWYLzd"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1510:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1510:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1510:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1510:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1510:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1543:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1543:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1543:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1543:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1543:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_logreg_beta.csv')"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzxbq-1ZYLzd"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23fCsGalYLzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ce98cf9-49ec-4ac9-ccf2-9beb1b12102b"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1500, test_end=1900)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"RHT\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 17ms/step - loss: 0.5415 - accuracy: 0.7849 - val_loss: 0.7184 - val_accuracy: 0.6487\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5292 - accuracy: 0.7924 - val_loss: 0.6810 - val_accuracy: 0.6487\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5216 - accuracy: 0.7924 - val_loss: 0.6697 - val_accuracy: 0.6487\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5125 - accuracy: 0.7924 - val_loss: 0.6570 - val_accuracy: 0.6487\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5078 - accuracy: 0.7924 - val_loss: 0.6289 - val_accuracy: 0.6487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 15ms/step - loss: 0.5396 - accuracy: 0.7815 - val_loss: 0.6833 - val_accuracy: 0.6487\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5112 - accuracy: 0.7924 - val_loss: 0.6368 - val_accuracy: 0.6487\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4569 - accuracy: 0.8059 - val_loss: 0.5953 - val_accuracy: 0.6692\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4274 - accuracy: 0.8202 - val_loss: 0.5502 - val_accuracy: 0.7179\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4212 - accuracy: 0.8244 - val_loss: 0.5454 - val_accuracy: 0.7231\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.792634\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.826303\n",
            "[2]\tvalidation_0-auc:0.840469\n",
            "[3]\tvalidation_0-auc:0.839099\n",
            "[4]\tvalidation_0-auc:0.842229\n",
            "[5]\tvalidation_0-auc:0.839791\n",
            "[6]\tvalidation_0-auc:0.841508\n",
            "[7]\tvalidation_0-auc:0.848086\n",
            "[8]\tvalidation_0-auc:0.846672\n",
            "[9]\tvalidation_0-auc:0.847754\n",
            "[10]\tvalidation_0-auc:0.848821\n",
            "[11]\tvalidation_0-auc:0.848778\n",
            "[12]\tvalidation_0-auc:0.845691\n",
            "[13]\tvalidation_0-auc:0.846412\n",
            "[14]\tvalidation_0-auc:0.844335\n",
            "[15]\tvalidation_0-auc:0.845619\n",
            "[16]\tvalidation_0-auc:0.843369\n",
            "[17]\tvalidation_0-auc:0.841724\n",
            "[18]\tvalidation_0-auc:0.838666\n",
            "[19]\tvalidation_0-auc:0.837699\n",
            "[20]\tvalidation_0-auc:0.836733\n",
            "[21]\tvalidation_0-auc:0.837483\n",
            "[22]\tvalidation_0-auc:0.83604\n",
            "[23]\tvalidation_0-auc:0.835839\n",
            "[24]\tvalidation_0-auc:0.834396\n",
            "[25]\tvalidation_0-auc:0.834165\n",
            "[26]\tvalidation_0-auc:0.832405\n",
            "[27]\tvalidation_0-auc:0.831309\n",
            "[28]\tvalidation_0-auc:0.829059\n",
            "[29]\tvalidation_0-auc:0.827039\n",
            "[30]\tvalidation_0-auc:0.827443\n",
            "[31]\tvalidation_0-auc:0.825683\n",
            "[32]\tvalidation_0-auc:0.824067\n",
            "[33]\tvalidation_0-auc:0.823144\n",
            "[34]\tvalidation_0-auc:0.822942\n",
            "[35]\tvalidation_0-auc:0.823\n",
            "[36]\tvalidation_0-auc:0.81974\n",
            "[37]\tvalidation_0-auc:0.819971\n",
            "[38]\tvalidation_0-auc:0.820548\n",
            "[39]\tvalidation_0-auc:0.81948\n",
            "[40]\tvalidation_0-auc:0.818369\n",
            "[41]\tvalidation_0-auc:0.818225\n",
            "[42]\tvalidation_0-auc:0.818874\n",
            "[43]\tvalidation_0-auc:0.818167\n",
            "[44]\tvalidation_0-auc:0.817186\n",
            "[45]\tvalidation_0-auc:0.816956\n",
            "[46]\tvalidation_0-auc:0.817071\n",
            "[47]\tvalidation_0-auc:0.81684\n",
            "[48]\tvalidation_0-auc:0.816725\n",
            "[49]\tvalidation_0-auc:0.815816\n",
            "[50]\tvalidation_0-auc:0.816682\n",
            "[51]\tvalidation_0-auc:0.816364\n",
            "[52]\tvalidation_0-auc:0.816018\n",
            "[53]\tvalidation_0-auc:0.815326\n",
            "[54]\tvalidation_0-auc:0.814691\n",
            "[55]\tvalidation_0-auc:0.814662\n",
            "[56]\tvalidation_0-auc:0.81472\n",
            "[57]\tvalidation_0-auc:0.813739\n",
            "[58]\tvalidation_0-auc:0.813104\n",
            "[59]\tvalidation_0-auc:0.810825\n",
            "[60]\tvalidation_0-auc:0.808776\n",
            "Stopping. Best iteration:\n",
            "[10]\tvalidation_0-auc:0.848821\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 3s 17ms/step - loss: 0.5384 - accuracy: 0.7813 - val_loss: 0.6866 - val_accuracy: 0.6471\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.5348 - accuracy: 0.7865 - val_loss: 0.6775 - val_accuracy: 0.6471\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.5288 - accuracy: 0.7865 - val_loss: 0.6952 - val_accuracy: 0.6471\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.5183 - accuracy: 0.7865 - val_loss: 0.7286 - val_accuracy: 0.6471\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.5155 - accuracy: 0.7865 - val_loss: 0.6946 - val_accuracy: 0.6471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 3s 15ms/step - loss: 0.5371 - accuracy: 0.7857 - val_loss: 0.6468 - val_accuracy: 0.6471\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4760 - accuracy: 0.7926 - val_loss: 0.5462 - val_accuracy: 0.6583\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.4265 - accuracy: 0.8245 - val_loss: 0.4807 - val_accuracy: 0.7871\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.4129 - accuracy: 0.8194 - val_loss: 0.5100 - val_accuracy: 0.7479\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.4047 - accuracy: 0.8289 - val_loss: 0.5189 - val_accuracy: 0.7731\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.719697\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.743403\n",
            "[2]\tvalidation_0-auc:0.774153\n",
            "[3]\tvalidation_0-auc:0.772195\n",
            "[4]\tvalidation_0-auc:0.759191\n",
            "[5]\tvalidation_0-auc:0.772933\n",
            "[6]\tvalidation_0-auc:0.774222\n",
            "[7]\tvalidation_0-auc:0.787673\n",
            "[8]\tvalidation_0-auc:0.791538\n",
            "[9]\tvalidation_0-auc:0.791933\n",
            "[10]\tvalidation_0-auc:0.794596\n",
            "[11]\tvalidation_0-auc:0.796657\n",
            "[12]\tvalidation_0-auc:0.796537\n",
            "[13]\tvalidation_0-auc:0.797499\n",
            "[14]\tvalidation_0-auc:0.80346\n",
            "[15]\tvalidation_0-auc:0.807428\n",
            "[16]\tvalidation_0-auc:0.806707\n",
            "[17]\tvalidation_0-auc:0.813681\n",
            "[18]\tvalidation_0-auc:0.814231\n",
            "[19]\tvalidation_0-auc:0.814815\n",
            "[20]\tvalidation_0-auc:0.816395\n",
            "[21]\tvalidation_0-auc:0.814317\n",
            "[22]\tvalidation_0-auc:0.815416\n",
            "[23]\tvalidation_0-auc:0.816997\n",
            "[24]\tvalidation_0-auc:0.81545\n",
            "[25]\tvalidation_0-auc:0.815708\n",
            "[26]\tvalidation_0-auc:0.816086\n",
            "[27]\tvalidation_0-auc:0.816258\n",
            "[28]\tvalidation_0-auc:0.815296\n",
            "[29]\tvalidation_0-auc:0.814987\n",
            "[30]\tvalidation_0-auc:0.815502\n",
            "[31]\tvalidation_0-auc:0.814523\n",
            "[32]\tvalidation_0-auc:0.815571\n",
            "[33]\tvalidation_0-auc:0.817478\n",
            "[34]\tvalidation_0-auc:0.816808\n",
            "[35]\tvalidation_0-auc:0.815502\n",
            "[36]\tvalidation_0-auc:0.815485\n",
            "[37]\tvalidation_0-auc:0.813784\n",
            "[38]\tvalidation_0-auc:0.816034\n",
            "[39]\tvalidation_0-auc:0.817323\n",
            "[40]\tvalidation_0-auc:0.815983\n",
            "[41]\tvalidation_0-auc:0.816704\n",
            "[42]\tvalidation_0-auc:0.815931\n",
            "[43]\tvalidation_0-auc:0.81789\n",
            "[44]\tvalidation_0-auc:0.817374\n",
            "[45]\tvalidation_0-auc:0.817873\n",
            "[46]\tvalidation_0-auc:0.818938\n",
            "[47]\tvalidation_0-auc:0.820089\n",
            "[48]\tvalidation_0-auc:0.821291\n",
            "[49]\tvalidation_0-auc:0.82191\n",
            "[50]\tvalidation_0-auc:0.822253\n",
            "[51]\tvalidation_0-auc:0.820638\n",
            "[52]\tvalidation_0-auc:0.821016\n",
            "[53]\tvalidation_0-auc:0.82203\n",
            "[54]\tvalidation_0-auc:0.821205\n",
            "[55]\tvalidation_0-auc:0.821137\n",
            "[56]\tvalidation_0-auc:0.820879\n",
            "[57]\tvalidation_0-auc:0.820432\n",
            "[58]\tvalidation_0-auc:0.820123\n",
            "[59]\tvalidation_0-auc:0.818457\n",
            "[60]\tvalidation_0-auc:0.819281\n",
            "[61]\tvalidation_0-auc:0.819178\n",
            "[62]\tvalidation_0-auc:0.820071\n",
            "[63]\tvalidation_0-auc:0.820484\n",
            "[64]\tvalidation_0-auc:0.820587\n",
            "[65]\tvalidation_0-auc:0.820381\n",
            "[66]\tvalidation_0-auc:0.819058\n",
            "[67]\tvalidation_0-auc:0.818989\n",
            "[68]\tvalidation_0-auc:0.818199\n",
            "[69]\tvalidation_0-auc:0.818216\n",
            "[70]\tvalidation_0-auc:0.818491\n",
            "[71]\tvalidation_0-auc:0.817374\n",
            "[72]\tvalidation_0-auc:0.817958\n",
            "[73]\tvalidation_0-auc:0.816928\n",
            "[74]\tvalidation_0-auc:0.816722\n",
            "[75]\tvalidation_0-auc:0.816653\n",
            "[76]\tvalidation_0-auc:0.816223\n",
            "[77]\tvalidation_0-auc:0.817289\n",
            "[78]\tvalidation_0-auc:0.817014\n",
            "[79]\tvalidation_0-auc:0.816808\n",
            "[80]\tvalidation_0-auc:0.81722\n",
            "[81]\tvalidation_0-auc:0.816636\n",
            "[82]\tvalidation_0-auc:0.816258\n",
            "[83]\tvalidation_0-auc:0.815674\n",
            "[84]\tvalidation_0-auc:0.815502\n",
            "[85]\tvalidation_0-auc:0.815914\n",
            "[86]\tvalidation_0-auc:0.81588\n",
            "[87]\tvalidation_0-auc:0.815399\n",
            "[88]\tvalidation_0-auc:0.815536\n",
            "[89]\tvalidation_0-auc:0.815296\n",
            "[90]\tvalidation_0-auc:0.815468\n",
            "[91]\tvalidation_0-auc:0.815433\n",
            "[92]\tvalidation_0-auc:0.81399\n",
            "[93]\tvalidation_0-auc:0.813956\n",
            "[94]\tvalidation_0-auc:0.81399\n",
            "[95]\tvalidation_0-auc:0.813784\n",
            "[96]\tvalidation_0-auc:0.815227\n",
            "[97]\tvalidation_0-auc:0.814746\n",
            "[98]\tvalidation_0-auc:0.814334\n",
            "[99]\tvalidation_0-auc:0.815365\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.6487179487179487 |        0.0         |         0.0         |         0.0         |\n",
            "|     GRU 0.1      | 0.7230769230769231 | 0.8918918918918919 | 0.24087591240875914 | 0.37931034482758624 |\n",
            "|   XGBoost 0.1    | 0.764102564102564  | 0.8688524590163934 | 0.38686131386861317 |  0.5353535353535354 |\n",
            "|    Logreg 0.1    | 0.7153846153846154 | 0.8611111111111112 | 0.22627737226277372 |  0.3583815028901734 |\n",
            "|     SVM 0.1      | 0.7256410256410256 |       0.8125       |  0.2846715328467153 | 0.42162162162162165 |\n",
            "|  LSTM beta 0.1   | 0.6470588235294118 |        0.0         |         0.0         |         0.0         |\n",
            "|   GRU beta 0.1   | 0.773109243697479  | 0.8169014084507042 |  0.4603174603174603 |  0.5888324873096447 |\n",
            "| XGBoost beta 0.1 | 0.7703081232492998 | 0.7075471698113207 |  0.5952380952380952 |  0.646551724137931  |\n",
            "| logreg beta 0.1  | 0.7254901960784313 |        0.78        | 0.30952380952380953 |  0.4431818181818182 |\n",
            "|   svm beta 0.1   | 0.7450980392156863 | 0.6666666666666666 |  0.5555555555555556 |  0.606060606060606  |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 17ms/step - loss: 0.3895 - accuracy: 0.8908 - val_loss: 1.8053 - val_accuracy: 0.2308\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3505 - accuracy: 0.8933 - val_loss: 1.5961 - val_accuracy: 0.2308\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3597 - accuracy: 0.8933 - val_loss: 1.6346 - val_accuracy: 0.2308\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3459 - accuracy: 0.8933 - val_loss: 1.8764 - val_accuracy: 0.2308\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3414 - accuracy: 0.8933 - val_loss: 1.7166 - val_accuracy: 0.2308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 15ms/step - loss: 0.4012 - accuracy: 0.8857 - val_loss: 1.8550 - val_accuracy: 0.2308\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3618 - accuracy: 0.8933 - val_loss: 1.9007 - val_accuracy: 0.2308\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3406 - accuracy: 0.8933 - val_loss: 1.6994 - val_accuracy: 0.2308\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3124 - accuracy: 0.8958 - val_loss: 1.4173 - val_accuracy: 0.2590\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3034 - accuracy: 0.8958 - val_loss: 1.7068 - val_accuracy: 0.2641\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.614796\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.613796\n",
            "[2]\tvalidation_0-auc:0.613407\n",
            "[3]\tvalidation_0-auc:0.62937\n",
            "[4]\tvalidation_0-auc:0.684204\n",
            "[5]\tvalidation_0-auc:0.686296\n",
            "[6]\tvalidation_0-auc:0.702074\n",
            "[7]\tvalidation_0-auc:0.70063\n",
            "[8]\tvalidation_0-auc:0.700741\n",
            "[9]\tvalidation_0-auc:0.700444\n",
            "[10]\tvalidation_0-auc:0.700907\n",
            "[11]\tvalidation_0-auc:0.699352\n",
            "[12]\tvalidation_0-auc:0.710741\n",
            "[13]\tvalidation_0-auc:0.704648\n",
            "[14]\tvalidation_0-auc:0.705056\n",
            "[15]\tvalidation_0-auc:0.707056\n",
            "[16]\tvalidation_0-auc:0.709537\n",
            "[17]\tvalidation_0-auc:0.710722\n",
            "[18]\tvalidation_0-auc:0.704278\n",
            "[19]\tvalidation_0-auc:0.708407\n",
            "[20]\tvalidation_0-auc:0.710981\n",
            "[21]\tvalidation_0-auc:0.711926\n",
            "[22]\tvalidation_0-auc:0.715556\n",
            "[23]\tvalidation_0-auc:0.711796\n",
            "[24]\tvalidation_0-auc:0.715944\n",
            "[25]\tvalidation_0-auc:0.71863\n",
            "[26]\tvalidation_0-auc:0.715889\n",
            "[27]\tvalidation_0-auc:0.713333\n",
            "[28]\tvalidation_0-auc:0.711981\n",
            "[29]\tvalidation_0-auc:0.709815\n",
            "[30]\tvalidation_0-auc:0.708926\n",
            "[31]\tvalidation_0-auc:0.705778\n",
            "[32]\tvalidation_0-auc:0.705926\n",
            "[33]\tvalidation_0-auc:0.703593\n",
            "[34]\tvalidation_0-auc:0.700926\n",
            "[35]\tvalidation_0-auc:0.699907\n",
            "[36]\tvalidation_0-auc:0.695982\n",
            "[37]\tvalidation_0-auc:0.694574\n",
            "[38]\tvalidation_0-auc:0.690833\n",
            "[39]\tvalidation_0-auc:0.689241\n",
            "[40]\tvalidation_0-auc:0.688352\n",
            "[41]\tvalidation_0-auc:0.688463\n",
            "[42]\tvalidation_0-auc:0.687556\n",
            "[43]\tvalidation_0-auc:0.687074\n",
            "[44]\tvalidation_0-auc:0.688296\n",
            "[45]\tvalidation_0-auc:0.687\n",
            "[46]\tvalidation_0-auc:0.687426\n",
            "[47]\tvalidation_0-auc:0.686389\n",
            "[48]\tvalidation_0-auc:0.685167\n",
            "[49]\tvalidation_0-auc:0.683278\n",
            "[50]\tvalidation_0-auc:0.681574\n",
            "[51]\tvalidation_0-auc:0.681426\n",
            "[52]\tvalidation_0-auc:0.68163\n",
            "[53]\tvalidation_0-auc:0.682259\n",
            "[54]\tvalidation_0-auc:0.681889\n",
            "[55]\tvalidation_0-auc:0.68063\n",
            "[56]\tvalidation_0-auc:0.681593\n",
            "[57]\tvalidation_0-auc:0.681074\n",
            "[58]\tvalidation_0-auc:0.681074\n",
            "[59]\tvalidation_0-auc:0.680333\n",
            "[60]\tvalidation_0-auc:0.682222\n",
            "[61]\tvalidation_0-auc:0.680741\n",
            "[62]\tvalidation_0-auc:0.681593\n",
            "[63]\tvalidation_0-auc:0.681407\n",
            "[64]\tvalidation_0-auc:0.679259\n",
            "[65]\tvalidation_0-auc:0.679741\n",
            "[66]\tvalidation_0-auc:0.677889\n",
            "[67]\tvalidation_0-auc:0.679556\n",
            "[68]\tvalidation_0-auc:0.678926\n",
            "[69]\tvalidation_0-auc:0.679333\n",
            "[70]\tvalidation_0-auc:0.679482\n",
            "[71]\tvalidation_0-auc:0.678296\n",
            "[72]\tvalidation_0-auc:0.68\n",
            "[73]\tvalidation_0-auc:0.679407\n",
            "[74]\tvalidation_0-auc:0.677741\n",
            "[75]\tvalidation_0-auc:0.677481\n",
            "Stopping. Best iteration:\n",
            "[25]\tvalidation_0-auc:0.71863\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 3s 17ms/step - loss: 0.3945 - accuracy: 0.8859 - val_loss: 1.7395 - val_accuracy: 0.2521\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 12ms/step - loss: 0.3584 - accuracy: 0.8902 - val_loss: 1.5328 - val_accuracy: 0.2521\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 12ms/step - loss: 0.3732 - accuracy: 0.8902 - val_loss: 1.5412 - val_accuracy: 0.2521\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.3561 - accuracy: 0.8902 - val_loss: 1.6267 - val_accuracy: 0.2521\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.3543 - accuracy: 0.8902 - val_loss: 1.6588 - val_accuracy: 0.2521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 3s 16ms/step - loss: 0.3994 - accuracy: 0.8825 - val_loss: 1.8079 - val_accuracy: 0.2521\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.3263 - accuracy: 0.8937 - val_loss: 1.8832 - val_accuracy: 0.2521\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.2860 - accuracy: 0.9032 - val_loss: 1.7415 - val_accuracy: 0.2577\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.2777 - accuracy: 0.9118 - val_loss: 1.6003 - val_accuracy: 0.2829\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.2654 - accuracy: 0.9127 - val_loss: 1.4509 - val_accuracy: 0.3417\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.670454\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.664836\n",
            "[2]\tvalidation_0-auc:0.64124\n",
            "[3]\tvalidation_0-auc:0.633999\n",
            "[4]\tvalidation_0-auc:0.639638\n",
            "[5]\tvalidation_0-auc:0.634623\n",
            "[6]\tvalidation_0-auc:0.636871\n",
            "[7]\tvalidation_0-auc:0.636829\n",
            "[8]\tvalidation_0-auc:0.63633\n",
            "[9]\tvalidation_0-auc:0.63094\n",
            "[10]\tvalidation_0-auc:0.635976\n",
            "[11]\tvalidation_0-auc:0.631273\n",
            "[12]\tvalidation_0-auc:0.637828\n",
            "[13]\tvalidation_0-auc:0.636704\n",
            "[14]\tvalidation_0-auc:0.637474\n",
            "[15]\tvalidation_0-auc:0.638951\n",
            "[16]\tvalidation_0-auc:0.639243\n",
            "[17]\tvalidation_0-auc:0.651352\n",
            "[18]\tvalidation_0-auc:0.65901\n",
            "[19]\tvalidation_0-auc:0.666146\n",
            "[20]\tvalidation_0-auc:0.674241\n",
            "[21]\tvalidation_0-auc:0.680275\n",
            "[22]\tvalidation_0-auc:0.686517\n",
            "[23]\tvalidation_0-auc:0.69149\n",
            "[24]\tvalidation_0-auc:0.687765\n",
            "[25]\tvalidation_0-auc:0.696005\n",
            "[26]\tvalidation_0-auc:0.691677\n",
            "[27]\tvalidation_0-auc:0.687682\n",
            "[28]\tvalidation_0-auc:0.694008\n",
            "[29]\tvalidation_0-auc:0.689305\n",
            "[30]\tvalidation_0-auc:0.694965\n",
            "[31]\tvalidation_0-auc:0.691968\n",
            "[32]\tvalidation_0-auc:0.695839\n",
            "[33]\tvalidation_0-auc:0.697628\n",
            "[34]\tvalidation_0-auc:0.693154\n",
            "[35]\tvalidation_0-auc:0.6964\n",
            "[36]\tvalidation_0-auc:0.701456\n",
            "[37]\tvalidation_0-auc:0.702809\n",
            "[38]\tvalidation_0-auc:0.698564\n",
            "[39]\tvalidation_0-auc:0.702226\n",
            "[40]\tvalidation_0-auc:0.708489\n",
            "[41]\tvalidation_0-auc:0.713005\n",
            "[42]\tvalidation_0-auc:0.713587\n",
            "[43]\tvalidation_0-auc:0.710633\n",
            "[44]\tvalidation_0-auc:0.711715\n",
            "[45]\tvalidation_0-auc:0.707241\n",
            "[46]\tvalidation_0-auc:0.710737\n",
            "[47]\tvalidation_0-auc:0.708864\n",
            "[48]\tvalidation_0-auc:0.702518\n",
            "[49]\tvalidation_0-auc:0.701685\n",
            "[50]\tvalidation_0-auc:0.700895\n",
            "[51]\tvalidation_0-auc:0.701977\n",
            "[52]\tvalidation_0-auc:0.705327\n",
            "[53]\tvalidation_0-auc:0.703288\n",
            "[54]\tvalidation_0-auc:0.705868\n",
            "[55]\tvalidation_0-auc:0.706076\n",
            "[56]\tvalidation_0-auc:0.703953\n",
            "[57]\tvalidation_0-auc:0.700749\n",
            "[58]\tvalidation_0-auc:0.70516\n",
            "[59]\tvalidation_0-auc:0.705077\n",
            "[60]\tvalidation_0-auc:0.707241\n",
            "[61]\tvalidation_0-auc:0.708739\n",
            "[62]\tvalidation_0-auc:0.709571\n",
            "[63]\tvalidation_0-auc:0.709238\n",
            "[64]\tvalidation_0-auc:0.707407\n",
            "[65]\tvalidation_0-auc:0.704869\n",
            "[66]\tvalidation_0-auc:0.703912\n",
            "[67]\tvalidation_0-auc:0.704203\n",
            "[68]\tvalidation_0-auc:0.705368\n",
            "[69]\tvalidation_0-auc:0.704661\n",
            "[70]\tvalidation_0-auc:0.699709\n",
            "[71]\tvalidation_0-auc:0.701956\n",
            "[72]\tvalidation_0-auc:0.701956\n",
            "[73]\tvalidation_0-auc:0.700208\n",
            "[74]\tvalidation_0-auc:0.700083\n",
            "[75]\tvalidation_0-auc:0.703163\n",
            "[76]\tvalidation_0-auc:0.708406\n",
            "[77]\tvalidation_0-auc:0.704494\n",
            "[78]\tvalidation_0-auc:0.70258\n",
            "[79]\tvalidation_0-auc:0.706409\n",
            "[80]\tvalidation_0-auc:0.710071\n",
            "[81]\tvalidation_0-auc:0.708281\n",
            "[82]\tvalidation_0-auc:0.707158\n",
            "[83]\tvalidation_0-auc:0.706617\n",
            "[84]\tvalidation_0-auc:0.705452\n",
            "[85]\tvalidation_0-auc:0.705035\n",
            "[86]\tvalidation_0-auc:0.701914\n",
            "[87]\tvalidation_0-auc:0.700208\n",
            "[88]\tvalidation_0-auc:0.699459\n",
            "[89]\tvalidation_0-auc:0.702871\n",
            "[90]\tvalidation_0-auc:0.701706\n",
            "[91]\tvalidation_0-auc:0.701165\n",
            "[92]\tvalidation_0-auc:0.701873\n",
            "Stopping. Best iteration:\n",
            "[42]\tvalidation_0-auc:0.713587\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+---------------------+--------------------+----------------------+---------------------+\n",
            "|      Model       |       Accuracy      |     Precision      |        Recall        |       F1 score      |\n",
            "+------------------+---------------------+--------------------+----------------------+---------------------+\n",
            "|     LSTM 0.2     | 0.23076923076923078 |        0.0         |         0.0          |         0.0         |\n",
            "|     GRU 0.2      |  0.2641025641025641 |        1.0         | 0.043333333333333335 | 0.08306709265175719 |\n",
            "|   XGBoost 0.2    |  0.2923076923076923 | 0.9615384615384616 | 0.08333333333333333  | 0.15337423312883433 |\n",
            "|    Logreg 0.2    | 0.24102564102564103 |        1.0         | 0.013333333333333334 | 0.02631578947368421 |\n",
            "|     SVM 0.2      |  0.258974358974359  |        1.0         | 0.03666666666666667  |  0.0707395498392283 |\n",
            "|  LSTM beta 0.2   | 0.25210084033613445 |        0.0         |         0.0          |         0.0         |\n",
            "|   GRU beta 0.2   | 0.34173669467787116 |        0.9         |  0.1348314606741573  | 0.23452768729641693 |\n",
            "| XGBoost beta 0.2 | 0.29971988795518206 | 0.8695652173913043 |  0.0749063670411985  | 0.13793103448275865 |\n",
            "| logreg beta 0.2  |  0.2689075630252101 |        1.0         | 0.02247191011235955  | 0.04395604395604395 |\n",
            "|   svm beta 0.2   |  0.2689075630252101 |        1.0         | 0.02247191011235955  | 0.04395604395604395 |\n",
            "+------------------+---------------------+--------------------+----------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 18ms/step - loss: 0.5061 - accuracy: 0.8118 - val_loss: 0.7827 - val_accuracy: 0.6128\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4922 - accuracy: 0.8151 - val_loss: 0.7886 - val_accuracy: 0.6128\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4693 - accuracy: 0.8193 - val_loss: 0.8042 - val_accuracy: 0.6128\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4597 - accuracy: 0.8176 - val_loss: 0.6197 - val_accuracy: 0.6667\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4168 - accuracy: 0.8294 - val_loss: 0.6199 - val_accuracy: 0.7103\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "75/75 [==============================] - 3s 16ms/step - loss: 0.5132 - accuracy: 0.8151 - val_loss: 0.7858 - val_accuracy: 0.6128\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4795 - accuracy: 0.8151 - val_loss: 0.7423 - val_accuracy: 0.6128\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4349 - accuracy: 0.8235 - val_loss: 0.5956 - val_accuracy: 0.6692\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4205 - accuracy: 0.8395 - val_loss: 0.5755 - val_accuracy: 0.7282\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3951 - accuracy: 0.8538 - val_loss: 0.5766 - val_accuracy: 0.7359\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.75045\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.749799\n",
            "[2]\tvalidation_0-auc:0.760633\n",
            "[3]\tvalidation_0-auc:0.77144\n",
            "[4]\tvalidation_0-auc:0.767491\n",
            "[5]\tvalidation_0-auc:0.775596\n",
            "[6]\tvalidation_0-auc:0.777356\n",
            "[7]\tvalidation_0-auc:0.772271\n",
            "[8]\tvalidation_0-auc:0.77931\n",
            "[9]\tvalidation_0-auc:0.783452\n",
            "[10]\tvalidation_0-auc:0.783036\n",
            "[11]\tvalidation_0-auc:0.780944\n",
            "[12]\tvalidation_0-auc:0.780016\n",
            "[13]\tvalidation_0-auc:0.778561\n",
            "[14]\tvalidation_0-auc:0.773352\n",
            "[15]\tvalidation_0-auc:0.771094\n",
            "[16]\tvalidation_0-auc:0.772396\n",
            "[17]\tvalidation_0-auc:0.767505\n",
            "[18]\tvalidation_0-auc:0.767201\n",
            "[19]\tvalidation_0-auc:0.764901\n",
            "[20]\tvalidation_0-auc:0.767187\n",
            "[21]\tvalidation_0-auc:0.764277\n",
            "[22]\tvalidation_0-auc:0.762684\n",
            "[23]\tvalidation_0-auc:0.764277\n",
            "[24]\tvalidation_0-auc:0.76267\n",
            "[25]\tvalidation_0-auc:0.758486\n",
            "[26]\tvalidation_0-auc:0.757461\n",
            "[27]\tvalidation_0-auc:0.757211\n",
            "[28]\tvalidation_0-auc:0.755812\n",
            "[29]\tvalidation_0-auc:0.755563\n",
            "[30]\tvalidation_0-auc:0.755105\n",
            "[31]\tvalidation_0-auc:0.753789\n",
            "[32]\tvalidation_0-auc:0.751046\n",
            "[33]\tvalidation_0-auc:0.749938\n",
            "[34]\tvalidation_0-auc:0.748358\n",
            "[35]\tvalidation_0-auc:0.746585\n",
            "[36]\tvalidation_0-auc:0.745546\n",
            "[37]\tvalidation_0-auc:0.744077\n",
            "[38]\tvalidation_0-auc:0.743897\n",
            "[39]\tvalidation_0-auc:0.745172\n",
            "[40]\tvalidation_0-auc:0.743357\n",
            "[41]\tvalidation_0-auc:0.743855\n",
            "[42]\tvalidation_0-auc:0.744784\n",
            "[43]\tvalidation_0-auc:0.745532\n",
            "[44]\tvalidation_0-auc:0.745338\n",
            "[45]\tvalidation_0-auc:0.742955\n",
            "[46]\tvalidation_0-auc:0.742359\n",
            "[47]\tvalidation_0-auc:0.741666\n",
            "[48]\tvalidation_0-auc:0.741057\n",
            "[49]\tvalidation_0-auc:0.739921\n",
            "[50]\tvalidation_0-auc:0.739616\n",
            "[51]\tvalidation_0-auc:0.7383\n",
            "[52]\tvalidation_0-auc:0.738688\n",
            "[53]\tvalidation_0-auc:0.738133\n",
            "[54]\tvalidation_0-auc:0.736443\n",
            "[55]\tvalidation_0-auc:0.737579\n",
            "[56]\tvalidation_0-auc:0.737164\n",
            "[57]\tvalidation_0-auc:0.737219\n",
            "[58]\tvalidation_0-auc:0.738078\n",
            "[59]\tvalidation_0-auc:0.738535\n",
            "Stopping. Best iteration:\n",
            "[9]\tvalidation_0-auc:0.783452\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 3s 17ms/step - loss: 0.5201 - accuracy: 0.8064 - val_loss: 0.7033 - val_accuracy: 0.6078\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.5024 - accuracy: 0.8099 - val_loss: 0.7287 - val_accuracy: 0.6078\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4957 - accuracy: 0.8099 - val_loss: 0.7615 - val_accuracy: 0.6078\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4638 - accuracy: 0.8099 - val_loss: 0.7089 - val_accuracy: 0.6078\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4343 - accuracy: 0.8099 - val_loss: 0.6486 - val_accuracy: 0.6078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "73/73 [==============================] - 3s 16ms/step - loss: 0.5251 - accuracy: 0.8107 - val_loss: 0.7633 - val_accuracy: 0.6078\n",
            "Epoch 2/5\n",
            "73/73 [==============================] - 1s 11ms/step - loss: 0.4553 - accuracy: 0.8133 - val_loss: 0.6700 - val_accuracy: 0.6218\n",
            "Epoch 3/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.4092 - accuracy: 0.8211 - val_loss: 0.5350 - val_accuracy: 0.7199\n",
            "Epoch 4/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.3807 - accuracy: 0.8418 - val_loss: 0.6008 - val_accuracy: 0.7283\n",
            "Epoch 5/5\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.3832 - accuracy: 0.8418 - val_loss: 0.5198 - val_accuracy: 0.7283\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.759414\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.772992\n",
            "[2]\tvalidation_0-auc:0.769108\n",
            "[3]\tvalidation_0-auc:0.773173\n",
            "[4]\tvalidation_0-auc:0.779526\n",
            "[5]\tvalidation_0-auc:0.789121\n",
            "[6]\tvalidation_0-auc:0.790076\n",
            "[7]\tvalidation_0-auc:0.788891\n",
            "[8]\tvalidation_0-auc:0.791656\n",
            "[9]\tvalidation_0-auc:0.792594\n",
            "[10]\tvalidation_0-auc:0.793845\n",
            "[11]\tvalidation_0-auc:0.791754\n",
            "[12]\tvalidation_0-auc:0.794289\n",
            "[13]\tvalidation_0-auc:0.796215\n",
            "[14]\tvalidation_0-auc:0.800592\n",
            "[15]\tvalidation_0-auc:0.80102\n",
            "[16]\tvalidation_0-auc:0.795935\n",
            "[17]\tvalidation_0-auc:0.794124\n",
            "[18]\tvalidation_0-auc:0.794207\n",
            "[19]\tvalidation_0-auc:0.794009\n",
            "[20]\tvalidation_0-auc:0.796725\n",
            "[21]\tvalidation_0-auc:0.798535\n",
            "[22]\tvalidation_0-auc:0.79972\n",
            "[23]\tvalidation_0-auc:0.800872\n",
            "[24]\tvalidation_0-auc:0.800625\n",
            "[25]\tvalidation_0-auc:0.801086\n",
            "[26]\tvalidation_0-auc:0.801662\n",
            "[27]\tvalidation_0-auc:0.800675\n",
            "[28]\tvalidation_0-auc:0.8013\n",
            "[29]\tvalidation_0-auc:0.803242\n",
            "[30]\tvalidation_0-auc:0.800642\n",
            "[31]\tvalidation_0-auc:0.80209\n",
            "[32]\tvalidation_0-auc:0.802123\n",
            "[33]\tvalidation_0-auc:0.801827\n",
            "[34]\tvalidation_0-auc:0.803176\n",
            "[35]\tvalidation_0-auc:0.803111\n",
            "[36]\tvalidation_0-auc:0.804427\n",
            "[37]\tvalidation_0-auc:0.803127\n",
            "[38]\tvalidation_0-auc:0.80293\n",
            "[39]\tvalidation_0-auc:0.802419\n",
            "[40]\tvalidation_0-auc:0.802172\n",
            "[41]\tvalidation_0-auc:0.801745\n",
            "[42]\tvalidation_0-auc:0.801909\n",
            "[43]\tvalidation_0-auc:0.801843\n",
            "[44]\tvalidation_0-auc:0.801777\n",
            "[45]\tvalidation_0-auc:0.802337\n",
            "[46]\tvalidation_0-auc:0.803522\n",
            "[47]\tvalidation_0-auc:0.804378\n",
            "[48]\tvalidation_0-auc:0.805283\n",
            "[49]\tvalidation_0-auc:0.801926\n",
            "[50]\tvalidation_0-auc:0.802814\n",
            "[51]\tvalidation_0-auc:0.802386\n",
            "[52]\tvalidation_0-auc:0.801531\n",
            "[53]\tvalidation_0-auc:0.79944\n",
            "[54]\tvalidation_0-auc:0.799078\n",
            "[55]\tvalidation_0-auc:0.799243\n",
            "[56]\tvalidation_0-auc:0.79865\n",
            "[57]\tvalidation_0-auc:0.798453\n",
            "[58]\tvalidation_0-auc:0.79865\n",
            "[59]\tvalidation_0-auc:0.798288\n",
            "[60]\tvalidation_0-auc:0.798058\n",
            "[61]\tvalidation_0-auc:0.797893\n",
            "[62]\tvalidation_0-auc:0.797696\n",
            "[63]\tvalidation_0-auc:0.797795\n",
            "[64]\tvalidation_0-auc:0.797005\n",
            "[65]\tvalidation_0-auc:0.796643\n",
            "[66]\tvalidation_0-auc:0.796215\n",
            "[67]\tvalidation_0-auc:0.795359\n",
            "[68]\tvalidation_0-auc:0.796017\n",
            "[69]\tvalidation_0-auc:0.796313\n",
            "[70]\tvalidation_0-auc:0.794964\n",
            "[71]\tvalidation_0-auc:0.795145\n",
            "[72]\tvalidation_0-auc:0.795704\n",
            "[73]\tvalidation_0-auc:0.795112\n",
            "[74]\tvalidation_0-auc:0.795013\n",
            "[75]\tvalidation_0-auc:0.794421\n",
            "[76]\tvalidation_0-auc:0.793828\n",
            "[77]\tvalidation_0-auc:0.794618\n",
            "[78]\tvalidation_0-auc:0.795079\n",
            "[79]\tvalidation_0-auc:0.794519\n",
            "[80]\tvalidation_0-auc:0.794059\n",
            "[81]\tvalidation_0-auc:0.793861\n",
            "[82]\tvalidation_0-auc:0.792972\n",
            "[83]\tvalidation_0-auc:0.791936\n",
            "[84]\tvalidation_0-auc:0.792001\n",
            "[85]\tvalidation_0-auc:0.790191\n",
            "[86]\tvalidation_0-auc:0.791277\n",
            "[87]\tvalidation_0-auc:0.791146\n",
            "[88]\tvalidation_0-auc:0.790092\n",
            "[89]\tvalidation_0-auc:0.79131\n",
            "[90]\tvalidation_0-auc:0.79108\n",
            "[91]\tvalidation_0-auc:0.791804\n",
            "[92]\tvalidation_0-auc:0.790783\n",
            "[93]\tvalidation_0-auc:0.790685\n",
            "[94]\tvalidation_0-auc:0.79029\n",
            "[95]\tvalidation_0-auc:0.789533\n",
            "[96]\tvalidation_0-auc:0.78894\n",
            "[97]\tvalidation_0-auc:0.787031\n",
            "[98]\tvalidation_0-auc:0.786669\n",
            "Stopping. Best iteration:\n",
            "[48]\tvalidation_0-auc:0.805283\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.7102564102564103 | 0.7794117647058824 |  0.3509933774834437 |  0.4840182648401827 |\n",
            "|      GRU 0.15     | 0.735897435897436  | 0.8157894736842105 |  0.4105960264900662 |  0.5462555066079295 |\n",
            "|    XGBoost 0.15   | 0.6974358974358974 | 0.8235294117647058 |  0.2781456953642384 |  0.4158415841584159 |\n",
            "|    Logreg 0.15    | 0.6487179487179487 | 0.8181818181818182 | 0.11920529801324503 | 0.20809248554913296 |\n",
            "|      SVM 0.15     | 0.6743589743589744 | 0.8157894736842105 |  0.2052980132450331 |  0.328042328042328  |\n",
            "|   LSTM beta 0.15  | 0.6078431372549019 |        0.0         |         0.0         |         0.0         |\n",
            "|   GRU beta 0.15   | 0.7282913165266106 | 0.6776859504132231 |  0.5857142857142857 |  0.6283524904214559 |\n",
            "| XGBoost beta 0.15 | 0.7170868347338936 | 0.696969696969697  |  0.4928571428571429 |  0.5774058577405858 |\n",
            "|  logreg beta 0.15 | 0.6862745098039216 | 0.868421052631579  |  0.2357142857142857 |  0.3707865168539326 |\n",
            "|   svm beta 0.15   | 0.7226890756302521 | 0.7204301075268817 |  0.4785714285714286 |  0.5751072961373391 |\n",
            "+-------------------+--------------------+--------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSy3uKygYLzd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "91f6a0af-d153-4936-808a-9f519af2121a"
      },
      "source": [
        "Result_purging.to_csv('RHT_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.638889</td>\n",
              "      <td>0.685279</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.319444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.755556</td>\n",
              "      <td>0.751269</td>\n",
              "      <td>0.581197</td>\n",
              "      <td>0.472222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.708333</td>\n",
              "      <td>0.736041</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.472222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.779412</td>\n",
              "      <td>0.730964</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.368056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.641975</td>\n",
              "      <td>0.692893</td>\n",
              "      <td>0.462222</td>\n",
              "      <td>0.361111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.620499</td>\n",
              "      <td>0.104575</td>\n",
              "      <td>0.055556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.708738</td>\n",
              "      <td>0.720222</td>\n",
              "      <td>0.591093</td>\n",
              "      <td>0.506944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.768421</td>\n",
              "      <td>0.742382</td>\n",
              "      <td>0.610879</td>\n",
              "      <td>0.506944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.795181</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.581498</td>\n",
              "      <td>0.458333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.835052</td>\n",
              "      <td>0.781163</td>\n",
              "      <td>0.672199</td>\n",
              "      <td>0.562500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.497462</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.956522</td>\n",
              "      <td>0.604061</td>\n",
              "      <td>0.360656</td>\n",
              "      <td>0.222222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.855263</td>\n",
              "      <td>0.634518</td>\n",
              "      <td>0.474453</td>\n",
              "      <td>0.328283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.588832</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.196970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.655556</td>\n",
              "      <td>0.568528</td>\n",
              "      <td>0.409722</td>\n",
              "      <td>0.297980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.484765</td>\n",
              "      <td>0.114286</td>\n",
              "      <td>0.060606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.686981</td>\n",
              "      <td>0.638978</td>\n",
              "      <td>0.505051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.904110</td>\n",
              "      <td>0.614958</td>\n",
              "      <td>0.487085</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.824324</td>\n",
              "      <td>0.584488</td>\n",
              "      <td>0.448529</td>\n",
              "      <td>0.308081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.876923</td>\n",
              "      <td>0.587258</td>\n",
              "      <td>0.433460</td>\n",
              "      <td>0.287879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.796296</td>\n",
              "      <td>0.718274</td>\n",
              "      <td>0.607774</td>\n",
              "      <td>0.491429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.853659</td>\n",
              "      <td>0.703046</td>\n",
              "      <td>0.544747</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.867470</td>\n",
              "      <td>0.710660</td>\n",
              "      <td>0.558140</td>\n",
              "      <td>0.411429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.953846</td>\n",
              "      <td>0.705584</td>\n",
              "      <td>0.516667</td>\n",
              "      <td>0.354286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.861538</td>\n",
              "      <td>0.675127</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.320000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.858268</td>\n",
              "      <td>0.767313</td>\n",
              "      <td>0.721854</td>\n",
              "      <td>0.622857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.935484</td>\n",
              "      <td>0.739612</td>\n",
              "      <td>0.649254</td>\n",
              "      <td>0.497143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.922330</td>\n",
              "      <td>0.756233</td>\n",
              "      <td>0.683453</td>\n",
              "      <td>0.542857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.988372</td>\n",
              "      <td>0.747922</td>\n",
              "      <td>0.651341</td>\n",
              "      <td>0.485714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.942529</td>\n",
              "      <td>0.728532</td>\n",
              "      <td>0.625954</td>\n",
              "      <td>0.468571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.648718</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.723077</td>\n",
              "      <td>0.379310</td>\n",
              "      <td>0.240876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.868852</td>\n",
              "      <td>0.764103</td>\n",
              "      <td>0.535354</td>\n",
              "      <td>0.386861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.861111</td>\n",
              "      <td>0.715385</td>\n",
              "      <td>0.358382</td>\n",
              "      <td>0.226277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.725641</td>\n",
              "      <td>0.421622</td>\n",
              "      <td>0.284672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.816901</td>\n",
              "      <td>0.773109</td>\n",
              "      <td>0.588832</td>\n",
              "      <td>0.460317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.707547</td>\n",
              "      <td>0.770308</td>\n",
              "      <td>0.646552</td>\n",
              "      <td>0.595238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.780000</td>\n",
              "      <td>0.725490</td>\n",
              "      <td>0.443182</td>\n",
              "      <td>0.309524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.745098</td>\n",
              "      <td>0.606061</td>\n",
              "      <td>0.555556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.264103</td>\n",
              "      <td>0.083067</td>\n",
              "      <td>0.043333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.961538</td>\n",
              "      <td>0.292308</td>\n",
              "      <td>0.153374</td>\n",
              "      <td>0.083333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.241026</td>\n",
              "      <td>0.026316</td>\n",
              "      <td>0.013333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.258974</td>\n",
              "      <td>0.070740</td>\n",
              "      <td>0.036667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.252101</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.341737</td>\n",
              "      <td>0.234528</td>\n",
              "      <td>0.134831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.299720</td>\n",
              "      <td>0.137931</td>\n",
              "      <td>0.074906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.268908</td>\n",
              "      <td>0.043956</td>\n",
              "      <td>0.022472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>RHT</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.268908</td>\n",
              "      <td>0.043956</td>\n",
              "      <td>0.022472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.779412</td>\n",
              "      <td>0.710256</td>\n",
              "      <td>0.484018</td>\n",
              "      <td>0.350993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.815789</td>\n",
              "      <td>0.735897</td>\n",
              "      <td>0.546256</td>\n",
              "      <td>0.410596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.697436</td>\n",
              "      <td>0.415842</td>\n",
              "      <td>0.278146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.648718</td>\n",
              "      <td>0.208092</td>\n",
              "      <td>0.119205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.815789</td>\n",
              "      <td>0.674359</td>\n",
              "      <td>0.328042</td>\n",
              "      <td>0.205298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.607843</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.677686</td>\n",
              "      <td>0.728291</td>\n",
              "      <td>0.628352</td>\n",
              "      <td>0.585714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.696970</td>\n",
              "      <td>0.717087</td>\n",
              "      <td>0.577406</td>\n",
              "      <td>0.492857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.868421</td>\n",
              "      <td>0.686275</td>\n",
              "      <td>0.370787</td>\n",
              "      <td>0.235714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>RHT</td>\n",
              "      <td>0.720430</td>\n",
              "      <td>0.722689</td>\n",
              "      <td>0.575107</td>\n",
              "      <td>0.478571</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  RHT  0.638889  0.685279  0.425926  0.319444\n",
              "1            GRU 0.1  RHT  0.755556  0.751269  0.581197  0.472222\n",
              "2        XGBoost 0.1  RHT  0.708333  0.736041  0.566667  0.472222\n",
              "3         Logreg 0.1  RHT  0.779412  0.730964  0.500000  0.368056\n",
              "4            SVM 0.1  RHT  0.641975  0.692893  0.462222  0.361111\n",
              "5      LSTM beta 0.1  RHT  0.888889  0.620499  0.104575  0.055556\n",
              "6       GRU beta 0.1  RHT  0.708738  0.720222  0.591093  0.506944\n",
              "7   XGBoost beta 0.1  RHT  0.768421  0.742382  0.610879  0.506944\n",
              "8    logreg beta 0.1  RHT  0.795181  0.736842  0.581498  0.458333\n",
              "9       svm beta 0.1  RHT  0.835052  0.781163  0.672199  0.562500\n",
              "0           LSTM 0.2  RHT  0.000000  0.497462  0.000000  0.000000\n",
              "1            GRU 0.2  RHT  0.956522  0.604061  0.360656  0.222222\n",
              "2        XGBoost 0.2  RHT  0.855263  0.634518  0.474453  0.328283\n",
              "3         Logreg 0.2  RHT  0.928571  0.588832  0.325000  0.196970\n",
              "4            SVM 0.2  RHT  0.655556  0.568528  0.409722  0.297980\n",
              "5      LSTM beta 0.2  RHT  1.000000  0.484765  0.114286  0.060606\n",
              "6       GRU beta 0.2  RHT  0.869565  0.686981  0.638978  0.505051\n",
              "7   XGBoost beta 0.2  RHT  0.904110  0.614958  0.487085  0.333333\n",
              "8    logreg beta 0.2  RHT  0.824324  0.584488  0.448529  0.308081\n",
              "9       svm beta 0.2  RHT  0.876923  0.587258  0.433460  0.287879\n",
              "0          LSTM 0.15  RHT  0.796296  0.718274  0.607774  0.491429\n",
              "1           GRU 0.15  RHT  0.853659  0.703046  0.544747  0.400000\n",
              "2       XGBoost 0.15  RHT  0.867470  0.710660  0.558140  0.411429\n",
              "3        Logreg 0.15  RHT  0.953846  0.705584  0.516667  0.354286\n",
              "4           SVM 0.15  RHT  0.861538  0.675127  0.466667  0.320000\n",
              "5     LSTM beta 0.15  RHT  0.858268  0.767313  0.721854  0.622857\n",
              "6      GRU beta 0.15  RHT  0.935484  0.739612  0.649254  0.497143\n",
              "7  XGBoost beta 0.15  RHT  0.922330  0.756233  0.683453  0.542857\n",
              "8   logreg beta 0.15  RHT  0.988372  0.747922  0.651341  0.485714\n",
              "9      svm beta 0.15  RHT  0.942529  0.728532  0.625954  0.468571\n",
              "0           LSTM 0.1  RHT  0.000000  0.648718  0.000000  0.000000\n",
              "1            GRU 0.1  RHT  0.891892  0.723077  0.379310  0.240876\n",
              "2        XGBoost 0.1  RHT  0.868852  0.764103  0.535354  0.386861\n",
              "3         Logreg 0.1  RHT  0.861111  0.715385  0.358382  0.226277\n",
              "4            SVM 0.1  RHT  0.812500  0.725641  0.421622  0.284672\n",
              "5      LSTM beta 0.1  RHT  0.000000  0.647059  0.000000  0.000000\n",
              "6       GRU beta 0.1  RHT  0.816901  0.773109  0.588832  0.460317\n",
              "7   XGBoost beta 0.1  RHT  0.707547  0.770308  0.646552  0.595238\n",
              "8    logreg beta 0.1  RHT  0.780000  0.725490  0.443182  0.309524\n",
              "9       svm beta 0.1  RHT  0.666667  0.745098  0.606061  0.555556\n",
              "0           LSTM 0.2  RHT  0.000000  0.230769  0.000000  0.000000\n",
              "1            GRU 0.2  RHT  1.000000  0.264103  0.083067  0.043333\n",
              "2        XGBoost 0.2  RHT  0.961538  0.292308  0.153374  0.083333\n",
              "3         Logreg 0.2  RHT  1.000000  0.241026  0.026316  0.013333\n",
              "4            SVM 0.2  RHT  1.000000  0.258974  0.070740  0.036667\n",
              "5      LSTM beta 0.2  RHT  0.000000  0.252101  0.000000  0.000000\n",
              "6       GRU beta 0.2  RHT  0.900000  0.341737  0.234528  0.134831\n",
              "7   XGBoost beta 0.2  RHT  0.869565  0.299720  0.137931  0.074906\n",
              "8    logreg beta 0.2  RHT  1.000000  0.268908  0.043956  0.022472\n",
              "9       svm beta 0.2  RHT  1.000000  0.268908  0.043956  0.022472\n",
              "0          LSTM 0.15  RHT  0.779412  0.710256  0.484018  0.350993\n",
              "1           GRU 0.15  RHT  0.815789  0.735897  0.546256  0.410596\n",
              "2       XGBoost 0.15  RHT  0.823529  0.697436  0.415842  0.278146\n",
              "3        Logreg 0.15  RHT  0.818182  0.648718  0.208092  0.119205\n",
              "4           SVM 0.15  RHT  0.815789  0.674359  0.328042  0.205298\n",
              "5     LSTM beta 0.15  RHT  0.000000  0.607843  0.000000  0.000000\n",
              "6      GRU beta 0.15  RHT  0.677686  0.728291  0.628352  0.585714\n",
              "7  XGBoost beta 0.15  RHT  0.696970  0.717087  0.577406  0.492857\n",
              "8   logreg beta 0.15  RHT  0.868421  0.686275  0.370787  0.235714\n",
              "9      svm beta 0.15  RHT  0.720430  0.722689  0.575107  0.478571"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlOBh5ljYLze"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1510:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1510:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1510:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1510:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1510:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1543:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1543:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1543:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1543:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1543:1900].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RHT_logreg_beta_p.csv')"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIanpF7WYLze"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvl0pxlkYiNp"
      },
      "source": [
        "## RMD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knMZBC-_YiNv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "41faebcf-f6b4-4113-b4e7-e1c785b919d2"
      },
      "source": [
        "dfs = pd.read_csv(\"RMD.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "dfs"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2768</td>\n",
              "      <td>US1.RMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>261.430</td>\n",
              "      <td>263.250</td>\n",
              "      <td>258.43</td>\n",
              "      <td>262.05</td>\n",
              "      <td>12671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2767</td>\n",
              "      <td>US1.RMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>268.770</td>\n",
              "      <td>269.040</td>\n",
              "      <td>263.96</td>\n",
              "      <td>263.96</td>\n",
              "      <td>7497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.RMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>264.905</td>\n",
              "      <td>266.600</td>\n",
              "      <td>263.85</td>\n",
              "      <td>265.42</td>\n",
              "      <td>15648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.RMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>265.150</td>\n",
              "      <td>266.290</td>\n",
              "      <td>262.26</td>\n",
              "      <td>264.74</td>\n",
              "      <td>15954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.RMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>277.170</td>\n",
              "      <td>277.170</td>\n",
              "      <td>268.44</td>\n",
              "      <td>270.48</td>\n",
              "      <td>7537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.RMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>32.650</td>\n",
              "      <td>33.030</td>\n",
              "      <td>32.43</td>\n",
              "      <td>32.64</td>\n",
              "      <td>689901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.RMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>33.330</td>\n",
              "      <td>33.330</td>\n",
              "      <td>32.59</td>\n",
              "      <td>32.73</td>\n",
              "      <td>328524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.RMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>33.240</td>\n",
              "      <td>33.460</td>\n",
              "      <td>33.16</td>\n",
              "      <td>33.23</td>\n",
              "      <td>589273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.RMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>32.650</td>\n",
              "      <td>33.205</td>\n",
              "      <td>32.65</td>\n",
              "      <td>33.15</td>\n",
              "      <td>653616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.RMD</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>32.830</td>\n",
              "      <td>32.870</td>\n",
              "      <td>32.28</td>\n",
              "      <td>32.41</td>\n",
              "      <td>416669</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2769 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...   <HIGH>   <LOW>  <CLOSE>   <VOL>\n",
              "0      2768  US1.RMD     D  20211001  ...  263.250  258.43   262.05   12671\n",
              "1      2767  US1.RMD     D  20210930  ...  269.040  263.96   263.96    7497\n",
              "2      2766  US1.RMD     D  20210929  ...  266.600  263.85   265.42   15648\n",
              "3      2765  US1.RMD     D  20210928  ...  266.290  262.26   264.74   15954\n",
              "4      2764  US1.RMD     D  20210927  ...  277.170  268.44   270.48    7537\n",
              "...     ...      ...   ...       ...  ...      ...     ...      ...     ...\n",
              "2764      4  US1.RMD     D  20101008  ...   33.030   32.43    32.64  689901\n",
              "2765      3  US1.RMD     D  20101007  ...   33.330   32.59    32.73  328524\n",
              "2766      2  US1.RMD     D  20101006  ...   33.460   33.16    33.23  589273\n",
              "2767      1  US1.RMD     D  20101005  ...   33.205   32.65    33.15  653616\n",
              "2768      0  US1.RMD     D  20101004  ...   32.870   32.28    32.41  416669\n",
              "\n",
              "[2769 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrF1s7fvYiNv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2670531f-91e4-484d-8c77-87de2ed40ff2"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"3d08d984-4ab7-49cd-a61b-6a28ca5d90bb\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"3d08d984-4ab7-49cd-a61b-6a28ca5d90bb\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '3d08d984-4ab7-49cd-a61b-6a28ca5d90bb',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [130.24, 132.16, 132.4, 130.3, 128.94, 133.59, 135.2, 133.6, 134.61, 136.0, 135.67, 135.5, 134.5, 133.49, 132.56, 133.88, 130.68, 132.17, 133.96, 133.72, 134.34, 135.13, 140.27, 140.35, 137.84, 137.79, 139.24, 138.38, 137.03, 135.96, 135.13, 132.11, 136.01, 136.6, 135.37, 136.01, 134.38, 132.37, 131.08, 135.02, 131.6, 131.38, 131.42, 128.97, 129.43, 126.9, 130.28, 131.57, 128.68, 130.26, 129.4, 126.86, 123.92, 125.44, 123.83, 123.64, 123.45, 124.76, 123.78, 123.38, 123.77, 124.37, 124.72, 124.01, 124.02, 123.82, 124.35, 123.88, 123.37, 123.72, 121.98, 120.54, 119.5, 119.88, 121.13, 120.08, 123.24, 120.54, 119.61, 118.5, 118.09, 119.48, 119.55, 118.19, 118.26, 117.4, 116.9, 115.15, 114.1, 112.76, 114.12, 113.3, 112.15, 113.37, 113.96, 112.75, 113.12, 112.51, 112.01, 112.79, 113.46, 112.81, 112.06, 110.84, 112.09, 112.05, 111.43, 112.36, 112.61, 112.68, 105.75, 104.43, 104.5, 103.07, 103.925, 102.25, 101.73, 101.49, 100.01, 99.23, 97.38, 101.85, 103.13, 102.29, 100.86, 101.64, 101.35, 101.48, 100.92, 101.29, 103.02, 105.02, 104.64, 103.98, 102.68, 101.5, 102.33, 101.42, 100.14, 100.98, 98.83, 97.92, 97.4, 100.32, 101.0, 102.62, 102.77, 101.4, 101.0, 100.5, 100.61, 102.4, 101.9, 103.11, 102.43, 101.82, 100.81, 101.65, 101.24, 100.12, 99.77, 99.76, 100.46, 99.46, 98.94, 98.22, 97.21, 96.84, 95.73, 95.6, 95.16, 94.33, 93.57, 95.19, 93.77, 92.04, 91.52, 94.56, 117.26, 117.22, 117.47, 117.84, 117.42, 116.88, 115.65, 113.58, 114.8, 115.36, 114.55, 113.49, 111.24, 110.17, 106.52, 111.77, 113.89, 112.04, 111.14, 108.88, 103.52, 106.4, 107.41, 109.53, 111.65, 110.76, 112.94, 115.31, 115.16, 113.94, 112.85, 110.0, 112.35, 112.13, 113.0, 111.79, 110.51, 108.89, 105.61, 104.74, 103.69, 102.85, 101.61, 102.56, 104.19, 104.23, 101.63, 101.68, 103.74, 106.92, 108.52, 107.57, 104.99, 105.0, 105.7, 105.99, 105.87, 105.19, 103.18, 104.14, 98.5, 98.01, 100.65, 102.62, 103.02, 105.05, 106.67, 106.0, 102.67, 103.23, 101.84, 103.17, 106.3, 107.88, 109.81, 109.77, 111.75, 113.86, 115.18, 115.34, 114.56, 115.55, 115.36, 113.99, 113.83, 113.5, 112.61, 113.46, 112.75, 114.21, 114.08, 111.64, 111.33, 112.02, 112.04, 111.92, 111.98, 111.85, 111.4, 112.41, 112.38, 111.56, 110.73, 108.89, 108.24, 107.25, 106.88, 106.4, 106.79, 106.85, 107.15, 108.26, 106.72, 107.78, 108.08, 107.92, 107.73, 106.79, 104.73, 108.63, 106.99, 105.77, 104.65, 106.47, 108.66, 109.32, 108.97, 109.04, 109.32, 108.8, 108.53, 109.39, 108.96, 109.11, 109.01, 107.58, 107.99, 107.23, 106.81, 105.26, 103.52, 103.73, 103.59, 104.1, 103.43, 106.36, 105.52, 107.94, 107.02, 106.8, 106.1, 106.28, 105.8, 106.75, 106.8, 106.95, 106.57, 106.79, 105.9, 106.65, 105.55, 105.13, 103.44, 102.8, 102.58, 101.13, 103.13, 103.11, 102.71, 102.05, 102.85, 101.62, 101.4, 102.38, 101.19, 102.16, 102.01, 101.06, 98.16, 97.83, 97.83, 97.72, 96.99, 95.84, 95.06, 94.6, 94.38, 97.19, 96.45, 96.55, 97.91, 98.8, 100.34, 101.19, 100.71, 99.7, 98.85, 98.5, 96.44, 96.66, 94.64, 93.6, 95.22, 95.85, 96.91, 95.76, 98.47, 96.25, 97.23, 98.29, 95.46, 96.48, 98.35, 98.19, 97.75, 98.19, 98.1, 98.6, 98.87, 99.56, 99.64, 97.61, 96.63, 96.17, 95.4, 94.95, 93.91, 95.24, 95.51, 96.38, 95.42, 93.78, 93.18, 92.83, 93.82, 94.21, 93.46, 91.52, 91.72, 91.04, 89.29, 92.57, 95.08, 93.35, 96.95, 99.7, 100.79, 100.68, 102.8, 102.79, 101.62, 100.22, 100.35, 87.5, 88.68, 88.81, 88.72, 87.84, 87.74, 87.04, 87.1, 88.02, 88.17, 87.62, 86.52, 86.39, 85.5, 84.7, 86.25, 85.75, 85.57, 85.13, 84.95, 85.57, 86.11, 85.66, 85.29, 83.94, 85.04, 85.84, 86.45, 86.36, 85.74, 85.16, 84.33, 83.69, 84.36, 85.41, 85.25, 85.19, 84.68, 85.11, 85.18, 85.62, 84.71, 84.61, 84.99, 84.23, 84.23, 84.01, 83.06, 82.73, 82.17, 82.49, 83.24, 83.11, 82.88, 85.78, 84.17, 82.99, 84.43, 78.65, 78.39, 79.06, 79.83, 78.98, 78.79, 78.87, 78.43, 77.66, 77.62, 77.59, 76.55, 76.21, 76.16, 77.24, 77.29, 77.23, 76.53, 77.42, 76.94, 76.09, 76.16, 76.31, 77.26, 77.37, 77.33, 79.6, 79.45, 79.535, 79.225, 80.55, 79.6092, 80.89, 81.38, 79.83, 78.83, 77.66, 76.58, 77.6, 77.57, 75.58, 74.96, 75.54, 74.23, 73.9, 73.94, 74.56, 73.44, 73.08, 72.8, 72.68, 72.52, 73.53, 72.57, 72.62, 74.1, 74.4, 75.05, 75.08, 73.44, 72.51, 77.48, 77.11, 77.4, 77.47, 77.68, 77.83, 77.16, 76.99, 78.02, 78.07, 77.5, 77.62, 77.59, 77.11, 77.1, 76.44, 76.81, 76.81, 76.08, 77.07, 77.79, 77.86, 77.09, 77.17, 77.02, 77.67, 78.47, 77.63, 78.36, 78.46, 78.9, 77.1, 76.17, 75.37, 75.17, 73.05, 73.49, 73.28, 73.34, 72.55, 72.78, 71.94, 71.9, 71.08, 69.99, 70.05, 70.05, 69.96, 69.6, 69.5, 68.41, 67.51, 67.7, 68.51, 68.89, 68.41, 67.96, 68.2, 67.48, 68.53, 69.37, 69.0, 68.95, 69.8, 68.91, 67.9, 73.06, 71.76, 70.94, 70.74, 69.83, 69.625, 69.0925, 68.65, 69.27, 68.12, 68.4, 69.41, 68.99, 69.01, 69.04, 69.22, 70.55, 71.34, 71.97, 71.52, 72.49, 71.97, 71.68, 71.58, 71.32, 71.28, 70.63, 71.31, 71.61, 71.26, 71.49, 70.98, 71.66, 71.42, 70.78, 70.73, 71.53, 71.82, 72.36, 72.48, 72.95, 72.02, 72.0, 71.95, 71.63, 72.38, 72.18, 72.27, 71.93, 71.64, 71.04, 70.86, 70.21, 69.78, 69.35, 68.91, 68.69, 68.21, 67.0, 66.94, 67.55, 67.68, 68.14, 68.4, 69.84, 69.729, 63.6, 63.64, 62.89, 63.29, 64.17, 65.33, 64.26, 64.12, 64.25, 63.27, 62.13, 61.84, 62.34, 61.95, 62.06, 62.42, 61.67, 61.87, 62.03, 61.61, 61.62, 62.03, 62.3, 62.52, 62.33, 62.22, 62.63, 62.78, 62.23, 61.37, 60.35, 59.79, 59.92, 59.53, 59.57, 61.48, 62.11, 60.61, 59.03, 58.22, 58.06, 58.53, 58.53, 58.8, 57.77, 58.52, 58.57, 59.85, 59.32, 58.36, 57.58, 57.97, 57.42, 57.33, 57.55, 58.17, 59.77, 60.34, 59.84, 60.56, 64.13, 63.75, 63.69, 63.55, 64.51, 64.99, 64.18, 64.655, 63.87, 64.08, 64.09, 65.14, 63.93, 64.5, 63.91, 64.13, 64.32, 64.8, 64.24, 64.71, 64.48, 63.6, 63.25, 63.98, 63.8, 63.45, 63.73, 64.45, 64.96, 64.9, 64.73, 65.66, 65.45, 67.06, 67.61, 67.47, 67.07, 67.13, 66.69, 67.24, 67.91, 69.05, 68.44, 68.01, 67.32, 69.78, 70.065, 69.56, 68.5, 69.09, 70.57, 70.42, 70.5, 70.15, 70.67, 70.58, 69.83, 69.09, 68.81, 69.38, 69.85, 68.87, 65.72, 65.15, 65.94, 65.53, 65.94, 65.88, 66.14, 65.51, 65.87, 64.81, 65.1, 64.45, 64.02, 64.29, 64.51, 63.34, 63.92, 63.58, 63.55, 63.24, 62.04, 60.34, 59.22, 60.51, 64.09, 62.31, 61.79, 59.86, 58.35, 58.76, 58.98, 59.57, 59.22, 59.8, 60.47, 60.25, 59.88, 59.87, 59.53, 59.56, 59.17, 59.06, 58.46, 58.23, 57.73, 58.1399, 57.42, 57.63, 56.83, 56.9, 57.21, 58.09, 57.44, 57.81, 57.56, 57.545, 56.825, 56.49, 56.41, 56.02, 55.65, 56.12, 55.81, 56.5, 56.65, 60.95, 61.16, 60.72, 60.45, 60.03, 59.66, 59.89, 59.75, 59.46, 60.19, 59.41, 58.71, 58.58, 58.99, 59.53, 58.17, 58.78, 58.99, 57.825, 56.89, 56.27, 55.97, 55.47, 56.33, 56.38, 56.18, 56.35, 56.0, 57.3, 58.78, 59.07, 60.36, 58.8, 58.0, 57.95, 58.1, 57.84, 57.45, 57.23, 57.33, 56.91, 57.72, 57.4, 57.77, 57.995, 59.26, 58.895, 59.19, 59.23, 59.03, 57.88, 57.1, 57.73, 57.18, 57.05, 57.42, 57.83, 57.58, 57.05, 57.23, 56.7, 56.24, 57.57, 59.04, 58.65, 57.6, 54.06, 53.75, 53.64, 53.29, 54.16, 53.1, 53.125, 52.31, 52.12, 51.85, 51.415, 52.43, 52.22, 53.68, 54.35, 54.5, 54.11, 53.92, 54.12, 53.535, 52.91, 53.98, 55.19, 56.13, 55.7, 55.64, 55.34, 55.66, 55.06, 56.45, 56.53, 57.81, 58.3, 59.89, 59.93, 59.55, 60.02, 59.54, 59.11, 58.46, 59.11, 58.23, 58.14, 57.58, 57.44, 57.34, 57.35, 58.36, 57.9, 57.98, 58.37, 58.5, 58.53, 58.59, 58.23, 57.61, 57.3, 57.7, 57.37, 56.25, 56.26, 55.5, 56.22, 56.47, 57.91, 56.52, 56.33, 55.23, 54.51, 54.54, 54.29, 53.94, 54.1, 52.9, 52.66, 51.67, 51.27, 50.98, 50.63, 49.44, 50.52, 51.72, 49.73, 49.8, 50.27, 50.98, 51.25, 50.91, 50.89, 50.96, 51.48, 50.69, 50.39, 51.91, 51.19, 51.65, 51.89, 50.72, 51.93, 52.46, 51.65, 51.79, 50.52, 49.57, 52.48, 53.93, 54.46, 54.89, 55.3, 55.86, 55.29, 55.18, 55.18, 55.44, 54.88, 54.82, 55.56, 55.89, 55.39, 57.96, 56.14, 55.9, 56.72, 55.15, 55.89, 56.27, 56.52, 56.78, 57.09, 57.33, 57.79, 57.31, 57.58, 57.41, 57.02, 56.77, 56.08, 56.64, 55.4, 55.28, 55.27, 56.38, 55.44, 55.89, 56.19, 56.31, 56.75, 57.97, 57.56, 57.18, 57.14, 57.75, 58.57, 58.55, 59.1, 58.77, 58.69, 58.9, 59.64, 59.57, 59.58, 59.69, 59.36, 58.82, 58.66, 58.14, 57.44, 57.1, 57.3, 57.27, 56.83, 56.81, 56.17, 56.41, 55.48, 65.38, 65.61, 65.62, 65.19, 64.83, 64.62, 65.67, 64.78, 63.96, 64.07, 64.67, 64.54, 65.7, 73.4, 73.26, 73.1, 72.92, 71.88, 72.25, 73.02, 73.76, 74.76, 74.83, 73.84, 72.94, 72.42, 73.89, 72.52, 71.39, 71.78, 71.67, 71.55, 70.26, 70.38, 71.32, 72.33, 71.75, 71.69, 72.44, 70.86, 70.99, 67.8, 68.71, 67.58, 66.32, 67.29, 65.84, 65.74, 64.87, 64.48, 64.7, 64.34, 64.22, 65.26, 65.36, 65.81, 65.76, 65.15, 65.97, 66.43, 66.27, 65.49, 65.11, 63.78, 63.26, 63.81, 63.89, 63.09, 63.54, 62.58, 62.47, 63.03, 63.32, 65.19, 67.14, 65.24, 61.84, 60.09, 60.2, 60.72, 60.26, 60.33, 59.86, 58.69, 58.14, 57.97, 57.04, 56.65, 57.04, 57.185, 56.07, 56.75, 56.89, 57.36, 57.39, 56.97, 56.8, 56.29, 55.9, 54.96, 54.5, 53.355, 53.43, 53.6, 53.34, 54.455, 54.74, 54.47, 54.24, 54.005, 53.59, 53.31, 53.2, 53.6, 53.06, 52.83, 52.84, 52.52, 52.02, 52.32, 52.27, 52.38, 51.99, 52.22, 51.16, 51.01, 51.15, 51.565, 51.45, 51.82, 51.705, 52.23, 52.08, 51.81, 52.33, 51.56, 52.03, 47.73, 47.5, 48.03, 47.05, 46.79, 46.27, 46.86, 47.81, 48.46, 48.52, 48.95, 50.87, 50.72, 51.33, 50.24, 49.84, 49.5, 49.26, 49.985, 49.75, 49.83, 50.17, 50.16, 50.4, 51.29, 51.58, 51.57, 52.1, 52.42, 52.49, 52.77, 52.61, 52.49, 52.8, 52.985, 53.36, 53.31, 52.87, 53.06, 52.94, 52.49, 52.12, 51.46, 51.88, 51.8, 52.07, 51.53, 51.68, 51.38, 51.56, 51.33, 51.17, 51.38, 50.4, 49.98, 50.22, 49.4, 49.37, 49.65, 51.71, 51.77, 51.43, 51.0, 51.08, 50.26, 50.17, 49.75, 48.96, 49.43, 48.66, 48.705, 49.14, 49.33, 49.27, 49.6, 49.34, 48.95, 49.6, 50.08, 50.5, 49.86, 50.63, 52.48, 51.82, 51.61, 51.21, 51.88, 51.55, 53.2, 53.08, 52.73, 53.03, 53.16, 53.77, 53.02, 52.57, 51.61, 51.33, 51.3, 50.19, 49.75, 49.87, 50.07, 50.21, 49.62, 49.63, 50.03, 50.93, 50.23, 50.08, 50.64, 50.59, 50.1, 50.6, 51.01, 51.42, 50.65, 49.99, 49.96, 49.82, 50.12, 49.88, 49.87, 49.86, 50.04, 48.13, 47.83, 47.4, 46.77, 47.61, 47.37, 47.45, 46.5, 46.86, 45.96, 44.43, 45.2, 45.44, 45.32, 45.0, 44.81, 44.77, 45.36, 45.25, 44.71, 43.89, 43.69, 43.58, 43.64, 42.98, 43.53, 43.87, 44.54, 44.24, 43.73, 43.31, 43.97, 45.1, 44.49, 44.53, 44.21, 43.99, 44.23, 44.09, 43.49, 44.02, 44.2, 43.83, 44.66, 46.27, 46.26, 46.53, 46.14, 46.13, 45.77, 45.95, 45.93, 45.52, 45.57, 43.67, 43.69, 43.49, 43.13, 42.05, 43.59, 43.94, 43.6, 44.11, 43.8103, 43.89, 46.88, 46.53, 46.1, 45.62, 45.81, 46.66, 45.86, 46.84, 47.79, 47.81, 47.79, 47.4, 46.34, 46.74, 46.23, 47.1, 47.2, 46.94, 47.28, 46.95, 46.63, 46.1, 45.66, 45.77, 45.87, 46.155, 46.49, 45.35, 45.49, 47.43, 48.13, 47.89, 47.23, 47.19, 48.72, 48.86, 48.81, 48.67, 49.55, 50.73, 51.11, 50.83, 51.26, 50.9, 50.73, 51.03, 51.5, 51.25, 49.97, 50.71, 50.6, 49.28, 49.85, 50.41, 50.76, 50.9, 51.74, 51.48, 50.7, 50.12, 49.55, 56.33, 57.12]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('3d08d984-4ab7-49cd-a61b-6a28ca5d90bb');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"4ef55e5f-5e36-44f7-923b-7496551fd04e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"4ef55e5f-5e36-44f7-923b-7496551fd04e\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '4ef55e5f-5e36-44f7-923b-7496551fd04e',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4ef55e5f-5e36-44f7-923b-7496551fd04e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRkSCtJoYiNw"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqMU1Uh2YiNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2db8128f-53e3-4681-8642-a96197cc578c"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .2, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"RMD\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 4s 15ms/step - loss: 0.5751 - accuracy: 0.7611 - val_loss: 0.6654 - val_accuracy: 0.6816\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5613 - accuracy: 0.7604 - val_loss: 0.6411 - val_accuracy: 0.6816\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5564 - accuracy: 0.7604 - val_loss: 0.6418 - val_accuracy: 0.6816\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5469 - accuracy: 0.7604 - val_loss: 0.5971 - val_accuracy: 0.6816\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5346 - accuracy: 0.7611 - val_loss: 0.6068 - val_accuracy: 0.6816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.5594 - accuracy: 0.7597 - val_loss: 0.6364 - val_accuracy: 0.6816\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5109 - accuracy: 0.7725 - val_loss: 0.5431 - val_accuracy: 0.7245\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4895 - accuracy: 0.7859 - val_loss: 0.5894 - val_accuracy: 0.6898\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4668 - accuracy: 0.7960 - val_loss: 0.6014 - val_accuracy: 0.7102\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4699 - accuracy: 0.7826 - val_loss: 0.5362 - val_accuracy: 0.7265\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.746181\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.746565\n",
            "[2]\tvalidation_0-auc:0.746027\n",
            "[3]\tvalidation_0-auc:0.746661\n",
            "[4]\tvalidation_0-auc:0.753733\n",
            "[5]\tvalidation_0-auc:0.753052\n",
            "[6]\tvalidation_0-auc:0.758368\n",
            "[7]\tvalidation_0-auc:0.761918\n",
            "[8]\tvalidation_0-auc:0.76544\n",
            "[9]\tvalidation_0-auc:0.763866\n",
            "[10]\tvalidation_0-auc:0.764788\n",
            "[11]\tvalidation_0-auc:0.76331\n",
            "[12]\tvalidation_0-auc:0.76403\n",
            "[13]\tvalidation_0-auc:0.763531\n",
            "[14]\tvalidation_0-auc:0.763646\n",
            "[15]\tvalidation_0-auc:0.763463\n",
            "[16]\tvalidation_0-auc:0.768559\n",
            "[17]\tvalidation_0-auc:0.768684\n",
            "[18]\tvalidation_0-auc:0.769375\n",
            "[19]\tvalidation_0-auc:0.768981\n",
            "[20]\tvalidation_0-auc:0.768405\n",
            "[21]\tvalidation_0-auc:0.767302\n",
            "[22]\tvalidation_0-auc:0.767235\n",
            "[23]\tvalidation_0-auc:0.768358\n",
            "[24]\tvalidation_0-auc:0.767571\n",
            "[25]\tvalidation_0-auc:0.767628\n",
            "[26]\tvalidation_0-auc:0.767571\n",
            "[27]\tvalidation_0-auc:0.768636\n",
            "[28]\tvalidation_0-auc:0.767753\n",
            "[29]\tvalidation_0-auc:0.767484\n",
            "[30]\tvalidation_0-auc:0.767407\n",
            "[31]\tvalidation_0-auc:0.767475\n",
            "[32]\tvalidation_0-auc:0.76735\n",
            "[33]\tvalidation_0-auc:0.76735\n",
            "[34]\tvalidation_0-auc:0.766956\n",
            "[35]\tvalidation_0-auc:0.767417\n",
            "[36]\tvalidation_0-auc:0.768271\n",
            "[37]\tvalidation_0-auc:0.768617\n",
            "[38]\tvalidation_0-auc:0.766112\n",
            "[39]\tvalidation_0-auc:0.766131\n",
            "[40]\tvalidation_0-auc:0.766362\n",
            "[41]\tvalidation_0-auc:0.766956\n",
            "[42]\tvalidation_0-auc:0.766736\n",
            "[43]\tvalidation_0-auc:0.765891\n",
            "[44]\tvalidation_0-auc:0.765968\n",
            "[45]\tvalidation_0-auc:0.765911\n",
            "[46]\tvalidation_0-auc:0.765488\n",
            "[47]\tvalidation_0-auc:0.765239\n",
            "[48]\tvalidation_0-auc:0.765392\n",
            "[49]\tvalidation_0-auc:0.764903\n",
            "[50]\tvalidation_0-auc:0.764682\n",
            "[51]\tvalidation_0-auc:0.765066\n",
            "[52]\tvalidation_0-auc:0.765527\n",
            "[53]\tvalidation_0-auc:0.765431\n",
            "[54]\tvalidation_0-auc:0.765296\n",
            "[55]\tvalidation_0-auc:0.765507\n",
            "[56]\tvalidation_0-auc:0.765795\n",
            "[57]\tvalidation_0-auc:0.765795\n",
            "[58]\tvalidation_0-auc:0.766218\n",
            "[59]\tvalidation_0-auc:0.766448\n",
            "[60]\tvalidation_0-auc:0.767216\n",
            "[61]\tvalidation_0-auc:0.767657\n",
            "[62]\tvalidation_0-auc:0.766678\n",
            "[63]\tvalidation_0-auc:0.766851\n",
            "[64]\tvalidation_0-auc:0.767158\n",
            "[65]\tvalidation_0-auc:0.76712\n",
            "[66]\tvalidation_0-auc:0.76712\n",
            "[67]\tvalidation_0-auc:0.767388\n",
            "[68]\tvalidation_0-auc:0.767561\n",
            "Stopping. Best iteration:\n",
            "[18]\tvalidation_0-auc:0.769375\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.5725 - accuracy: 0.7529 - val_loss: 0.5856 - val_accuracy: 0.7265\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5665 - accuracy: 0.7557 - val_loss: 0.5792 - val_accuracy: 0.7265\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5576 - accuracy: 0.7557 - val_loss: 0.5635 - val_accuracy: 0.7265\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5451 - accuracy: 0.7557 - val_loss: 0.5395 - val_accuracy: 0.7265\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5160 - accuracy: 0.7557 - val_loss: 0.4851 - val_accuracy: 0.7265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5686 - accuracy: 0.7563 - val_loss: 0.5648 - val_accuracy: 0.7265\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5215 - accuracy: 0.7666 - val_loss: 0.4789 - val_accuracy: 0.7243\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4971 - accuracy: 0.7824 - val_loss: 0.4842 - val_accuracy: 0.7440\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4727 - accuracy: 0.7914 - val_loss: 0.4744 - val_accuracy: 0.7549\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4750 - accuracy: 0.7920 - val_loss: 0.4441 - val_accuracy: 0.8053\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.707639\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.709964\n",
            "[2]\tvalidation_0-auc:0.710157\n",
            "[3]\tvalidation_0-auc:0.708349\n",
            "[4]\tvalidation_0-auc:0.732602\n",
            "[5]\tvalidation_0-auc:0.739193\n",
            "[6]\tvalidation_0-auc:0.733892\n",
            "[7]\tvalidation_0-auc:0.779398\n",
            "[8]\tvalidation_0-auc:0.781795\n",
            "[9]\tvalidation_0-auc:0.788422\n",
            "[10]\tvalidation_0-auc:0.791\n",
            "[11]\tvalidation_0-auc:0.806482\n",
            "[12]\tvalidation_0-auc:0.808337\n",
            "[13]\tvalidation_0-auc:0.808831\n",
            "[14]\tvalidation_0-auc:0.809904\n",
            "[15]\tvalidation_0-auc:0.815952\n",
            "[16]\tvalidation_0-auc:0.816988\n",
            "[17]\tvalidation_0-auc:0.818819\n",
            "[18]\tvalidation_0-auc:0.821687\n",
            "[19]\tvalidation_0-auc:0.827759\n",
            "[20]\tvalidation_0-auc:0.832675\n",
            "[21]\tvalidation_0-auc:0.834506\n",
            "[22]\tvalidation_0-auc:0.834325\n",
            "[23]\tvalidation_0-auc:0.836964\n",
            "[24]\tvalidation_0-auc:0.835578\n",
            "[25]\tvalidation_0-auc:0.835831\n",
            "[26]\tvalidation_0-auc:0.836771\n",
            "[27]\tvalidation_0-auc:0.838253\n",
            "[28]\tvalidation_0-auc:0.836759\n",
            "[29]\tvalidation_0-auc:0.839554\n",
            "[30]\tvalidation_0-auc:0.839265\n",
            "[31]\tvalidation_0-auc:0.838819\n",
            "[32]\tvalidation_0-auc:0.843193\n",
            "[33]\tvalidation_0-auc:0.843578\n",
            "[34]\tvalidation_0-auc:0.841627\n",
            "[35]\tvalidation_0-auc:0.843181\n",
            "[36]\tvalidation_0-auc:0.84306\n",
            "[37]\tvalidation_0-auc:0.844349\n",
            "[38]\tvalidation_0-auc:0.845639\n",
            "[39]\tvalidation_0-auc:0.846771\n",
            "[40]\tvalidation_0-auc:0.845277\n",
            "[41]\tvalidation_0-auc:0.846723\n",
            "[42]\tvalidation_0-auc:0.847518\n",
            "[43]\tvalidation_0-auc:0.84759\n",
            "[44]\tvalidation_0-auc:0.848614\n",
            "[45]\tvalidation_0-auc:0.849506\n",
            "[46]\tvalidation_0-auc:0.85141\n",
            "[47]\tvalidation_0-auc:0.852145\n",
            "[48]\tvalidation_0-auc:0.851518\n",
            "[49]\tvalidation_0-auc:0.851554\n",
            "[50]\tvalidation_0-auc:0.852988\n",
            "[51]\tvalidation_0-auc:0.852843\n",
            "[52]\tvalidation_0-auc:0.853181\n",
            "[53]\tvalidation_0-auc:0.852675\n",
            "[54]\tvalidation_0-auc:0.852048\n",
            "[55]\tvalidation_0-auc:0.852506\n",
            "[56]\tvalidation_0-auc:0.853301\n",
            "[57]\tvalidation_0-auc:0.852602\n",
            "[58]\tvalidation_0-auc:0.852699\n",
            "[59]\tvalidation_0-auc:0.85441\n",
            "[60]\tvalidation_0-auc:0.854518\n",
            "[61]\tvalidation_0-auc:0.85459\n",
            "[62]\tvalidation_0-auc:0.854566\n",
            "[63]\tvalidation_0-auc:0.854265\n",
            "[64]\tvalidation_0-auc:0.854337\n",
            "[65]\tvalidation_0-auc:0.854253\n",
            "[66]\tvalidation_0-auc:0.854566\n",
            "[67]\tvalidation_0-auc:0.85659\n",
            "[68]\tvalidation_0-auc:0.857\n",
            "[69]\tvalidation_0-auc:0.856855\n",
            "[70]\tvalidation_0-auc:0.857687\n",
            "[71]\tvalidation_0-auc:0.857446\n",
            "[72]\tvalidation_0-auc:0.856241\n",
            "[73]\tvalidation_0-auc:0.857036\n",
            "[74]\tvalidation_0-auc:0.856602\n",
            "[75]\tvalidation_0-auc:0.857301\n",
            "[76]\tvalidation_0-auc:0.856373\n",
            "[77]\tvalidation_0-auc:0.85647\n",
            "[78]\tvalidation_0-auc:0.855482\n",
            "[79]\tvalidation_0-auc:0.855337\n",
            "[80]\tvalidation_0-auc:0.85647\n",
            "[81]\tvalidation_0-auc:0.85612\n",
            "[82]\tvalidation_0-auc:0.855518\n",
            "[83]\tvalidation_0-auc:0.855892\n",
            "[84]\tvalidation_0-auc:0.855771\n",
            "[85]\tvalidation_0-auc:0.856036\n",
            "[86]\tvalidation_0-auc:0.856928\n",
            "[87]\tvalidation_0-auc:0.857675\n",
            "[88]\tvalidation_0-auc:0.85706\n",
            "[89]\tvalidation_0-auc:0.857301\n",
            "[90]\tvalidation_0-auc:0.858\n",
            "[91]\tvalidation_0-auc:0.857349\n",
            "[92]\tvalidation_0-auc:0.858096\n",
            "[93]\tvalidation_0-auc:0.858506\n",
            "[94]\tvalidation_0-auc:0.858819\n",
            "[95]\tvalidation_0-auc:0.859036\n",
            "[96]\tvalidation_0-auc:0.858626\n",
            "[97]\tvalidation_0-auc:0.858361\n",
            "[98]\tvalidation_0-auc:0.858313\n",
            "[99]\tvalidation_0-auc:0.858072\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.6816326530612244 |        0.0         |         0.0         |         0.0         |\n",
            "|     GRU 0.1      | 0.726530612244898  | 0.6964285714285714 |         0.25        | 0.36792452830188677 |\n",
            "|   XGBoost 0.1    | 0.7510204081632653 | 0.6666666666666666 |  0.4358974358974359 |  0.5271317829457364 |\n",
            "|    Logreg 0.1    | 0.7142857142857143 |        0.75        | 0.15384615384615385 | 0.25531914893617025 |\n",
            "|     SVM 0.1      | 0.7183673469387755 |       0.6875       | 0.21153846153846154 |  0.3235294117647059 |\n",
            "|  LSTM beta 0.1   | 0.7264770240700219 |        0.0         |         0.0         |         0.0         |\n",
            "|   GRU beta 0.1   | 0.8052516411378556 | 0.7647058823529411 |        0.416        |  0.538860103626943  |\n",
            "| XGBoost beta 0.1 | 0.8008752735229759 |       0.7125       |        0.456        |  0.5560975609756097 |\n",
            "| logreg beta 0.1  | 0.7636761487964989 | 0.7297297297297297 |        0.216        |  0.3333333333333333 |\n",
            "|   svm beta 0.1   | 0.8030634573304157 | 0.7215189873417721 |        0.456        |  0.5588235294117647 |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 20ms/step - loss: 0.4311 - accuracy: 0.8523 - val_loss: 0.4095 - val_accuracy: 0.8592\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4219 - accuracy: 0.8604 - val_loss: 0.4060 - val_accuracy: 0.8592\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4168 - accuracy: 0.8604 - val_loss: 0.4082 - val_accuracy: 0.8592\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4163 - accuracy: 0.8604 - val_loss: 0.4028 - val_accuracy: 0.8592\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.3987 - accuracy: 0.8604 - val_loss: 0.3922 - val_accuracy: 0.8592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.4289 - accuracy: 0.8597 - val_loss: 0.4017 - val_accuracy: 0.8592\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.3984 - accuracy: 0.8604 - val_loss: 0.4407 - val_accuracy: 0.8510\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.3804 - accuracy: 0.8638 - val_loss: 0.3792 - val_accuracy: 0.8531\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.3612 - accuracy: 0.8698 - val_loss: 0.3754 - val_accuracy: 0.8612\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.3616 - accuracy: 0.8705 - val_loss: 0.3832 - val_accuracy: 0.8510\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.634256\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.636321\n",
            "[2]\tvalidation_0-auc:0.636614\n",
            "[3]\tvalidation_0-auc:0.63892\n",
            "[4]\tvalidation_0-auc:0.641261\n",
            "[5]\tvalidation_0-auc:0.653792\n",
            "[6]\tvalidation_0-auc:0.649781\n",
            "[7]\tvalidation_0-auc:0.663293\n",
            "[8]\tvalidation_0-auc:0.662777\n",
            "[9]\tvalidation_0-auc:0.661228\n",
            "[10]\tvalidation_0-auc:0.659472\n",
            "[11]\tvalidation_0-auc:0.661365\n",
            "[12]\tvalidation_0-auc:0.675015\n",
            "[13]\tvalidation_0-auc:0.674756\n",
            "[14]\tvalidation_0-auc:0.671228\n",
            "[15]\tvalidation_0-auc:0.673001\n",
            "[16]\tvalidation_0-auc:0.673706\n",
            "[17]\tvalidation_0-auc:0.67023\n",
            "[18]\tvalidation_0-auc:0.671934\n",
            "[19]\tvalidation_0-auc:0.673018\n",
            "[20]\tvalidation_0-auc:0.67078\n",
            "[21]\tvalidation_0-auc:0.67276\n",
            "[22]\tvalidation_0-auc:0.672002\n",
            "[23]\tvalidation_0-auc:0.672002\n",
            "[24]\tvalidation_0-auc:0.670144\n",
            "[25]\tvalidation_0-auc:0.671658\n",
            "[26]\tvalidation_0-auc:0.676133\n",
            "[27]\tvalidation_0-auc:0.677235\n",
            "[28]\tvalidation_0-auc:0.675376\n",
            "[29]\tvalidation_0-auc:0.675961\n",
            "[30]\tvalidation_0-auc:0.675428\n",
            "[31]\tvalidation_0-auc:0.676288\n",
            "[32]\tvalidation_0-auc:0.679077\n",
            "[33]\tvalidation_0-auc:0.679111\n",
            "[34]\tvalidation_0-auc:0.678595\n",
            "[35]\tvalidation_0-auc:0.6777\n",
            "[36]\tvalidation_0-auc:0.680832\n",
            "[37]\tvalidation_0-auc:0.680798\n",
            "[38]\tvalidation_0-auc:0.678819\n",
            "[39]\tvalidation_0-auc:0.681056\n",
            "[40]\tvalidation_0-auc:0.679404\n",
            "[41]\tvalidation_0-auc:0.677407\n",
            "[42]\tvalidation_0-auc:0.67751\n",
            "[43]\tvalidation_0-auc:0.677476\n",
            "[44]\tvalidation_0-auc:0.676633\n",
            "[45]\tvalidation_0-auc:0.675806\n",
            "[46]\tvalidation_0-auc:0.675738\n",
            "[47]\tvalidation_0-auc:0.677424\n",
            "[48]\tvalidation_0-auc:0.67572\n",
            "[49]\tvalidation_0-auc:0.675135\n",
            "[50]\tvalidation_0-auc:0.675342\n",
            "[51]\tvalidation_0-auc:0.672175\n",
            "[52]\tvalidation_0-auc:0.673242\n",
            "[53]\tvalidation_0-auc:0.670419\n",
            "[54]\tvalidation_0-auc:0.670247\n",
            "[55]\tvalidation_0-auc:0.669937\n",
            "[56]\tvalidation_0-auc:0.669696\n",
            "[57]\tvalidation_0-auc:0.669214\n",
            "[58]\tvalidation_0-auc:0.669352\n",
            "[59]\tvalidation_0-auc:0.667872\n",
            "[60]\tvalidation_0-auc:0.666804\n",
            "[61]\tvalidation_0-auc:0.662914\n",
            "[62]\tvalidation_0-auc:0.661124\n",
            "[63]\tvalidation_0-auc:0.660333\n",
            "[64]\tvalidation_0-auc:0.662157\n",
            "[65]\tvalidation_0-auc:0.659386\n",
            "[66]\tvalidation_0-auc:0.659231\n",
            "[67]\tvalidation_0-auc:0.658542\n",
            "[68]\tvalidation_0-auc:0.657579\n",
            "[69]\tvalidation_0-auc:0.657475\n",
            "[70]\tvalidation_0-auc:0.65627\n",
            "[71]\tvalidation_0-auc:0.658267\n",
            "[72]\tvalidation_0-auc:0.655857\n",
            "[73]\tvalidation_0-auc:0.653482\n",
            "[74]\tvalidation_0-auc:0.653792\n",
            "[75]\tvalidation_0-auc:0.653826\n",
            "[76]\tvalidation_0-auc:0.652139\n",
            "[77]\tvalidation_0-auc:0.651244\n",
            "[78]\tvalidation_0-auc:0.650969\n",
            "[79]\tvalidation_0-auc:0.650797\n",
            "[80]\tvalidation_0-auc:0.650005\n",
            "[81]\tvalidation_0-auc:0.649592\n",
            "[82]\tvalidation_0-auc:0.651348\n",
            "[83]\tvalidation_0-auc:0.651313\n",
            "[84]\tvalidation_0-auc:0.648146\n",
            "[85]\tvalidation_0-auc:0.647217\n",
            "[86]\tvalidation_0-auc:0.64942\n",
            "[87]\tvalidation_0-auc:0.648663\n",
            "[88]\tvalidation_0-auc:0.649386\n",
            "[89]\tvalidation_0-auc:0.647458\n",
            "Stopping. Best iteration:\n",
            "[39]\tvalidation_0-auc:0.681056\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.4289 - accuracy: 0.8524 - val_loss: 0.4374 - val_accuracy: 0.8490\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4241 - accuracy: 0.8572 - val_loss: 0.4093 - val_accuracy: 0.8490\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.3903 - accuracy: 0.8545 - val_loss: 0.3260 - val_accuracy: 0.8490\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.3732 - accuracy: 0.8566 - val_loss: 0.3483 - val_accuracy: 0.8490\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.3607 - accuracy: 0.8566 - val_loss: 0.4169 - val_accuracy: 0.8490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.4339 - accuracy: 0.8511 - val_loss: 0.4155 - val_accuracy: 0.8490\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.3714 - accuracy: 0.8572 - val_loss: 0.3400 - val_accuracy: 0.8359\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.3373 - accuracy: 0.8627 - val_loss: 0.3306 - val_accuracy: 0.8381\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.3148 - accuracy: 0.8744 - val_loss: 0.3051 - val_accuracy: 0.8425\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.3314 - accuracy: 0.8737 - val_loss: 0.3086 - val_accuracy: 0.8315\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.748805\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.797792\n",
            "[2]\tvalidation_0-auc:0.801939\n",
            "[3]\tvalidation_0-auc:0.80009\n",
            "[4]\tvalidation_0-auc:0.799772\n",
            "[5]\tvalidation_0-auc:0.801154\n",
            "[6]\tvalidation_0-auc:0.868108\n",
            "[7]\tvalidation_0-auc:0.860993\n",
            "[8]\tvalidation_0-auc:0.866334\n",
            "[9]\tvalidation_0-auc:0.863178\n",
            "[10]\tvalidation_0-auc:0.86456\n",
            "[11]\tvalidation_0-auc:0.889605\n",
            "[12]\tvalidation_0-auc:0.892182\n",
            "[13]\tvalidation_0-auc:0.890837\n",
            "[14]\tvalidation_0-auc:0.889792\n",
            "[15]\tvalidation_0-auc:0.887999\n",
            "[16]\tvalidation_0-auc:0.888746\n",
            "[17]\tvalidation_0-auc:0.8877\n",
            "[18]\tvalidation_0-auc:0.886579\n",
            "[19]\tvalidation_0-auc:0.883311\n",
            "[20]\tvalidation_0-auc:0.880734\n",
            "[21]\tvalidation_0-auc:0.878343\n",
            "[22]\tvalidation_0-auc:0.877372\n",
            "[23]\tvalidation_0-auc:0.876924\n",
            "[24]\tvalidation_0-auc:0.87726\n",
            "[25]\tvalidation_0-auc:0.876793\n",
            "[26]\tvalidation_0-auc:0.879688\n",
            "[27]\tvalidation_0-auc:0.879874\n",
            "[28]\tvalidation_0-auc:0.882919\n",
            "[29]\tvalidation_0-auc:0.883516\n",
            "[30]\tvalidation_0-auc:0.884114\n",
            "[31]\tvalidation_0-auc:0.884263\n",
            "[32]\tvalidation_0-auc:0.884805\n",
            "[33]\tvalidation_0-auc:0.885403\n",
            "[34]\tvalidation_0-auc:0.887046\n",
            "[35]\tvalidation_0-auc:0.888241\n",
            "[36]\tvalidation_0-auc:0.888092\n",
            "[37]\tvalidation_0-auc:0.891827\n",
            "[38]\tvalidation_0-auc:0.88996\n",
            "[39]\tvalidation_0-auc:0.890931\n",
            "[40]\tvalidation_0-auc:0.891454\n",
            "[41]\tvalidation_0-auc:0.891958\n",
            "[42]\tvalidation_0-auc:0.891846\n",
            "[43]\tvalidation_0-auc:0.890389\n",
            "[44]\tvalidation_0-auc:0.889978\n",
            "[45]\tvalidation_0-auc:0.891192\n",
            "[46]\tvalidation_0-auc:0.891827\n",
            "[47]\tvalidation_0-auc:0.89179\n",
            "[48]\tvalidation_0-auc:0.891566\n",
            "[49]\tvalidation_0-auc:0.892014\n",
            "[50]\tvalidation_0-auc:0.89235\n",
            "[51]\tvalidation_0-auc:0.89291\n",
            "[52]\tvalidation_0-auc:0.894442\n",
            "[53]\tvalidation_0-auc:0.89433\n",
            "[54]\tvalidation_0-auc:0.892985\n",
            "[55]\tvalidation_0-auc:0.893583\n",
            "[56]\tvalidation_0-auc:0.893583\n",
            "[57]\tvalidation_0-auc:0.893433\n",
            "[58]\tvalidation_0-auc:0.893546\n",
            "[59]\tvalidation_0-auc:0.893807\n",
            "[60]\tvalidation_0-auc:0.893546\n",
            "[61]\tvalidation_0-auc:0.895563\n",
            "[62]\tvalidation_0-auc:0.895973\n",
            "[63]\tvalidation_0-auc:0.896011\n",
            "[64]\tvalidation_0-auc:0.895749\n",
            "[65]\tvalidation_0-auc:0.896384\n",
            "[66]\tvalidation_0-auc:0.896758\n",
            "[67]\tvalidation_0-auc:0.896758\n",
            "[68]\tvalidation_0-auc:0.897057\n",
            "[69]\tvalidation_0-auc:0.896945\n",
            "[70]\tvalidation_0-auc:0.897281\n",
            "[71]\tvalidation_0-auc:0.897169\n",
            "[72]\tvalidation_0-auc:0.897804\n",
            "[73]\tvalidation_0-auc:0.898962\n",
            "[74]\tvalidation_0-auc:0.898401\n",
            "[75]\tvalidation_0-auc:0.898364\n",
            "[76]\tvalidation_0-auc:0.900232\n",
            "[77]\tvalidation_0-auc:0.90053\n",
            "[78]\tvalidation_0-auc:0.900829\n",
            "[79]\tvalidation_0-auc:0.900605\n",
            "[80]\tvalidation_0-auc:0.900456\n",
            "[81]\tvalidation_0-auc:0.900642\n",
            "[82]\tvalidation_0-auc:0.899671\n",
            "[83]\tvalidation_0-auc:0.899447\n",
            "[84]\tvalidation_0-auc:0.899335\n",
            "[85]\tvalidation_0-auc:0.898812\n",
            "[86]\tvalidation_0-auc:0.898028\n",
            "[87]\tvalidation_0-auc:0.897841\n",
            "[88]\tvalidation_0-auc:0.898177\n",
            "[89]\tvalidation_0-auc:0.897654\n",
            "[90]\tvalidation_0-auc:0.89758\n",
            "[91]\tvalidation_0-auc:0.897094\n",
            "[92]\tvalidation_0-auc:0.897467\n",
            "[93]\tvalidation_0-auc:0.897206\n",
            "[94]\tvalidation_0-auc:0.896795\n",
            "[95]\tvalidation_0-auc:0.89758\n",
            "[96]\tvalidation_0-auc:0.897094\n",
            "[97]\tvalidation_0-auc:0.896907\n",
            "[98]\tvalidation_0-auc:0.895861\n",
            "[99]\tvalidation_0-auc:0.895338\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+----------------------+----------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall        |       F1 score       |\n",
            "+------------------+--------------------+---------------------+----------------------+----------------------+\n",
            "|     LSTM 0.2     | 0.8591836734693877 |         0.0         |         0.0          |         0.0          |\n",
            "|     GRU 0.2      | 0.8510204081632653 |         0.3         | 0.043478260869565216 | 0.07594936708860758  |\n",
            "|   XGBoost 0.2    | 0.8551020408163266 |  0.3333333333333333 | 0.028985507246376812 | 0.05333333333333333  |\n",
            "|    Logreg 0.2    | 0.8571428571428571 |  0.3333333333333333 | 0.014492753623188406 | 0.027777777777777776 |\n",
            "|     SVM 0.2      | 0.8448979591836735 |  0.1111111111111111 | 0.014492753623188406 | 0.02564102564102564  |\n",
            "|  LSTM beta 0.2   | 0.849015317286652  |         0.0         |         0.0          |         0.0          |\n",
            "|   GRU beta 0.2   | 0.8315098468271335 | 0.36666666666666664 | 0.15942028985507245  |  0.2222222222222222  |\n",
            "| XGBoost beta 0.2 | 0.849015317286652  |         0.5         | 0.43478260869565216  | 0.46511627906976744  |\n",
            "| logreg beta 0.2  | 0.838074398249453  |  0.2727272727272727 | 0.043478260869565216 | 0.07500000000000001  |\n",
            "|   svm beta 0.2   | 0.849015317286652  |         0.5         |  0.4057971014492754  |        0.448         |\n",
            "+------------------+--------------------+---------------------+----------------------+----------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.5344 - accuracy: 0.7893 - val_loss: 0.6093 - val_accuracy: 0.7163\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5174 - accuracy: 0.7946 - val_loss: 0.6000 - val_accuracy: 0.7163\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5125 - accuracy: 0.7946 - val_loss: 0.6029 - val_accuracy: 0.7163\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5148 - accuracy: 0.7940 - val_loss: 0.6362 - val_accuracy: 0.7163\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5145 - accuracy: 0.7946 - val_loss: 0.6239 - val_accuracy: 0.7163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.5324 - accuracy: 0.7933 - val_loss: 0.6339 - val_accuracy: 0.7163\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4975 - accuracy: 0.7966 - val_loss: 0.5464 - val_accuracy: 0.7122\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4424 - accuracy: 0.8107 - val_loss: 0.5455 - val_accuracy: 0.7306\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4388 - accuracy: 0.8121 - val_loss: 0.5436 - val_accuracy: 0.7347\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4206 - accuracy: 0.8208 - val_loss: 0.5784 - val_accuracy: 0.7143\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.736252\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.74318\n",
            "[2]\tvalidation_0-auc:0.742298\n",
            "[3]\tvalidation_0-auc:0.742596\n",
            "[4]\tvalidation_0-auc:0.745598\n",
            "[5]\tvalidation_0-auc:0.746941\n",
            "[6]\tvalidation_0-auc:0.749226\n",
            "[7]\tvalidation_0-auc:0.747341\n",
            "[8]\tvalidation_0-auc:0.746213\n",
            "[9]\tvalidation_0-auc:0.748509\n",
            "[10]\tvalidation_0-auc:0.751337\n",
            "[11]\tvalidation_0-auc:0.750968\n",
            "[12]\tvalidation_0-auc:0.751542\n",
            "[13]\tvalidation_0-auc:0.754402\n",
            "[14]\tvalidation_0-auc:0.754975\n",
            "[15]\tvalidation_0-auc:0.756339\n",
            "[16]\tvalidation_0-auc:0.755549\n",
            "[17]\tvalidation_0-auc:0.75559\n",
            "[18]\tvalidation_0-auc:0.753397\n",
            "[19]\tvalidation_0-auc:0.753018\n",
            "[20]\tvalidation_0-auc:0.754258\n",
            "[21]\tvalidation_0-auc:0.755058\n",
            "[22]\tvalidation_0-auc:0.753951\n",
            "[23]\tvalidation_0-auc:0.753367\n",
            "[24]\tvalidation_0-auc:0.753971\n",
            "[25]\tvalidation_0-auc:0.752608\n",
            "[26]\tvalidation_0-auc:0.75517\n",
            "[27]\tvalidation_0-auc:0.755447\n",
            "[28]\tvalidation_0-auc:0.755836\n",
            "[29]\tvalidation_0-auc:0.755857\n",
            "[30]\tvalidation_0-auc:0.75515\n",
            "[31]\tvalidation_0-auc:0.755068\n",
            "[32]\tvalidation_0-auc:0.755139\n",
            "[33]\tvalidation_0-auc:0.756246\n",
            "[34]\tvalidation_0-auc:0.756554\n",
            "[35]\tvalidation_0-auc:0.755703\n",
            "[36]\tvalidation_0-auc:0.753233\n",
            "[37]\tvalidation_0-auc:0.753889\n",
            "[38]\tvalidation_0-auc:0.752998\n",
            "[39]\tvalidation_0-auc:0.753377\n",
            "[40]\tvalidation_0-auc:0.752844\n",
            "[41]\tvalidation_0-auc:0.752803\n",
            "[42]\tvalidation_0-auc:0.753254\n",
            "[43]\tvalidation_0-auc:0.754443\n",
            "[44]\tvalidation_0-auc:0.752557\n",
            "[45]\tvalidation_0-auc:0.753172\n",
            "[46]\tvalidation_0-auc:0.751706\n",
            "[47]\tvalidation_0-auc:0.752157\n",
            "[48]\tvalidation_0-auc:0.752465\n",
            "[49]\tvalidation_0-auc:0.751809\n",
            "[50]\tvalidation_0-auc:0.751829\n",
            "[51]\tvalidation_0-auc:0.751255\n",
            "[52]\tvalidation_0-auc:0.751522\n",
            "[53]\tvalidation_0-auc:0.751665\n",
            "[54]\tvalidation_0-auc:0.750702\n",
            "[55]\tvalidation_0-auc:0.750784\n",
            "[56]\tvalidation_0-auc:0.751153\n",
            "[57]\tvalidation_0-auc:0.750784\n",
            "[58]\tvalidation_0-auc:0.750763\n",
            "[59]\tvalidation_0-auc:0.750518\n",
            "[60]\tvalidation_0-auc:0.750845\n",
            "[61]\tvalidation_0-auc:0.749985\n",
            "[62]\tvalidation_0-auc:0.749821\n",
            "[63]\tvalidation_0-auc:0.750272\n",
            "[64]\tvalidation_0-auc:0.7506\n",
            "[65]\tvalidation_0-auc:0.750845\n",
            "[66]\tvalidation_0-auc:0.750825\n",
            "[67]\tvalidation_0-auc:0.751214\n",
            "[68]\tvalidation_0-auc:0.749165\n",
            "[69]\tvalidation_0-auc:0.748755\n",
            "[70]\tvalidation_0-auc:0.748693\n",
            "[71]\tvalidation_0-auc:0.749185\n",
            "[72]\tvalidation_0-auc:0.749411\n",
            "[73]\tvalidation_0-auc:0.748632\n",
            "[74]\tvalidation_0-auc:0.747956\n",
            "[75]\tvalidation_0-auc:0.747259\n",
            "[76]\tvalidation_0-auc:0.746849\n",
            "[77]\tvalidation_0-auc:0.746562\n",
            "[78]\tvalidation_0-auc:0.747341\n",
            "[79]\tvalidation_0-auc:0.747771\n",
            "[80]\tvalidation_0-auc:0.748058\n",
            "[81]\tvalidation_0-auc:0.748529\n",
            "[82]\tvalidation_0-auc:0.749124\n",
            "[83]\tvalidation_0-auc:0.749452\n",
            "[84]\tvalidation_0-auc:0.749185\n",
            "Stopping. Best iteration:\n",
            "[34]\tvalidation_0-auc:0.756554\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.5410 - accuracy: 0.7838 - val_loss: 0.5489 - val_accuracy: 0.7637\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5266 - accuracy: 0.7900 - val_loss: 0.5397 - val_accuracy: 0.7637\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4971 - accuracy: 0.7900 - val_loss: 0.4925 - val_accuracy: 0.7637\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.4587 - accuracy: 0.7982 - val_loss: 0.4288 - val_accuracy: 0.7724\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4439 - accuracy: 0.8037 - val_loss: 0.4179 - val_accuracy: 0.7681\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5285 - accuracy: 0.7852 - val_loss: 0.5166 - val_accuracy: 0.7637\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4644 - accuracy: 0.7941 - val_loss: 0.4461 - val_accuracy: 0.8009\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4271 - accuracy: 0.8264 - val_loss: 0.4054 - val_accuracy: 0.8031\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4074 - accuracy: 0.8277 - val_loss: 0.3913 - val_accuracy: 0.8162\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4038 - accuracy: 0.8387 - val_loss: 0.3839 - val_accuracy: 0.8271\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.849146\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.852767\n",
            "[2]\tvalidation_0-auc:0.847925\n",
            "[3]\tvalidation_0-auc:0.849464\n",
            "[4]\tvalidation_0-auc:0.849265\n",
            "[5]\tvalidation_0-auc:0.850525\n",
            "[6]\tvalidation_0-auc:0.842486\n",
            "[7]\tvalidation_0-auc:0.840974\n",
            "[8]\tvalidation_0-auc:0.859254\n",
            "[9]\tvalidation_0-auc:0.856972\n",
            "[10]\tvalidation_0-auc:0.856176\n",
            "[11]\tvalidation_0-auc:0.859785\n",
            "[12]\tvalidation_0-auc:0.860461\n",
            "[13]\tvalidation_0-auc:0.862292\n",
            "[14]\tvalidation_0-auc:0.862464\n",
            "[15]\tvalidation_0-auc:0.864348\n",
            "[16]\tvalidation_0-auc:0.861894\n",
            "[17]\tvalidation_0-auc:0.862557\n",
            "[18]\tvalidation_0-auc:0.863831\n",
            "[19]\tvalidation_0-auc:0.86655\n",
            "[20]\tvalidation_0-auc:0.863884\n",
            "[21]\tvalidation_0-auc:0.862849\n",
            "[22]\tvalidation_0-auc:0.862637\n",
            "[23]\tvalidation_0-auc:0.863247\n",
            "[24]\tvalidation_0-auc:0.861124\n",
            "[25]\tvalidation_0-auc:0.862544\n",
            "[26]\tvalidation_0-auc:0.861934\n",
            "[27]\tvalidation_0-auc:0.860289\n",
            "[28]\tvalidation_0-auc:0.863472\n",
            "[29]\tvalidation_0-auc:0.862252\n",
            "[30]\tvalidation_0-auc:0.862743\n",
            "[31]\tvalidation_0-auc:0.863459\n",
            "[32]\tvalidation_0-auc:0.861867\n",
            "[33]\tvalidation_0-auc:0.86123\n",
            "[34]\tvalidation_0-auc:0.86269\n",
            "[35]\tvalidation_0-auc:0.864733\n",
            "[36]\tvalidation_0-auc:0.863592\n",
            "[37]\tvalidation_0-auc:0.863618\n",
            "[38]\tvalidation_0-auc:0.864441\n",
            "[39]\tvalidation_0-auc:0.864242\n",
            "[40]\tvalidation_0-auc:0.865383\n",
            "[41]\tvalidation_0-auc:0.866179\n",
            "[42]\tvalidation_0-auc:0.866523\n",
            "[43]\tvalidation_0-auc:0.866179\n",
            "[44]\tvalidation_0-auc:0.86716\n",
            "[45]\tvalidation_0-auc:0.869044\n",
            "[46]\tvalidation_0-auc:0.868513\n",
            "[47]\tvalidation_0-auc:0.868062\n",
            "[48]\tvalidation_0-auc:0.869362\n",
            "[49]\tvalidation_0-auc:0.868261\n",
            "[50]\tvalidation_0-auc:0.869203\n",
            "[51]\tvalidation_0-auc:0.86923\n",
            "[52]\tvalidation_0-auc:0.869681\n",
            "[53]\tvalidation_0-auc:0.87114\n",
            "[54]\tvalidation_0-auc:0.870795\n",
            "[55]\tvalidation_0-auc:0.87045\n",
            "[56]\tvalidation_0-auc:0.871299\n",
            "[57]\tvalidation_0-auc:0.872002\n",
            "[58]\tvalidation_0-auc:0.871949\n",
            "[59]\tvalidation_0-auc:0.873222\n",
            "[60]\tvalidation_0-auc:0.873939\n",
            "[61]\tvalidation_0-auc:0.873886\n",
            "[62]\tvalidation_0-auc:0.873435\n",
            "[63]\tvalidation_0-auc:0.873262\n",
            "[64]\tvalidation_0-auc:0.874615\n",
            "[65]\tvalidation_0-auc:0.874562\n",
            "[66]\tvalidation_0-auc:0.873819\n",
            "[67]\tvalidation_0-auc:0.874244\n",
            "[68]\tvalidation_0-auc:0.875013\n",
            "[69]\tvalidation_0-auc:0.875491\n",
            "[70]\tvalidation_0-auc:0.875756\n",
            "[71]\tvalidation_0-auc:0.87573\n",
            "[72]\tvalidation_0-auc:0.875597\n",
            "[73]\tvalidation_0-auc:0.87687\n",
            "[74]\tvalidation_0-auc:0.876897\n",
            "[75]\tvalidation_0-auc:0.876579\n",
            "[76]\tvalidation_0-auc:0.876074\n",
            "[77]\tvalidation_0-auc:0.875902\n",
            "[78]\tvalidation_0-auc:0.876565\n",
            "[79]\tvalidation_0-auc:0.876194\n",
            "[80]\tvalidation_0-auc:0.875849\n",
            "[81]\tvalidation_0-auc:0.875637\n",
            "[82]\tvalidation_0-auc:0.876327\n",
            "[83]\tvalidation_0-auc:0.87699\n",
            "[84]\tvalidation_0-auc:0.876884\n",
            "[85]\tvalidation_0-auc:0.877069\n",
            "[86]\tvalidation_0-auc:0.877149\n",
            "[87]\tvalidation_0-auc:0.876486\n",
            "[88]\tvalidation_0-auc:0.876963\n",
            "[89]\tvalidation_0-auc:0.877494\n",
            "[90]\tvalidation_0-auc:0.877361\n",
            "[91]\tvalidation_0-auc:0.877547\n",
            "[92]\tvalidation_0-auc:0.87821\n",
            "[93]\tvalidation_0-auc:0.8776\n",
            "[94]\tvalidation_0-auc:0.877786\n",
            "[95]\tvalidation_0-auc:0.877998\n",
            "[96]\tvalidation_0-auc:0.878184\n",
            "[97]\tvalidation_0-auc:0.878025\n",
            "[98]\tvalidation_0-auc:0.878767\n",
            "[99]\tvalidation_0-auc:0.878608\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.7163265306122449 |         0.0         |         0.0         |         0.0         |\n",
            "|      GRU 0.15     | 0.7142857142857143 | 0.45454545454545453 | 0.03597122302158273 | 0.06666666666666667 |\n",
            "|    XGBoost 0.15   | 0.7591836734693878 |  0.6842105263157895 |  0.2805755395683453 |  0.3979591836734694 |\n",
            "|    Logreg 0.15    | 0.7285714285714285 |  0.6363636363636364 | 0.10071942446043165 | 0.17391304347826086 |\n",
            "|      SVM 0.15     | 0.7183673469387755 |  0.5238095238095238 | 0.07913669064748201 |        0.1375       |\n",
            "|   LSTM beta 0.15  | 0.7680525164113785 |  0.5384615384615384 | 0.12962962962962962 |  0.208955223880597  |\n",
            "|   GRU beta 0.15   | 0.8271334792122538 |  0.7543859649122807 | 0.39814814814814814 |  0.5212121212121213 |\n",
            "| XGBoost beta 0.15 | 0.824945295404814  |  0.6842105263157895 | 0.48148148148148145 |  0.5652173913043478 |\n",
            "|  logreg beta 0.15 | 0.7768052516411379 |  0.5882352941176471 | 0.18518518518518517 |  0.2816901408450704 |\n",
            "|   svm beta 0.15   | 0.8205689277899344 |  0.6805555555555556 |  0.4537037037037037 |  0.5444444444444445 |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eErc1XoJYiNw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0ab36270-701a-4755-e23e-28d0719a3cad"
      },
      "source": [
        "Result_cross.to_csv('RMD_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.681633</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.726531</td>\n",
              "      <td>0.367925</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.751020</td>\n",
              "      <td>0.527132</td>\n",
              "      <td>0.435897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.255319</td>\n",
              "      <td>0.153846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.718367</td>\n",
              "      <td>0.323529</td>\n",
              "      <td>0.211538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.726477</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.764706</td>\n",
              "      <td>0.805252</td>\n",
              "      <td>0.538860</td>\n",
              "      <td>0.416000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.712500</td>\n",
              "      <td>0.800875</td>\n",
              "      <td>0.556098</td>\n",
              "      <td>0.456000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.729730</td>\n",
              "      <td>0.763676</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.216000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.721519</td>\n",
              "      <td>0.803063</td>\n",
              "      <td>0.558824</td>\n",
              "      <td>0.456000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.859184</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.851020</td>\n",
              "      <td>0.075949</td>\n",
              "      <td>0.043478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.855102</td>\n",
              "      <td>0.053333</td>\n",
              "      <td>0.028986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.027778</td>\n",
              "      <td>0.014493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.844898</td>\n",
              "      <td>0.025641</td>\n",
              "      <td>0.014493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.849015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.366667</td>\n",
              "      <td>0.831510</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.159420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.849015</td>\n",
              "      <td>0.465116</td>\n",
              "      <td>0.434783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.838074</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.043478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.849015</td>\n",
              "      <td>0.448000</td>\n",
              "      <td>0.405797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.716327</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.035971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.759184</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>0.280576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.728571</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>0.100719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.523810</td>\n",
              "      <td>0.718367</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.079137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.768053</td>\n",
              "      <td>0.208955</td>\n",
              "      <td>0.129630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.754386</td>\n",
              "      <td>0.827133</td>\n",
              "      <td>0.521212</td>\n",
              "      <td>0.398148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.824945</td>\n",
              "      <td>0.565217</td>\n",
              "      <td>0.481481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>0.776805</td>\n",
              "      <td>0.281690</td>\n",
              "      <td>0.185185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.680556</td>\n",
              "      <td>0.820569</td>\n",
              "      <td>0.544444</td>\n",
              "      <td>0.453704</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  RMD  0.000000  0.681633  0.000000  0.000000\n",
              "1            GRU 0.1  RMD  0.696429  0.726531  0.367925  0.250000\n",
              "2        XGBoost 0.1  RMD  0.666667  0.751020  0.527132  0.435897\n",
              "3         Logreg 0.1  RMD  0.750000  0.714286  0.255319  0.153846\n",
              "4            SVM 0.1  RMD  0.687500  0.718367  0.323529  0.211538\n",
              "5      LSTM beta 0.1  RMD  0.000000  0.726477  0.000000  0.000000\n",
              "6       GRU beta 0.1  RMD  0.764706  0.805252  0.538860  0.416000\n",
              "7   XGBoost beta 0.1  RMD  0.712500  0.800875  0.556098  0.456000\n",
              "8    logreg beta 0.1  RMD  0.729730  0.763676  0.333333  0.216000\n",
              "9       svm beta 0.1  RMD  0.721519  0.803063  0.558824  0.456000\n",
              "0           LSTM 0.2  RMD  0.000000  0.859184  0.000000  0.000000\n",
              "1            GRU 0.2  RMD  0.300000  0.851020  0.075949  0.043478\n",
              "2        XGBoost 0.2  RMD  0.333333  0.855102  0.053333  0.028986\n",
              "3         Logreg 0.2  RMD  0.333333  0.857143  0.027778  0.014493\n",
              "4            SVM 0.2  RMD  0.111111  0.844898  0.025641  0.014493\n",
              "5      LSTM beta 0.2  RMD  0.000000  0.849015  0.000000  0.000000\n",
              "6       GRU beta 0.2  RMD  0.366667  0.831510  0.222222  0.159420\n",
              "7   XGBoost beta 0.2  RMD  0.500000  0.849015  0.465116  0.434783\n",
              "8    logreg beta 0.2  RMD  0.272727  0.838074  0.075000  0.043478\n",
              "9       svm beta 0.2  RMD  0.500000  0.849015  0.448000  0.405797\n",
              "0          LSTM 0.15  RMD  0.000000  0.716327  0.000000  0.000000\n",
              "1           GRU 0.15  RMD  0.454545  0.714286  0.066667  0.035971\n",
              "2       XGBoost 0.15  RMD  0.684211  0.759184  0.397959  0.280576\n",
              "3        Logreg 0.15  RMD  0.636364  0.728571  0.173913  0.100719\n",
              "4           SVM 0.15  RMD  0.523810  0.718367  0.137500  0.079137\n",
              "5     LSTM beta 0.15  RMD  0.538462  0.768053  0.208955  0.129630\n",
              "6      GRU beta 0.15  RMD  0.754386  0.827133  0.521212  0.398148\n",
              "7  XGBoost beta 0.15  RMD  0.684211  0.824945  0.565217  0.481481\n",
              "8   logreg beta 0.15  RMD  0.588235  0.776805  0.281690  0.185185\n",
              "9      svm beta 0.15  RMD  0.680556  0.820569  0.544444  0.453704"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2kQniZqYiNw"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_logreg_beta.csv')"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QRVzX6pYiNw"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUqD3VarYiNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "873ceea6-58f9-4857-a41d-a9ef863544bd"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"RMD\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.5781 - accuracy: 0.7577 - val_loss: 0.6341 - val_accuracy: 0.6816\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5586 - accuracy: 0.7604 - val_loss: 0.6259 - val_accuracy: 0.6816\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5530 - accuracy: 0.7604 - val_loss: 0.6380 - val_accuracy: 0.6816\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5547 - accuracy: 0.7611 - val_loss: 0.6399 - val_accuracy: 0.6816\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5388 - accuracy: 0.7631 - val_loss: 0.6197 - val_accuracy: 0.6816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.5655 - accuracy: 0.7570 - val_loss: 0.6128 - val_accuracy: 0.6816\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5099 - accuracy: 0.7671 - val_loss: 0.5601 - val_accuracy: 0.7041\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4898 - accuracy: 0.7819 - val_loss: 0.5936 - val_accuracy: 0.6939\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4725 - accuracy: 0.7926 - val_loss: 0.5581 - val_accuracy: 0.7224\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4659 - accuracy: 0.7980 - val_loss: 0.5367 - val_accuracy: 0.7286\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.746181\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.746565\n",
            "[2]\tvalidation_0-auc:0.746027\n",
            "[3]\tvalidation_0-auc:0.746661\n",
            "[4]\tvalidation_0-auc:0.753733\n",
            "[5]\tvalidation_0-auc:0.753052\n",
            "[6]\tvalidation_0-auc:0.758368\n",
            "[7]\tvalidation_0-auc:0.761918\n",
            "[8]\tvalidation_0-auc:0.76544\n",
            "[9]\tvalidation_0-auc:0.763866\n",
            "[10]\tvalidation_0-auc:0.764788\n",
            "[11]\tvalidation_0-auc:0.76331\n",
            "[12]\tvalidation_0-auc:0.76403\n",
            "[13]\tvalidation_0-auc:0.763531\n",
            "[14]\tvalidation_0-auc:0.763646\n",
            "[15]\tvalidation_0-auc:0.763463\n",
            "[16]\tvalidation_0-auc:0.768559\n",
            "[17]\tvalidation_0-auc:0.768684\n",
            "[18]\tvalidation_0-auc:0.769375\n",
            "[19]\tvalidation_0-auc:0.768981\n",
            "[20]\tvalidation_0-auc:0.768405\n",
            "[21]\tvalidation_0-auc:0.767302\n",
            "[22]\tvalidation_0-auc:0.767235\n",
            "[23]\tvalidation_0-auc:0.768358\n",
            "[24]\tvalidation_0-auc:0.767571\n",
            "[25]\tvalidation_0-auc:0.767628\n",
            "[26]\tvalidation_0-auc:0.767571\n",
            "[27]\tvalidation_0-auc:0.768636\n",
            "[28]\tvalidation_0-auc:0.767753\n",
            "[29]\tvalidation_0-auc:0.767484\n",
            "[30]\tvalidation_0-auc:0.767407\n",
            "[31]\tvalidation_0-auc:0.767475\n",
            "[32]\tvalidation_0-auc:0.76735\n",
            "[33]\tvalidation_0-auc:0.76735\n",
            "[34]\tvalidation_0-auc:0.766956\n",
            "[35]\tvalidation_0-auc:0.767417\n",
            "[36]\tvalidation_0-auc:0.768271\n",
            "[37]\tvalidation_0-auc:0.768617\n",
            "[38]\tvalidation_0-auc:0.766112\n",
            "[39]\tvalidation_0-auc:0.766131\n",
            "[40]\tvalidation_0-auc:0.766362\n",
            "[41]\tvalidation_0-auc:0.766956\n",
            "[42]\tvalidation_0-auc:0.766736\n",
            "[43]\tvalidation_0-auc:0.765891\n",
            "[44]\tvalidation_0-auc:0.765968\n",
            "[45]\tvalidation_0-auc:0.765911\n",
            "[46]\tvalidation_0-auc:0.765488\n",
            "[47]\tvalidation_0-auc:0.765239\n",
            "[48]\tvalidation_0-auc:0.765392\n",
            "[49]\tvalidation_0-auc:0.764903\n",
            "[50]\tvalidation_0-auc:0.764682\n",
            "[51]\tvalidation_0-auc:0.765066\n",
            "[52]\tvalidation_0-auc:0.765527\n",
            "[53]\tvalidation_0-auc:0.765431\n",
            "[54]\tvalidation_0-auc:0.765296\n",
            "[55]\tvalidation_0-auc:0.765507\n",
            "[56]\tvalidation_0-auc:0.765795\n",
            "[57]\tvalidation_0-auc:0.765795\n",
            "[58]\tvalidation_0-auc:0.766218\n",
            "[59]\tvalidation_0-auc:0.766448\n",
            "[60]\tvalidation_0-auc:0.767216\n",
            "[61]\tvalidation_0-auc:0.767657\n",
            "[62]\tvalidation_0-auc:0.766678\n",
            "[63]\tvalidation_0-auc:0.766851\n",
            "[64]\tvalidation_0-auc:0.767158\n",
            "[65]\tvalidation_0-auc:0.76712\n",
            "[66]\tvalidation_0-auc:0.76712\n",
            "[67]\tvalidation_0-auc:0.767388\n",
            "[68]\tvalidation_0-auc:0.767561\n",
            "Stopping. Best iteration:\n",
            "[18]\tvalidation_0-auc:0.769375\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.5743 - accuracy: 0.7522 - val_loss: 0.5864 - val_accuracy: 0.7265\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5586 - accuracy: 0.7557 - val_loss: 0.5907 - val_accuracy: 0.7265\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5658 - accuracy: 0.7557 - val_loss: 0.5832 - val_accuracy: 0.7265\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5524 - accuracy: 0.7557 - val_loss: 0.5410 - val_accuracy: 0.7265\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5229 - accuracy: 0.7550 - val_loss: 0.5008 - val_accuracy: 0.7265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5748 - accuracy: 0.7522 - val_loss: 0.5772 - val_accuracy: 0.7265\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5223 - accuracy: 0.7632 - val_loss: 0.4969 - val_accuracy: 0.7834\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4976 - accuracy: 0.7763 - val_loss: 0.4633 - val_accuracy: 0.7593\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4694 - accuracy: 0.7893 - val_loss: 0.4496 - val_accuracy: 0.7877\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4707 - accuracy: 0.7872 - val_loss: 0.4563 - val_accuracy: 0.8140\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.707639\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.709964\n",
            "[2]\tvalidation_0-auc:0.710157\n",
            "[3]\tvalidation_0-auc:0.708349\n",
            "[4]\tvalidation_0-auc:0.732602\n",
            "[5]\tvalidation_0-auc:0.739193\n",
            "[6]\tvalidation_0-auc:0.733892\n",
            "[7]\tvalidation_0-auc:0.779398\n",
            "[8]\tvalidation_0-auc:0.781795\n",
            "[9]\tvalidation_0-auc:0.788422\n",
            "[10]\tvalidation_0-auc:0.791\n",
            "[11]\tvalidation_0-auc:0.806482\n",
            "[12]\tvalidation_0-auc:0.808337\n",
            "[13]\tvalidation_0-auc:0.808831\n",
            "[14]\tvalidation_0-auc:0.809904\n",
            "[15]\tvalidation_0-auc:0.815952\n",
            "[16]\tvalidation_0-auc:0.816988\n",
            "[17]\tvalidation_0-auc:0.818819\n",
            "[18]\tvalidation_0-auc:0.821687\n",
            "[19]\tvalidation_0-auc:0.827759\n",
            "[20]\tvalidation_0-auc:0.832675\n",
            "[21]\tvalidation_0-auc:0.834506\n",
            "[22]\tvalidation_0-auc:0.834325\n",
            "[23]\tvalidation_0-auc:0.836964\n",
            "[24]\tvalidation_0-auc:0.835578\n",
            "[25]\tvalidation_0-auc:0.835831\n",
            "[26]\tvalidation_0-auc:0.836771\n",
            "[27]\tvalidation_0-auc:0.838253\n",
            "[28]\tvalidation_0-auc:0.836759\n",
            "[29]\tvalidation_0-auc:0.839554\n",
            "[30]\tvalidation_0-auc:0.839265\n",
            "[31]\tvalidation_0-auc:0.838819\n",
            "[32]\tvalidation_0-auc:0.843193\n",
            "[33]\tvalidation_0-auc:0.843578\n",
            "[34]\tvalidation_0-auc:0.841627\n",
            "[35]\tvalidation_0-auc:0.843181\n",
            "[36]\tvalidation_0-auc:0.84306\n",
            "[37]\tvalidation_0-auc:0.844349\n",
            "[38]\tvalidation_0-auc:0.845639\n",
            "[39]\tvalidation_0-auc:0.846771\n",
            "[40]\tvalidation_0-auc:0.845277\n",
            "[41]\tvalidation_0-auc:0.846723\n",
            "[42]\tvalidation_0-auc:0.847518\n",
            "[43]\tvalidation_0-auc:0.84759\n",
            "[44]\tvalidation_0-auc:0.848614\n",
            "[45]\tvalidation_0-auc:0.849506\n",
            "[46]\tvalidation_0-auc:0.85141\n",
            "[47]\tvalidation_0-auc:0.852145\n",
            "[48]\tvalidation_0-auc:0.851518\n",
            "[49]\tvalidation_0-auc:0.851554\n",
            "[50]\tvalidation_0-auc:0.852988\n",
            "[51]\tvalidation_0-auc:0.852843\n",
            "[52]\tvalidation_0-auc:0.853181\n",
            "[53]\tvalidation_0-auc:0.852675\n",
            "[54]\tvalidation_0-auc:0.852048\n",
            "[55]\tvalidation_0-auc:0.852506\n",
            "[56]\tvalidation_0-auc:0.853301\n",
            "[57]\tvalidation_0-auc:0.852602\n",
            "[58]\tvalidation_0-auc:0.852699\n",
            "[59]\tvalidation_0-auc:0.85441\n",
            "[60]\tvalidation_0-auc:0.854518\n",
            "[61]\tvalidation_0-auc:0.85459\n",
            "[62]\tvalidation_0-auc:0.854566\n",
            "[63]\tvalidation_0-auc:0.854265\n",
            "[64]\tvalidation_0-auc:0.854337\n",
            "[65]\tvalidation_0-auc:0.854253\n",
            "[66]\tvalidation_0-auc:0.854566\n",
            "[67]\tvalidation_0-auc:0.85659\n",
            "[68]\tvalidation_0-auc:0.857\n",
            "[69]\tvalidation_0-auc:0.856855\n",
            "[70]\tvalidation_0-auc:0.857687\n",
            "[71]\tvalidation_0-auc:0.857446\n",
            "[72]\tvalidation_0-auc:0.856241\n",
            "[73]\tvalidation_0-auc:0.857036\n",
            "[74]\tvalidation_0-auc:0.856602\n",
            "[75]\tvalidation_0-auc:0.857301\n",
            "[76]\tvalidation_0-auc:0.856373\n",
            "[77]\tvalidation_0-auc:0.85647\n",
            "[78]\tvalidation_0-auc:0.855482\n",
            "[79]\tvalidation_0-auc:0.855337\n",
            "[80]\tvalidation_0-auc:0.85647\n",
            "[81]\tvalidation_0-auc:0.85612\n",
            "[82]\tvalidation_0-auc:0.855518\n",
            "[83]\tvalidation_0-auc:0.855892\n",
            "[84]\tvalidation_0-auc:0.855771\n",
            "[85]\tvalidation_0-auc:0.856036\n",
            "[86]\tvalidation_0-auc:0.856928\n",
            "[87]\tvalidation_0-auc:0.857675\n",
            "[88]\tvalidation_0-auc:0.85706\n",
            "[89]\tvalidation_0-auc:0.857301\n",
            "[90]\tvalidation_0-auc:0.858\n",
            "[91]\tvalidation_0-auc:0.857349\n",
            "[92]\tvalidation_0-auc:0.858096\n",
            "[93]\tvalidation_0-auc:0.858506\n",
            "[94]\tvalidation_0-auc:0.858819\n",
            "[95]\tvalidation_0-auc:0.859036\n",
            "[96]\tvalidation_0-auc:0.858626\n",
            "[97]\tvalidation_0-auc:0.858361\n",
            "[98]\tvalidation_0-auc:0.858313\n",
            "[99]\tvalidation_0-auc:0.858072\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.6816326530612244 |        0.0         |         0.0         |         0.0         |\n",
            "|     GRU 0.1      | 0.7285714285714285 | 0.7169811320754716 | 0.24358974358974358 | 0.36363636363636365 |\n",
            "|   XGBoost 0.1    | 0.7510204081632653 | 0.6666666666666666 |  0.4358974358974359 |  0.5271317829457364 |\n",
            "|    Logreg 0.1    | 0.7142857142857143 |        0.75        | 0.15384615384615385 | 0.25531914893617025 |\n",
            "|     SVM 0.1      | 0.7183673469387755 |       0.6875       | 0.21153846153846154 |  0.3235294117647059 |\n",
            "|  LSTM beta 0.1   | 0.7264770240700219 |        0.0         |         0.0         |         0.0         |\n",
            "|   GRU beta 0.1   | 0.8140043763676149 | 0.7380952380952381 |        0.496        |  0.5933014354066987 |\n",
            "| XGBoost beta 0.1 | 0.8008752735229759 |       0.7125       |        0.456        |  0.5560975609756097 |\n",
            "| logreg beta 0.1  | 0.7636761487964989 | 0.7297297297297297 |        0.216        |  0.3333333333333333 |\n",
            "|   svm beta 0.1   | 0.8030634573304157 | 0.7215189873417721 |        0.456        |  0.5588235294117647 |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.4454 - accuracy: 0.8523 - val_loss: 0.4078 - val_accuracy: 0.8592\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4128 - accuracy: 0.8604 - val_loss: 0.4054 - val_accuracy: 0.8592\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4111 - accuracy: 0.8604 - val_loss: 0.3822 - val_accuracy: 0.8592\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4159 - accuracy: 0.8597 - val_loss: 0.4028 - val_accuracy: 0.8592\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.3946 - accuracy: 0.8604 - val_loss: 0.3959 - val_accuracy: 0.8592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.4341 - accuracy: 0.8557 - val_loss: 0.3996 - val_accuracy: 0.8592\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.3870 - accuracy: 0.8671 - val_loss: 0.4028 - val_accuracy: 0.8571\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.3669 - accuracy: 0.8685 - val_loss: 0.3760 - val_accuracy: 0.8592\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.3648 - accuracy: 0.8705 - val_loss: 0.3788 - val_accuracy: 0.8510\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.3572 - accuracy: 0.8711 - val_loss: 0.3787 - val_accuracy: 0.8531\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.634256\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.636321\n",
            "[2]\tvalidation_0-auc:0.636614\n",
            "[3]\tvalidation_0-auc:0.63892\n",
            "[4]\tvalidation_0-auc:0.641261\n",
            "[5]\tvalidation_0-auc:0.653792\n",
            "[6]\tvalidation_0-auc:0.649781\n",
            "[7]\tvalidation_0-auc:0.663293\n",
            "[8]\tvalidation_0-auc:0.662777\n",
            "[9]\tvalidation_0-auc:0.661228\n",
            "[10]\tvalidation_0-auc:0.659472\n",
            "[11]\tvalidation_0-auc:0.661365\n",
            "[12]\tvalidation_0-auc:0.675015\n",
            "[13]\tvalidation_0-auc:0.674756\n",
            "[14]\tvalidation_0-auc:0.671228\n",
            "[15]\tvalidation_0-auc:0.673001\n",
            "[16]\tvalidation_0-auc:0.673706\n",
            "[17]\tvalidation_0-auc:0.67023\n",
            "[18]\tvalidation_0-auc:0.671934\n",
            "[19]\tvalidation_0-auc:0.673018\n",
            "[20]\tvalidation_0-auc:0.67078\n",
            "[21]\tvalidation_0-auc:0.67276\n",
            "[22]\tvalidation_0-auc:0.672002\n",
            "[23]\tvalidation_0-auc:0.672002\n",
            "[24]\tvalidation_0-auc:0.670144\n",
            "[25]\tvalidation_0-auc:0.671658\n",
            "[26]\tvalidation_0-auc:0.676133\n",
            "[27]\tvalidation_0-auc:0.677235\n",
            "[28]\tvalidation_0-auc:0.675376\n",
            "[29]\tvalidation_0-auc:0.675961\n",
            "[30]\tvalidation_0-auc:0.675428\n",
            "[31]\tvalidation_0-auc:0.676288\n",
            "[32]\tvalidation_0-auc:0.679077\n",
            "[33]\tvalidation_0-auc:0.679111\n",
            "[34]\tvalidation_0-auc:0.678595\n",
            "[35]\tvalidation_0-auc:0.6777\n",
            "[36]\tvalidation_0-auc:0.680832\n",
            "[37]\tvalidation_0-auc:0.680798\n",
            "[38]\tvalidation_0-auc:0.678819\n",
            "[39]\tvalidation_0-auc:0.681056\n",
            "[40]\tvalidation_0-auc:0.679404\n",
            "[41]\tvalidation_0-auc:0.677407\n",
            "[42]\tvalidation_0-auc:0.67751\n",
            "[43]\tvalidation_0-auc:0.677476\n",
            "[44]\tvalidation_0-auc:0.676633\n",
            "[45]\tvalidation_0-auc:0.675806\n",
            "[46]\tvalidation_0-auc:0.675738\n",
            "[47]\tvalidation_0-auc:0.677424\n",
            "[48]\tvalidation_0-auc:0.67572\n",
            "[49]\tvalidation_0-auc:0.675135\n",
            "[50]\tvalidation_0-auc:0.675342\n",
            "[51]\tvalidation_0-auc:0.672175\n",
            "[52]\tvalidation_0-auc:0.673242\n",
            "[53]\tvalidation_0-auc:0.670419\n",
            "[54]\tvalidation_0-auc:0.670247\n",
            "[55]\tvalidation_0-auc:0.669937\n",
            "[56]\tvalidation_0-auc:0.669696\n",
            "[57]\tvalidation_0-auc:0.669214\n",
            "[58]\tvalidation_0-auc:0.669352\n",
            "[59]\tvalidation_0-auc:0.667872\n",
            "[60]\tvalidation_0-auc:0.666804\n",
            "[61]\tvalidation_0-auc:0.662914\n",
            "[62]\tvalidation_0-auc:0.661124\n",
            "[63]\tvalidation_0-auc:0.660333\n",
            "[64]\tvalidation_0-auc:0.662157\n",
            "[65]\tvalidation_0-auc:0.659386\n",
            "[66]\tvalidation_0-auc:0.659231\n",
            "[67]\tvalidation_0-auc:0.658542\n",
            "[68]\tvalidation_0-auc:0.657579\n",
            "[69]\tvalidation_0-auc:0.657475\n",
            "[70]\tvalidation_0-auc:0.65627\n",
            "[71]\tvalidation_0-auc:0.658267\n",
            "[72]\tvalidation_0-auc:0.655857\n",
            "[73]\tvalidation_0-auc:0.653482\n",
            "[74]\tvalidation_0-auc:0.653792\n",
            "[75]\tvalidation_0-auc:0.653826\n",
            "[76]\tvalidation_0-auc:0.652139\n",
            "[77]\tvalidation_0-auc:0.651244\n",
            "[78]\tvalidation_0-auc:0.650969\n",
            "[79]\tvalidation_0-auc:0.650797\n",
            "[80]\tvalidation_0-auc:0.650005\n",
            "[81]\tvalidation_0-auc:0.649592\n",
            "[82]\tvalidation_0-auc:0.651348\n",
            "[83]\tvalidation_0-auc:0.651313\n",
            "[84]\tvalidation_0-auc:0.648146\n",
            "[85]\tvalidation_0-auc:0.647217\n",
            "[86]\tvalidation_0-auc:0.64942\n",
            "[87]\tvalidation_0-auc:0.648663\n",
            "[88]\tvalidation_0-auc:0.649386\n",
            "[89]\tvalidation_0-auc:0.647458\n",
            "Stopping. Best iteration:\n",
            "[39]\tvalidation_0-auc:0.681056\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.4434 - accuracy: 0.8531 - val_loss: 0.4324 - val_accuracy: 0.8490\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4254 - accuracy: 0.8572 - val_loss: 0.4195 - val_accuracy: 0.8490\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4131 - accuracy: 0.8572 - val_loss: 0.3685 - val_accuracy: 0.8490\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4030 - accuracy: 0.8572 - val_loss: 0.3503 - val_accuracy: 0.8490\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.3666 - accuracy: 0.8572 - val_loss: 0.3485 - val_accuracy: 0.8490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.4704 - accuracy: 0.8531 - val_loss: 0.4228 - val_accuracy: 0.8490\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.3832 - accuracy: 0.8600 - val_loss: 0.3492 - val_accuracy: 0.8359\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.3520 - accuracy: 0.8641 - val_loss: 0.3303 - val_accuracy: 0.8337\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.3394 - accuracy: 0.8703 - val_loss: 0.3280 - val_accuracy: 0.8293\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.3338 - accuracy: 0.8703 - val_loss: 0.3235 - val_accuracy: 0.8315\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.748805\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.797792\n",
            "[2]\tvalidation_0-auc:0.801939\n",
            "[3]\tvalidation_0-auc:0.80009\n",
            "[4]\tvalidation_0-auc:0.799772\n",
            "[5]\tvalidation_0-auc:0.801154\n",
            "[6]\tvalidation_0-auc:0.868108\n",
            "[7]\tvalidation_0-auc:0.860993\n",
            "[8]\tvalidation_0-auc:0.866334\n",
            "[9]\tvalidation_0-auc:0.863178\n",
            "[10]\tvalidation_0-auc:0.86456\n",
            "[11]\tvalidation_0-auc:0.889605\n",
            "[12]\tvalidation_0-auc:0.892182\n",
            "[13]\tvalidation_0-auc:0.890837\n",
            "[14]\tvalidation_0-auc:0.889792\n",
            "[15]\tvalidation_0-auc:0.887999\n",
            "[16]\tvalidation_0-auc:0.888746\n",
            "[17]\tvalidation_0-auc:0.8877\n",
            "[18]\tvalidation_0-auc:0.886579\n",
            "[19]\tvalidation_0-auc:0.883311\n",
            "[20]\tvalidation_0-auc:0.880734\n",
            "[21]\tvalidation_0-auc:0.878343\n",
            "[22]\tvalidation_0-auc:0.877372\n",
            "[23]\tvalidation_0-auc:0.876924\n",
            "[24]\tvalidation_0-auc:0.87726\n",
            "[25]\tvalidation_0-auc:0.876793\n",
            "[26]\tvalidation_0-auc:0.879688\n",
            "[27]\tvalidation_0-auc:0.879874\n",
            "[28]\tvalidation_0-auc:0.882919\n",
            "[29]\tvalidation_0-auc:0.883516\n",
            "[30]\tvalidation_0-auc:0.884114\n",
            "[31]\tvalidation_0-auc:0.884263\n",
            "[32]\tvalidation_0-auc:0.884805\n",
            "[33]\tvalidation_0-auc:0.885403\n",
            "[34]\tvalidation_0-auc:0.887046\n",
            "[35]\tvalidation_0-auc:0.888241\n",
            "[36]\tvalidation_0-auc:0.888092\n",
            "[37]\tvalidation_0-auc:0.891827\n",
            "[38]\tvalidation_0-auc:0.88996\n",
            "[39]\tvalidation_0-auc:0.890931\n",
            "[40]\tvalidation_0-auc:0.891454\n",
            "[41]\tvalidation_0-auc:0.891958\n",
            "[42]\tvalidation_0-auc:0.891846\n",
            "[43]\tvalidation_0-auc:0.890389\n",
            "[44]\tvalidation_0-auc:0.889978\n",
            "[45]\tvalidation_0-auc:0.891192\n",
            "[46]\tvalidation_0-auc:0.891827\n",
            "[47]\tvalidation_0-auc:0.89179\n",
            "[48]\tvalidation_0-auc:0.891566\n",
            "[49]\tvalidation_0-auc:0.892014\n",
            "[50]\tvalidation_0-auc:0.89235\n",
            "[51]\tvalidation_0-auc:0.89291\n",
            "[52]\tvalidation_0-auc:0.894442\n",
            "[53]\tvalidation_0-auc:0.89433\n",
            "[54]\tvalidation_0-auc:0.892985\n",
            "[55]\tvalidation_0-auc:0.893583\n",
            "[56]\tvalidation_0-auc:0.893583\n",
            "[57]\tvalidation_0-auc:0.893433\n",
            "[58]\tvalidation_0-auc:0.893546\n",
            "[59]\tvalidation_0-auc:0.893807\n",
            "[60]\tvalidation_0-auc:0.893546\n",
            "[61]\tvalidation_0-auc:0.895563\n",
            "[62]\tvalidation_0-auc:0.895973\n",
            "[63]\tvalidation_0-auc:0.896011\n",
            "[64]\tvalidation_0-auc:0.895749\n",
            "[65]\tvalidation_0-auc:0.896384\n",
            "[66]\tvalidation_0-auc:0.896758\n",
            "[67]\tvalidation_0-auc:0.896758\n",
            "[68]\tvalidation_0-auc:0.897057\n",
            "[69]\tvalidation_0-auc:0.896945\n",
            "[70]\tvalidation_0-auc:0.897281\n",
            "[71]\tvalidation_0-auc:0.897169\n",
            "[72]\tvalidation_0-auc:0.897804\n",
            "[73]\tvalidation_0-auc:0.898962\n",
            "[74]\tvalidation_0-auc:0.898401\n",
            "[75]\tvalidation_0-auc:0.898364\n",
            "[76]\tvalidation_0-auc:0.900232\n",
            "[77]\tvalidation_0-auc:0.90053\n",
            "[78]\tvalidation_0-auc:0.900829\n",
            "[79]\tvalidation_0-auc:0.900605\n",
            "[80]\tvalidation_0-auc:0.900456\n",
            "[81]\tvalidation_0-auc:0.900642\n",
            "[82]\tvalidation_0-auc:0.899671\n",
            "[83]\tvalidation_0-auc:0.899447\n",
            "[84]\tvalidation_0-auc:0.899335\n",
            "[85]\tvalidation_0-auc:0.898812\n",
            "[86]\tvalidation_0-auc:0.898028\n",
            "[87]\tvalidation_0-auc:0.897841\n",
            "[88]\tvalidation_0-auc:0.898177\n",
            "[89]\tvalidation_0-auc:0.897654\n",
            "[90]\tvalidation_0-auc:0.89758\n",
            "[91]\tvalidation_0-auc:0.897094\n",
            "[92]\tvalidation_0-auc:0.897467\n",
            "[93]\tvalidation_0-auc:0.897206\n",
            "[94]\tvalidation_0-auc:0.896795\n",
            "[95]\tvalidation_0-auc:0.89758\n",
            "[96]\tvalidation_0-auc:0.897094\n",
            "[97]\tvalidation_0-auc:0.896907\n",
            "[98]\tvalidation_0-auc:0.895861\n",
            "[99]\tvalidation_0-auc:0.895338\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+----------------------+----------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall        |       F1 score       |\n",
            "+------------------+--------------------+--------------------+----------------------+----------------------+\n",
            "|     LSTM 0.2     | 0.8591836734693877 |        0.0         |         0.0          |         0.0          |\n",
            "|     GRU 0.2      | 0.8530612244897959 | 0.3333333333333333 | 0.043478260869565216 | 0.07692307692307691  |\n",
            "|   XGBoost 0.2    | 0.8551020408163266 | 0.3333333333333333 | 0.028985507246376812 | 0.05333333333333333  |\n",
            "|    Logreg 0.2    | 0.8571428571428571 | 0.3333333333333333 | 0.014492753623188406 | 0.027777777777777776 |\n",
            "|     SVM 0.2      | 0.8448979591836735 | 0.1111111111111111 | 0.014492753623188406 | 0.02564102564102564  |\n",
            "|  LSTM beta 0.2   | 0.849015317286652  |        0.0         |         0.0          |         0.0          |\n",
            "|   GRU beta 0.2   | 0.8315098468271335 | 0.3333333333333333 | 0.11594202898550725  | 0.17204301075268816  |\n",
            "| XGBoost beta 0.2 | 0.849015317286652  |        0.5         | 0.43478260869565216  | 0.46511627906976744  |\n",
            "| logreg beta 0.2  | 0.838074398249453  | 0.2727272727272727 | 0.043478260869565216 | 0.07500000000000001  |\n",
            "|   svm beta 0.2   | 0.849015317286652  |        0.5         |  0.4057971014492754  |        0.448         |\n",
            "+------------------+--------------------+--------------------+----------------------+----------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.5270 - accuracy: 0.7919 - val_loss: 0.6311 - val_accuracy: 0.7163\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5174 - accuracy: 0.7946 - val_loss: 0.5996 - val_accuracy: 0.7163\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4956 - accuracy: 0.7980 - val_loss: 0.5722 - val_accuracy: 0.7143\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4687 - accuracy: 0.7987 - val_loss: 0.5844 - val_accuracy: 0.7163\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.4569 - accuracy: 0.8020 - val_loss: 0.5943 - val_accuracy: 0.7082\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.5261 - accuracy: 0.7926 - val_loss: 0.5807 - val_accuracy: 0.7163\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4648 - accuracy: 0.8087 - val_loss: 0.5240 - val_accuracy: 0.7612\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4426 - accuracy: 0.8161 - val_loss: 0.5628 - val_accuracy: 0.7245\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4356 - accuracy: 0.8154 - val_loss: 0.5558 - val_accuracy: 0.7327\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.4228 - accuracy: 0.8154 - val_loss: 0.5301 - val_accuracy: 0.7531\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.736252\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.74318\n",
            "[2]\tvalidation_0-auc:0.742298\n",
            "[3]\tvalidation_0-auc:0.742596\n",
            "[4]\tvalidation_0-auc:0.745598\n",
            "[5]\tvalidation_0-auc:0.746941\n",
            "[6]\tvalidation_0-auc:0.749226\n",
            "[7]\tvalidation_0-auc:0.747341\n",
            "[8]\tvalidation_0-auc:0.746213\n",
            "[9]\tvalidation_0-auc:0.748509\n",
            "[10]\tvalidation_0-auc:0.751337\n",
            "[11]\tvalidation_0-auc:0.750968\n",
            "[12]\tvalidation_0-auc:0.751542\n",
            "[13]\tvalidation_0-auc:0.754402\n",
            "[14]\tvalidation_0-auc:0.754975\n",
            "[15]\tvalidation_0-auc:0.756339\n",
            "[16]\tvalidation_0-auc:0.755549\n",
            "[17]\tvalidation_0-auc:0.75559\n",
            "[18]\tvalidation_0-auc:0.753397\n",
            "[19]\tvalidation_0-auc:0.753018\n",
            "[20]\tvalidation_0-auc:0.754258\n",
            "[21]\tvalidation_0-auc:0.755058\n",
            "[22]\tvalidation_0-auc:0.753951\n",
            "[23]\tvalidation_0-auc:0.753367\n",
            "[24]\tvalidation_0-auc:0.753971\n",
            "[25]\tvalidation_0-auc:0.752608\n",
            "[26]\tvalidation_0-auc:0.75517\n",
            "[27]\tvalidation_0-auc:0.755447\n",
            "[28]\tvalidation_0-auc:0.755836\n",
            "[29]\tvalidation_0-auc:0.755857\n",
            "[30]\tvalidation_0-auc:0.75515\n",
            "[31]\tvalidation_0-auc:0.755068\n",
            "[32]\tvalidation_0-auc:0.755139\n",
            "[33]\tvalidation_0-auc:0.756246\n",
            "[34]\tvalidation_0-auc:0.756554\n",
            "[35]\tvalidation_0-auc:0.755703\n",
            "[36]\tvalidation_0-auc:0.753233\n",
            "[37]\tvalidation_0-auc:0.753889\n",
            "[38]\tvalidation_0-auc:0.752998\n",
            "[39]\tvalidation_0-auc:0.753377\n",
            "[40]\tvalidation_0-auc:0.752844\n",
            "[41]\tvalidation_0-auc:0.752803\n",
            "[42]\tvalidation_0-auc:0.753254\n",
            "[43]\tvalidation_0-auc:0.754443\n",
            "[44]\tvalidation_0-auc:0.752557\n",
            "[45]\tvalidation_0-auc:0.753172\n",
            "[46]\tvalidation_0-auc:0.751706\n",
            "[47]\tvalidation_0-auc:0.752157\n",
            "[48]\tvalidation_0-auc:0.752465\n",
            "[49]\tvalidation_0-auc:0.751809\n",
            "[50]\tvalidation_0-auc:0.751829\n",
            "[51]\tvalidation_0-auc:0.751255\n",
            "[52]\tvalidation_0-auc:0.751522\n",
            "[53]\tvalidation_0-auc:0.751665\n",
            "[54]\tvalidation_0-auc:0.750702\n",
            "[55]\tvalidation_0-auc:0.750784\n",
            "[56]\tvalidation_0-auc:0.751153\n",
            "[57]\tvalidation_0-auc:0.750784\n",
            "[58]\tvalidation_0-auc:0.750763\n",
            "[59]\tvalidation_0-auc:0.750518\n",
            "[60]\tvalidation_0-auc:0.750845\n",
            "[61]\tvalidation_0-auc:0.749985\n",
            "[62]\tvalidation_0-auc:0.749821\n",
            "[63]\tvalidation_0-auc:0.750272\n",
            "[64]\tvalidation_0-auc:0.7506\n",
            "[65]\tvalidation_0-auc:0.750845\n",
            "[66]\tvalidation_0-auc:0.750825\n",
            "[67]\tvalidation_0-auc:0.751214\n",
            "[68]\tvalidation_0-auc:0.749165\n",
            "[69]\tvalidation_0-auc:0.748755\n",
            "[70]\tvalidation_0-auc:0.748693\n",
            "[71]\tvalidation_0-auc:0.749185\n",
            "[72]\tvalidation_0-auc:0.749411\n",
            "[73]\tvalidation_0-auc:0.748632\n",
            "[74]\tvalidation_0-auc:0.747956\n",
            "[75]\tvalidation_0-auc:0.747259\n",
            "[76]\tvalidation_0-auc:0.746849\n",
            "[77]\tvalidation_0-auc:0.746562\n",
            "[78]\tvalidation_0-auc:0.747341\n",
            "[79]\tvalidation_0-auc:0.747771\n",
            "[80]\tvalidation_0-auc:0.748058\n",
            "[81]\tvalidation_0-auc:0.748529\n",
            "[82]\tvalidation_0-auc:0.749124\n",
            "[83]\tvalidation_0-auc:0.749452\n",
            "[84]\tvalidation_0-auc:0.749185\n",
            "Stopping. Best iteration:\n",
            "[34]\tvalidation_0-auc:0.756554\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 4s 15ms/step - loss: 0.5419 - accuracy: 0.7852 - val_loss: 0.5464 - val_accuracy: 0.7637\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5198 - accuracy: 0.7900 - val_loss: 0.5434 - val_accuracy: 0.7637\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4950 - accuracy: 0.7900 - val_loss: 0.4693 - val_accuracy: 0.7637\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4638 - accuracy: 0.7900 - val_loss: 0.4437 - val_accuracy: 0.7637\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4428 - accuracy: 0.8051 - val_loss: 0.4146 - val_accuracy: 0.7768\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5204 - accuracy: 0.7886 - val_loss: 0.4951 - val_accuracy: 0.7637\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4528 - accuracy: 0.8030 - val_loss: 0.4107 - val_accuracy: 0.7987\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4160 - accuracy: 0.8257 - val_loss: 0.4120 - val_accuracy: 0.8315\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4209 - accuracy: 0.8332 - val_loss: 0.4209 - val_accuracy: 0.7921\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4154 - accuracy: 0.8243 - val_loss: 0.3978 - val_accuracy: 0.8337\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.849146\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.852767\n",
            "[2]\tvalidation_0-auc:0.847925\n",
            "[3]\tvalidation_0-auc:0.849464\n",
            "[4]\tvalidation_0-auc:0.849265\n",
            "[5]\tvalidation_0-auc:0.850525\n",
            "[6]\tvalidation_0-auc:0.842486\n",
            "[7]\tvalidation_0-auc:0.840974\n",
            "[8]\tvalidation_0-auc:0.859254\n",
            "[9]\tvalidation_0-auc:0.856972\n",
            "[10]\tvalidation_0-auc:0.856176\n",
            "[11]\tvalidation_0-auc:0.859785\n",
            "[12]\tvalidation_0-auc:0.860461\n",
            "[13]\tvalidation_0-auc:0.862292\n",
            "[14]\tvalidation_0-auc:0.862464\n",
            "[15]\tvalidation_0-auc:0.864348\n",
            "[16]\tvalidation_0-auc:0.861894\n",
            "[17]\tvalidation_0-auc:0.862557\n",
            "[18]\tvalidation_0-auc:0.863831\n",
            "[19]\tvalidation_0-auc:0.86655\n",
            "[20]\tvalidation_0-auc:0.863884\n",
            "[21]\tvalidation_0-auc:0.862849\n",
            "[22]\tvalidation_0-auc:0.862637\n",
            "[23]\tvalidation_0-auc:0.863247\n",
            "[24]\tvalidation_0-auc:0.861124\n",
            "[25]\tvalidation_0-auc:0.862544\n",
            "[26]\tvalidation_0-auc:0.861934\n",
            "[27]\tvalidation_0-auc:0.860289\n",
            "[28]\tvalidation_0-auc:0.863472\n",
            "[29]\tvalidation_0-auc:0.862252\n",
            "[30]\tvalidation_0-auc:0.862743\n",
            "[31]\tvalidation_0-auc:0.863459\n",
            "[32]\tvalidation_0-auc:0.861867\n",
            "[33]\tvalidation_0-auc:0.86123\n",
            "[34]\tvalidation_0-auc:0.86269\n",
            "[35]\tvalidation_0-auc:0.864733\n",
            "[36]\tvalidation_0-auc:0.863592\n",
            "[37]\tvalidation_0-auc:0.863618\n",
            "[38]\tvalidation_0-auc:0.864441\n",
            "[39]\tvalidation_0-auc:0.864242\n",
            "[40]\tvalidation_0-auc:0.865383\n",
            "[41]\tvalidation_0-auc:0.866179\n",
            "[42]\tvalidation_0-auc:0.866523\n",
            "[43]\tvalidation_0-auc:0.866179\n",
            "[44]\tvalidation_0-auc:0.86716\n",
            "[45]\tvalidation_0-auc:0.869044\n",
            "[46]\tvalidation_0-auc:0.868513\n",
            "[47]\tvalidation_0-auc:0.868062\n",
            "[48]\tvalidation_0-auc:0.869362\n",
            "[49]\tvalidation_0-auc:0.868261\n",
            "[50]\tvalidation_0-auc:0.869203\n",
            "[51]\tvalidation_0-auc:0.86923\n",
            "[52]\tvalidation_0-auc:0.869681\n",
            "[53]\tvalidation_0-auc:0.87114\n",
            "[54]\tvalidation_0-auc:0.870795\n",
            "[55]\tvalidation_0-auc:0.87045\n",
            "[56]\tvalidation_0-auc:0.871299\n",
            "[57]\tvalidation_0-auc:0.872002\n",
            "[58]\tvalidation_0-auc:0.871949\n",
            "[59]\tvalidation_0-auc:0.873222\n",
            "[60]\tvalidation_0-auc:0.873939\n",
            "[61]\tvalidation_0-auc:0.873886\n",
            "[62]\tvalidation_0-auc:0.873435\n",
            "[63]\tvalidation_0-auc:0.873262\n",
            "[64]\tvalidation_0-auc:0.874615\n",
            "[65]\tvalidation_0-auc:0.874562\n",
            "[66]\tvalidation_0-auc:0.873819\n",
            "[67]\tvalidation_0-auc:0.874244\n",
            "[68]\tvalidation_0-auc:0.875013\n",
            "[69]\tvalidation_0-auc:0.875491\n",
            "[70]\tvalidation_0-auc:0.875756\n",
            "[71]\tvalidation_0-auc:0.87573\n",
            "[72]\tvalidation_0-auc:0.875597\n",
            "[73]\tvalidation_0-auc:0.87687\n",
            "[74]\tvalidation_0-auc:0.876897\n",
            "[75]\tvalidation_0-auc:0.876579\n",
            "[76]\tvalidation_0-auc:0.876074\n",
            "[77]\tvalidation_0-auc:0.875902\n",
            "[78]\tvalidation_0-auc:0.876565\n",
            "[79]\tvalidation_0-auc:0.876194\n",
            "[80]\tvalidation_0-auc:0.875849\n",
            "[81]\tvalidation_0-auc:0.875637\n",
            "[82]\tvalidation_0-auc:0.876327\n",
            "[83]\tvalidation_0-auc:0.87699\n",
            "[84]\tvalidation_0-auc:0.876884\n",
            "[85]\tvalidation_0-auc:0.877069\n",
            "[86]\tvalidation_0-auc:0.877149\n",
            "[87]\tvalidation_0-auc:0.876486\n",
            "[88]\tvalidation_0-auc:0.876963\n",
            "[89]\tvalidation_0-auc:0.877494\n",
            "[90]\tvalidation_0-auc:0.877361\n",
            "[91]\tvalidation_0-auc:0.877547\n",
            "[92]\tvalidation_0-auc:0.87821\n",
            "[93]\tvalidation_0-auc:0.8776\n",
            "[94]\tvalidation_0-auc:0.877786\n",
            "[95]\tvalidation_0-auc:0.877998\n",
            "[96]\tvalidation_0-auc:0.878184\n",
            "[97]\tvalidation_0-auc:0.878025\n",
            "[98]\tvalidation_0-auc:0.878767\n",
            "[99]\tvalidation_0-auc:0.878608\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+----------------------+----------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |        Recall        |       F1 score       |\n",
            "+-------------------+--------------------+--------------------+----------------------+----------------------+\n",
            "|     LSTM 0.15     | 0.7081632653061225 |        0.25        | 0.014388489208633094 | 0.027210884353741496 |\n",
            "|      GRU 0.15     | 0.753061224489796  | 0.6956521739130435 |  0.2302158273381295  | 0.34594594594594597  |\n",
            "|    XGBoost 0.15   | 0.7591836734693878 | 0.6842105263157895 |  0.2805755395683453  |  0.3979591836734694  |\n",
            "|    Logreg 0.15    | 0.7285714285714285 | 0.6363636363636364 | 0.10071942446043165  | 0.17391304347826086  |\n",
            "|      SVM 0.15     | 0.7183673469387755 | 0.5238095238095238 | 0.07913669064748201  |        0.1375        |\n",
            "|   LSTM beta 0.15  | 0.7768052516411379 |        1.0         | 0.05555555555555555  | 0.10526315789473684  |\n",
            "|   GRU beta 0.15   | 0.8336980306345733 | 0.6777777777777778 |  0.5648148148148148  |  0.6161616161616162  |\n",
            "| XGBoost beta 0.15 | 0.824945295404814  | 0.6842105263157895 | 0.48148148148148145  |  0.5652173913043478  |\n",
            "|  logreg beta 0.15 | 0.7768052516411379 | 0.5882352941176471 | 0.18518518518518517  |  0.2816901408450704  |\n",
            "|   svm beta 0.15   | 0.8205689277899344 | 0.6805555555555556 |  0.4537037037037037  |  0.5444444444444445  |\n",
            "+-------------------+--------------------+--------------------+----------------------+----------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZeyIeftYiNw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "bf651f3d-1ee1-41c6-81e7-95b8b484b4a5"
      },
      "source": [
        "Result_purging.to_csv('RMD_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.681633</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.716981</td>\n",
              "      <td>0.728571</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.243590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.751020</td>\n",
              "      <td>0.527132</td>\n",
              "      <td>0.435897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.255319</td>\n",
              "      <td>0.153846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.718367</td>\n",
              "      <td>0.323529</td>\n",
              "      <td>0.211538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.726477</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.738095</td>\n",
              "      <td>0.814004</td>\n",
              "      <td>0.593301</td>\n",
              "      <td>0.496000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.712500</td>\n",
              "      <td>0.800875</td>\n",
              "      <td>0.556098</td>\n",
              "      <td>0.456000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.729730</td>\n",
              "      <td>0.763676</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.216000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.721519</td>\n",
              "      <td>0.803063</td>\n",
              "      <td>0.558824</td>\n",
              "      <td>0.456000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.859184</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.853061</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.043478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.855102</td>\n",
              "      <td>0.053333</td>\n",
              "      <td>0.028986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.027778</td>\n",
              "      <td>0.014493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.844898</td>\n",
              "      <td>0.025641</td>\n",
              "      <td>0.014493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.849015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.831510</td>\n",
              "      <td>0.172043</td>\n",
              "      <td>0.115942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.849015</td>\n",
              "      <td>0.465116</td>\n",
              "      <td>0.434783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.838074</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.043478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.849015</td>\n",
              "      <td>0.448000</td>\n",
              "      <td>0.405797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.708163</td>\n",
              "      <td>0.027211</td>\n",
              "      <td>0.014388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.695652</td>\n",
              "      <td>0.753061</td>\n",
              "      <td>0.345946</td>\n",
              "      <td>0.230216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.759184</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>0.280576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.728571</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>0.100719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.523810</td>\n",
              "      <td>0.718367</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.079137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>RMD</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.776805</td>\n",
              "      <td>0.105263</td>\n",
              "      <td>0.055556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.677778</td>\n",
              "      <td>0.833698</td>\n",
              "      <td>0.616162</td>\n",
              "      <td>0.564815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.824945</td>\n",
              "      <td>0.565217</td>\n",
              "      <td>0.481481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>0.776805</td>\n",
              "      <td>0.281690</td>\n",
              "      <td>0.185185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>RMD</td>\n",
              "      <td>0.680556</td>\n",
              "      <td>0.820569</td>\n",
              "      <td>0.544444</td>\n",
              "      <td>0.453704</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  RMD  0.000000  0.681633  0.000000  0.000000\n",
              "1            GRU 0.1  RMD  0.716981  0.728571  0.363636  0.243590\n",
              "2        XGBoost 0.1  RMD  0.666667  0.751020  0.527132  0.435897\n",
              "3         Logreg 0.1  RMD  0.750000  0.714286  0.255319  0.153846\n",
              "4            SVM 0.1  RMD  0.687500  0.718367  0.323529  0.211538\n",
              "5      LSTM beta 0.1  RMD  0.000000  0.726477  0.000000  0.000000\n",
              "6       GRU beta 0.1  RMD  0.738095  0.814004  0.593301  0.496000\n",
              "7   XGBoost beta 0.1  RMD  0.712500  0.800875  0.556098  0.456000\n",
              "8    logreg beta 0.1  RMD  0.729730  0.763676  0.333333  0.216000\n",
              "9       svm beta 0.1  RMD  0.721519  0.803063  0.558824  0.456000\n",
              "0           LSTM 0.2  RMD  0.000000  0.859184  0.000000  0.000000\n",
              "1            GRU 0.2  RMD  0.333333  0.853061  0.076923  0.043478\n",
              "2        XGBoost 0.2  RMD  0.333333  0.855102  0.053333  0.028986\n",
              "3         Logreg 0.2  RMD  0.333333  0.857143  0.027778  0.014493\n",
              "4            SVM 0.2  RMD  0.111111  0.844898  0.025641  0.014493\n",
              "5      LSTM beta 0.2  RMD  0.000000  0.849015  0.000000  0.000000\n",
              "6       GRU beta 0.2  RMD  0.333333  0.831510  0.172043  0.115942\n",
              "7   XGBoost beta 0.2  RMD  0.500000  0.849015  0.465116  0.434783\n",
              "8    logreg beta 0.2  RMD  0.272727  0.838074  0.075000  0.043478\n",
              "9       svm beta 0.2  RMD  0.500000  0.849015  0.448000  0.405797\n",
              "0          LSTM 0.15  RMD  0.250000  0.708163  0.027211  0.014388\n",
              "1           GRU 0.15  RMD  0.695652  0.753061  0.345946  0.230216\n",
              "2       XGBoost 0.15  RMD  0.684211  0.759184  0.397959  0.280576\n",
              "3        Logreg 0.15  RMD  0.636364  0.728571  0.173913  0.100719\n",
              "4           SVM 0.15  RMD  0.523810  0.718367  0.137500  0.079137\n",
              "5     LSTM beta 0.15  RMD  1.000000  0.776805  0.105263  0.055556\n",
              "6      GRU beta 0.15  RMD  0.677778  0.833698  0.616162  0.564815\n",
              "7  XGBoost beta 0.15  RMD  0.684211  0.824945  0.565217  0.481481\n",
              "8   logreg beta 0.15  RMD  0.588235  0.776805  0.281690  0.185185\n",
              "9      svm beta 0.15  RMD  0.680556  0.820569  0.544444  0.453704"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH2Iq8EcYiNw"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('RMD_logreg_beta_p.csv')"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vozi8lmOYiNx"
      },
      "source": [
        ""
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY_JW9t6Y6WY"
      },
      "source": [
        "## STX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WEFTQ7GY6We",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b2c1efc3-bb86-4656-dadc-8df00cddbc6a"
      },
      "source": [
        "dfs = pd.read_csv(\"STX.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "dfs"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2767</td>\n",
              "      <td>US1.STX</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>83.16</td>\n",
              "      <td>85.00</td>\n",
              "      <td>81.38</td>\n",
              "      <td>84.41</td>\n",
              "      <td>43575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.STX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>83.38</td>\n",
              "      <td>83.69</td>\n",
              "      <td>82.50</td>\n",
              "      <td>82.56</td>\n",
              "      <td>40131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.STX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>83.86</td>\n",
              "      <td>84.45</td>\n",
              "      <td>82.50</td>\n",
              "      <td>83.05</td>\n",
              "      <td>36683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.STX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>86.20</td>\n",
              "      <td>87.00</td>\n",
              "      <td>83.96</td>\n",
              "      <td>84.17</td>\n",
              "      <td>65032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2763</td>\n",
              "      <td>US1.STX</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>86.01</td>\n",
              "      <td>88.17</td>\n",
              "      <td>86.01</td>\n",
              "      <td>86.89</td>\n",
              "      <td>36916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2763</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.STX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>12.23</td>\n",
              "      <td>12.41</td>\n",
              "      <td>11.92</td>\n",
              "      <td>12.20</td>\n",
              "      <td>19259065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.STX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>11.66</td>\n",
              "      <td>11.93</td>\n",
              "      <td>11.38</td>\n",
              "      <td>11.88</td>\n",
              "      <td>15252399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.STX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>11.78</td>\n",
              "      <td>11.92</td>\n",
              "      <td>11.42</td>\n",
              "      <td>11.49</td>\n",
              "      <td>20353875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.STX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>11.68</td>\n",
              "      <td>11.95</td>\n",
              "      <td>11.29</td>\n",
              "      <td>11.94</td>\n",
              "      <td>18677214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.STX</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>11.63</td>\n",
              "      <td>11.67</td>\n",
              "      <td>11.30</td>\n",
              "      <td>11.48</td>\n",
              "      <td>10203081</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2768 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...  <HIGH>  <LOW>  <CLOSE>     <VOL>\n",
              "0      2767  US1.STX     D  20211001  ...   85.00  81.38    84.41     43575\n",
              "1      2766  US1.STX     D  20210930  ...   83.69  82.50    82.56     40131\n",
              "2      2765  US1.STX     D  20210929  ...   84.45  82.50    83.05     36683\n",
              "3      2764  US1.STX     D  20210928  ...   87.00  83.96    84.17     65032\n",
              "4      2763  US1.STX     D  20210927  ...   88.17  86.01    86.89     36916\n",
              "...     ...      ...   ...       ...  ...     ...    ...      ...       ...\n",
              "2763      4  US1.STX     D  20101008  ...   12.41  11.92    12.20  19259065\n",
              "2764      3  US1.STX     D  20101007  ...   11.93  11.38    11.88  15252399\n",
              "2765      2  US1.STX     D  20101006  ...   11.92  11.42    11.49  20353875\n",
              "2766      1  US1.STX     D  20101005  ...   11.95  11.29    11.94  18677214\n",
              "2767      0  US1.STX     D  20101004  ...   11.67  11.30    11.48  10203081\n",
              "\n",
              "[2768 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ef-MU4CY6We",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1677dd2c-7325-4d9a-c79a-c53c8ec5c814"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"b341c840-c80b-4586-bc3d-0cf17dbcf845\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"b341c840-c80b-4586-bc3d-0cf17dbcf845\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'b341c840-c80b-4586-bc3d-0cf17dbcf845',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [53.62, 53.67, 53.11, 52.24, 52.86, 53.78, 53.81, 53.68, 53.08, 51.85, 52.75, 52.39, 52.94, 56.76, 56.04, 55.7, 56.15, 56.31, 56.2, 55.32, 54.77, 54.35, 54.11, 52.18, 49.66, 50.22, 48.93, 47.16, 46.94, 47.09, 46.95, 47.59, 47.24, 46.38, 46.44, 46.02, 44.57, 44.64, 45.83, 44.81, 44.91, 45.15, 44.58, 44.7, 44.45, 44.94, 46.28, 46.33, 47.44, 47.48, 47.8, 48.08, 48.94, 47.72, 47.37, 47.31, 47.4, 47.01, 47.37, 48.35, 47.63, 46.66, 47.24, 46.04, 46.59, 47.26, 47.42, 47.41, 47.51, 47.12, 46.72, 47.25, 46.21, 47.32, 46.46, 46.18, 45.45, 45.54, 44.37, 44.27, 44.55, 43.64, 43.8, 43.93, 43.56, 44.41, 44.05, 44.15, 42.51, 41.85, 43.92, 43.73, 42.61, 43.19, 43.74, 44.72, 46.16, 44.72, 45.2159, 45.69, 45.6, 44.62, 44.04, 47.41, 47.57, 49.01, 48.56, 49.38, 49.61, 48.39, 48.32, 48.33, 44.94, 44.99, 46.08, 46.99, 48.21, 48.95, 50.49, 50.29, 50.645, 49.38, 49.66, 49.48, 49.72, 48.9, 49.37, 49.21, 49.23, 49.59, 48.75, 48.43, 47.9, 45.99, 45.82, 47.2, 46.84, 46.93, 48.96, 46.21, 47.27, 48.15, 47.68, 47.2, 47.68, 48.09, 48.63, 46.22, 45.18, 46.28, 46.29, 46.43, 46.35, 46.56, 46.82, 47.49, 48.06, 47.01, 45.12, 45.9, 45.88, 45.71, 45.78, 45.73, 45.37, 44.66, 45.02, 44.46, 45.24, 45.15, 45.57, 45.25, 44.29, 43.75, 42.78, 43.26, 43.66, 40.98, 38.76, 38.5, 40.23, 38.725, 39.72, 39.64, 40.13, 40.68, 40.6, 40.15, 39.13, 39.16, 38.14, 36.5, 39.08, 38.64, 38.36, 38.45, 37.91, 35.8684, 36.35, 36.58, 36.17, 37.32, 37.44, 38.54, 39.38, 39.99, 39.51, 39.8, 39.9, 41.65, 41.57, 44.77, 43.09, 42.65, 44.09, 44.09, 43.91, 42.9, 42.53, 42.12, 42.07, 43.3, 43.86, 42.72, 42.55, 41.88, 43.02, 45.35, 45.8, 45.42, 44.95, 44.27, 42.98, 40.23, 38.86, 37.83, 38.79, 42.6, 41.68, 43.47, 43.61, 43.82, 43.76, 44.13, 44.1, 42.75, 43.0, 43.34, 43.67, 44.01, 45.86, 46.81, 46.66, 47.79, 46.93, 46.97, 47.35, 46.58, 47.07, 48.5, 49.15, 49.27, 49.01, 48.09, 47.98, 47.94, 48.29, 48.21, 49.47, 48.98, 50.3, 49.8, 49.38, 49.37, 49.41, 53.52, 53.14, 55.99, 56.46, 56.36, 55.76, 55.73, 55.31, 54.8, 54.55, 54.26, 53.06, 52.9, 53.23, 51.01, 50.87, 51.03, 55.04, 54.87, 54.23, 53.88, 53.41, 53.07, 52.62, 54.0, 54.7, 57.54, 56.43, 55.73, 56.35, 56.47, 57.19, 58.1, 58.11, 58.45, 59.01, 59.03, 57.25, 58.89, 58.74, 58.6, 58.6, 56.7, 57.63, 56.46, 56.36, 55.68, 56.51, 55.83, 58.22, 58.45, 58.08, 57.94, 58.97, 58.44, 59.08, 56.94, 57.11, 57.94, 55.97, 55.24, 58.68, 59.18, 57.47, 57.89, 56.35, 58.03, 57.25, 57.47, 58.3, 58.28, 58.01, 57.25, 56.31, 59.01, 59.37, 57.77, 57.08, 56.71, 57.27, 57.23, 56.49, 55.94, 55.68, 54.97, 54.7, 54.22, 57.89, 59.52, 59.34, 58.3, 58.27, 58.22, 59.63, 59.91, 62.111, 61.9, 61.14, 60.61, 59.87, 58.57, 59.34, 56.27, 56.63, 58.24, 58.93, 58.3, 57.76, 58.52, 56.9, 57.37, 58.14, 56.28, 59.03, 59.9, 59.9, 60.41, 60.09, 59.56, 59.93, 59.68, 60.21, 60.18, 58.88, 56.29, 56.23, 55.14, 54.12, 53.98, 53.415, 52.86, 53.54, 53.3, 51.365, 51.43, 52.13, 51.41, 51.52, 51.37, 49.63, 49.16, 47.82, 47.44, 49.53, 48.7, 49.65, 51.77, 54.45, 55.2, 54.98, 55.12, 54.93, 53.18, 52.96, 54.06, 53.16, 52.62, 52.01, 52.14, 51.505, 51.25, 49.0, 47.06, 46.42, 46.01, 42.93, 42.63, 42.83, 42.93, 41.85, 42.02, 42.095, 42.14, 42.18, 41.95, 41.54, 42.18, 43.0, 42.04, 41.95, 42.1, 42.12, 40.46, 40.49, 39.5901, 39.17, 39.13, 39.21, 39.1, 38.57, 38.755, 39.9801, 39.08, 40.29, 40.34, 40.15, 39.86, 39.42, 39.13, 37.81, 37.49, 37.66, 37.98, 37.41, 37.71, 37.12, 37.45, 36.34, 36.77, 36.97, 36.98, 36.67, 36.89, 38.18, 38.16, 38.85, 39.3501, 34.93, 34.35, 34.26, 33.97, 34.5, 34.4, 33.62, 33.5, 33.76, 33.85, 33.79, 34.11, 33.98, 34.06, 34.2, 33.18, 32.98, 33.17, 32.875, 33.16, 33.58, 32.72, 32.385, 32.18, 33.52, 33.53, 32.65, 32.78, 31.9, 30.94, 32.0, 32.34, 32.78, 32.75, 31.79, 31.54, 31.5, 31.45, 31.48, 31.26, 31.01, 31.01, 32.28, 31.46, 31.37, 31.51, 32.47, 32.29, 32.56, 32.3, 31.54, 32.61, 32.99, 33.48, 33.43, 33.58, 33.43, 33.39, 32.96, 32.6, 32.55, 32.34, 33.14, 39.78, 39.59, 39.43, 39.24, 38.82, 39.07, 39.49, 38.32, 39.69, 38.72, 38.39, 38.2, 37.29, 39.0148, 38.59, 38.75, 38.91, 40.17, 39.51, 42.4, 42.67, 42.01, 41.66, 41.04, 41.83, 42.02, 41.51, 41.28, 42.03, 41.54, 42.37, 42.77, 41.94, 41.89, 42.09, 43.22, 42.88, 43.56, 43.24, 42.52, 43.11, 42.73, 42.63, 42.77, 42.51, 42.26, 42.16, 44.0, 43.48, 43.04, 42.475, 43.11, 42.83, 42.87, 43.55, 42.575, 42.52, 42.44, 42.18, 42.13, 42.3, 42.03, 50.53, 49.42, 48.265, 48.55, 48.56, 48.54, 48.49, 48.17, 48.34, 49.1, 47.605, 46.85, 46.245, 46.0, 45.97, 45.3, 45.93, 47.13, 45.67, 45.28, 44.91, 45.24, 45.13, 45.01, 44.51, 46.08, 46.88, 47.23, 47.06, 46.7, 47.57, 47.28, 46.68, 48.1, 48.28, 48.51, 48.97, 48.92, 49.48, 48.19, 47.69, 47.75, 47.39, 47.35, 47.43, 47.14, 47.57, 48.14, 47.87, 47.81, 46.46, 46.36, 45.8, 46.09, 45.44, 46.28, 45.27, 44.79, 45.15, 44.9, 44.5, 43.89, 42.66, 37.42, 36.37, 36.47, 36.33, 37.24, 37.02, 36.9, 36.75, 36.85, 37.64, 38.04, 38.49, 39.055, 39.12, 39.625, 38.18, 38.02, 38.8, 39.375, 39.11, 39.02, 38.91, 38.76, 38.72, 39.1, 40.61, 40.2, 40.85, 39.16, 39.38, 39.22, 40.26, 39.15, 39.16, 38.87, 38.11, 40.1, 39.5, 39.37, 39.07, 38.82, 38.95, 39.82, 39.29, 39.23, 38.26, 38.51, 37.99, 37.25, 36.17, 34.77, 34.49, 34.65, 33.0, 33.07, 32.64, 33.71, 34.32, 34.16, 34.24, 34.84, 34.26, 34.53, 34.5, 34.97, 34.32, 35.34, 35.01, 35.16, 35.17, 35.23, 35.1, 37.97, 37.88, 37.81, 38.54, 38.24, 38.41, 38.55, 37.79, 38.21, 38.43, 37.21, 36.46, 36.4, 36.68, 35.53, 35.95, 36.41, 36.46, 36.07, 36.16, 36.55, 36.1, 36.31, 36.5, 34.47, 33.92, 33.72, 33.73, 33.92, 33.72, 33.33, 32.05, 31.58, 32.12, 31.87, 32.33, 32.07, 31.78, 32.46, 32.0, 31.55, 31.75, 31.38, 32.49, 32.61, 32.13, 31.26, 30.72, 30.65, 32.415, 32.03, 32.59, 32.05, 32.45, 31.44, 31.58, 31.22, 30.5, 29.99, 30.16, 28.89, 29.18, 29.84, 29.35, 24.09, 24.01, 23.73, 23.52, 23.17, 24.03, 24.35, 23.94, 22.525, 20.865, 23.16, 24.48, 23.88, 23.86, 23.43, 23.21, 22.69, 22.71, 22.76, 22.68, 23.14, 24.14, 24.25, 24.65, 23.89, 23.64, 23.755, 23.01, 22.56, 21.66, 21.55, 21.48, 20.88, 20.8, 20.41, 20.52, 20.24, 19.635, 19.15, 19.09, 18.79, 18.72, 19.25, 18.86, 19.07, 20.01, 20.155, 19.44, 20.48, 21.7, 26.9, 27.34, 27.0, 26.3, 25.32, 25.43, 25.67, 25.33, 25.78, 25.59, 27.11, 33.93, 35.04, 34.98, 33.22, 33.11, 33.72, 32.86, 33.55, 33.69, 34.45, 34.16, 33.81, 33.55, 33.23, 34.81, 37.04, 36.54, 36.58, 34.93, 34.49, 34.62, 35.36, 36.17, 34.95, 34.21, 34.24, 35.48, 34.53, 34.76, 33.935, 33.23, 31.36, 31.7, 30.92, 31.08, 30.75, 32.22, 31.34, 33.1, 31.97, 31.33, 29.87, 29.02, 30.19, 29.41, 30.36, 31.6, 32.2, 30.52, 28.6, 30.41, 29.05, 26.76, 26.97, 27.75, 26.4, 27.93, 27.57, 29.21, 29.83, 30.87, 32.11, 31.08, 31.575, 31.35, 32.52, 34.52, 34.75, 36.34, 36.27, 36.65, 37.18, 37.15, 36.74, 36.65, 36.63, 35.45, 34.7, 34.1045, 33.91, 34.78, 34.25, 33.66, 35.9899, 36.225, 35.72, 35.43, 35.1, 34.45, 33.77, 34.545, 36.52, 35.94, 35.19, 34.57, 34.27, 34.71, 34.4, 34.28, 34.38, 33.89, 34.3, 33.07, 34.07, 36.11, 36.54, 38.28, 38.82, 38.86, 38.81, 40.38, 38.88, 38.07, 38.06, 39.34, 39.18, 39.55, 41.15, 39.24, 37.13, 38.52, 38.74, 39.45, 41.3938, 47.8, 48.61, 49.0, 49.03, 48.34, 47.3, 46.2, 44.85, 43.18, 42.86, 44.78, 41.77, 42.06, 42.89, 44.1, 43.56, 43.8, 46.19, 45.6, 47.76, 49.16, 48.62, 48.33, 48.93, 49.21, 50.11, 49.61, 48.19, 49.48, 49.77, 50.61, 51.39, 50.68, 50.05, 47.9, 46.33, 47.63, 48.73, 48.65, 49.42, 52.05, 52.45, 52.0, 51.37, 51.63, 51.01, 51.15, 49.48, 51.32, 51.87, 51.38, 51.46, 50.6, 51.29, 49.42, 48.88, 47.9, 47.32, 48.02, 49.06, 48.11, 47.72, 48.15, 47.91, 47.31, 47.61, 46.28, 45.94, 46.06, 46.4, 47.82, 47.02, 48.15, 47.575, 47.51, 49.03, 50.06, 50.68, 51.98, 52.29, 53.87, 53.34, 54.14, 53.18, 53.2, 53.36, 53.54, 54.3, 53.59, 52.63, 53.05, 54.42, 54.57, 55.66, 55.24, 55.21, 55.65, 56.36, 55.35, 54.58, 55.78, 55.37, 54.6, 55.14, 56.88, 56.9, 57.42, 56.59, 56.16, 56.76, 57.27, 56.43, 56.3, 57.7, 59.22, 59.61, 58.72, 58.16, 59.0, 57.91, 58.33, 58.75, 58.23, 58.41, 59.14, 57.44, 55.97, 56.66, 55.77, 55.39, 55.59, 54.7, 53.31, 53.755, 52.74, 52.44, 51.79, 52.03, 53.08, 52.755, 53.52, 53.61, 55.33, 55.85, 56.5, 55.7, 55.31, 53.66, 54.84, 53.75, 54.54, 54.36, 54.41, 56.97, 56.91, 58.42, 58.09, 58.83, 61.56, 61.11, 60.69, 60.02, 60.7, 61.99, 62.19, 61.46, 61.18, 61.14, 61.82, 61.17, 60.8, 60.2, 60.39, 59.94, 60.41, 59.92, 59.25, 57.8, 56.4399, 58.68, 57.53, 57.07, 59.06, 63.98, 63.94, 63.29, 63.12, 63.41, 62.29, 64.59, 64.74, 65.05, 66.44, 65.43, 64.26, 63.84, 65.64, 66.01, 66.5, 67.26, 68.19, 68.76, 68.82, 68.65, 68.63, 68.41, 67.61, 64.99, 63.28, 63.71, 64.04, 65.51, 65.22, 65.77, 66.24, 66.39, 66.09, 66.84, 67.53, 65.94, 66.2484, 65.85, 65.99, 66.03, 64.93, 65.54, 64.63, 64.65, 64.0, 62.88, 62.12, 62.315, 61.81, 61.76, 61.57, 62.535, 63.75, 63.885, 64.18, 62.82, 60.14, 60.22, 59.47, 58.73, 58.35, 56.51, 54.28, 55.205, 53.85, 52.945, 52.37, 51.96, 52.56, 52.99, 54.94, 56.24, 57.06, 55.24, 56.15, 56.09, 55.51, 55.11, 57.27, 57.25, 56.78, 56.26, 57.445, 56.78, 57.6, 58.31, 59.795, 58.97, 59.245, 58.84, 61.0, 60.87, 61.25, 60.82, 61.29, 61.31, 60.96, 61.69, 61.83, 62.58, 62.1, 61.18, 60.65, 60.96, 60.365, 60.43, 59.84, 59.58, 59.22, 58.75, 58.38, 57.935, 56.8, 56.99, 56.97, 56.84, 56.93, 57.38, 58.7, 58.19, 58.6, 60.09, 59.39, 59.14, 59.5, 59.58, 59.19, 59.24, 58.57, 58.8, 59.5, 61.31, 59.68, 59.67, 59.45, 58.83, 58.48, 58.69, 58.96, 59.11, 59.03, 58.87, 56.81, 56.925, 56.71, 56.77, 56.0, 55.95, 56.27, 56.58, 57.11, 56.81, 55.91, 55.36, 54.22, 54.5, 54.52, 54.66, 55.5, 54.37, 53.63, 53.55, 53.82, 53.74, 53.06, 53.12, 53.36, 51.95, 51.61, 51.16, 50.88, 51.71, 51.19, 50.89, 51.25, 50.77, 49.79, 49.54, 49.28, 49.98, 50.26, 50.51, 51.03, 50.73, 52.58, 53.5, 52.59, 52.73, 53.94, 54.11, 55.65, 55.69, 55.57, 55.65, 54.87, 54.82, 53.39, 53.96, 55.69, 55.54, 54.96, 56.06, 57.38, 57.16, 57.0, 56.17, 54.92, 54.8, 55.56, 55.13, 53.0, 53.5, 53.11, 51.67, 51.92, 50.72, 50.16, 49.75, 50.75, 49.88, 50.13, 49.29, 50.11, 52.56, 53.19, 52.29, 52.19, 51.94, 51.73, 50.69, 51.15, 50.76, 50.02, 49.04, 49.71, 50.14, 49.6, 49.21, 49.37, 49.89, 49.52, 49.19, 49.11, 49.99, 51.09, 52.87, 53.11, 53.66, 51.515, 58.06, 58.56, 60.74, 61.0, 61.24, 61.19, 60.665, 60.84, 60.32, 58.54, 58.39, 58.02, 59.63, 58.8, 56.94, 56.83, 55.43, 56.16, 56.53, 55.65, 56.01, 55.9, 56.01, 55.68, 52.49, 52.67, 52.97, 51.34, 49.99, 49.41, 49.57, 50.94, 50.71, 51.41, 49.85, 49.96, 49.62, 49.85, 49.02, 49.01, 48.79, 48.45, 48.34, 48.22, 47.72, 47.86, 48.51, 48.95, 50.09, 48.77, 47.8, 47.9, 48.17, 47.04, 48.48, 48.49, 49.26, 49.22, 48.67, 49.51, 48.25, 49.81, 49.8, 48.56, 48.09, 49.13]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b341c840-c80b-4586-bc3d-0cf17dbcf845');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"b2de36ed-2928-47f8-9e76-4b2c3733d8da\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"b2de36ed-2928-47f8-9e76-4b2c3733d8da\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'b2de36ed-2928-47f8-9e76-4b2c3733d8da',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b2de36ed-2928-47f8-9e76-4b2c3733d8da');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUWgGHO3Y6We"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tQNHd60Y6We",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e29174b-8e47-4ed0-bc6a-d0f97a2379d4"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .2, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"STX\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6957 - accuracy: 0.5007 - val_loss: 0.6993 - val_accuracy: 0.3531\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6674 - accuracy: 0.5926 - val_loss: 0.5859 - val_accuracy: 0.7388\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6353 - accuracy: 0.6732 - val_loss: 0.6101 - val_accuracy: 0.6898\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6274 - accuracy: 0.6456 - val_loss: 0.5332 - val_accuracy: 0.7592\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6103 - accuracy: 0.6718 - val_loss: 0.5403 - val_accuracy: 0.7469\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6749 - accuracy: 0.5752 - val_loss: 0.5725 - val_accuracy: 0.7531\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5907 - accuracy: 0.6812 - val_loss: 0.5382 - val_accuracy: 0.7367\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5753 - accuracy: 0.7228 - val_loss: 0.4936 - val_accuracy: 0.7755\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5703 - accuracy: 0.7013 - val_loss: 0.4913 - val_accuracy: 0.7755\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5643 - accuracy: 0.7248 - val_loss: 0.4948 - val_accuracy: 0.7694\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.816615\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.821292\n",
            "[2]\tvalidation_0-auc:0.81884\n",
            "[3]\tvalidation_0-auc:0.820764\n",
            "[4]\tvalidation_0-auc:0.81925\n",
            "[5]\tvalidation_0-auc:0.820891\n",
            "[6]\tvalidation_0-auc:0.820372\n",
            "[7]\tvalidation_0-auc:0.821292\n",
            "[8]\tvalidation_0-auc:0.821055\n",
            "[9]\tvalidation_0-auc:0.821694\n",
            "[10]\tvalidation_0-auc:0.823727\n",
            "[11]\tvalidation_0-auc:0.822879\n",
            "[12]\tvalidation_0-auc:0.823936\n",
            "[13]\tvalidation_0-auc:0.82824\n",
            "[14]\tvalidation_0-auc:0.827902\n",
            "[15]\tvalidation_0-auc:0.828112\n",
            "[16]\tvalidation_0-auc:0.828687\n",
            "[17]\tvalidation_0-auc:0.82865\n",
            "[18]\tvalidation_0-auc:0.828677\n",
            "[19]\tvalidation_0-auc:0.829252\n",
            "[20]\tvalidation_0-auc:0.829288\n",
            "[21]\tvalidation_0-auc:0.828924\n",
            "[22]\tvalidation_0-auc:0.828933\n",
            "[23]\tvalidation_0-auc:0.829398\n",
            "[24]\tvalidation_0-auc:0.829398\n",
            "[25]\tvalidation_0-auc:0.830483\n",
            "[26]\tvalidation_0-auc:0.829808\n",
            "[27]\tvalidation_0-auc:0.8302\n",
            "[28]\tvalidation_0-auc:0.830319\n",
            "[29]\tvalidation_0-auc:0.830091\n",
            "[30]\tvalidation_0-auc:0.830765\n",
            "[31]\tvalidation_0-auc:0.830847\n",
            "[32]\tvalidation_0-auc:0.830501\n",
            "[33]\tvalidation_0-auc:0.830656\n",
            "[34]\tvalidation_0-auc:0.830638\n",
            "[35]\tvalidation_0-auc:0.831221\n",
            "[36]\tvalidation_0-auc:0.832096\n",
            "[37]\tvalidation_0-auc:0.832251\n",
            "[38]\tvalidation_0-auc:0.831923\n",
            "[39]\tvalidation_0-auc:0.831704\n",
            "[40]\tvalidation_0-auc:0.831832\n",
            "[41]\tvalidation_0-auc:0.831887\n",
            "[42]\tvalidation_0-auc:0.83278\n",
            "[43]\tvalidation_0-auc:0.832561\n",
            "[44]\tvalidation_0-auc:0.832343\n",
            "[45]\tvalidation_0-auc:0.832124\n",
            "[46]\tvalidation_0-auc:0.831978\n",
            "[47]\tvalidation_0-auc:0.832169\n",
            "[48]\tvalidation_0-auc:0.831914\n",
            "[49]\tvalidation_0-auc:0.830729\n",
            "[50]\tvalidation_0-auc:0.831312\n",
            "[51]\tvalidation_0-auc:0.830236\n",
            "[52]\tvalidation_0-auc:0.830474\n",
            "[53]\tvalidation_0-auc:0.830027\n",
            "[54]\tvalidation_0-auc:0.830355\n",
            "[55]\tvalidation_0-auc:0.830391\n",
            "[56]\tvalidation_0-auc:0.830446\n",
            "[57]\tvalidation_0-auc:0.830337\n",
            "[58]\tvalidation_0-auc:0.830246\n",
            "[59]\tvalidation_0-auc:0.829972\n",
            "[60]\tvalidation_0-auc:0.829753\n",
            "[61]\tvalidation_0-auc:0.830629\n",
            "[62]\tvalidation_0-auc:0.830483\n",
            "[63]\tvalidation_0-auc:0.83061\n",
            "[64]\tvalidation_0-auc:0.831048\n",
            "[65]\tvalidation_0-auc:0.830556\n",
            "[66]\tvalidation_0-auc:0.830227\n",
            "[67]\tvalidation_0-auc:0.8303\n",
            "[68]\tvalidation_0-auc:0.829635\n",
            "[69]\tvalidation_0-auc:0.829544\n",
            "[70]\tvalidation_0-auc:0.829507\n",
            "[71]\tvalidation_0-auc:0.829489\n",
            "[72]\tvalidation_0-auc:0.829416\n",
            "[73]\tvalidation_0-auc:0.829379\n",
            "[74]\tvalidation_0-auc:0.829252\n",
            "[75]\tvalidation_0-auc:0.829033\n",
            "[76]\tvalidation_0-auc:0.829471\n",
            "[77]\tvalidation_0-auc:0.829762\n",
            "[78]\tvalidation_0-auc:0.82958\n",
            "[79]\tvalidation_0-auc:0.829197\n",
            "[80]\tvalidation_0-auc:0.828978\n",
            "[81]\tvalidation_0-auc:0.828869\n",
            "[82]\tvalidation_0-auc:0.829434\n",
            "[83]\tvalidation_0-auc:0.829106\n",
            "[84]\tvalidation_0-auc:0.828431\n",
            "[85]\tvalidation_0-auc:0.828413\n",
            "[86]\tvalidation_0-auc:0.827866\n",
            "[87]\tvalidation_0-auc:0.828067\n",
            "[88]\tvalidation_0-auc:0.827921\n",
            "[89]\tvalidation_0-auc:0.827757\n",
            "[90]\tvalidation_0-auc:0.827793\n",
            "[91]\tvalidation_0-auc:0.827793\n",
            "[92]\tvalidation_0-auc:0.827629\n",
            "Stopping. Best iteration:\n",
            "[42]\tvalidation_0-auc:0.83278\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6950 - accuracy: 0.5045 - val_loss: 0.6808 - val_accuracy: 0.6214\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6910 - accuracy: 0.5333 - val_loss: 0.6637 - val_accuracy: 0.6214\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6623 - accuracy: 0.6019 - val_loss: 0.6077 - val_accuracy: 0.6958\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6117 - accuracy: 0.6767 - val_loss: 0.6396 - val_accuracy: 0.6433\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5937 - accuracy: 0.6911 - val_loss: 0.6289 - val_accuracy: 0.6499\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6819 - accuracy: 0.5635 - val_loss: 0.6372 - val_accuracy: 0.7352\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6265 - accuracy: 0.6719 - val_loss: 0.5634 - val_accuracy: 0.7177\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5809 - accuracy: 0.7179 - val_loss: 0.5333 - val_accuracy: 0.7418\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5827 - accuracy: 0.7152 - val_loss: 0.5350 - val_accuracy: 0.7112\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5690 - accuracy: 0.7200 - val_loss: 0.5275 - val_accuracy: 0.7330\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.740118\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.737951\n",
            "[2]\tvalidation_0-auc:0.727998\n",
            "[3]\tvalidation_0-auc:0.742958\n",
            "[4]\tvalidation_0-auc:0.739142\n",
            "[5]\tvalidation_0-auc:0.751272\n",
            "[6]\tvalidation_0-auc:0.755902\n",
            "[7]\tvalidation_0-auc:0.772867\n",
            "[8]\tvalidation_0-auc:0.756198\n",
            "[9]\tvalidation_0-auc:0.761245\n",
            "[10]\tvalidation_0-auc:0.765937\n",
            "[11]\tvalidation_0-auc:0.767504\n",
            "[12]\tvalidation_0-auc:0.766792\n",
            "[13]\tvalidation_0-auc:0.771463\n",
            "[14]\tvalidation_0-auc:0.769366\n",
            "[15]\tvalidation_0-auc:0.772989\n",
            "[16]\tvalidation_0-auc:0.774414\n",
            "[17]\tvalidation_0-auc:0.777508\n",
            "[18]\tvalidation_0-auc:0.779319\n",
            "[19]\tvalidation_0-auc:0.781293\n",
            "[20]\tvalidation_0-auc:0.786667\n",
            "[21]\tvalidation_0-auc:0.787664\n",
            "[22]\tvalidation_0-auc:0.786392\n",
            "[23]\tvalidation_0-auc:0.788712\n",
            "[24]\tvalidation_0-auc:0.78973\n",
            "[25]\tvalidation_0-auc:0.786717\n",
            "[26]\tvalidation_0-auc:0.79089\n",
            "[27]\tvalidation_0-auc:0.793098\n",
            "[28]\tvalidation_0-auc:0.795144\n",
            "[29]\tvalidation_0-auc:0.792864\n",
            "[30]\tvalidation_0-auc:0.795408\n",
            "[31]\tvalidation_0-auc:0.796528\n",
            "[32]\tvalidation_0-auc:0.795815\n",
            "[33]\tvalidation_0-auc:0.794238\n",
            "[34]\tvalidation_0-auc:0.791582\n",
            "[35]\tvalidation_0-auc:0.788071\n",
            "[36]\tvalidation_0-auc:0.789251\n",
            "[37]\tvalidation_0-auc:0.786962\n",
            "[38]\tvalidation_0-auc:0.785913\n",
            "[39]\tvalidation_0-auc:0.785771\n",
            "[40]\tvalidation_0-auc:0.785567\n",
            "[41]\tvalidation_0-auc:0.787257\n",
            "[42]\tvalidation_0-auc:0.78454\n",
            "[43]\tvalidation_0-auc:0.785608\n",
            "[44]\tvalidation_0-auc:0.785466\n",
            "[45]\tvalidation_0-auc:0.784061\n",
            "[46]\tvalidation_0-auc:0.784713\n",
            "[47]\tvalidation_0-auc:0.786524\n",
            "[48]\tvalidation_0-auc:0.789302\n",
            "[49]\tvalidation_0-auc:0.789282\n",
            "[50]\tvalidation_0-auc:0.787511\n",
            "[51]\tvalidation_0-auc:0.785842\n",
            "[52]\tvalidation_0-auc:0.784845\n",
            "[53]\tvalidation_0-auc:0.784519\n",
            "[54]\tvalidation_0-auc:0.784967\n",
            "[55]\tvalidation_0-auc:0.784764\n",
            "[56]\tvalidation_0-auc:0.78515\n",
            "[57]\tvalidation_0-auc:0.783664\n",
            "[58]\tvalidation_0-auc:0.782229\n",
            "[59]\tvalidation_0-auc:0.782616\n",
            "[60]\tvalidation_0-auc:0.782494\n",
            "[61]\tvalidation_0-auc:0.782698\n",
            "[62]\tvalidation_0-auc:0.782759\n",
            "[63]\tvalidation_0-auc:0.782474\n",
            "[64]\tvalidation_0-auc:0.779604\n",
            "[65]\tvalidation_0-auc:0.77995\n",
            "[66]\tvalidation_0-auc:0.778118\n",
            "[67]\tvalidation_0-auc:0.777701\n",
            "[68]\tvalidation_0-auc:0.777599\n",
            "[69]\tvalidation_0-auc:0.778596\n",
            "[70]\tvalidation_0-auc:0.776174\n",
            "[71]\tvalidation_0-auc:0.774322\n",
            "[72]\tvalidation_0-auc:0.775482\n",
            "[73]\tvalidation_0-auc:0.776195\n",
            "[74]\tvalidation_0-auc:0.776459\n",
            "[75]\tvalidation_0-auc:0.777212\n",
            "[76]\tvalidation_0-auc:0.776622\n",
            "[77]\tvalidation_0-auc:0.775075\n",
            "[78]\tvalidation_0-auc:0.774628\n",
            "[79]\tvalidation_0-auc:0.774628\n",
            "[80]\tvalidation_0-auc:0.773101\n",
            "[81]\tvalidation_0-auc:0.771005\n",
            "Stopping. Best iteration:\n",
            "[31]\tvalidation_0-auc:0.796528\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.746938775510204  | 0.6310160427807486 | 0.6820809248554913 | 0.6555555555555556 |\n",
            "|     GRU 0.1      | 0.7693877551020408 |        0.7         | 0.6069364161849711 | 0.6501547987616099 |\n",
            "|   XGBoost 0.1    | 0.7571428571428571 | 0.6534090909090909 | 0.6647398843930635 | 0.6590257879656161 |\n",
            "|    Logreg 0.1    | 0.7448979591836735 | 0.6348314606741573 | 0.653179190751445  | 0.6438746438746439 |\n",
            "|     SVM 0.1      | 0.7571428571428571 | 0.651685393258427  | 0.6705202312138728 | 0.6609686609686609 |\n",
            "|  LSTM beta 0.1   | 0.649890590809628  | 0.525096525096525  | 0.7861271676300579 | 0.6296296296296297 |\n",
            "|   GRU beta 0.1   | 0.7330415754923414 | 0.7142857142857143 | 0.4913294797687861 | 0.5821917808219178 |\n",
            "| XGBoost beta 0.1 | 0.7199124726477024 | 0.6285714285714286 | 0.6358381502890174 | 0.632183908045977  |\n",
            "| logreg beta 0.1  | 0.7177242888402626 | 0.6309523809523809 | 0.6127167630057804 | 0.6217008797653959 |\n",
            "|   svm beta 0.1   | 0.7045951859956237 | 0.6079545454545454 | 0.6184971098265896 | 0.6131805157593122 |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6928 - accuracy: 0.5309 - val_loss: 0.6809 - val_accuracy: 0.7612\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6685 - accuracy: 0.6174 - val_loss: 0.6673 - val_accuracy: 0.6286\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6379 - accuracy: 0.6544 - val_loss: 0.5525 - val_accuracy: 0.7510\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6367 - accuracy: 0.6477 - val_loss: 0.5871 - val_accuracy: 0.7592\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6341 - accuracy: 0.6564 - val_loss: 0.5865 - val_accuracy: 0.7388\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6719 - accuracy: 0.5705 - val_loss: 0.5948 - val_accuracy: 0.7347\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6252 - accuracy: 0.6779 - val_loss: 0.5616 - val_accuracy: 0.7388\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6126 - accuracy: 0.6691 - val_loss: 0.5162 - val_accuracy: 0.7714\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6146 - accuracy: 0.6584 - val_loss: 0.5160 - val_accuracy: 0.7776\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6054 - accuracy: 0.6819 - val_loss: 0.5295 - val_accuracy: 0.7551\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.773453\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.790804\n",
            "[2]\tvalidation_0-auc:0.796306\n",
            "[3]\tvalidation_0-auc:0.793243\n",
            "[4]\tvalidation_0-auc:0.793333\n",
            "[5]\tvalidation_0-auc:0.797142\n",
            "[6]\tvalidation_0-auc:0.793716\n",
            "[7]\tvalidation_0-auc:0.79421\n",
            "[8]\tvalidation_0-auc:0.794281\n",
            "[9]\tvalidation_0-auc:0.791147\n",
            "[10]\tvalidation_0-auc:0.792104\n",
            "[11]\tvalidation_0-auc:0.793716\n",
            "[12]\tvalidation_0-auc:0.792386\n",
            "[13]\tvalidation_0-auc:0.792034\n",
            "[14]\tvalidation_0-auc:0.793616\n",
            "[15]\tvalidation_0-auc:0.792598\n",
            "[16]\tvalidation_0-auc:0.79156\n",
            "[17]\tvalidation_0-auc:0.792578\n",
            "[18]\tvalidation_0-auc:0.793041\n",
            "[19]\tvalidation_0-auc:0.793948\n",
            "[20]\tvalidation_0-auc:0.793696\n",
            "[21]\tvalidation_0-auc:0.79417\n",
            "[22]\tvalidation_0-auc:0.793908\n",
            "[23]\tvalidation_0-auc:0.792054\n",
            "[24]\tvalidation_0-auc:0.792527\n",
            "[25]\tvalidation_0-auc:0.79283\n",
            "[26]\tvalidation_0-auc:0.792487\n",
            "[27]\tvalidation_0-auc:0.791782\n",
            "[28]\tvalidation_0-auc:0.791409\n",
            "[29]\tvalidation_0-auc:0.791288\n",
            "[30]\tvalidation_0-auc:0.790814\n",
            "[31]\tvalidation_0-auc:0.790552\n",
            "[32]\tvalidation_0-auc:0.788376\n",
            "[33]\tvalidation_0-auc:0.789797\n",
            "[34]\tvalidation_0-auc:0.789192\n",
            "[35]\tvalidation_0-auc:0.78895\n",
            "[36]\tvalidation_0-auc:0.789293\n",
            "[37]\tvalidation_0-auc:0.79026\n",
            "[38]\tvalidation_0-auc:0.792678\n",
            "[39]\tvalidation_0-auc:0.792628\n",
            "[40]\tvalidation_0-auc:0.791107\n",
            "[41]\tvalidation_0-auc:0.792054\n",
            "[42]\tvalidation_0-auc:0.792457\n",
            "[43]\tvalidation_0-auc:0.792477\n",
            "[44]\tvalidation_0-auc:0.791973\n",
            "[45]\tvalidation_0-auc:0.79157\n",
            "[46]\tvalidation_0-auc:0.791379\n",
            "[47]\tvalidation_0-auc:0.791016\n",
            "[48]\tvalidation_0-auc:0.790734\n",
            "[49]\tvalidation_0-auc:0.790925\n",
            "[50]\tvalidation_0-auc:0.791046\n",
            "[51]\tvalidation_0-auc:0.791268\n",
            "[52]\tvalidation_0-auc:0.790885\n",
            "[53]\tvalidation_0-auc:0.79151\n",
            "[54]\tvalidation_0-auc:0.792527\n",
            "[55]\tvalidation_0-auc:0.792427\n",
            "Stopping. Best iteration:\n",
            "[5]\tvalidation_0-auc:0.797142\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 20ms/step - loss: 0.6758 - accuracy: 0.5738 - val_loss: 0.7044 - val_accuracy: 0.5842\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6350 - accuracy: 0.6658 - val_loss: 0.8899 - val_accuracy: 0.4967\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6341 - accuracy: 0.6575 - val_loss: 0.6135 - val_accuracy: 0.6608\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5929 - accuracy: 0.7255 - val_loss: 0.5864 - val_accuracy: 0.6761\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5645 - accuracy: 0.7358 - val_loss: 0.6097 - val_accuracy: 0.7024\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6368 - accuracy: 0.6424 - val_loss: 0.5928 - val_accuracy: 0.7330\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5595 - accuracy: 0.7412 - val_loss: 0.5656 - val_accuracy: 0.7112\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5432 - accuracy: 0.7433 - val_loss: 0.6217 - val_accuracy: 0.7046\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5335 - accuracy: 0.7358 - val_loss: 0.6082 - val_accuracy: 0.7177\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5391 - accuracy: 0.7412 - val_loss: 0.6116 - val_accuracy: 0.7155\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.660717\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.672075\n",
            "[2]\tvalidation_0-auc:0.671741\n",
            "[3]\tvalidation_0-auc:0.677832\n",
            "[4]\tvalidation_0-auc:0.678912\n",
            "[5]\tvalidation_0-auc:0.678845\n",
            "[6]\tvalidation_0-auc:0.681484\n",
            "[7]\tvalidation_0-auc:0.67968\n",
            "[8]\tvalidation_0-auc:0.676651\n",
            "[9]\tvalidation_0-auc:0.703231\n",
            "[10]\tvalidation_0-auc:0.677375\n",
            "[11]\tvalidation_0-auc:0.69125\n",
            "[12]\tvalidation_0-auc:0.694802\n",
            "[13]\tvalidation_0-auc:0.695136\n",
            "[14]\tvalidation_0-auc:0.711483\n",
            "[15]\tvalidation_0-auc:0.706082\n",
            "[16]\tvalidation_0-auc:0.711995\n",
            "[17]\tvalidation_0-auc:0.715959\n",
            "[18]\tvalidation_0-auc:0.710425\n",
            "[19]\tvalidation_0-auc:0.713109\n",
            "[20]\tvalidation_0-auc:0.713198\n",
            "[21]\tvalidation_0-auc:0.715191\n",
            "[22]\tvalidation_0-auc:0.715814\n",
            "[23]\tvalidation_0-auc:0.719578\n",
            "[24]\tvalidation_0-auc:0.71724\n",
            "[25]\tvalidation_0-auc:0.718977\n",
            "[26]\tvalidation_0-auc:0.709467\n",
            "[27]\tvalidation_0-auc:0.706361\n",
            "[28]\tvalidation_0-auc:0.708499\n",
            "[29]\tvalidation_0-auc:0.70832\n",
            "[30]\tvalidation_0-auc:0.709612\n",
            "[31]\tvalidation_0-auc:0.708287\n",
            "[32]\tvalidation_0-auc:0.709167\n",
            "[33]\tvalidation_0-auc:0.709033\n",
            "[34]\tvalidation_0-auc:0.706316\n",
            "[35]\tvalidation_0-auc:0.706795\n",
            "[36]\tvalidation_0-auc:0.706973\n",
            "[37]\tvalidation_0-auc:0.706984\n",
            "[38]\tvalidation_0-auc:0.707541\n",
            "[39]\tvalidation_0-auc:0.707407\n",
            "[40]\tvalidation_0-auc:0.705024\n",
            "[41]\tvalidation_0-auc:0.70645\n",
            "[42]\tvalidation_0-auc:0.708788\n",
            "[43]\tvalidation_0-auc:0.707897\n",
            "[44]\tvalidation_0-auc:0.707519\n",
            "[45]\tvalidation_0-auc:0.708231\n",
            "[46]\tvalidation_0-auc:0.709478\n",
            "[47]\tvalidation_0-auc:0.70224\n",
            "[48]\tvalidation_0-auc:0.701884\n",
            "[49]\tvalidation_0-auc:0.701149\n",
            "[50]\tvalidation_0-auc:0.700637\n",
            "[51]\tvalidation_0-auc:0.699011\n",
            "[52]\tvalidation_0-auc:0.699212\n",
            "[53]\tvalidation_0-auc:0.695448\n",
            "[54]\tvalidation_0-auc:0.697096\n",
            "[55]\tvalidation_0-auc:0.697719\n",
            "[56]\tvalidation_0-auc:0.698254\n",
            "[57]\tvalidation_0-auc:0.698187\n",
            "[58]\tvalidation_0-auc:0.699412\n",
            "[59]\tvalidation_0-auc:0.700102\n",
            "[60]\tvalidation_0-auc:0.700214\n",
            "[61]\tvalidation_0-auc:0.699368\n",
            "[62]\tvalidation_0-auc:0.700214\n",
            "[63]\tvalidation_0-auc:0.695002\n",
            "[64]\tvalidation_0-auc:0.696561\n",
            "[65]\tvalidation_0-auc:0.694691\n",
            "[66]\tvalidation_0-auc:0.693778\n",
            "[67]\tvalidation_0-auc:0.694023\n",
            "[68]\tvalidation_0-auc:0.694067\n",
            "[69]\tvalidation_0-auc:0.695069\n",
            "[70]\tvalidation_0-auc:0.696205\n",
            "[71]\tvalidation_0-auc:0.696895\n",
            "[72]\tvalidation_0-auc:0.695292\n",
            "[73]\tvalidation_0-auc:0.698677\n",
            "Stopping. Best iteration:\n",
            "[23]\tvalidation_0-auc:0.719578\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|     LSTM 0.2     | 0.7387755102040816 | 0.543859649122807  |  0.6503496503496503 | 0.5923566878980893 |\n",
            "|     GRU 0.2      | 0.7551020408163265 | 0.583941605839416  |  0.5594405594405595 | 0.5714285714285715 |\n",
            "|   XGBoost 0.2    | 0.736734693877551  | 0.5426829268292683 |  0.6223776223776224 | 0.5798045602605864 |\n",
            "|    Logreg 0.2    | 0.7448979591836735 |      0.55625       |  0.6223776223776224 | 0.5874587458745875 |\n",
            "|     SVM 0.2      | 0.7510204081632653 | 0.5686274509803921 |  0.6083916083916084 | 0.5878378378378378 |\n",
            "|  LSTM beta 0.2   | 0.7024070021881839 |        0.52        |  0.6363636363636364 | 0.5723270440251573 |\n",
            "|   GRU beta 0.2   | 0.7155361050328227 | 0.5474452554744526 |  0.5244755244755245 | 0.5357142857142857 |\n",
            "| XGBoost beta 0.2 | 0.6980306345733042 | 0.5229357798165137 |  0.3986013986013986 | 0.4523809523809524 |\n",
            "| logreg beta 0.2  | 0.7155361050328227 | 0.5488721804511278 |  0.5104895104895105 | 0.5289855072463768 |\n",
            "|   svm beta 0.2   | 0.6892778993435449 | 0.5041322314049587 | 0.42657342657342656 | 0.4621212121212121 |\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6950 - accuracy: 0.5054 - val_loss: 0.6874 - val_accuracy: 0.7306\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6699 - accuracy: 0.6007 - val_loss: 0.6197 - val_accuracy: 0.7286\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6381 - accuracy: 0.6369 - val_loss: 0.6039 - val_accuracy: 0.7184\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6262 - accuracy: 0.6772 - val_loss: 0.5836 - val_accuracy: 0.7306\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6251 - accuracy: 0.6631 - val_loss: 0.5855 - val_accuracy: 0.7408\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6758 - accuracy: 0.5698 - val_loss: 0.5984 - val_accuracy: 0.7184\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6257 - accuracy: 0.6591 - val_loss: 0.5636 - val_accuracy: 0.7286\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6062 - accuracy: 0.6826 - val_loss: 0.5771 - val_accuracy: 0.7163\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5942 - accuracy: 0.6852 - val_loss: 0.5467 - val_accuracy: 0.7367\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6002 - accuracy: 0.6859 - val_loss: 0.5687 - val_accuracy: 0.7367\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.773817\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.78442\n",
            "[2]\tvalidation_0-auc:0.786919\n",
            "[3]\tvalidation_0-auc:0.786374\n",
            "[4]\tvalidation_0-auc:0.789358\n",
            "[5]\tvalidation_0-auc:0.796587\n",
            "[6]\tvalidation_0-auc:0.796475\n",
            "[7]\tvalidation_0-auc:0.796267\n",
            "[8]\tvalidation_0-auc:0.796994\n",
            "[9]\tvalidation_0-auc:0.797262\n",
            "[10]\tvalidation_0-auc:0.797314\n",
            "[11]\tvalidation_0-auc:0.794503\n",
            "[12]\tvalidation_0-auc:0.795792\n",
            "[13]\tvalidation_0-auc:0.79478\n",
            "[14]\tvalidation_0-auc:0.793967\n",
            "[15]\tvalidation_0-auc:0.794512\n",
            "[16]\tvalidation_0-auc:0.799173\n",
            "[17]\tvalidation_0-auc:0.799848\n",
            "[18]\tvalidation_0-auc:0.799294\n",
            "[19]\tvalidation_0-auc:0.798499\n",
            "[20]\tvalidation_0-auc:0.796994\n",
            "[21]\tvalidation_0-auc:0.795091\n",
            "[22]\tvalidation_0-auc:0.794175\n",
            "[23]\tvalidation_0-auc:0.793846\n",
            "[24]\tvalidation_0-auc:0.793543\n",
            "[25]\tvalidation_0-auc:0.792506\n",
            "[26]\tvalidation_0-auc:0.791113\n",
            "[27]\tvalidation_0-auc:0.79056\n",
            "[28]\tvalidation_0-auc:0.790127\n",
            "[29]\tvalidation_0-auc:0.790084\n",
            "[30]\tvalidation_0-auc:0.788856\n",
            "[31]\tvalidation_0-auc:0.788908\n",
            "[32]\tvalidation_0-auc:0.788493\n",
            "[33]\tvalidation_0-auc:0.788285\n",
            "[34]\tvalidation_0-auc:0.789859\n",
            "[35]\tvalidation_0-auc:0.790274\n",
            "[36]\tvalidation_0-auc:0.789375\n",
            "[37]\tvalidation_0-auc:0.787905\n",
            "[38]\tvalidation_0-auc:0.787749\n",
            "[39]\tvalidation_0-auc:0.787801\n",
            "[40]\tvalidation_0-auc:0.787126\n",
            "[41]\tvalidation_0-auc:0.786936\n",
            "[42]\tvalidation_0-auc:0.787593\n",
            "[43]\tvalidation_0-auc:0.788354\n",
            "[44]\tvalidation_0-auc:0.788545\n",
            "[45]\tvalidation_0-auc:0.788216\n",
            "[46]\tvalidation_0-auc:0.788726\n",
            "[47]\tvalidation_0-auc:0.788986\n",
            "[48]\tvalidation_0-auc:0.788432\n",
            "[49]\tvalidation_0-auc:0.78838\n",
            "[50]\tvalidation_0-auc:0.788225\n",
            "[51]\tvalidation_0-auc:0.787567\n",
            "[52]\tvalidation_0-auc:0.787671\n",
            "[53]\tvalidation_0-auc:0.78883\n",
            "[54]\tvalidation_0-auc:0.789842\n",
            "[55]\tvalidation_0-auc:0.790188\n",
            "[56]\tvalidation_0-auc:0.789392\n",
            "[57]\tvalidation_0-auc:0.787784\n",
            "[58]\tvalidation_0-auc:0.787887\n",
            "[59]\tvalidation_0-auc:0.787853\n",
            "[60]\tvalidation_0-auc:0.785795\n",
            "[61]\tvalidation_0-auc:0.786089\n",
            "[62]\tvalidation_0-auc:0.786192\n",
            "[63]\tvalidation_0-auc:0.785042\n",
            "[64]\tvalidation_0-auc:0.785284\n",
            "[65]\tvalidation_0-auc:0.785648\n",
            "[66]\tvalidation_0-auc:0.784956\n",
            "[67]\tvalidation_0-auc:0.784247\n",
            "Stopping. Best iteration:\n",
            "[17]\tvalidation_0-auc:0.799848\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6889 - accuracy: 0.5415 - val_loss: 0.6728 - val_accuracy: 0.6893\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6564 - accuracy: 0.6294 - val_loss: 0.6626 - val_accuracy: 0.6521\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6105 - accuracy: 0.6843 - val_loss: 0.6602 - val_accuracy: 0.6849\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5826 - accuracy: 0.7001 - val_loss: 0.6216 - val_accuracy: 0.6893\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5955 - accuracy: 0.6740 - val_loss: 0.5912 - val_accuracy: 0.7046\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6567 - accuracy: 0.6239 - val_loss: 0.6151 - val_accuracy: 0.6389\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5952 - accuracy: 0.6898 - val_loss: 0.6176 - val_accuracy: 0.6674\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5759 - accuracy: 0.7200 - val_loss: 0.5953 - val_accuracy: 0.6718\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5657 - accuracy: 0.7268 - val_loss: 0.5950 - val_accuracy: 0.6871\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5638 - accuracy: 0.7248 - val_loss: 0.5928 - val_accuracy: 0.7002\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.692972\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.713038\n",
            "[2]\tvalidation_0-auc:0.737062\n",
            "[3]\tvalidation_0-auc:0.735619\n",
            "[4]\tvalidation_0-auc:0.740806\n",
            "[5]\tvalidation_0-auc:0.745788\n",
            "[6]\tvalidation_0-auc:0.745515\n",
            "[7]\tvalidation_0-auc:0.743506\n",
            "[8]\tvalidation_0-auc:0.741868\n",
            "[9]\tvalidation_0-auc:0.74101\n",
            "[10]\tvalidation_0-auc:0.748879\n",
            "[11]\tvalidation_0-auc:0.751258\n",
            "[12]\tvalidation_0-auc:0.758161\n",
            "[13]\tvalidation_0-auc:0.760082\n",
            "[14]\tvalidation_0-auc:0.764381\n",
            "[15]\tvalidation_0-auc:0.764898\n",
            "[16]\tvalidation_0-auc:0.766282\n",
            "[17]\tvalidation_0-auc:0.765259\n",
            "[18]\tvalidation_0-auc:0.761583\n",
            "[19]\tvalidation_0-auc:0.766361\n",
            "[20]\tvalidation_0-auc:0.765668\n",
            "[21]\tvalidation_0-auc:0.76716\n",
            "[22]\tvalidation_0-auc:0.768272\n",
            "[23]\tvalidation_0-auc:0.7656\n",
            "[24]\tvalidation_0-auc:0.765395\n",
            "[25]\tvalidation_0-auc:0.764381\n",
            "[26]\tvalidation_0-auc:0.765181\n",
            "[27]\tvalidation_0-auc:0.764537\n",
            "[28]\tvalidation_0-auc:0.765454\n",
            "[29]\tvalidation_0-auc:0.765464\n",
            "[30]\tvalidation_0-auc:0.762324\n",
            "[31]\tvalidation_0-auc:0.762695\n",
            "[32]\tvalidation_0-auc:0.762402\n",
            "[33]\tvalidation_0-auc:0.764879\n",
            "[34]\tvalidation_0-auc:0.762617\n",
            "[35]\tvalidation_0-auc:0.761837\n",
            "[36]\tvalidation_0-auc:0.762324\n",
            "[37]\tvalidation_0-auc:0.759068\n",
            "[38]\tvalidation_0-auc:0.759828\n",
            "[39]\tvalidation_0-auc:0.759301\n",
            "[40]\tvalidation_0-auc:0.756572\n",
            "[41]\tvalidation_0-auc:0.75663\n",
            "[42]\tvalidation_0-auc:0.756981\n",
            "[43]\tvalidation_0-auc:0.756532\n",
            "[44]\tvalidation_0-auc:0.75624\n",
            "[45]\tvalidation_0-auc:0.755177\n",
            "[46]\tvalidation_0-auc:0.753705\n",
            "[47]\tvalidation_0-auc:0.753919\n",
            "[48]\tvalidation_0-auc:0.754573\n",
            "[49]\tvalidation_0-auc:0.755236\n",
            "[50]\tvalidation_0-auc:0.754553\n",
            "[51]\tvalidation_0-auc:0.754729\n",
            "[52]\tvalidation_0-auc:0.754553\n",
            "[53]\tvalidation_0-auc:0.755948\n",
            "[54]\tvalidation_0-auc:0.753422\n",
            "[55]\tvalidation_0-auc:0.753305\n",
            "[56]\tvalidation_0-auc:0.75193\n",
            "[57]\tvalidation_0-auc:0.75158\n",
            "[58]\tvalidation_0-auc:0.751326\n",
            "[59]\tvalidation_0-auc:0.751384\n",
            "[60]\tvalidation_0-auc:0.752496\n",
            "[61]\tvalidation_0-auc:0.753452\n",
            "[62]\tvalidation_0-auc:0.753471\n",
            "[63]\tvalidation_0-auc:0.753783\n",
            "[64]\tvalidation_0-auc:0.750253\n",
            "[65]\tvalidation_0-auc:0.75119\n",
            "[66]\tvalidation_0-auc:0.750039\n",
            "[67]\tvalidation_0-auc:0.749591\n",
            "[68]\tvalidation_0-auc:0.750546\n",
            "[69]\tvalidation_0-auc:0.75115\n",
            "[70]\tvalidation_0-auc:0.749668\n",
            "[71]\tvalidation_0-auc:0.748128\n",
            "[72]\tvalidation_0-auc:0.749298\n",
            "Stopping. Best iteration:\n",
            "[22]\tvalidation_0-auc:0.768272\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |        Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.7408163265306122 | 0.8034188034188035 | 0.47474747474747475 | 0.5968253968253968 |\n",
            "|      GRU 0.15     | 0.736734693877551  | 0.6994219653179191 |  0.6111111111111112 | 0.652291105121294  |\n",
            "|    XGBoost 0.15   | 0.7387755102040816 | 0.6966292134831461 |  0.6262626262626263 | 0.6595744680851064 |\n",
            "|    Logreg 0.15    | 0.7306122448979592 | 0.675531914893617  |  0.6414141414141414 | 0.6580310880829016 |\n",
            "|      SVM 0.15     | 0.7387755102040816 | 0.6988636363636364 |  0.6212121212121212 | 0.6577540106951871 |\n",
            "|   LSTM beta 0.15  | 0.7045951859956237 | 0.7032258064516129 |  0.5505050505050505 | 0.6175637393767706 |\n",
            "|   GRU beta 0.15   | 0.700218818380744  | 0.6942675159235668 |  0.5505050505050505 | 0.6140845070422536 |\n",
            "| XGBoost beta 0.15 | 0.6608315098468271 | 0.6869565217391305 |  0.398989898989899  | 0.5047923322683706 |\n",
            "|  logreg beta 0.15 | 0.7177242888402626 | 0.7315436241610739 |  0.5505050505050505 | 0.6282420749279539 |\n",
            "|   svm beta 0.15   | 0.6761487964989059 | 0.6865671641791045 | 0.46464646464646464 | 0.5542168674698794 |\n",
            "+-------------------+--------------------+--------------------+---------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMuMAwCjY6We",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "926c39dc-8775-425b-e050-137ebc358b37"
      },
      "source": [
        "Result_cross.to_csv('RMD_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.631016</td>\n",
              "      <td>0.746939</td>\n",
              "      <td>0.655556</td>\n",
              "      <td>0.682081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.769388</td>\n",
              "      <td>0.650155</td>\n",
              "      <td>0.606936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.653409</td>\n",
              "      <td>0.757143</td>\n",
              "      <td>0.659026</td>\n",
              "      <td>0.664740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.634831</td>\n",
              "      <td>0.744898</td>\n",
              "      <td>0.643875</td>\n",
              "      <td>0.653179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.651685</td>\n",
              "      <td>0.757143</td>\n",
              "      <td>0.660969</td>\n",
              "      <td>0.670520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.525097</td>\n",
              "      <td>0.649891</td>\n",
              "      <td>0.629630</td>\n",
              "      <td>0.786127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.733042</td>\n",
              "      <td>0.582192</td>\n",
              "      <td>0.491329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.628571</td>\n",
              "      <td>0.719912</td>\n",
              "      <td>0.632184</td>\n",
              "      <td>0.635838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.630952</td>\n",
              "      <td>0.717724</td>\n",
              "      <td>0.621701</td>\n",
              "      <td>0.612717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.607955</td>\n",
              "      <td>0.704595</td>\n",
              "      <td>0.613181</td>\n",
              "      <td>0.618497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.543860</td>\n",
              "      <td>0.738776</td>\n",
              "      <td>0.592357</td>\n",
              "      <td>0.650350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.583942</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.559441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.542683</td>\n",
              "      <td>0.736735</td>\n",
              "      <td>0.579805</td>\n",
              "      <td>0.622378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.556250</td>\n",
              "      <td>0.744898</td>\n",
              "      <td>0.587459</td>\n",
              "      <td>0.622378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.568627</td>\n",
              "      <td>0.751020</td>\n",
              "      <td>0.587838</td>\n",
              "      <td>0.608392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.702407</td>\n",
              "      <td>0.572327</td>\n",
              "      <td>0.636364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.547445</td>\n",
              "      <td>0.715536</td>\n",
              "      <td>0.535714</td>\n",
              "      <td>0.524476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.522936</td>\n",
              "      <td>0.698031</td>\n",
              "      <td>0.452381</td>\n",
              "      <td>0.398601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.548872</td>\n",
              "      <td>0.715536</td>\n",
              "      <td>0.528986</td>\n",
              "      <td>0.510490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.504132</td>\n",
              "      <td>0.689278</td>\n",
              "      <td>0.462121</td>\n",
              "      <td>0.426573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.803419</td>\n",
              "      <td>0.740816</td>\n",
              "      <td>0.596825</td>\n",
              "      <td>0.474747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.699422</td>\n",
              "      <td>0.736735</td>\n",
              "      <td>0.652291</td>\n",
              "      <td>0.611111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.696629</td>\n",
              "      <td>0.738776</td>\n",
              "      <td>0.659574</td>\n",
              "      <td>0.626263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.675532</td>\n",
              "      <td>0.730612</td>\n",
              "      <td>0.658031</td>\n",
              "      <td>0.641414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.698864</td>\n",
              "      <td>0.738776</td>\n",
              "      <td>0.657754</td>\n",
              "      <td>0.621212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.703226</td>\n",
              "      <td>0.704595</td>\n",
              "      <td>0.617564</td>\n",
              "      <td>0.550505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.694268</td>\n",
              "      <td>0.700219</td>\n",
              "      <td>0.614085</td>\n",
              "      <td>0.550505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.686957</td>\n",
              "      <td>0.660832</td>\n",
              "      <td>0.504792</td>\n",
              "      <td>0.398990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.731544</td>\n",
              "      <td>0.717724</td>\n",
              "      <td>0.628242</td>\n",
              "      <td>0.550505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.686567</td>\n",
              "      <td>0.676149</td>\n",
              "      <td>0.554217</td>\n",
              "      <td>0.464646</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  STX  0.631016  0.746939  0.655556  0.682081\n",
              "1            GRU 0.1  STX  0.700000  0.769388  0.650155  0.606936\n",
              "2        XGBoost 0.1  STX  0.653409  0.757143  0.659026  0.664740\n",
              "3         Logreg 0.1  STX  0.634831  0.744898  0.643875  0.653179\n",
              "4            SVM 0.1  STX  0.651685  0.757143  0.660969  0.670520\n",
              "5      LSTM beta 0.1  STX  0.525097  0.649891  0.629630  0.786127\n",
              "6       GRU beta 0.1  STX  0.714286  0.733042  0.582192  0.491329\n",
              "7   XGBoost beta 0.1  STX  0.628571  0.719912  0.632184  0.635838\n",
              "8    logreg beta 0.1  STX  0.630952  0.717724  0.621701  0.612717\n",
              "9       svm beta 0.1  STX  0.607955  0.704595  0.613181  0.618497\n",
              "0           LSTM 0.2  STX  0.543860  0.738776  0.592357  0.650350\n",
              "1            GRU 0.2  STX  0.583942  0.755102  0.571429  0.559441\n",
              "2        XGBoost 0.2  STX  0.542683  0.736735  0.579805  0.622378\n",
              "3         Logreg 0.2  STX  0.556250  0.744898  0.587459  0.622378\n",
              "4            SVM 0.2  STX  0.568627  0.751020  0.587838  0.608392\n",
              "5      LSTM beta 0.2  STX  0.520000  0.702407  0.572327  0.636364\n",
              "6       GRU beta 0.2  STX  0.547445  0.715536  0.535714  0.524476\n",
              "7   XGBoost beta 0.2  STX  0.522936  0.698031  0.452381  0.398601\n",
              "8    logreg beta 0.2  STX  0.548872  0.715536  0.528986  0.510490\n",
              "9       svm beta 0.2  STX  0.504132  0.689278  0.462121  0.426573\n",
              "0          LSTM 0.15  STX  0.803419  0.740816  0.596825  0.474747\n",
              "1           GRU 0.15  STX  0.699422  0.736735  0.652291  0.611111\n",
              "2       XGBoost 0.15  STX  0.696629  0.738776  0.659574  0.626263\n",
              "3        Logreg 0.15  STX  0.675532  0.730612  0.658031  0.641414\n",
              "4           SVM 0.15  STX  0.698864  0.738776  0.657754  0.621212\n",
              "5     LSTM beta 0.15  STX  0.703226  0.704595  0.617564  0.550505\n",
              "6      GRU beta 0.15  STX  0.694268  0.700219  0.614085  0.550505\n",
              "7  XGBoost beta 0.15  STX  0.686957  0.660832  0.504792  0.398990\n",
              "8   logreg beta 0.15  STX  0.731544  0.717724  0.628242  0.550505\n",
              "9      svm beta 0.15  STX  0.686567  0.676149  0.554217  0.464646"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs3mNVC3Y6We"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_logreg_beta.csv')"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZggcPhqY6We"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWbEskqKY6We",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7771bfba-2641-4fe2-cc40-0199da8156f1"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"STX\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 17ms/step - loss: 0.6957 - accuracy: 0.5141 - val_loss: 0.6664 - val_accuracy: 0.6469\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6691 - accuracy: 0.5893 - val_loss: 0.6234 - val_accuracy: 0.7327\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6227 - accuracy: 0.6772 - val_loss: 0.5741 - val_accuracy: 0.7041\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5941 - accuracy: 0.6886 - val_loss: 0.6018 - val_accuracy: 0.6857\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5832 - accuracy: 0.7034 - val_loss: 0.5182 - val_accuracy: 0.7551\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6735 - accuracy: 0.5658 - val_loss: 0.5501 - val_accuracy: 0.7388\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5900 - accuracy: 0.6899 - val_loss: 0.5157 - val_accuracy: 0.7429\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5769 - accuracy: 0.7047 - val_loss: 0.4862 - val_accuracy: 0.7673\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5610 - accuracy: 0.7188 - val_loss: 0.4803 - val_accuracy: 0.7653\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5624 - accuracy: 0.7201 - val_loss: 0.4879 - val_accuracy: 0.7653\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.816615\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.821292\n",
            "[2]\tvalidation_0-auc:0.81884\n",
            "[3]\tvalidation_0-auc:0.820764\n",
            "[4]\tvalidation_0-auc:0.81925\n",
            "[5]\tvalidation_0-auc:0.820891\n",
            "[6]\tvalidation_0-auc:0.820372\n",
            "[7]\tvalidation_0-auc:0.821292\n",
            "[8]\tvalidation_0-auc:0.821055\n",
            "[9]\tvalidation_0-auc:0.821694\n",
            "[10]\tvalidation_0-auc:0.823727\n",
            "[11]\tvalidation_0-auc:0.822879\n",
            "[12]\tvalidation_0-auc:0.823936\n",
            "[13]\tvalidation_0-auc:0.82824\n",
            "[14]\tvalidation_0-auc:0.827902\n",
            "[15]\tvalidation_0-auc:0.828112\n",
            "[16]\tvalidation_0-auc:0.828687\n",
            "[17]\tvalidation_0-auc:0.82865\n",
            "[18]\tvalidation_0-auc:0.828677\n",
            "[19]\tvalidation_0-auc:0.829252\n",
            "[20]\tvalidation_0-auc:0.829288\n",
            "[21]\tvalidation_0-auc:0.828924\n",
            "[22]\tvalidation_0-auc:0.828933\n",
            "[23]\tvalidation_0-auc:0.829398\n",
            "[24]\tvalidation_0-auc:0.829398\n",
            "[25]\tvalidation_0-auc:0.830483\n",
            "[26]\tvalidation_0-auc:0.829808\n",
            "[27]\tvalidation_0-auc:0.8302\n",
            "[28]\tvalidation_0-auc:0.830319\n",
            "[29]\tvalidation_0-auc:0.830091\n",
            "[30]\tvalidation_0-auc:0.830765\n",
            "[31]\tvalidation_0-auc:0.830847\n",
            "[32]\tvalidation_0-auc:0.830501\n",
            "[33]\tvalidation_0-auc:0.830656\n",
            "[34]\tvalidation_0-auc:0.830638\n",
            "[35]\tvalidation_0-auc:0.831221\n",
            "[36]\tvalidation_0-auc:0.832096\n",
            "[37]\tvalidation_0-auc:0.832251\n",
            "[38]\tvalidation_0-auc:0.831923\n",
            "[39]\tvalidation_0-auc:0.831704\n",
            "[40]\tvalidation_0-auc:0.831832\n",
            "[41]\tvalidation_0-auc:0.831887\n",
            "[42]\tvalidation_0-auc:0.83278\n",
            "[43]\tvalidation_0-auc:0.832561\n",
            "[44]\tvalidation_0-auc:0.832343\n",
            "[45]\tvalidation_0-auc:0.832124\n",
            "[46]\tvalidation_0-auc:0.831978\n",
            "[47]\tvalidation_0-auc:0.832169\n",
            "[48]\tvalidation_0-auc:0.831914\n",
            "[49]\tvalidation_0-auc:0.830729\n",
            "[50]\tvalidation_0-auc:0.831312\n",
            "[51]\tvalidation_0-auc:0.830236\n",
            "[52]\tvalidation_0-auc:0.830474\n",
            "[53]\tvalidation_0-auc:0.830027\n",
            "[54]\tvalidation_0-auc:0.830355\n",
            "[55]\tvalidation_0-auc:0.830391\n",
            "[56]\tvalidation_0-auc:0.830446\n",
            "[57]\tvalidation_0-auc:0.830337\n",
            "[58]\tvalidation_0-auc:0.830246\n",
            "[59]\tvalidation_0-auc:0.829972\n",
            "[60]\tvalidation_0-auc:0.829753\n",
            "[61]\tvalidation_0-auc:0.830629\n",
            "[62]\tvalidation_0-auc:0.830483\n",
            "[63]\tvalidation_0-auc:0.83061\n",
            "[64]\tvalidation_0-auc:0.831048\n",
            "[65]\tvalidation_0-auc:0.830556\n",
            "[66]\tvalidation_0-auc:0.830227\n",
            "[67]\tvalidation_0-auc:0.8303\n",
            "[68]\tvalidation_0-auc:0.829635\n",
            "[69]\tvalidation_0-auc:0.829544\n",
            "[70]\tvalidation_0-auc:0.829507\n",
            "[71]\tvalidation_0-auc:0.829489\n",
            "[72]\tvalidation_0-auc:0.829416\n",
            "[73]\tvalidation_0-auc:0.829379\n",
            "[74]\tvalidation_0-auc:0.829252\n",
            "[75]\tvalidation_0-auc:0.829033\n",
            "[76]\tvalidation_0-auc:0.829471\n",
            "[77]\tvalidation_0-auc:0.829762\n",
            "[78]\tvalidation_0-auc:0.82958\n",
            "[79]\tvalidation_0-auc:0.829197\n",
            "[80]\tvalidation_0-auc:0.828978\n",
            "[81]\tvalidation_0-auc:0.828869\n",
            "[82]\tvalidation_0-auc:0.829434\n",
            "[83]\tvalidation_0-auc:0.829106\n",
            "[84]\tvalidation_0-auc:0.828431\n",
            "[85]\tvalidation_0-auc:0.828413\n",
            "[86]\tvalidation_0-auc:0.827866\n",
            "[87]\tvalidation_0-auc:0.828067\n",
            "[88]\tvalidation_0-auc:0.827921\n",
            "[89]\tvalidation_0-auc:0.827757\n",
            "[90]\tvalidation_0-auc:0.827793\n",
            "[91]\tvalidation_0-auc:0.827793\n",
            "[92]\tvalidation_0-auc:0.827629\n",
            "Stopping. Best iteration:\n",
            "[42]\tvalidation_0-auc:0.83278\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6949 - accuracy: 0.5086 - val_loss: 0.6839 - val_accuracy: 0.6214\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6919 - accuracy: 0.5175 - val_loss: 0.6529 - val_accuracy: 0.6214\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6658 - accuracy: 0.5951 - val_loss: 0.6428 - val_accuracy: 0.6980\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6184 - accuracy: 0.6726 - val_loss: 0.5684 - val_accuracy: 0.7309\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.5904 - accuracy: 0.6960 - val_loss: 0.5548 - val_accuracy: 0.7112\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6937 - accuracy: 0.5285 - val_loss: 0.6804 - val_accuracy: 0.6302\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6647 - accuracy: 0.5957 - val_loss: 0.5841 - val_accuracy: 0.7352\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6123 - accuracy: 0.6925 - val_loss: 0.6173 - val_accuracy: 0.6718\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5892 - accuracy: 0.7213 - val_loss: 0.5575 - val_accuracy: 0.7046\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5827 - accuracy: 0.6911 - val_loss: 0.5342 - val_accuracy: 0.7090\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.740118\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.737951\n",
            "[2]\tvalidation_0-auc:0.727998\n",
            "[3]\tvalidation_0-auc:0.742958\n",
            "[4]\tvalidation_0-auc:0.739142\n",
            "[5]\tvalidation_0-auc:0.751272\n",
            "[6]\tvalidation_0-auc:0.755902\n",
            "[7]\tvalidation_0-auc:0.772867\n",
            "[8]\tvalidation_0-auc:0.756198\n",
            "[9]\tvalidation_0-auc:0.761245\n",
            "[10]\tvalidation_0-auc:0.765937\n",
            "[11]\tvalidation_0-auc:0.767504\n",
            "[12]\tvalidation_0-auc:0.766792\n",
            "[13]\tvalidation_0-auc:0.771463\n",
            "[14]\tvalidation_0-auc:0.769366\n",
            "[15]\tvalidation_0-auc:0.772989\n",
            "[16]\tvalidation_0-auc:0.774414\n",
            "[17]\tvalidation_0-auc:0.777508\n",
            "[18]\tvalidation_0-auc:0.779319\n",
            "[19]\tvalidation_0-auc:0.781293\n",
            "[20]\tvalidation_0-auc:0.786667\n",
            "[21]\tvalidation_0-auc:0.787664\n",
            "[22]\tvalidation_0-auc:0.786392\n",
            "[23]\tvalidation_0-auc:0.788712\n",
            "[24]\tvalidation_0-auc:0.78973\n",
            "[25]\tvalidation_0-auc:0.786717\n",
            "[26]\tvalidation_0-auc:0.79089\n",
            "[27]\tvalidation_0-auc:0.793098\n",
            "[28]\tvalidation_0-auc:0.795144\n",
            "[29]\tvalidation_0-auc:0.792864\n",
            "[30]\tvalidation_0-auc:0.795408\n",
            "[31]\tvalidation_0-auc:0.796528\n",
            "[32]\tvalidation_0-auc:0.795815\n",
            "[33]\tvalidation_0-auc:0.794238\n",
            "[34]\tvalidation_0-auc:0.791582\n",
            "[35]\tvalidation_0-auc:0.788071\n",
            "[36]\tvalidation_0-auc:0.789251\n",
            "[37]\tvalidation_0-auc:0.786962\n",
            "[38]\tvalidation_0-auc:0.785913\n",
            "[39]\tvalidation_0-auc:0.785771\n",
            "[40]\tvalidation_0-auc:0.785567\n",
            "[41]\tvalidation_0-auc:0.787257\n",
            "[42]\tvalidation_0-auc:0.78454\n",
            "[43]\tvalidation_0-auc:0.785608\n",
            "[44]\tvalidation_0-auc:0.785466\n",
            "[45]\tvalidation_0-auc:0.784061\n",
            "[46]\tvalidation_0-auc:0.784713\n",
            "[47]\tvalidation_0-auc:0.786524\n",
            "[48]\tvalidation_0-auc:0.789302\n",
            "[49]\tvalidation_0-auc:0.789282\n",
            "[50]\tvalidation_0-auc:0.787511\n",
            "[51]\tvalidation_0-auc:0.785842\n",
            "[52]\tvalidation_0-auc:0.784845\n",
            "[53]\tvalidation_0-auc:0.784519\n",
            "[54]\tvalidation_0-auc:0.784967\n",
            "[55]\tvalidation_0-auc:0.784764\n",
            "[56]\tvalidation_0-auc:0.78515\n",
            "[57]\tvalidation_0-auc:0.783664\n",
            "[58]\tvalidation_0-auc:0.782229\n",
            "[59]\tvalidation_0-auc:0.782616\n",
            "[60]\tvalidation_0-auc:0.782494\n",
            "[61]\tvalidation_0-auc:0.782698\n",
            "[62]\tvalidation_0-auc:0.782759\n",
            "[63]\tvalidation_0-auc:0.782474\n",
            "[64]\tvalidation_0-auc:0.779604\n",
            "[65]\tvalidation_0-auc:0.77995\n",
            "[66]\tvalidation_0-auc:0.778118\n",
            "[67]\tvalidation_0-auc:0.777701\n",
            "[68]\tvalidation_0-auc:0.777599\n",
            "[69]\tvalidation_0-auc:0.778596\n",
            "[70]\tvalidation_0-auc:0.776174\n",
            "[71]\tvalidation_0-auc:0.774322\n",
            "[72]\tvalidation_0-auc:0.775482\n",
            "[73]\tvalidation_0-auc:0.776195\n",
            "[74]\tvalidation_0-auc:0.776459\n",
            "[75]\tvalidation_0-auc:0.777212\n",
            "[76]\tvalidation_0-auc:0.776622\n",
            "[77]\tvalidation_0-auc:0.775075\n",
            "[78]\tvalidation_0-auc:0.774628\n",
            "[79]\tvalidation_0-auc:0.774628\n",
            "[80]\tvalidation_0-auc:0.773101\n",
            "[81]\tvalidation_0-auc:0.771005\n",
            "Stopping. Best iteration:\n",
            "[31]\tvalidation_0-auc:0.796528\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.7551020408163265 | 0.6432432432432432 | 0.6878612716763006 | 0.664804469273743  |\n",
            "|     GRU 0.1      | 0.7653061224489796 | 0.6858974358974359 | 0.6184971098265896 | 0.6504559270516718 |\n",
            "|   XGBoost 0.1    | 0.7571428571428571 | 0.6534090909090909 | 0.6647398843930635 | 0.6590257879656161 |\n",
            "|    Logreg 0.1    | 0.7448979591836735 | 0.6348314606741573 | 0.653179190751445  | 0.6438746438746439 |\n",
            "|     SVM 0.1      | 0.7571428571428571 | 0.651685393258427  | 0.6705202312138728 | 0.6609686609686609 |\n",
            "|  LSTM beta 0.1   | 0.7111597374179431 | 0.6518518518518519 | 0.5086705202312138 | 0.5714285714285715 |\n",
            "|   GRU beta 0.1   | 0.7089715536105032 | 0.631578947368421  | 0.5549132947976878 | 0.5907692307692307 |\n",
            "| XGBoost beta 0.1 | 0.7199124726477024 | 0.6285714285714286 | 0.6358381502890174 | 0.632183908045977  |\n",
            "| logreg beta 0.1  | 0.7177242888402626 | 0.6309523809523809 | 0.6127167630057804 | 0.6217008797653959 |\n",
            "|   svm beta 0.1   | 0.7045951859956237 | 0.6079545454545454 | 0.6184971098265896 | 0.6131805157593122 |\n",
            "+------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6928 - accuracy: 0.5282 - val_loss: 0.7995 - val_accuracy: 0.2918\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6683 - accuracy: 0.5906 - val_loss: 0.5728 - val_accuracy: 0.7429\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6392 - accuracy: 0.6470 - val_loss: 0.5614 - val_accuracy: 0.7429\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6395 - accuracy: 0.6450 - val_loss: 0.5953 - val_accuracy: 0.7388\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6392 - accuracy: 0.6517 - val_loss: 0.6217 - val_accuracy: 0.7163\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6897 - accuracy: 0.5503 - val_loss: 0.6095 - val_accuracy: 0.7408\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6394 - accuracy: 0.6255 - val_loss: 0.5749 - val_accuracy: 0.7143\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6125 - accuracy: 0.6725 - val_loss: 0.5639 - val_accuracy: 0.7286\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6170 - accuracy: 0.6738 - val_loss: 0.5252 - val_accuracy: 0.7776\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6141 - accuracy: 0.6705 - val_loss: 0.6021 - val_accuracy: 0.7020\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.773453\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.790804\n",
            "[2]\tvalidation_0-auc:0.796306\n",
            "[3]\tvalidation_0-auc:0.793243\n",
            "[4]\tvalidation_0-auc:0.793333\n",
            "[5]\tvalidation_0-auc:0.797142\n",
            "[6]\tvalidation_0-auc:0.793716\n",
            "[7]\tvalidation_0-auc:0.79421\n",
            "[8]\tvalidation_0-auc:0.794281\n",
            "[9]\tvalidation_0-auc:0.791147\n",
            "[10]\tvalidation_0-auc:0.792104\n",
            "[11]\tvalidation_0-auc:0.793716\n",
            "[12]\tvalidation_0-auc:0.792386\n",
            "[13]\tvalidation_0-auc:0.792034\n",
            "[14]\tvalidation_0-auc:0.793616\n",
            "[15]\tvalidation_0-auc:0.792598\n",
            "[16]\tvalidation_0-auc:0.79156\n",
            "[17]\tvalidation_0-auc:0.792578\n",
            "[18]\tvalidation_0-auc:0.793041\n",
            "[19]\tvalidation_0-auc:0.793948\n",
            "[20]\tvalidation_0-auc:0.793696\n",
            "[21]\tvalidation_0-auc:0.79417\n",
            "[22]\tvalidation_0-auc:0.793908\n",
            "[23]\tvalidation_0-auc:0.792054\n",
            "[24]\tvalidation_0-auc:0.792527\n",
            "[25]\tvalidation_0-auc:0.79283\n",
            "[26]\tvalidation_0-auc:0.792487\n",
            "[27]\tvalidation_0-auc:0.791782\n",
            "[28]\tvalidation_0-auc:0.791409\n",
            "[29]\tvalidation_0-auc:0.791288\n",
            "[30]\tvalidation_0-auc:0.790814\n",
            "[31]\tvalidation_0-auc:0.790552\n",
            "[32]\tvalidation_0-auc:0.788376\n",
            "[33]\tvalidation_0-auc:0.789797\n",
            "[34]\tvalidation_0-auc:0.789192\n",
            "[35]\tvalidation_0-auc:0.78895\n",
            "[36]\tvalidation_0-auc:0.789293\n",
            "[37]\tvalidation_0-auc:0.79026\n",
            "[38]\tvalidation_0-auc:0.792678\n",
            "[39]\tvalidation_0-auc:0.792628\n",
            "[40]\tvalidation_0-auc:0.791107\n",
            "[41]\tvalidation_0-auc:0.792054\n",
            "[42]\tvalidation_0-auc:0.792457\n",
            "[43]\tvalidation_0-auc:0.792477\n",
            "[44]\tvalidation_0-auc:0.791973\n",
            "[45]\tvalidation_0-auc:0.79157\n",
            "[46]\tvalidation_0-auc:0.791379\n",
            "[47]\tvalidation_0-auc:0.791016\n",
            "[48]\tvalidation_0-auc:0.790734\n",
            "[49]\tvalidation_0-auc:0.790925\n",
            "[50]\tvalidation_0-auc:0.791046\n",
            "[51]\tvalidation_0-auc:0.791268\n",
            "[52]\tvalidation_0-auc:0.790885\n",
            "[53]\tvalidation_0-auc:0.79151\n",
            "[54]\tvalidation_0-auc:0.792527\n",
            "[55]\tvalidation_0-auc:0.792427\n",
            "Stopping. Best iteration:\n",
            "[5]\tvalidation_0-auc:0.797142\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6637 - accuracy: 0.6012 - val_loss: 0.6522 - val_accuracy: 0.6346\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5949 - accuracy: 0.6987 - val_loss: 0.6196 - val_accuracy: 0.6718\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5719 - accuracy: 0.7159 - val_loss: 0.6072 - val_accuracy: 0.6915\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5560 - accuracy: 0.7303 - val_loss: 0.6342 - val_accuracy: 0.6871\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5393 - accuracy: 0.7529 - val_loss: 0.6038 - val_accuracy: 0.7002\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6197 - accuracy: 0.6685 - val_loss: 0.5771 - val_accuracy: 0.7199\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5539 - accuracy: 0.7289 - val_loss: 0.6460 - val_accuracy: 0.7068\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5501 - accuracy: 0.7522 - val_loss: 0.5468 - val_accuracy: 0.7068\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5527 - accuracy: 0.7378 - val_loss: 0.5793 - val_accuracy: 0.7024\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5450 - accuracy: 0.7378 - val_loss: 0.5774 - val_accuracy: 0.6980\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.660717\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.672075\n",
            "[2]\tvalidation_0-auc:0.671741\n",
            "[3]\tvalidation_0-auc:0.677832\n",
            "[4]\tvalidation_0-auc:0.678912\n",
            "[5]\tvalidation_0-auc:0.678845\n",
            "[6]\tvalidation_0-auc:0.681484\n",
            "[7]\tvalidation_0-auc:0.67968\n",
            "[8]\tvalidation_0-auc:0.676651\n",
            "[9]\tvalidation_0-auc:0.703231\n",
            "[10]\tvalidation_0-auc:0.677375\n",
            "[11]\tvalidation_0-auc:0.69125\n",
            "[12]\tvalidation_0-auc:0.694802\n",
            "[13]\tvalidation_0-auc:0.695136\n",
            "[14]\tvalidation_0-auc:0.711483\n",
            "[15]\tvalidation_0-auc:0.706082\n",
            "[16]\tvalidation_0-auc:0.711995\n",
            "[17]\tvalidation_0-auc:0.715959\n",
            "[18]\tvalidation_0-auc:0.710425\n",
            "[19]\tvalidation_0-auc:0.713109\n",
            "[20]\tvalidation_0-auc:0.713198\n",
            "[21]\tvalidation_0-auc:0.715191\n",
            "[22]\tvalidation_0-auc:0.715814\n",
            "[23]\tvalidation_0-auc:0.719578\n",
            "[24]\tvalidation_0-auc:0.71724\n",
            "[25]\tvalidation_0-auc:0.718977\n",
            "[26]\tvalidation_0-auc:0.709467\n",
            "[27]\tvalidation_0-auc:0.706361\n",
            "[28]\tvalidation_0-auc:0.708499\n",
            "[29]\tvalidation_0-auc:0.70832\n",
            "[30]\tvalidation_0-auc:0.709612\n",
            "[31]\tvalidation_0-auc:0.708287\n",
            "[32]\tvalidation_0-auc:0.709167\n",
            "[33]\tvalidation_0-auc:0.709033\n",
            "[34]\tvalidation_0-auc:0.706316\n",
            "[35]\tvalidation_0-auc:0.706795\n",
            "[36]\tvalidation_0-auc:0.706973\n",
            "[37]\tvalidation_0-auc:0.706984\n",
            "[38]\tvalidation_0-auc:0.707541\n",
            "[39]\tvalidation_0-auc:0.707407\n",
            "[40]\tvalidation_0-auc:0.705024\n",
            "[41]\tvalidation_0-auc:0.70645\n",
            "[42]\tvalidation_0-auc:0.708788\n",
            "[43]\tvalidation_0-auc:0.707897\n",
            "[44]\tvalidation_0-auc:0.707519\n",
            "[45]\tvalidation_0-auc:0.708231\n",
            "[46]\tvalidation_0-auc:0.709478\n",
            "[47]\tvalidation_0-auc:0.70224\n",
            "[48]\tvalidation_0-auc:0.701884\n",
            "[49]\tvalidation_0-auc:0.701149\n",
            "[50]\tvalidation_0-auc:0.700637\n",
            "[51]\tvalidation_0-auc:0.699011\n",
            "[52]\tvalidation_0-auc:0.699212\n",
            "[53]\tvalidation_0-auc:0.695448\n",
            "[54]\tvalidation_0-auc:0.697096\n",
            "[55]\tvalidation_0-auc:0.697719\n",
            "[56]\tvalidation_0-auc:0.698254\n",
            "[57]\tvalidation_0-auc:0.698187\n",
            "[58]\tvalidation_0-auc:0.699412\n",
            "[59]\tvalidation_0-auc:0.700102\n",
            "[60]\tvalidation_0-auc:0.700214\n",
            "[61]\tvalidation_0-auc:0.699368\n",
            "[62]\tvalidation_0-auc:0.700214\n",
            "[63]\tvalidation_0-auc:0.695002\n",
            "[64]\tvalidation_0-auc:0.696561\n",
            "[65]\tvalidation_0-auc:0.694691\n",
            "[66]\tvalidation_0-auc:0.693778\n",
            "[67]\tvalidation_0-auc:0.694023\n",
            "[68]\tvalidation_0-auc:0.694067\n",
            "[69]\tvalidation_0-auc:0.695069\n",
            "[70]\tvalidation_0-auc:0.696205\n",
            "[71]\tvalidation_0-auc:0.696895\n",
            "[72]\tvalidation_0-auc:0.695292\n",
            "[73]\tvalidation_0-auc:0.698677\n",
            "Stopping. Best iteration:\n",
            "[23]\tvalidation_0-auc:0.719578\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.2     | 0.7163265306122449 | 0.5108695652173914 |  0.6573426573426573 |  0.5749235474006117 |\n",
            "|     GRU 0.2      | 0.7020408163265306 | 0.4928909952606635 |  0.7272727272727273 |  0.5875706214689266 |\n",
            "|   XGBoost 0.2    | 0.736734693877551  | 0.5426829268292683 |  0.6223776223776224 |  0.5798045602605864 |\n",
            "|    Logreg 0.2    | 0.7448979591836735 |      0.55625       |  0.6223776223776224 |  0.5874587458745875 |\n",
            "|     SVM 0.2      | 0.7510204081632653 | 0.5686274509803921 |  0.6083916083916084 |  0.5878378378378378 |\n",
            "|  LSTM beta 0.2   | 0.700218818380744  | 0.5214285714285715 |  0.5104895104895105 |  0.5159010600706714 |\n",
            "|   GRU beta 0.2   | 0.6980306345733042 | 0.5193798449612403 | 0.46853146853146854 | 0.49264705882352944 |\n",
            "| XGBoost beta 0.2 | 0.6980306345733042 | 0.5229357798165137 |  0.3986013986013986 |  0.4523809523809524 |\n",
            "| logreg beta 0.2  | 0.7155361050328227 | 0.5488721804511278 |  0.5104895104895105 |  0.5289855072463768 |\n",
            "|   svm beta 0.2   | 0.6892778993435449 | 0.5041322314049587 | 0.42657342657342656 |  0.4621212121212121 |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 4s 15ms/step - loss: 0.6950 - accuracy: 0.5081 - val_loss: 0.6734 - val_accuracy: 0.5959\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6657 - accuracy: 0.6121 - val_loss: 0.6244 - val_accuracy: 0.7306\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6395 - accuracy: 0.6456 - val_loss: 0.6260 - val_accuracy: 0.6776\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6342 - accuracy: 0.6517 - val_loss: 0.6059 - val_accuracy: 0.7102\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6182 - accuracy: 0.6591 - val_loss: 0.5923 - val_accuracy: 0.7327\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6841 - accuracy: 0.5383 - val_loss: 0.6398 - val_accuracy: 0.7082\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6403 - accuracy: 0.6315 - val_loss: 0.5805 - val_accuracy: 0.7245\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6045 - accuracy: 0.6711 - val_loss: 0.5801 - val_accuracy: 0.7143\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6085 - accuracy: 0.6745 - val_loss: 0.5733 - val_accuracy: 0.7327\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5904 - accuracy: 0.6812 - val_loss: 0.5566 - val_accuracy: 0.7224\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.773817\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.78442\n",
            "[2]\tvalidation_0-auc:0.786919\n",
            "[3]\tvalidation_0-auc:0.786374\n",
            "[4]\tvalidation_0-auc:0.789358\n",
            "[5]\tvalidation_0-auc:0.796587\n",
            "[6]\tvalidation_0-auc:0.796475\n",
            "[7]\tvalidation_0-auc:0.796267\n",
            "[8]\tvalidation_0-auc:0.796994\n",
            "[9]\tvalidation_0-auc:0.797262\n",
            "[10]\tvalidation_0-auc:0.797314\n",
            "[11]\tvalidation_0-auc:0.794503\n",
            "[12]\tvalidation_0-auc:0.795792\n",
            "[13]\tvalidation_0-auc:0.79478\n",
            "[14]\tvalidation_0-auc:0.793967\n",
            "[15]\tvalidation_0-auc:0.794512\n",
            "[16]\tvalidation_0-auc:0.799173\n",
            "[17]\tvalidation_0-auc:0.799848\n",
            "[18]\tvalidation_0-auc:0.799294\n",
            "[19]\tvalidation_0-auc:0.798499\n",
            "[20]\tvalidation_0-auc:0.796994\n",
            "[21]\tvalidation_0-auc:0.795091\n",
            "[22]\tvalidation_0-auc:0.794175\n",
            "[23]\tvalidation_0-auc:0.793846\n",
            "[24]\tvalidation_0-auc:0.793543\n",
            "[25]\tvalidation_0-auc:0.792506\n",
            "[26]\tvalidation_0-auc:0.791113\n",
            "[27]\tvalidation_0-auc:0.79056\n",
            "[28]\tvalidation_0-auc:0.790127\n",
            "[29]\tvalidation_0-auc:0.790084\n",
            "[30]\tvalidation_0-auc:0.788856\n",
            "[31]\tvalidation_0-auc:0.788908\n",
            "[32]\tvalidation_0-auc:0.788493\n",
            "[33]\tvalidation_0-auc:0.788285\n",
            "[34]\tvalidation_0-auc:0.789859\n",
            "[35]\tvalidation_0-auc:0.790274\n",
            "[36]\tvalidation_0-auc:0.789375\n",
            "[37]\tvalidation_0-auc:0.787905\n",
            "[38]\tvalidation_0-auc:0.787749\n",
            "[39]\tvalidation_0-auc:0.787801\n",
            "[40]\tvalidation_0-auc:0.787126\n",
            "[41]\tvalidation_0-auc:0.786936\n",
            "[42]\tvalidation_0-auc:0.787593\n",
            "[43]\tvalidation_0-auc:0.788354\n",
            "[44]\tvalidation_0-auc:0.788545\n",
            "[45]\tvalidation_0-auc:0.788216\n",
            "[46]\tvalidation_0-auc:0.788726\n",
            "[47]\tvalidation_0-auc:0.788986\n",
            "[48]\tvalidation_0-auc:0.788432\n",
            "[49]\tvalidation_0-auc:0.78838\n",
            "[50]\tvalidation_0-auc:0.788225\n",
            "[51]\tvalidation_0-auc:0.787567\n",
            "[52]\tvalidation_0-auc:0.787671\n",
            "[53]\tvalidation_0-auc:0.78883\n",
            "[54]\tvalidation_0-auc:0.789842\n",
            "[55]\tvalidation_0-auc:0.790188\n",
            "[56]\tvalidation_0-auc:0.789392\n",
            "[57]\tvalidation_0-auc:0.787784\n",
            "[58]\tvalidation_0-auc:0.787887\n",
            "[59]\tvalidation_0-auc:0.787853\n",
            "[60]\tvalidation_0-auc:0.785795\n",
            "[61]\tvalidation_0-auc:0.786089\n",
            "[62]\tvalidation_0-auc:0.786192\n",
            "[63]\tvalidation_0-auc:0.785042\n",
            "[64]\tvalidation_0-auc:0.785284\n",
            "[65]\tvalidation_0-auc:0.785648\n",
            "[66]\tvalidation_0-auc:0.784956\n",
            "[67]\tvalidation_0-auc:0.784247\n",
            "Stopping. Best iteration:\n",
            "[17]\tvalidation_0-auc:0.799848\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6905 - accuracy: 0.5498 - val_loss: 0.6675 - val_accuracy: 0.5952\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6608 - accuracy: 0.6349 - val_loss: 0.6786 - val_accuracy: 0.6280\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6285 - accuracy: 0.6630 - val_loss: 0.6411 - val_accuracy: 0.6915\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5909 - accuracy: 0.7021 - val_loss: 0.6431 - val_accuracy: 0.6674\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5787 - accuracy: 0.7014 - val_loss: 0.6130 - val_accuracy: 0.7177\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6439 - accuracy: 0.6376 - val_loss: 0.6121 - val_accuracy: 0.7309\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5919 - accuracy: 0.7035 - val_loss: 0.5994 - val_accuracy: 0.7243\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5789 - accuracy: 0.7159 - val_loss: 0.6032 - val_accuracy: 0.7243\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5676 - accuracy: 0.7111 - val_loss: 0.6019 - val_accuracy: 0.7265\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5729 - accuracy: 0.7268 - val_loss: 0.6334 - val_accuracy: 0.6455\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.692972\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.713038\n",
            "[2]\tvalidation_0-auc:0.737062\n",
            "[3]\tvalidation_0-auc:0.735619\n",
            "[4]\tvalidation_0-auc:0.740806\n",
            "[5]\tvalidation_0-auc:0.745788\n",
            "[6]\tvalidation_0-auc:0.745515\n",
            "[7]\tvalidation_0-auc:0.743506\n",
            "[8]\tvalidation_0-auc:0.741868\n",
            "[9]\tvalidation_0-auc:0.74101\n",
            "[10]\tvalidation_0-auc:0.748879\n",
            "[11]\tvalidation_0-auc:0.751258\n",
            "[12]\tvalidation_0-auc:0.758161\n",
            "[13]\tvalidation_0-auc:0.760082\n",
            "[14]\tvalidation_0-auc:0.764381\n",
            "[15]\tvalidation_0-auc:0.764898\n",
            "[16]\tvalidation_0-auc:0.766282\n",
            "[17]\tvalidation_0-auc:0.765259\n",
            "[18]\tvalidation_0-auc:0.761583\n",
            "[19]\tvalidation_0-auc:0.766361\n",
            "[20]\tvalidation_0-auc:0.765668\n",
            "[21]\tvalidation_0-auc:0.76716\n",
            "[22]\tvalidation_0-auc:0.768272\n",
            "[23]\tvalidation_0-auc:0.7656\n",
            "[24]\tvalidation_0-auc:0.765395\n",
            "[25]\tvalidation_0-auc:0.764381\n",
            "[26]\tvalidation_0-auc:0.765181\n",
            "[27]\tvalidation_0-auc:0.764537\n",
            "[28]\tvalidation_0-auc:0.765454\n",
            "[29]\tvalidation_0-auc:0.765464\n",
            "[30]\tvalidation_0-auc:0.762324\n",
            "[31]\tvalidation_0-auc:0.762695\n",
            "[32]\tvalidation_0-auc:0.762402\n",
            "[33]\tvalidation_0-auc:0.764879\n",
            "[34]\tvalidation_0-auc:0.762617\n",
            "[35]\tvalidation_0-auc:0.761837\n",
            "[36]\tvalidation_0-auc:0.762324\n",
            "[37]\tvalidation_0-auc:0.759068\n",
            "[38]\tvalidation_0-auc:0.759828\n",
            "[39]\tvalidation_0-auc:0.759301\n",
            "[40]\tvalidation_0-auc:0.756572\n",
            "[41]\tvalidation_0-auc:0.75663\n",
            "[42]\tvalidation_0-auc:0.756981\n",
            "[43]\tvalidation_0-auc:0.756532\n",
            "[44]\tvalidation_0-auc:0.75624\n",
            "[45]\tvalidation_0-auc:0.755177\n",
            "[46]\tvalidation_0-auc:0.753705\n",
            "[47]\tvalidation_0-auc:0.753919\n",
            "[48]\tvalidation_0-auc:0.754573\n",
            "[49]\tvalidation_0-auc:0.755236\n",
            "[50]\tvalidation_0-auc:0.754553\n",
            "[51]\tvalidation_0-auc:0.754729\n",
            "[52]\tvalidation_0-auc:0.754553\n",
            "[53]\tvalidation_0-auc:0.755948\n",
            "[54]\tvalidation_0-auc:0.753422\n",
            "[55]\tvalidation_0-auc:0.753305\n",
            "[56]\tvalidation_0-auc:0.75193\n",
            "[57]\tvalidation_0-auc:0.75158\n",
            "[58]\tvalidation_0-auc:0.751326\n",
            "[59]\tvalidation_0-auc:0.751384\n",
            "[60]\tvalidation_0-auc:0.752496\n",
            "[61]\tvalidation_0-auc:0.753452\n",
            "[62]\tvalidation_0-auc:0.753471\n",
            "[63]\tvalidation_0-auc:0.753783\n",
            "[64]\tvalidation_0-auc:0.750253\n",
            "[65]\tvalidation_0-auc:0.75119\n",
            "[66]\tvalidation_0-auc:0.750039\n",
            "[67]\tvalidation_0-auc:0.749591\n",
            "[68]\tvalidation_0-auc:0.750546\n",
            "[69]\tvalidation_0-auc:0.75115\n",
            "[70]\tvalidation_0-auc:0.749668\n",
            "[71]\tvalidation_0-auc:0.748128\n",
            "[72]\tvalidation_0-auc:0.749298\n",
            "Stopping. Best iteration:\n",
            "[22]\tvalidation_0-auc:0.768272\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |        Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.7326530612244898 | 0.7596899224806202 |  0.494949494949495  | 0.599388379204893  |\n",
            "|      GRU 0.15     | 0.7224489795918367 | 0.7246376811594203 |  0.5050505050505051 | 0.5952380952380952 |\n",
            "|    XGBoost 0.15   | 0.7387755102040816 | 0.6966292134831461 |  0.6262626262626263 | 0.6595744680851064 |\n",
            "|    Logreg 0.15    | 0.7306122448979592 | 0.675531914893617  |  0.6414141414141414 | 0.6580310880829016 |\n",
            "|      SVM 0.15     | 0.7387755102040816 | 0.6988636363636364 |  0.6212121212121212 | 0.6577540106951871 |\n",
            "|   LSTM beta 0.15  | 0.7177242888402626 | 0.7142857142857143 |  0.5808080808080808 | 0.6406685236768802 |\n",
            "|   GRU beta 0.15   | 0.6455142231947484 | 0.6836734693877551 |  0.3383838383838384 | 0.4527027027027027 |\n",
            "| XGBoost beta 0.15 | 0.6608315098468271 | 0.6869565217391305 |  0.398989898989899  | 0.5047923322683706 |\n",
            "|  logreg beta 0.15 | 0.7177242888402626 | 0.7315436241610739 |  0.5505050505050505 | 0.6282420749279539 |\n",
            "|   svm beta 0.15   | 0.6761487964989059 | 0.6865671641791045 | 0.46464646464646464 | 0.5542168674698794 |\n",
            "+-------------------+--------------------+--------------------+---------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVrluUZhY6Wf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5ab6430f-2dd3-403e-ad64-77a6cab761b4"
      },
      "source": [
        "Result_purging.to_csv('STX_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.643243</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.664804</td>\n",
              "      <td>0.687861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.685897</td>\n",
              "      <td>0.765306</td>\n",
              "      <td>0.650456</td>\n",
              "      <td>0.618497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.653409</td>\n",
              "      <td>0.757143</td>\n",
              "      <td>0.659026</td>\n",
              "      <td>0.664740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.634831</td>\n",
              "      <td>0.744898</td>\n",
              "      <td>0.643875</td>\n",
              "      <td>0.653179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.651685</td>\n",
              "      <td>0.757143</td>\n",
              "      <td>0.660969</td>\n",
              "      <td>0.670520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.651852</td>\n",
              "      <td>0.711160</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.508671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.631579</td>\n",
              "      <td>0.708972</td>\n",
              "      <td>0.590769</td>\n",
              "      <td>0.554913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.628571</td>\n",
              "      <td>0.719912</td>\n",
              "      <td>0.632184</td>\n",
              "      <td>0.635838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.630952</td>\n",
              "      <td>0.717724</td>\n",
              "      <td>0.621701</td>\n",
              "      <td>0.612717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.607955</td>\n",
              "      <td>0.704595</td>\n",
              "      <td>0.613181</td>\n",
              "      <td>0.618497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.510870</td>\n",
              "      <td>0.716327</td>\n",
              "      <td>0.574924</td>\n",
              "      <td>0.657343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.492891</td>\n",
              "      <td>0.702041</td>\n",
              "      <td>0.587571</td>\n",
              "      <td>0.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.542683</td>\n",
              "      <td>0.736735</td>\n",
              "      <td>0.579805</td>\n",
              "      <td>0.622378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.556250</td>\n",
              "      <td>0.744898</td>\n",
              "      <td>0.587459</td>\n",
              "      <td>0.622378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.568627</td>\n",
              "      <td>0.751020</td>\n",
              "      <td>0.587838</td>\n",
              "      <td>0.608392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.521429</td>\n",
              "      <td>0.700219</td>\n",
              "      <td>0.515901</td>\n",
              "      <td>0.510490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.519380</td>\n",
              "      <td>0.698031</td>\n",
              "      <td>0.492647</td>\n",
              "      <td>0.468531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.522936</td>\n",
              "      <td>0.698031</td>\n",
              "      <td>0.452381</td>\n",
              "      <td>0.398601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.548872</td>\n",
              "      <td>0.715536</td>\n",
              "      <td>0.528986</td>\n",
              "      <td>0.510490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.504132</td>\n",
              "      <td>0.689278</td>\n",
              "      <td>0.462121</td>\n",
              "      <td>0.426573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.759690</td>\n",
              "      <td>0.732653</td>\n",
              "      <td>0.599388</td>\n",
              "      <td>0.494949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.724638</td>\n",
              "      <td>0.722449</td>\n",
              "      <td>0.595238</td>\n",
              "      <td>0.505051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.696629</td>\n",
              "      <td>0.738776</td>\n",
              "      <td>0.659574</td>\n",
              "      <td>0.626263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.675532</td>\n",
              "      <td>0.730612</td>\n",
              "      <td>0.658031</td>\n",
              "      <td>0.641414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.698864</td>\n",
              "      <td>0.738776</td>\n",
              "      <td>0.657754</td>\n",
              "      <td>0.621212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.717724</td>\n",
              "      <td>0.640669</td>\n",
              "      <td>0.580808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.683673</td>\n",
              "      <td>0.645514</td>\n",
              "      <td>0.452703</td>\n",
              "      <td>0.338384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.686957</td>\n",
              "      <td>0.660832</td>\n",
              "      <td>0.504792</td>\n",
              "      <td>0.398990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.731544</td>\n",
              "      <td>0.717724</td>\n",
              "      <td>0.628242</td>\n",
              "      <td>0.550505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>STX</td>\n",
              "      <td>0.686567</td>\n",
              "      <td>0.676149</td>\n",
              "      <td>0.554217</td>\n",
              "      <td>0.464646</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  STX  0.643243  0.755102  0.664804  0.687861\n",
              "1            GRU 0.1  STX  0.685897  0.765306  0.650456  0.618497\n",
              "2        XGBoost 0.1  STX  0.653409  0.757143  0.659026  0.664740\n",
              "3         Logreg 0.1  STX  0.634831  0.744898  0.643875  0.653179\n",
              "4            SVM 0.1  STX  0.651685  0.757143  0.660969  0.670520\n",
              "5      LSTM beta 0.1  STX  0.651852  0.711160  0.571429  0.508671\n",
              "6       GRU beta 0.1  STX  0.631579  0.708972  0.590769  0.554913\n",
              "7   XGBoost beta 0.1  STX  0.628571  0.719912  0.632184  0.635838\n",
              "8    logreg beta 0.1  STX  0.630952  0.717724  0.621701  0.612717\n",
              "9       svm beta 0.1  STX  0.607955  0.704595  0.613181  0.618497\n",
              "0           LSTM 0.2  STX  0.510870  0.716327  0.574924  0.657343\n",
              "1            GRU 0.2  STX  0.492891  0.702041  0.587571  0.727273\n",
              "2        XGBoost 0.2  STX  0.542683  0.736735  0.579805  0.622378\n",
              "3         Logreg 0.2  STX  0.556250  0.744898  0.587459  0.622378\n",
              "4            SVM 0.2  STX  0.568627  0.751020  0.587838  0.608392\n",
              "5      LSTM beta 0.2  STX  0.521429  0.700219  0.515901  0.510490\n",
              "6       GRU beta 0.2  STX  0.519380  0.698031  0.492647  0.468531\n",
              "7   XGBoost beta 0.2  STX  0.522936  0.698031  0.452381  0.398601\n",
              "8    logreg beta 0.2  STX  0.548872  0.715536  0.528986  0.510490\n",
              "9       svm beta 0.2  STX  0.504132  0.689278  0.462121  0.426573\n",
              "0          LSTM 0.15  STX  0.759690  0.732653  0.599388  0.494949\n",
              "1           GRU 0.15  STX  0.724638  0.722449  0.595238  0.505051\n",
              "2       XGBoost 0.15  STX  0.696629  0.738776  0.659574  0.626263\n",
              "3        Logreg 0.15  STX  0.675532  0.730612  0.658031  0.641414\n",
              "4           SVM 0.15  STX  0.698864  0.738776  0.657754  0.621212\n",
              "5     LSTM beta 0.15  STX  0.714286  0.717724  0.640669  0.580808\n",
              "6      GRU beta 0.15  STX  0.683673  0.645514  0.452703  0.338384\n",
              "7  XGBoost beta 0.15  STX  0.686957  0.660832  0.504792  0.398990\n",
              "8   logreg beta 0.15  STX  0.731544  0.717724  0.628242  0.550505\n",
              "9      svm beta 0.15  STX  0.686567  0.676149  0.554217  0.464646"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poMQY66RY6Wf"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('STX_logreg_beta_p.csv')"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2dWRfeBY6Wf"
      },
      "source": [
        ""
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzCPsuaHZPyL"
      },
      "source": [
        "## SYY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ISArb8fZPyL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "46aeabe9-e3a5-4f07-8e4a-77006a16c28d"
      },
      "source": [
        "dfs = pd.read_csv(\"SYY.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "dfs"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2768</td>\n",
              "      <td>US1.SYY</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>78.90</td>\n",
              "      <td>81.415</td>\n",
              "      <td>78.900</td>\n",
              "      <td>80.85</td>\n",
              "      <td>74171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2767</td>\n",
              "      <td>US1.SYY</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>80.37</td>\n",
              "      <td>80.500</td>\n",
              "      <td>78.430</td>\n",
              "      <td>78.51</td>\n",
              "      <td>98224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.SYY</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>80.62</td>\n",
              "      <td>81.760</td>\n",
              "      <td>80.240</td>\n",
              "      <td>80.94</td>\n",
              "      <td>67594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.SYY</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>81.48</td>\n",
              "      <td>82.660</td>\n",
              "      <td>80.540</td>\n",
              "      <td>80.66</td>\n",
              "      <td>79431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.SYY</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>80.87</td>\n",
              "      <td>82.520</td>\n",
              "      <td>80.870</td>\n",
              "      <td>81.68</td>\n",
              "      <td>116348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.SYY</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>28.59</td>\n",
              "      <td>28.630</td>\n",
              "      <td>28.325</td>\n",
              "      <td>28.52</td>\n",
              "      <td>2238435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.SYY</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>28.64</td>\n",
              "      <td>28.680</td>\n",
              "      <td>28.430</td>\n",
              "      <td>28.59</td>\n",
              "      <td>2226012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.SYY</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>28.84</td>\n",
              "      <td>28.840</td>\n",
              "      <td>28.480</td>\n",
              "      <td>28.51</td>\n",
              "      <td>2150778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.SYY</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>28.48</td>\n",
              "      <td>28.830</td>\n",
              "      <td>28.400</td>\n",
              "      <td>28.74</td>\n",
              "      <td>3718972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.SYY</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>28.53</td>\n",
              "      <td>28.710</td>\n",
              "      <td>28.220</td>\n",
              "      <td>28.33</td>\n",
              "      <td>3195008</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2769 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...  <HIGH>   <LOW>  <CLOSE>    <VOL>\n",
              "0      2768  US1.SYY     D  20211001  ...  81.415  78.900    80.85    74171\n",
              "1      2767  US1.SYY     D  20210930  ...  80.500  78.430    78.51    98224\n",
              "2      2766  US1.SYY     D  20210929  ...  81.760  80.240    80.94    67594\n",
              "3      2765  US1.SYY     D  20210928  ...  82.660  80.540    80.66    79431\n",
              "4      2764  US1.SYY     D  20210927  ...  82.520  80.870    81.68   116348\n",
              "...     ...      ...   ...       ...  ...     ...     ...      ...      ...\n",
              "2764      4  US1.SYY     D  20101008  ...  28.630  28.325    28.52  2238435\n",
              "2765      3  US1.SYY     D  20101007  ...  28.680  28.430    28.59  2226012\n",
              "2766      2  US1.SYY     D  20101006  ...  28.840  28.480    28.51  2150778\n",
              "2767      1  US1.SYY     D  20101005  ...  28.830  28.400    28.74  3718972\n",
              "2768      0  US1.SYY     D  20101004  ...  28.710  28.220    28.33  3195008\n",
              "\n",
              "[2769 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ygb5d7xZPyL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "180b94e6-2163-4621-d2d3-4f86f6716018"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"bc59e7fa-c1b2-4749-8620-a7729c88f151\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"bc59e7fa-c1b2-4749-8620-a7729c88f151\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'bc59e7fa-c1b2-4749-8620-a7729c88f151',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [77.59, 78.22, 78.07, 77.96, 78.63, 79.35, 79.36, 78.89, 78.8, 78.7, 78.62, 78.66, 78.13, 78.39, 78.86, 78.39, 77.18, 78.27, 77.76, 77.06, 76.77, 76.69, 75.69, 74.38, 75.09, 74.46, 74.32, 74.48, 74.39, 73.84, 73.36, 72.8, 73.31, 72.25, 72.82, 73.57, 72.98, 72.265, 72.26, 73.35, 72.2, 70.02, 70.64, 69.88, 68.6, 67.14, 68.92, 68.95, 68.56, 69.63, 69.94, 70.88, 70.14, 70.1, 70.17, 70.26, 70.87, 71.87, 71.75, 72.64, 72.83, 72.59, 71.97, 72.05, 71.49, 72.0, 71.75, 71.75, 71.42, 70.67, 70.6, 70.42, 70.31, 71.71, 71.12, 70.67, 70.78, 71.07, 71.68, 71.82, 72.26, 72.54, 72.97, 72.78, 71.72, 71.5, 70.97, 70.81, 69.37, 68.93, 68.8, 74.53, 74.39, 74.54, 75.05, 74.66, 75.4, 75.48, 74.94, 74.91, 74.67, 73.9, 73.62, 73.37, 73.99, 73.56, 73.03, 72.91, 72.97, 70.52, 70.81, 69.97, 70.35, 69.75, 69.11, 69.36, 70.03, 70.41, 70.56, 70.98, 70.63, 70.11, 69.66, 69.01, 68.13, 67.43, 66.97, 67.25, 67.14, 66.85, 66.46, 66.33, 66.77, 66.75, 66.26, 65.62, 66.29, 66.19, 65.56, 65.92, 65.4, 66.18, 66.47, 66.21, 66.14, 66.45, 66.2, 66.46, 65.76, 65.87, 66.27, 66.57, 67.12, 67.58, 67.56, 67.1, 67.42, 67.27, 67.33, 67.21, 67.36, 67.01, 67.54, 66.56, 66.53, 66.94, 66.29, 66.22, 65.12, 65.34, 65.36, 66.64, 63.58, 63.84, 62.9, 62.07, 62.27, 62.01, 62.53, 62.9, 62.67, 62.67, 62.24, 62.22, 62.73, 62.92, 63.04, 63.24, 62.46, 62.785, 61.81, 61.84, 61.34, 62.04, 62.67, 62.27, 62.04, 61.18, 59.95, 60.9, 61.8, 63.49, 64.03, 64.19, 65.44, 66.0, 65.56, 65.31, 64.9, 65.02, 65.95, 66.16, 67.03, 67.4, 67.55, 66.86, 65.34, 64.91, 64.9, 64.5, 65.1, 66.03, 66.49, 66.23, 66.47, 66.88, 67.39, 66.79, 66.57, 66.46, 64.44, 64.51, 71.25, 71.79, 71.33, 72.01, 70.35, 69.8, 71.69, 71.98, 71.49, 70.78, 71.13, 70.55, 70.39, 70.15, 68.96, 68.82, 68.47, 70.4, 71.78, 72.68, 72.01, 72.13, 72.57, 73.28, 73.04, 73.25, 72.84, 72.95, 73.07, 72.97, 72.96, 73.3, 72.92, 73.05, 73.5, 73.54, 73.61, 74.48, 74.94, 75.05, 74.7, 74.97, 75.0, 75.34, 74.82, 73.97, 74.19, 74.41, 74.54, 75.78, 75.21, 75.08, 74.36, 75.18, 74.82, 73.89, 74.89, 74.19, 72.73, 68.4, 70.03, 69.87, 69.56, 69.75, 68.57, 68.04, 67.07, 67.21, 66.67, 71.5, 71.73, 70.92, 70.22, 70.86, 71.0, 70.89, 70.315, 71.08, 70.49, 70.73, 69.84, 69.39, 69.66, 69.32, 69.37, 69.38, 68.54, 67.95, 68.3, 68.35, 68.06, 68.21, 68.22, 67.77, 68.08, 67.42, 66.89, 66.78, 66.89, 66.48, 65.91, 66.22, 66.42, 66.15, 65.76, 65.78, 65.84, 65.74, 65.41, 65.02, 66.05, 65.15, 65.01, 64.6, 64.09, 63.69, 63.64, 63.54, 63.84, 63.34, 62.93, 62.9, 62.82, 62.74, 63.49, 62.75, 63.49, 62.29, 62.18, 62.82, 63.09, 62.5, 63.38, 62.95, 62.46, 61.93, 61.35, 61.15, 61.58, 61.86, 62.5, 61.24, 60.44, 60.52, 60.52, 60.62, 59.87, 59.72, 60.325, 61.2, 59.46, 58.37, 59.96, 59.38, 59.36, 59.97, 59.32, 59.77, 60.3, 60.44, 60.18, 60.12, 59.37, 59.88, 60.7, 60.52, 61.27, 60.47, 60.13, 60.87, 60.02, 59.67, 59.47, 59.66, 59.86, 60.26, 59.67, 58.86, 58.56, 58.9, 60.0, 59.36, 58.36, 58.43, 58.12, 58.31, 57.01, 58.41, 59.64, 57.82, 61.15, 61.99, 62.9, 63.31, 63.39, 64.12, 63.99, 63.74, 63.345, 63.58, 63.22, 62.22, 62.21, 61.72, 61.38, 61.2193, 61.35, 61.39, 61.55, 61.44, 60.84, 60.65, 60.53, 60.74, 61.06, 60.87, 60.76, 60.72, 60.58, 60.78, 61.02, 61.34, 60.94, 60.9, 61.63, 61.82, 61.98, 62.62, 60.92, 59.54, 59.29, 59.52, 57.77, 57.73, 57.69, 56.33, 55.15, 54.92, 54.91, 54.665, 54.97, 54.9, 54.54, 54.1, 54.93, 54.03, 54.19, 54.03, 54.0, 52.59, 54.19, 56.65, 55.85, 55.57, 55.61, 54.7, 54.81, 54.86, 54.56, 54.74, 54.95, 54.89, 54.75, 54.87, 54.81, 54.75, 54.48, 54.28, 53.79, 53.93, 53.98, 54.21, 54.32, 54.24, 53.75, 54.0, 53.95, 54.03, 54.13, 54.13, 54.09, 53.34, 53.9, 54.41, 53.93, 53.92, 53.65, 52.68, 53.5, 52.86, 53.28, 52.93, 53.12, 52.89, 53.03, 52.965, 52.67, 52.72, 52.31, 52.13, 52.33, 51.23, 51.67, 51.62, 51.18, 51.18, 51.2, 51.19, 51.61, 51.06, 51.59, 50.88, 51.3, 52.08, 52.48, 52.33, 52.08, 52.66, 52.57, 52.64, 52.25, 52.57, 52.11, 51.97, 51.55, 51.12, 50.83, 51.02, 50.4, 50.72, 50.57, 50.4, 50.21, 49.24, 49.39, 49.45, 49.105, 49.38, 50.54, 50.34, 49.88, 49.87, 49.29, 49.93, 49.91, 52.8, 53.37, 53.87, 54.49, 54.38, 55.54, 55.44, 55.39, 55.34, 55.18, 54.995, 54.75, 55.09, 55.46, 55.715, 55.4, 54.55, 54.21, 54.42, 54.3, 54.22, 54.24, 54.96, 54.59, 54.44, 54.4, 54.17, 54.03, 53.94, 55.01, 54.55, 54.92, 54.9, 55.33, 54.62, 52.92, 52.865, 52.87, 52.87, 53.43, 53.22, 53.12, 52.85, 52.21, 52.46, 52.11, 52.41, 52.11, 51.9, 52.21, 52.09, 52.025, 51.78, 51.65, 51.55, 51.59, 51.68, 51.93, 52.005, 51.81, 52.2, 52.05, 52.82, 52.655, 52.53, 51.81, 52.11, 52.74, 52.31, 52.58, 51.87, 51.88, 52.17, 52.43, 52.33, 52.31, 52.15, 52.18, 52.76, 53.13, 52.71, 52.85, 53.2, 52.87, 52.75, 52.8, 52.35, 52.61, 52.73, 52.71, 52.8, 52.62, 52.26, 51.82, 52.22, 51.21, 52.5592, 52.38, 52.07, 52.47, 52.73, 52.68, 53.03, 53.06, 53.52, 53.52, 53.81, 53.62, 55.26, 54.89, 54.97, 54.89, 55.29, 55.33, 55.67, 55.39, 55.0, 55.16, 54.99, 55.38, 55.93, 55.72, 56.165, 56.22, 56.26, 56.28, 56.49, 56.61, 56.38, 55.95, 55.51, 55.62, 55.02, 55.15, 54.33, 54.53, 53.97, 53.49, 53.61, 53.1, 53.26, 53.9, 53.7, 53.93, 53.55, 54.11, 53.72, 53.16, 53.36, 53.13, 53.52, 53.49, 53.12, 53.23, 54.16, 54.2, 52.77, 48.04, 47.85, 47.765, 48.075, 48.12, 48.0, 47.47, 47.265, 47.45, 47.73, 47.475, 47.585, 47.93, 48.26, 48.25, 48.33, 48.875, 48.62, 48.02, 48.19, 48.07, 48.28, 47.79, 48.13, 48.91, 49.015, 49.03, 49.44, 49.25, 48.89, 49.55, 50.01, 49.7, 49.1008, 49.24, 49.37, 49.59, 49.63, 49.535, 50.34, 49.81, 52.62, 53.09, 53.455, 53.24, 52.05, 51.87, 51.95, 52.415, 52.14, 52.46, 52.44, 52.75, 52.9, 52.47, 52.45, 52.23, 52.1, 52.17, 52.205, 51.76, 51.83, 51.8, 51.22, 51.24, 51.5, 51.31, 51.93, 51.91, 51.8, 51.82, 51.6, 51.94, 52.13, 52.08, 51.74, 51.82, 52.15, 51.94, 52.005, 51.96, 52.02, 51.08, 51.62, 51.615, 51.16, 51.255, 51.2, 50.71, 50.74, 50.21, 49.5, 49.71, 49.87, 50.28, 49.95, 50.06, 49.95, 49.81, 49.76, 48.67, 48.4797, 48.43, 48.79, 49.0, 48.68, 48.67, 48.78, 48.89, 48.75, 48.79, 48.11, 48.65, 48.84, 48.58, 48.74, 48.41, 48.57, 49.06, 48.7, 49.29, 50.21, 50.14, 50.19, 50.23, 50.18, 49.73, 49.35, 48.88, 48.53, 48.5, 48.59, 46.08, 46.27, 46.42, 46.3, 46.31, 45.77, 45.59, 46.74, 46.99, 46.86, 46.81, 46.6, 47.1, 46.95, 46.72, 46.96, 46.77, 46.97, 46.69, 46.985, 47.1, 46.73, 46.63, 46.46, 46.24, 46.245, 46.35, 46.36, 46.34, 46.61, 46.12, 46.03, 45.87, 45.81, 45.835, 45.11, 45.04, 44.88, 45.1, 45.03, 45.005, 45.04, 44.56, 44.11, 43.25, 43.57, 43.09, 42.91, 42.77, 45.005, 44.49, 44.37, 43.74, 43.19, 42.6, 42.7, 42.6, 43.12, 43.06, 43.44, 43.23, 42.78, 43.12, 39.78, 39.48, 39.3201, 39.25, 39.12, 39.6, 39.405, 39.66, 39.97, 39.85, 40.13, 40.05, 40.69, 40.1399, 39.79, 39.81, 40.18, 40.66, 40.63, 40.99, 41.45, 41.53, 41.52, 41.3, 41.54, 41.29, 40.89, 40.56, 41.27, 41.54, 41.1201, 41.22, 40.91, 41.11, 41.11, 41.17, 41.07, 41.1, 40.5, 41.015, 41.04, 41.1, 41.55, 41.5, 41.31, 41.41, 40.91, 41.41, 41.155, 40.47, 40.59, 39.97, 40.21, 40.35, 40.52, 40.475, 40.94, 41.08, 40.78, 40.95, 41.04, 41.25, 41.8, 41.58, 41.63, 41.48, 41.77, 41.71, 41.55, 41.08, 41.42, 41.43, 40.8, 40.43, 41.23, 41.23, 40.93, 40.94, 40.55, 40.06, 40.205, 39.39, 38.6, 38.97, 38.92, 38.73, 39.7301, 39.72, 39.45, 39.35, 39.86, 39.41, 39.52, 39.52, 39.46, 39.88, 39.94, 39.37, 39.59, 40.06, 39.55, 39.91, 39.75, 39.09, 39.86, 39.91, 40.31, 39.96, 39.06, 39.28, 39.71, 40.54, 40.975, 41.15, 41.15, 41.39, 38.52, 38.64, 37.98, 37.58, 36.93, 36.26, 36.77, 36.46, 36.26, 36.33, 36.06, 36.26, 36.015, 35.79, 36.01, 36.15, 36.21, 36.3, 36.27, 36.345, 36.14, 36.02, 36.16, 36.1, 36.05, 36.01, 36.055, 36.05, 35.68, 35.92, 35.99, 36.09, 37.55, 38.41, 38.3, 38.75, 37.61, 37.62, 37.365, 37.4, 37.31, 37.25, 37.01, 36.93, 37.14, 37.22, 37.11, 37.08, 37.105, 37.56, 37.57, 37.29, 37.22, 37.17, 37.24, 37.54, 37.49, 37.68, 38.1, 38.07, 38.29, 37.475, 37.28, 37.42, 36.86, 36.61, 36.5, 36.62, 36.38, 35.96, 36.1, 36.66, 37.16, 37.02, 37.425, 37.74, 37.78, 37.92, 37.97, 37.62, 37.61, 37.29, 37.335, 37.31, 37.66, 37.81, 37.63, 38.18, 38.17, 38.16, 37.96, 37.84, 37.64, 37.55, 37.72, 38.26, 38.165, 37.88, 38.01, 38.25, 38.65, 38.6, 38.21, 38.535, 38.52, 38.7, 38.57, 38.8, 38.4, 38.46, 38.92, 38.64, 39.11, 38.8, 38.9, 38.97, 38.98, 39.14, 39.465, 39.555, 39.61, 39.68, 38.52, 39.93, 39.565, 39.55, 39.65, 38.91, 39.8, 40.1, 40.26, 39.86, 39.87, 39.52, 38.44, 39.18, 39.9, 40.11, 40.71, 40.92, 40.98, 40.7, 40.31, 39.56, 39.99, 40.25, 40.28, 40.76, 40.88, 41.25, 41.12, 40.4, 39.26, 39.44, 39.86, 39.69, 40.27, 40.69, 40.6, 40.59, 40.7, 40.91, 40.49, 40.74, 40.11, 39.73, 39.6, 39.33, 40.13, 39.41, 39.98, 40.01, 39.92, 39.82, 39.95, 39.97, 40.25, 40.26, 39.94, 39.47, 39.555, 39.69, 39.11, 39.14, 39.15, 38.89, 38.58, 38.74, 38.925, 38.53, 38.66, 38.14, 37.98, 37.83, 37.35, 37.47, 38.525, 38.56, 38.21, 38.21, 38.24, 38.04, 37.92, 37.66, 37.615, 36.99, 36.5099, 36.22, 36.6, 36.68, 36.44, 37.03, 37.17, 37.67, 37.05, 37.575, 37.69, 37.19, 36.99, 37.97, 37.95, 37.52, 37.46, 38.0, 36.75, 37.58, 37.975, 38.0, 38.0, 38.14, 37.78, 37.83, 38.25, 38.27, 38.26, 38.76, 38.62, 38.62, 38.51, 38.2, 37.81, 37.8, 37.76, 37.78, 37.79, 37.59, 37.67, 37.56, 37.67, 37.6, 37.44, 37.64, 37.74, 37.35, 37.45, 36.26, 36.28, 35.96, 35.53, 35.98, 35.78, 35.69, 36.25, 36.6, 36.61, 36.735, 36.9, 37.02, 36.95, 36.7, 36.74, 36.45, 37.07, 36.86, 36.85, 36.98, 36.94, 37.11, 36.77, 37.07, 37.24, 37.16, 37.15, 37.46, 37.85, 37.61, 37.46, 37.79, 37.54, 37.19, 37.24, 37.22, 36.9071, 36.89, 36.94, 37.65, 37.69, 37.82, 37.71, 37.67, 37.53, 37.22, 37.26, 37.63, 37.55, 37.17, 36.665, 36.76, 36.58, 36.48, 36.5, 36.48, 36.7, 36.58, 36.42, 36.63, 36.815, 36.76, 36.81, 36.6, 36.75, 36.57, 37.185, 36.2, 36.21, 36.42, 36.24, 36.46, 36.05, 36.39, 36.42, 36.37, 36.34, 36.04, 35.92, 35.77, 35.74, 35.35, 35.93, 36.18, 35.84, 35.87, 35.66, 35.85, 35.74, 36.11, 36.14, 35.97, 35.47, 35.85, 35.85, 35.85, 36.07, 36.25, 36.15, 36.5, 36.16, 35.95, 35.84, 36.21, 36.14, 36.2, 36.22, 36.3, 36.19, 36.39, 36.01, 36.02, 35.95, 36.0, 36.1, 35.98, 36.05, 35.84, 35.83, 35.92, 35.71, 35.62, 35.33, 35.67, 35.39, 35.51, 35.28, 34.755, 34.59, 34.52, 35.07, 35.15, 35.06, 35.74, 35.57, 35.48, 36.12, 36.42, 36.55, 36.52, 36.89, 36.76, 36.65, 36.29, 36.36, 35.94, 35.99, 36.46, 36.0, 36.06, 35.91, 36.09, 36.67, 36.82, 36.53, 36.47, 36.51, 36.47, 36.25, 36.27, 35.96, 36.19, 36.26, 36.24, 36.95, 36.98, 37.63, 34.31, 33.56, 33.64, 33.7, 33.64, 33.8043, 33.81, 34.17, 33.79, 33.95, 33.52, 33.33, 33.61, 33.64, 33.705, 33.58, 33.39, 33.1499, 33.08, 33.13, 33.14, 33.6501, 33.32, 33.96, 32.56, 32.35, 32.48, 32.86, 33.24, 32.98, 32.82, 32.55]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('bc59e7fa-c1b2-4749-8620-a7729c88f151');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"5f786ede-0bd7-4087-ad66-8c06b7d5e7f5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"5f786ede-0bd7-4087-ad66-8c06b7d5e7f5\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '5f786ede-0bd7-4087-ad66-8c06b7d5e7f5',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('5f786ede-0bd7-4087-ad66-8c06b7d5e7f5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgRpVZRpZPyL"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI-cTjlTZPyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f95a2f59-75cd-46e8-c452-0e08bb019b6a"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"SYY\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6592 - accuracy: 0.6396 - val_loss: 0.5518 - val_accuracy: 0.8857\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6544 - accuracy: 0.6450 - val_loss: 0.5046 - val_accuracy: 0.8857\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6386 - accuracy: 0.6557 - val_loss: 0.4101 - val_accuracy: 0.8898\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6114 - accuracy: 0.6752 - val_loss: 0.5532 - val_accuracy: 0.7735\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6017 - accuracy: 0.6866 - val_loss: 0.6022 - val_accuracy: 0.7408\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6565 - accuracy: 0.6423 - val_loss: 0.5127 - val_accuracy: 0.8857\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6374 - accuracy: 0.6611 - val_loss: 0.4194 - val_accuracy: 0.8857\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5989 - accuracy: 0.6819 - val_loss: 0.4868 - val_accuracy: 0.8510\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5890 - accuracy: 0.6805 - val_loss: 0.4766 - val_accuracy: 0.8510\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5704 - accuracy: 0.7121 - val_loss: 0.5043 - val_accuracy: 0.8265\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.678386\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.675177\n",
            "[2]\tvalidation_0-auc:0.670445\n",
            "[3]\tvalidation_0-auc:0.680855\n",
            "[4]\tvalidation_0-auc:0.67707\n",
            "[5]\tvalidation_0-auc:0.672914\n",
            "[6]\tvalidation_0-auc:0.672626\n",
            "[7]\tvalidation_0-auc:0.674375\n",
            "[8]\tvalidation_0-auc:0.674128\n",
            "[9]\tvalidation_0-auc:0.668388\n",
            "[10]\tvalidation_0-auc:0.669828\n",
            "[11]\tvalidation_0-auc:0.670054\n",
            "[12]\tvalidation_0-auc:0.667832\n",
            "[13]\tvalidation_0-auc:0.671721\n",
            "[14]\tvalidation_0-auc:0.672976\n",
            "[15]\tvalidation_0-auc:0.675856\n",
            "[16]\tvalidation_0-auc:0.674004\n",
            "[17]\tvalidation_0-auc:0.674416\n",
            "[18]\tvalidation_0-auc:0.674292\n",
            "[19]\tvalidation_0-auc:0.675774\n",
            "[20]\tvalidation_0-auc:0.676267\n",
            "[21]\tvalidation_0-auc:0.67707\n",
            "[22]\tvalidation_0-auc:0.675465\n",
            "[23]\tvalidation_0-auc:0.674889\n",
            "[24]\tvalidation_0-auc:0.674704\n",
            "[25]\tvalidation_0-auc:0.675341\n",
            "[26]\tvalidation_0-auc:0.675774\n",
            "[27]\tvalidation_0-auc:0.677625\n",
            "[28]\tvalidation_0-auc:0.677666\n",
            "[29]\tvalidation_0-auc:0.680176\n",
            "[30]\tvalidation_0-auc:0.680053\n",
            "[31]\tvalidation_0-auc:0.680382\n",
            "[32]\tvalidation_0-auc:0.679045\n",
            "[33]\tvalidation_0-auc:0.678427\n",
            "[34]\tvalidation_0-auc:0.678345\n",
            "[35]\tvalidation_0-auc:0.678427\n",
            "[36]\tvalidation_0-auc:0.680279\n",
            "[37]\tvalidation_0-auc:0.679291\n",
            "[38]\tvalidation_0-auc:0.680238\n",
            "[39]\tvalidation_0-auc:0.681596\n",
            "[40]\tvalidation_0-auc:0.68283\n",
            "[41]\tvalidation_0-auc:0.682871\n",
            "[42]\tvalidation_0-auc:0.682665\n",
            "[43]\tvalidation_0-auc:0.680958\n",
            "[44]\tvalidation_0-auc:0.68067\n",
            "[45]\tvalidation_0-auc:0.680999\n",
            "[46]\tvalidation_0-auc:0.68141\n",
            "[47]\tvalidation_0-auc:0.681328\n",
            "[48]\tvalidation_0-auc:0.682645\n",
            "[49]\tvalidation_0-auc:0.682316\n",
            "[50]\tvalidation_0-auc:0.681246\n",
            "[51]\tvalidation_0-auc:0.683427\n",
            "[52]\tvalidation_0-auc:0.683056\n",
            "[53]\tvalidation_0-auc:0.683673\n",
            "[54]\tvalidation_0-auc:0.683715\n",
            "[55]\tvalidation_0-auc:0.685772\n",
            "[56]\tvalidation_0-auc:0.686307\n",
            "[57]\tvalidation_0-auc:0.686266\n",
            "[58]\tvalidation_0-auc:0.685402\n",
            "[59]\tvalidation_0-auc:0.685155\n",
            "[60]\tvalidation_0-auc:0.685402\n",
            "[61]\tvalidation_0-auc:0.686986\n",
            "[62]\tvalidation_0-auc:0.686883\n",
            "[63]\tvalidation_0-auc:0.687006\n",
            "[64]\tvalidation_0-auc:0.686842\n",
            "[65]\tvalidation_0-auc:0.687582\n",
            "[66]\tvalidation_0-auc:0.687294\n",
            "[67]\tvalidation_0-auc:0.687047\n",
            "[68]\tvalidation_0-auc:0.687006\n",
            "[69]\tvalidation_0-auc:0.686842\n",
            "[70]\tvalidation_0-auc:0.687089\n",
            "[71]\tvalidation_0-auc:0.687006\n",
            "[72]\tvalidation_0-auc:0.688035\n",
            "[73]\tvalidation_0-auc:0.68857\n",
            "[74]\tvalidation_0-auc:0.688529\n",
            "[75]\tvalidation_0-auc:0.687911\n",
            "[76]\tvalidation_0-auc:0.687665\n",
            "[77]\tvalidation_0-auc:0.687047\n",
            "[78]\tvalidation_0-auc:0.688899\n",
            "[79]\tvalidation_0-auc:0.690051\n",
            "[80]\tvalidation_0-auc:0.691327\n",
            "[81]\tvalidation_0-auc:0.690586\n",
            "[82]\tvalidation_0-auc:0.691738\n",
            "[83]\tvalidation_0-auc:0.691615\n",
            "[84]\tvalidation_0-auc:0.690874\n",
            "[85]\tvalidation_0-auc:0.690504\n",
            "[86]\tvalidation_0-auc:0.690586\n",
            "[87]\tvalidation_0-auc:0.690174\n",
            "[88]\tvalidation_0-auc:0.689475\n",
            "[89]\tvalidation_0-auc:0.691327\n",
            "[90]\tvalidation_0-auc:0.691779\n",
            "[91]\tvalidation_0-auc:0.691738\n",
            "[92]\tvalidation_0-auc:0.691944\n",
            "[93]\tvalidation_0-auc:0.691944\n",
            "[94]\tvalidation_0-auc:0.692067\n",
            "[95]\tvalidation_0-auc:0.691738\n",
            "[96]\tvalidation_0-auc:0.69252\n",
            "[97]\tvalidation_0-auc:0.691697\n",
            "[98]\tvalidation_0-auc:0.691038\n",
            "[99]\tvalidation_0-auc:0.690997\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 4s 15ms/step - loss: 0.6575 - accuracy: 0.6541 - val_loss: 0.5482 - val_accuracy: 0.8775\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6307 - accuracy: 0.6664 - val_loss: 0.6455 - val_accuracy: 0.5886\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5908 - accuracy: 0.6973 - val_loss: 0.5784 - val_accuracy: 0.6937\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5562 - accuracy: 0.7207 - val_loss: 0.4211 - val_accuracy: 0.8862\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5387 - accuracy: 0.7461 - val_loss: 0.4394 - val_accuracy: 0.8425\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6357 - accuracy: 0.6568 - val_loss: 0.4939 - val_accuracy: 0.8775\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5632 - accuracy: 0.7282 - val_loss: 0.4355 - val_accuracy: 0.8884\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5281 - accuracy: 0.7598 - val_loss: 0.5175 - val_accuracy: 0.7615\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5250 - accuracy: 0.7618 - val_loss: 0.4065 - val_accuracy: 0.8862\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5094 - accuracy: 0.7653 - val_loss: 0.4568 - val_accuracy: 0.8118\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.66726\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.672805\n",
            "[2]\tvalidation_0-auc:0.678171\n",
            "[3]\tvalidation_0-auc:0.677124\n",
            "[4]\tvalidation_0-auc:0.672871\n",
            "[5]\tvalidation_0-auc:0.673762\n",
            "[6]\tvalidation_0-auc:0.673495\n",
            "[7]\tvalidation_0-auc:0.675699\n",
            "[8]\tvalidation_0-auc:0.672716\n",
            "[9]\tvalidation_0-auc:0.670756\n",
            "[10]\tvalidation_0-auc:0.67305\n",
            "[11]\tvalidation_0-auc:0.671981\n",
            "[12]\tvalidation_0-auc:0.669264\n",
            "[13]\tvalidation_0-auc:0.66824\n",
            "[14]\tvalidation_0-auc:0.671424\n",
            "[15]\tvalidation_0-auc:0.669598\n",
            "[16]\tvalidation_0-auc:0.66971\n",
            "[17]\tvalidation_0-auc:0.672315\n",
            "[18]\tvalidation_0-auc:0.673807\n",
            "[19]\tvalidation_0-auc:0.673361\n",
            "[20]\tvalidation_0-auc:0.67345\n",
            "[21]\tvalidation_0-auc:0.675321\n",
            "[22]\tvalidation_0-auc:0.677235\n",
            "[23]\tvalidation_0-auc:0.675566\n",
            "[24]\tvalidation_0-auc:0.677814\n",
            "[25]\tvalidation_0-auc:0.674831\n",
            "[26]\tvalidation_0-auc:0.676701\n",
            "[27]\tvalidation_0-auc:0.67826\n",
            "[28]\tvalidation_0-auc:0.679418\n",
            "[29]\tvalidation_0-auc:0.679551\n",
            "[30]\tvalidation_0-auc:0.680598\n",
            "[31]\tvalidation_0-auc:0.678638\n",
            "[32]\tvalidation_0-auc:0.677614\n",
            "[33]\tvalidation_0-auc:0.678772\n",
            "[34]\tvalidation_0-auc:0.680241\n",
            "[35]\tvalidation_0-auc:0.681043\n",
            "[36]\tvalidation_0-auc:0.680019\n",
            "[37]\tvalidation_0-auc:0.680642\n",
            "[38]\tvalidation_0-auc:0.68033\n",
            "[39]\tvalidation_0-auc:0.679596\n",
            "[40]\tvalidation_0-auc:0.676478\n",
            "[41]\tvalidation_0-auc:0.677102\n",
            "[42]\tvalidation_0-auc:0.677993\n",
            "[43]\tvalidation_0-auc:0.680108\n",
            "[44]\tvalidation_0-auc:0.68082\n",
            "[45]\tvalidation_0-auc:0.680932\n",
            "[46]\tvalidation_0-auc:0.679774\n",
            "[47]\tvalidation_0-auc:0.680553\n",
            "[48]\tvalidation_0-auc:0.680998\n",
            "[49]\tvalidation_0-auc:0.682023\n",
            "[50]\tvalidation_0-auc:0.682824\n",
            "[51]\tvalidation_0-auc:0.683537\n",
            "[52]\tvalidation_0-auc:0.682512\n",
            "[53]\tvalidation_0-auc:0.682824\n",
            "[54]\tvalidation_0-auc:0.683804\n",
            "[55]\tvalidation_0-auc:0.683626\n",
            "[56]\tvalidation_0-auc:0.683492\n",
            "[57]\tvalidation_0-auc:0.682446\n",
            "[58]\tvalidation_0-auc:0.683871\n",
            "[59]\tvalidation_0-auc:0.684004\n",
            "[60]\tvalidation_0-auc:0.684316\n",
            "[61]\tvalidation_0-auc:0.684806\n",
            "[62]\tvalidation_0-auc:0.683025\n",
            "[63]\tvalidation_0-auc:0.683114\n",
            "[64]\tvalidation_0-auc:0.681956\n",
            "[65]\tvalidation_0-auc:0.682624\n",
            "[66]\tvalidation_0-auc:0.680976\n",
            "[67]\tvalidation_0-auc:0.680753\n",
            "[68]\tvalidation_0-auc:0.680598\n",
            "[69]\tvalidation_0-auc:0.679974\n",
            "[70]\tvalidation_0-auc:0.680998\n",
            "[71]\tvalidation_0-auc:0.680642\n",
            "[72]\tvalidation_0-auc:0.681266\n",
            "[73]\tvalidation_0-auc:0.682646\n",
            "[74]\tvalidation_0-auc:0.683091\n",
            "[75]\tvalidation_0-auc:0.683626\n",
            "[76]\tvalidation_0-auc:0.682112\n",
            "[77]\tvalidation_0-auc:0.681755\n",
            "[78]\tvalidation_0-auc:0.681355\n",
            "[79]\tvalidation_0-auc:0.681132\n",
            "[80]\tvalidation_0-auc:0.681399\n",
            "[81]\tvalidation_0-auc:0.681488\n",
            "[82]\tvalidation_0-auc:0.684182\n",
            "[83]\tvalidation_0-auc:0.681555\n",
            "[84]\tvalidation_0-auc:0.681689\n",
            "[85]\tvalidation_0-auc:0.680843\n",
            "[86]\tvalidation_0-auc:0.680843\n",
            "[87]\tvalidation_0-auc:0.68111\n",
            "[88]\tvalidation_0-auc:0.683693\n",
            "[89]\tvalidation_0-auc:0.68347\n",
            "[90]\tvalidation_0-auc:0.684138\n",
            "[91]\tvalidation_0-auc:0.684182\n",
            "[92]\tvalidation_0-auc:0.685696\n",
            "[93]\tvalidation_0-auc:0.685607\n",
            "[94]\tvalidation_0-auc:0.685875\n",
            "[95]\tvalidation_0-auc:0.686721\n",
            "[96]\tvalidation_0-auc:0.687166\n",
            "[97]\tvalidation_0-auc:0.687122\n",
            "[98]\tvalidation_0-auc:0.686988\n",
            "[99]\tvalidation_0-auc:0.68583\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.7408163265306122 |  0.2248062015503876 |  0.5178571428571429 | 0.31351351351351353 |\n",
            "|     GRU 0.1      | 0.826530612244898  |  0.3013698630136986 | 0.39285714285714285 |  0.3410852713178294 |\n",
            "|   XGBoost 0.1    | 0.789795918367347  |  0.2761904761904762 |  0.5178571428571429 |  0.360248447204969  |\n",
            "|    Logreg 0.1    | 0.8775510204081632 |        0.4375       |         0.25        |  0.3181818181818182 |\n",
            "|     SVM 0.1      | 0.8489795918367347 |         0.35        |        0.375        |  0.3620689655172413 |\n",
            "|  LSTM beta 0.1   | 0.8424507658643327 |  0.3333333333333333 |  0.2857142857142857 | 0.30769230769230765 |\n",
            "|   GRU beta 0.1   | 0.811816192560175  |  0.2972972972972973 | 0.39285714285714285 |  0.3384615384615385 |\n",
            "| XGBoost beta 0.1 | 0.7571115973741794 | 0.22772277227722773 |  0.4107142857142857 |  0.2929936305732484 |\n",
            "| logreg beta 0.1  |  0.87527352297593  |  0.4864864864864865 | 0.32142857142857145 |  0.3870967741935484 |\n",
            "|   svm beta 0.1   | 0.8030634573304157 | 0.27631578947368424 |        0.375        |  0.3181818181818182 |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6182 - accuracy: 0.7087 - val_loss: 0.4258 - val_accuracy: 0.8857\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6065 - accuracy: 0.7101 - val_loss: 0.4425 - val_accuracy: 0.8857\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6066 - accuracy: 0.7101 - val_loss: 0.4261 - val_accuracy: 0.8857\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5975 - accuracy: 0.7094 - val_loss: 0.4350 - val_accuracy: 0.8857\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6089 - accuracy: 0.7047 - val_loss: 0.4283 - val_accuracy: 0.8857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6108 - accuracy: 0.7107 - val_loss: 0.4114 - val_accuracy: 0.8857\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5911 - accuracy: 0.7141 - val_loss: 0.3917 - val_accuracy: 0.8857\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5765 - accuracy: 0.7181 - val_loss: 0.4443 - val_accuracy: 0.8939\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5605 - accuracy: 0.7275 - val_loss: 0.4613 - val_accuracy: 0.8837\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5520 - accuracy: 0.7268 - val_loss: 0.3987 - val_accuracy: 0.8959\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.670178\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.666413\n",
            "[2]\tvalidation_0-auc:0.667791\n",
            "[3]\tvalidation_0-auc:0.657711\n",
            "[4]\tvalidation_0-auc:0.665076\n",
            "[5]\tvalidation_0-auc:0.677995\n",
            "[6]\tvalidation_0-auc:0.671597\n",
            "[7]\tvalidation_0-auc:0.669375\n",
            "[8]\tvalidation_0-auc:0.670095\n",
            "[9]\tvalidation_0-auc:0.668779\n",
            "[10]\tvalidation_0-auc:0.665569\n",
            "[11]\tvalidation_0-auc:0.67421\n",
            "[12]\tvalidation_0-auc:0.672832\n",
            "[13]\tvalidation_0-auc:0.676905\n",
            "[14]\tvalidation_0-auc:0.675527\n",
            "[15]\tvalidation_0-auc:0.676226\n",
            "[16]\tvalidation_0-auc:0.67635\n",
            "[17]\tvalidation_0-auc:0.675876\n",
            "[18]\tvalidation_0-auc:0.677255\n",
            "[19]\tvalidation_0-auc:0.677152\n",
            "[20]\tvalidation_0-auc:0.677255\n",
            "[21]\tvalidation_0-auc:0.68141\n",
            "[22]\tvalidation_0-auc:0.68104\n",
            "[23]\tvalidation_0-auc:0.68141\n",
            "[24]\tvalidation_0-auc:0.682028\n",
            "[25]\tvalidation_0-auc:0.681061\n",
            "[26]\tvalidation_0-auc:0.680937\n",
            "[27]\tvalidation_0-auc:0.683838\n",
            "[28]\tvalidation_0-auc:0.683344\n",
            "[29]\tvalidation_0-auc:0.685813\n",
            "[30]\tvalidation_0-auc:0.684208\n",
            "[31]\tvalidation_0-auc:0.686019\n",
            "[32]\tvalidation_0-auc:0.682707\n",
            "[33]\tvalidation_0-auc:0.683118\n",
            "[34]\tvalidation_0-auc:0.683241\n",
            "[35]\tvalidation_0-auc:0.683571\n",
            "[36]\tvalidation_0-auc:0.683818\n",
            "[37]\tvalidation_0-auc:0.683838\n",
            "[38]\tvalidation_0-auc:0.684908\n",
            "[39]\tvalidation_0-auc:0.68534\n",
            "[40]\tvalidation_0-auc:0.684414\n",
            "[41]\tvalidation_0-auc:0.685648\n",
            "[42]\tvalidation_0-auc:0.685566\n",
            "[43]\tvalidation_0-auc:0.686615\n",
            "[44]\tvalidation_0-auc:0.686759\n",
            "[45]\tvalidation_0-auc:0.686142\n",
            "[46]\tvalidation_0-auc:0.687089\n",
            "[47]\tvalidation_0-auc:0.685772\n",
            "[48]\tvalidation_0-auc:0.68499\n",
            "[49]\tvalidation_0-auc:0.685196\n",
            "[50]\tvalidation_0-auc:0.685895\n",
            "[51]\tvalidation_0-auc:0.682521\n",
            "[52]\tvalidation_0-auc:0.682686\n",
            "[53]\tvalidation_0-auc:0.680917\n",
            "[54]\tvalidation_0-auc:0.680094\n",
            "[55]\tvalidation_0-auc:0.679641\n",
            "[56]\tvalidation_0-auc:0.680382\n",
            "[57]\tvalidation_0-auc:0.679189\n",
            "[58]\tvalidation_0-auc:0.678448\n",
            "[59]\tvalidation_0-auc:0.679559\n",
            "[60]\tvalidation_0-auc:0.680217\n",
            "[61]\tvalidation_0-auc:0.679271\n",
            "[62]\tvalidation_0-auc:0.680258\n",
            "[63]\tvalidation_0-auc:0.6803\n",
            "[64]\tvalidation_0-auc:0.680217\n",
            "[65]\tvalidation_0-auc:0.679888\n",
            "[66]\tvalidation_0-auc:0.677831\n",
            "[67]\tvalidation_0-auc:0.677049\n",
            "[68]\tvalidation_0-auc:0.67779\n",
            "[69]\tvalidation_0-auc:0.679724\n",
            "[70]\tvalidation_0-auc:0.679271\n",
            "[71]\tvalidation_0-auc:0.678695\n",
            "[72]\tvalidation_0-auc:0.678818\n",
            "[73]\tvalidation_0-auc:0.677872\n",
            "[74]\tvalidation_0-auc:0.678201\n",
            "[75]\tvalidation_0-auc:0.678613\n",
            "[76]\tvalidation_0-auc:0.681246\n",
            "[77]\tvalidation_0-auc:0.681452\n",
            "[78]\tvalidation_0-auc:0.683097\n",
            "[79]\tvalidation_0-auc:0.682439\n",
            "[80]\tvalidation_0-auc:0.682192\n",
            "[81]\tvalidation_0-auc:0.683962\n",
            "[82]\tvalidation_0-auc:0.685319\n",
            "[83]\tvalidation_0-auc:0.683056\n",
            "[84]\tvalidation_0-auc:0.683756\n",
            "[85]\tvalidation_0-auc:0.681575\n",
            "[86]\tvalidation_0-auc:0.68141\n",
            "[87]\tvalidation_0-auc:0.680382\n",
            "[88]\tvalidation_0-auc:0.678942\n",
            "[89]\tvalidation_0-auc:0.679065\n",
            "[90]\tvalidation_0-auc:0.67853\n",
            "[91]\tvalidation_0-auc:0.678736\n",
            "[92]\tvalidation_0-auc:0.677913\n",
            "[93]\tvalidation_0-auc:0.674827\n",
            "[94]\tvalidation_0-auc:0.676391\n",
            "[95]\tvalidation_0-auc:0.676679\n",
            "[96]\tvalidation_0-auc:0.678777\n",
            "Stopping. Best iteration:\n",
            "[46]\tvalidation_0-auc:0.687089\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6057 - accuracy: 0.7186 - val_loss: 0.4166 - val_accuracy: 0.8775\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5927 - accuracy: 0.7145 - val_loss: 0.4068 - val_accuracy: 0.8621\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5846 - accuracy: 0.7303 - val_loss: 0.4349 - val_accuracy: 0.8775\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5657 - accuracy: 0.7481 - val_loss: 0.5090 - val_accuracy: 0.7374\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5696 - accuracy: 0.7385 - val_loss: 0.4139 - val_accuracy: 0.8775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.5750 - accuracy: 0.7248 - val_loss: 0.3878 - val_accuracy: 0.8753\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4836 - accuracy: 0.7721 - val_loss: 0.5257 - val_accuracy: 0.7374\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4627 - accuracy: 0.7900 - val_loss: 0.3933 - val_accuracy: 0.8862\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4531 - accuracy: 0.7996 - val_loss: 0.4293 - val_accuracy: 0.8796\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4449 - accuracy: 0.7962 - val_loss: 0.4669 - val_accuracy: 0.8118\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.662585\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.663275\n",
            "[2]\tvalidation_0-auc:0.667884\n",
            "[3]\tvalidation_0-auc:0.670244\n",
            "[4]\tvalidation_0-auc:0.633194\n",
            "[5]\tvalidation_0-auc:0.633639\n",
            "[6]\tvalidation_0-auc:0.642835\n",
            "[7]\tvalidation_0-auc:0.643859\n",
            "[8]\tvalidation_0-auc:0.641031\n",
            "[9]\tvalidation_0-auc:0.659623\n",
            "[10]\tvalidation_0-auc:0.665568\n",
            "[11]\tvalidation_0-auc:0.670155\n",
            "[12]\tvalidation_0-auc:0.676122\n",
            "[13]\tvalidation_0-auc:0.668374\n",
            "[14]\tvalidation_0-auc:0.67423\n",
            "[15]\tvalidation_0-auc:0.677859\n",
            "[16]\tvalidation_0-auc:0.677146\n",
            "[17]\tvalidation_0-auc:0.670556\n",
            "[18]\tvalidation_0-auc:0.67561\n",
            "[19]\tvalidation_0-auc:0.677169\n",
            "[20]\tvalidation_0-auc:0.676545\n",
            "[21]\tvalidation_0-auc:0.675944\n",
            "[22]\tvalidation_0-auc:0.679328\n",
            "[23]\tvalidation_0-auc:0.677926\n",
            "[24]\tvalidation_0-auc:0.679262\n",
            "[25]\tvalidation_0-auc:0.682713\n",
            "[26]\tvalidation_0-auc:0.683581\n",
            "[27]\tvalidation_0-auc:0.684873\n",
            "[28]\tvalidation_0-auc:0.686209\n",
            "[29]\tvalidation_0-auc:0.682045\n",
            "[30]\tvalidation_0-auc:0.68367\n",
            "[31]\tvalidation_0-auc:0.678082\n",
            "[32]\tvalidation_0-auc:0.678883\n",
            "[33]\tvalidation_0-auc:0.682646\n",
            "[34]\tvalidation_0-auc:0.681488\n",
            "[35]\tvalidation_0-auc:0.683002\n",
            "[36]\tvalidation_0-auc:0.685006\n",
            "[37]\tvalidation_0-auc:0.681911\n",
            "[38]\tvalidation_0-auc:0.683247\n",
            "[39]\tvalidation_0-auc:0.682357\n",
            "[40]\tvalidation_0-auc:0.680843\n",
            "[41]\tvalidation_0-auc:0.679818\n",
            "[42]\tvalidation_0-auc:0.680152\n",
            "[43]\tvalidation_0-auc:0.683091\n",
            "[44]\tvalidation_0-auc:0.68445\n",
            "[45]\tvalidation_0-auc:0.687255\n",
            "[46]\tvalidation_0-auc:0.688881\n",
            "[47]\tvalidation_0-auc:0.688836\n",
            "[48]\tvalidation_0-auc:0.690862\n",
            "[49]\tvalidation_0-auc:0.693\n",
            "[50]\tvalidation_0-auc:0.691575\n",
            "[51]\tvalidation_0-auc:0.691085\n",
            "[52]\tvalidation_0-auc:0.690773\n",
            "[53]\tvalidation_0-auc:0.690907\n",
            "[54]\tvalidation_0-auc:0.691174\n",
            "[55]\tvalidation_0-auc:0.690239\n",
            "[56]\tvalidation_0-auc:0.692599\n",
            "[57]\tvalidation_0-auc:0.694024\n",
            "[58]\tvalidation_0-auc:0.696206\n",
            "[59]\tvalidation_0-auc:0.697097\n",
            "[60]\tvalidation_0-auc:0.699412\n",
            "[61]\tvalidation_0-auc:0.700525\n",
            "[62]\tvalidation_0-auc:0.703732\n",
            "[63]\tvalidation_0-auc:0.699947\n",
            "[64]\tvalidation_0-auc:0.700347\n",
            "[65]\tvalidation_0-auc:0.699902\n",
            "[66]\tvalidation_0-auc:0.699412\n",
            "[67]\tvalidation_0-auc:0.699947\n",
            "[68]\tvalidation_0-auc:0.703821\n",
            "[69]\tvalidation_0-auc:0.705914\n",
            "[70]\tvalidation_0-auc:0.70725\n",
            "[71]\tvalidation_0-auc:0.707294\n",
            "[72]\tvalidation_0-auc:0.707918\n",
            "[73]\tvalidation_0-auc:0.70774\n",
            "[74]\tvalidation_0-auc:0.70892\n",
            "[75]\tvalidation_0-auc:0.708875\n",
            "[76]\tvalidation_0-auc:0.708697\n",
            "[77]\tvalidation_0-auc:0.710924\n",
            "[78]\tvalidation_0-auc:0.709209\n",
            "[79]\tvalidation_0-auc:0.709387\n",
            "[80]\tvalidation_0-auc:0.70725\n",
            "[81]\tvalidation_0-auc:0.707695\n",
            "[82]\tvalidation_0-auc:0.707962\n",
            "[83]\tvalidation_0-auc:0.708986\n",
            "[84]\tvalidation_0-auc:0.709788\n",
            "[85]\tvalidation_0-auc:0.709031\n",
            "[86]\tvalidation_0-auc:0.707561\n",
            "[87]\tvalidation_0-auc:0.708318\n",
            "[88]\tvalidation_0-auc:0.708096\n",
            "[89]\tvalidation_0-auc:0.707339\n",
            "[90]\tvalidation_0-auc:0.705246\n",
            "[91]\tvalidation_0-auc:0.7048\n",
            "[92]\tvalidation_0-auc:0.703776\n",
            "[93]\tvalidation_0-auc:0.703197\n",
            "[94]\tvalidation_0-auc:0.702841\n",
            "[95]\tvalidation_0-auc:0.703153\n",
            "[96]\tvalidation_0-auc:0.703865\n",
            "[97]\tvalidation_0-auc:0.702886\n",
            "[98]\tvalidation_0-auc:0.702218\n",
            "[99]\tvalidation_0-auc:0.703554\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.8857142857142857 |        0.0         |         0.0         |         0.0         |\n",
            "|      GRU 0.15     | 0.8959183673469387 | 0.8571428571428571 | 0.10714285714285714 | 0.19047619047619047 |\n",
            "|    XGBoost 0.15   | 0.8755102040816326 | 0.4074074074074074 | 0.19642857142857142 |  0.2650602409638554 |\n",
            "|    Logreg 0.15    | 0.8959183673469387 | 0.8571428571428571 | 0.10714285714285714 | 0.19047619047619047 |\n",
            "|      SVM 0.15     |        0.9         |        1.0         |        0.125        |  0.2222222222222222 |\n",
            "|   LSTM beta 0.15  | 0.8774617067833698 |        0.0         |         0.0         |         0.0         |\n",
            "|   GRU beta 0.15   | 0.811816192560175  | 0.2972972972972973 | 0.39285714285714285 |  0.3384615384615385 |\n",
            "| XGBoost beta 0.15 | 0.8096280087527352 | 0.2876712328767123 |        0.375        |  0.3255813953488372 |\n",
            "|  logreg beta 0.15 | 0.8905908096280087 | 0.6153846153846154 |  0.2857142857142857 |  0.3902439024390244 |\n",
            "|   svm beta 0.15   | 0.8905908096280087 | 0.6153846153846154 |  0.2857142857142857 |  0.3902439024390244 |\n",
            "+-------------------+--------------------+--------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJIfb2q0ZPyM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c4d96c7f-f682-4763-a70b-a3a99ac8dbe4"
      },
      "source": [
        "Result_cross.to_csv('SYY_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.224806</td>\n",
              "      <td>0.740816</td>\n",
              "      <td>0.313514</td>\n",
              "      <td>0.517857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.301370</td>\n",
              "      <td>0.826531</td>\n",
              "      <td>0.341085</td>\n",
              "      <td>0.392857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.276190</td>\n",
              "      <td>0.789796</td>\n",
              "      <td>0.360248</td>\n",
              "      <td>0.517857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.877551</td>\n",
              "      <td>0.318182</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.350000</td>\n",
              "      <td>0.848980</td>\n",
              "      <td>0.362069</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.842451</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.297297</td>\n",
              "      <td>0.811816</td>\n",
              "      <td>0.338462</td>\n",
              "      <td>0.392857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.227723</td>\n",
              "      <td>0.757112</td>\n",
              "      <td>0.292994</td>\n",
              "      <td>0.410714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.486486</td>\n",
              "      <td>0.875274</td>\n",
              "      <td>0.387097</td>\n",
              "      <td>0.321429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.276316</td>\n",
              "      <td>0.803063</td>\n",
              "      <td>0.318182</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.885714</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.895918</td>\n",
              "      <td>0.190476</td>\n",
              "      <td>0.107143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.407407</td>\n",
              "      <td>0.875510</td>\n",
              "      <td>0.265060</td>\n",
              "      <td>0.196429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.895918</td>\n",
              "      <td>0.190476</td>\n",
              "      <td>0.107143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.877462</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.297297</td>\n",
              "      <td>0.811816</td>\n",
              "      <td>0.338462</td>\n",
              "      <td>0.392857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.287671</td>\n",
              "      <td>0.809628</td>\n",
              "      <td>0.325581</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.890591</td>\n",
              "      <td>0.390244</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.890591</td>\n",
              "      <td>0.390244</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  SYY  0.224806  0.740816  0.313514  0.517857\n",
              "1            GRU 0.1  SYY  0.301370  0.826531  0.341085  0.392857\n",
              "2        XGBoost 0.1  SYY  0.276190  0.789796  0.360248  0.517857\n",
              "3         Logreg 0.1  SYY  0.437500  0.877551  0.318182  0.250000\n",
              "4            SVM 0.1  SYY  0.350000  0.848980  0.362069  0.375000\n",
              "5      LSTM beta 0.1  SYY  0.333333  0.842451  0.307692  0.285714\n",
              "6       GRU beta 0.1  SYY  0.297297  0.811816  0.338462  0.392857\n",
              "7   XGBoost beta 0.1  SYY  0.227723  0.757112  0.292994  0.410714\n",
              "8    logreg beta 0.1  SYY  0.486486  0.875274  0.387097  0.321429\n",
              "9       svm beta 0.1  SYY  0.276316  0.803063  0.318182  0.375000\n",
              "0          LSTM 0.15  SYY  0.000000  0.885714  0.000000  0.000000\n",
              "1           GRU 0.15  SYY  0.857143  0.895918  0.190476  0.107143\n",
              "2       XGBoost 0.15  SYY  0.407407  0.875510  0.265060  0.196429\n",
              "3        Logreg 0.15  SYY  0.857143  0.895918  0.190476  0.107143\n",
              "4           SVM 0.15  SYY  1.000000  0.900000  0.222222  0.125000\n",
              "5     LSTM beta 0.15  SYY  0.000000  0.877462  0.000000  0.000000\n",
              "6      GRU beta 0.15  SYY  0.297297  0.811816  0.338462  0.392857\n",
              "7  XGBoost beta 0.15  SYY  0.287671  0.809628  0.325581  0.375000\n",
              "8   logreg beta 0.15  SYY  0.615385  0.890591  0.390244  0.285714\n",
              "9      svm beta 0.15  SYY  0.615385  0.890591  0.390244  0.285714"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzJKjV8FZPyM"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_logreg_beta.csv')"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNCRxvhuZPyM"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmbbIM9VZPyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e04056df-2021-4381-a41c-a5a9771d4fd8"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"SYY\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6619 - accuracy: 0.6396 - val_loss: 0.5449 - val_accuracy: 0.8857\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6513 - accuracy: 0.6470 - val_loss: 0.6163 - val_accuracy: 0.8857\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6329 - accuracy: 0.6523 - val_loss: 0.5904 - val_accuracy: 0.6633\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6135 - accuracy: 0.6711 - val_loss: 0.4898 - val_accuracy: 0.8857\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6023 - accuracy: 0.6805 - val_loss: 0.5017 - val_accuracy: 0.8878\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6569 - accuracy: 0.6443 - val_loss: 0.5039 - val_accuracy: 0.8857\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6279 - accuracy: 0.6658 - val_loss: 0.5391 - val_accuracy: 0.8837\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6043 - accuracy: 0.6805 - val_loss: 0.5127 - val_accuracy: 0.8592\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5877 - accuracy: 0.7047 - val_loss: 0.5683 - val_accuracy: 0.7796\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5832 - accuracy: 0.7101 - val_loss: 0.4847 - val_accuracy: 0.8286\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.678386\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.675177\n",
            "[2]\tvalidation_0-auc:0.670445\n",
            "[3]\tvalidation_0-auc:0.680855\n",
            "[4]\tvalidation_0-auc:0.67707\n",
            "[5]\tvalidation_0-auc:0.672914\n",
            "[6]\tvalidation_0-auc:0.672626\n",
            "[7]\tvalidation_0-auc:0.674375\n",
            "[8]\tvalidation_0-auc:0.674128\n",
            "[9]\tvalidation_0-auc:0.668388\n",
            "[10]\tvalidation_0-auc:0.669828\n",
            "[11]\tvalidation_0-auc:0.670054\n",
            "[12]\tvalidation_0-auc:0.667832\n",
            "[13]\tvalidation_0-auc:0.671721\n",
            "[14]\tvalidation_0-auc:0.672976\n",
            "[15]\tvalidation_0-auc:0.675856\n",
            "[16]\tvalidation_0-auc:0.674004\n",
            "[17]\tvalidation_0-auc:0.674416\n",
            "[18]\tvalidation_0-auc:0.674292\n",
            "[19]\tvalidation_0-auc:0.675774\n",
            "[20]\tvalidation_0-auc:0.676267\n",
            "[21]\tvalidation_0-auc:0.67707\n",
            "[22]\tvalidation_0-auc:0.675465\n",
            "[23]\tvalidation_0-auc:0.674889\n",
            "[24]\tvalidation_0-auc:0.674704\n",
            "[25]\tvalidation_0-auc:0.675341\n",
            "[26]\tvalidation_0-auc:0.675774\n",
            "[27]\tvalidation_0-auc:0.677625\n",
            "[28]\tvalidation_0-auc:0.677666\n",
            "[29]\tvalidation_0-auc:0.680176\n",
            "[30]\tvalidation_0-auc:0.680053\n",
            "[31]\tvalidation_0-auc:0.680382\n",
            "[32]\tvalidation_0-auc:0.679045\n",
            "[33]\tvalidation_0-auc:0.678427\n",
            "[34]\tvalidation_0-auc:0.678345\n",
            "[35]\tvalidation_0-auc:0.678427\n",
            "[36]\tvalidation_0-auc:0.680279\n",
            "[37]\tvalidation_0-auc:0.679291\n",
            "[38]\tvalidation_0-auc:0.680238\n",
            "[39]\tvalidation_0-auc:0.681596\n",
            "[40]\tvalidation_0-auc:0.68283\n",
            "[41]\tvalidation_0-auc:0.682871\n",
            "[42]\tvalidation_0-auc:0.682665\n",
            "[43]\tvalidation_0-auc:0.680958\n",
            "[44]\tvalidation_0-auc:0.68067\n",
            "[45]\tvalidation_0-auc:0.680999\n",
            "[46]\tvalidation_0-auc:0.68141\n",
            "[47]\tvalidation_0-auc:0.681328\n",
            "[48]\tvalidation_0-auc:0.682645\n",
            "[49]\tvalidation_0-auc:0.682316\n",
            "[50]\tvalidation_0-auc:0.681246\n",
            "[51]\tvalidation_0-auc:0.683427\n",
            "[52]\tvalidation_0-auc:0.683056\n",
            "[53]\tvalidation_0-auc:0.683673\n",
            "[54]\tvalidation_0-auc:0.683715\n",
            "[55]\tvalidation_0-auc:0.685772\n",
            "[56]\tvalidation_0-auc:0.686307\n",
            "[57]\tvalidation_0-auc:0.686266\n",
            "[58]\tvalidation_0-auc:0.685402\n",
            "[59]\tvalidation_0-auc:0.685155\n",
            "[60]\tvalidation_0-auc:0.685402\n",
            "[61]\tvalidation_0-auc:0.686986\n",
            "[62]\tvalidation_0-auc:0.686883\n",
            "[63]\tvalidation_0-auc:0.687006\n",
            "[64]\tvalidation_0-auc:0.686842\n",
            "[65]\tvalidation_0-auc:0.687582\n",
            "[66]\tvalidation_0-auc:0.687294\n",
            "[67]\tvalidation_0-auc:0.687047\n",
            "[68]\tvalidation_0-auc:0.687006\n",
            "[69]\tvalidation_0-auc:0.686842\n",
            "[70]\tvalidation_0-auc:0.687089\n",
            "[71]\tvalidation_0-auc:0.687006\n",
            "[72]\tvalidation_0-auc:0.688035\n",
            "[73]\tvalidation_0-auc:0.68857\n",
            "[74]\tvalidation_0-auc:0.688529\n",
            "[75]\tvalidation_0-auc:0.687911\n",
            "[76]\tvalidation_0-auc:0.687665\n",
            "[77]\tvalidation_0-auc:0.687047\n",
            "[78]\tvalidation_0-auc:0.688899\n",
            "[79]\tvalidation_0-auc:0.690051\n",
            "[80]\tvalidation_0-auc:0.691327\n",
            "[81]\tvalidation_0-auc:0.690586\n",
            "[82]\tvalidation_0-auc:0.691738\n",
            "[83]\tvalidation_0-auc:0.691615\n",
            "[84]\tvalidation_0-auc:0.690874\n",
            "[85]\tvalidation_0-auc:0.690504\n",
            "[86]\tvalidation_0-auc:0.690586\n",
            "[87]\tvalidation_0-auc:0.690174\n",
            "[88]\tvalidation_0-auc:0.689475\n",
            "[89]\tvalidation_0-auc:0.691327\n",
            "[90]\tvalidation_0-auc:0.691779\n",
            "[91]\tvalidation_0-auc:0.691738\n",
            "[92]\tvalidation_0-auc:0.691944\n",
            "[93]\tvalidation_0-auc:0.691944\n",
            "[94]\tvalidation_0-auc:0.692067\n",
            "[95]\tvalidation_0-auc:0.691738\n",
            "[96]\tvalidation_0-auc:0.69252\n",
            "[97]\tvalidation_0-auc:0.691697\n",
            "[98]\tvalidation_0-auc:0.691038\n",
            "[99]\tvalidation_0-auc:0.690997\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6448 - accuracy: 0.6507 - val_loss: 0.5430 - val_accuracy: 0.8775\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6372 - accuracy: 0.6465 - val_loss: 0.4477 - val_accuracy: 0.8775\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5976 - accuracy: 0.6946 - val_loss: 0.4275 - val_accuracy: 0.8643\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5985 - accuracy: 0.6815 - val_loss: 0.4306 - val_accuracy: 0.8731\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5578 - accuracy: 0.7213 - val_loss: 0.4621 - val_accuracy: 0.8315\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6168 - accuracy: 0.6836 - val_loss: 0.3973 - val_accuracy: 0.8884\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5353 - accuracy: 0.7426 - val_loss: 0.4157 - val_accuracy: 0.8840\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5135 - accuracy: 0.7666 - val_loss: 0.4165 - val_accuracy: 0.8643\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5031 - accuracy: 0.7763 - val_loss: 0.4048 - val_accuracy: 0.8775\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5136 - accuracy: 0.7673 - val_loss: 0.5100 - val_accuracy: 0.7812\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.66726\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.672805\n",
            "[2]\tvalidation_0-auc:0.678171\n",
            "[3]\tvalidation_0-auc:0.677124\n",
            "[4]\tvalidation_0-auc:0.672871\n",
            "[5]\tvalidation_0-auc:0.673762\n",
            "[6]\tvalidation_0-auc:0.673495\n",
            "[7]\tvalidation_0-auc:0.675699\n",
            "[8]\tvalidation_0-auc:0.672716\n",
            "[9]\tvalidation_0-auc:0.670756\n",
            "[10]\tvalidation_0-auc:0.67305\n",
            "[11]\tvalidation_0-auc:0.671981\n",
            "[12]\tvalidation_0-auc:0.669264\n",
            "[13]\tvalidation_0-auc:0.66824\n",
            "[14]\tvalidation_0-auc:0.671424\n",
            "[15]\tvalidation_0-auc:0.669598\n",
            "[16]\tvalidation_0-auc:0.66971\n",
            "[17]\tvalidation_0-auc:0.672315\n",
            "[18]\tvalidation_0-auc:0.673807\n",
            "[19]\tvalidation_0-auc:0.673361\n",
            "[20]\tvalidation_0-auc:0.67345\n",
            "[21]\tvalidation_0-auc:0.675321\n",
            "[22]\tvalidation_0-auc:0.677235\n",
            "[23]\tvalidation_0-auc:0.675566\n",
            "[24]\tvalidation_0-auc:0.677814\n",
            "[25]\tvalidation_0-auc:0.674831\n",
            "[26]\tvalidation_0-auc:0.676701\n",
            "[27]\tvalidation_0-auc:0.67826\n",
            "[28]\tvalidation_0-auc:0.679418\n",
            "[29]\tvalidation_0-auc:0.679551\n",
            "[30]\tvalidation_0-auc:0.680598\n",
            "[31]\tvalidation_0-auc:0.678638\n",
            "[32]\tvalidation_0-auc:0.677614\n",
            "[33]\tvalidation_0-auc:0.678772\n",
            "[34]\tvalidation_0-auc:0.680241\n",
            "[35]\tvalidation_0-auc:0.681043\n",
            "[36]\tvalidation_0-auc:0.680019\n",
            "[37]\tvalidation_0-auc:0.680642\n",
            "[38]\tvalidation_0-auc:0.68033\n",
            "[39]\tvalidation_0-auc:0.679596\n",
            "[40]\tvalidation_0-auc:0.676478\n",
            "[41]\tvalidation_0-auc:0.677102\n",
            "[42]\tvalidation_0-auc:0.677993\n",
            "[43]\tvalidation_0-auc:0.680108\n",
            "[44]\tvalidation_0-auc:0.68082\n",
            "[45]\tvalidation_0-auc:0.680932\n",
            "[46]\tvalidation_0-auc:0.679774\n",
            "[47]\tvalidation_0-auc:0.680553\n",
            "[48]\tvalidation_0-auc:0.680998\n",
            "[49]\tvalidation_0-auc:0.682023\n",
            "[50]\tvalidation_0-auc:0.682824\n",
            "[51]\tvalidation_0-auc:0.683537\n",
            "[52]\tvalidation_0-auc:0.682512\n",
            "[53]\tvalidation_0-auc:0.682824\n",
            "[54]\tvalidation_0-auc:0.683804\n",
            "[55]\tvalidation_0-auc:0.683626\n",
            "[56]\tvalidation_0-auc:0.683492\n",
            "[57]\tvalidation_0-auc:0.682446\n",
            "[58]\tvalidation_0-auc:0.683871\n",
            "[59]\tvalidation_0-auc:0.684004\n",
            "[60]\tvalidation_0-auc:0.684316\n",
            "[61]\tvalidation_0-auc:0.684806\n",
            "[62]\tvalidation_0-auc:0.683025\n",
            "[63]\tvalidation_0-auc:0.683114\n",
            "[64]\tvalidation_0-auc:0.681956\n",
            "[65]\tvalidation_0-auc:0.682624\n",
            "[66]\tvalidation_0-auc:0.680976\n",
            "[67]\tvalidation_0-auc:0.680753\n",
            "[68]\tvalidation_0-auc:0.680598\n",
            "[69]\tvalidation_0-auc:0.679974\n",
            "[70]\tvalidation_0-auc:0.680998\n",
            "[71]\tvalidation_0-auc:0.680642\n",
            "[72]\tvalidation_0-auc:0.681266\n",
            "[73]\tvalidation_0-auc:0.682646\n",
            "[74]\tvalidation_0-auc:0.683091\n",
            "[75]\tvalidation_0-auc:0.683626\n",
            "[76]\tvalidation_0-auc:0.682112\n",
            "[77]\tvalidation_0-auc:0.681755\n",
            "[78]\tvalidation_0-auc:0.681355\n",
            "[79]\tvalidation_0-auc:0.681132\n",
            "[80]\tvalidation_0-auc:0.681399\n",
            "[81]\tvalidation_0-auc:0.681488\n",
            "[82]\tvalidation_0-auc:0.684182\n",
            "[83]\tvalidation_0-auc:0.681555\n",
            "[84]\tvalidation_0-auc:0.681689\n",
            "[85]\tvalidation_0-auc:0.680843\n",
            "[86]\tvalidation_0-auc:0.680843\n",
            "[87]\tvalidation_0-auc:0.68111\n",
            "[88]\tvalidation_0-auc:0.683693\n",
            "[89]\tvalidation_0-auc:0.68347\n",
            "[90]\tvalidation_0-auc:0.684138\n",
            "[91]\tvalidation_0-auc:0.684182\n",
            "[92]\tvalidation_0-auc:0.685696\n",
            "[93]\tvalidation_0-auc:0.685607\n",
            "[94]\tvalidation_0-auc:0.685875\n",
            "[95]\tvalidation_0-auc:0.686721\n",
            "[96]\tvalidation_0-auc:0.687166\n",
            "[97]\tvalidation_0-auc:0.687122\n",
            "[98]\tvalidation_0-auc:0.686988\n",
            "[99]\tvalidation_0-auc:0.68583\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.8877551020408163 |  0.5294117647058824 | 0.16071428571428573 | 0.24657534246575344 |\n",
            "|     GRU 0.1      | 0.8285714285714286 |         0.3         |        0.375        | 0.33333333333333326 |\n",
            "|   XGBoost 0.1    | 0.789795918367347  |  0.2761904761904762 |  0.5178571428571429 |  0.360248447204969  |\n",
            "|    Logreg 0.1    | 0.8775510204081632 |        0.4375       |         0.25        |  0.3181818181818182 |\n",
            "|     SVM 0.1      | 0.8489795918367347 |         0.35        |        0.375        |  0.3620689655172413 |\n",
            "|  LSTM beta 0.1   | 0.8315098468271335 | 0.29411764705882354 | 0.26785714285714285 |  0.2803738317757009 |\n",
            "|   GRU beta 0.1   | 0.7811816192560175 |  0.2755102040816326 | 0.48214285714285715 |  0.3506493506493506 |\n",
            "| XGBoost beta 0.1 | 0.7571115973741794 | 0.22772277227722773 |  0.4107142857142857 |  0.2929936305732484 |\n",
            "| logreg beta 0.1  |  0.87527352297593  |  0.4864864864864865 | 0.32142857142857145 |  0.3870967741935484 |\n",
            "|   svm beta 0.1   | 0.8030634573304157 | 0.27631578947368424 |        0.375        |  0.3181818181818182 |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6104 - accuracy: 0.7067 - val_loss: 0.4765 - val_accuracy: 0.8857\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6108 - accuracy: 0.7101 - val_loss: 0.4596 - val_accuracy: 0.8857\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6062 - accuracy: 0.7101 - val_loss: 0.4713 - val_accuracy: 0.8857\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6069 - accuracy: 0.7101 - val_loss: 0.4415 - val_accuracy: 0.8857\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6043 - accuracy: 0.7101 - val_loss: 0.4688 - val_accuracy: 0.8857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6191 - accuracy: 0.7081 - val_loss: 0.4938 - val_accuracy: 0.8857\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5998 - accuracy: 0.7141 - val_loss: 0.4642 - val_accuracy: 0.8857\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5708 - accuracy: 0.7275 - val_loss: 0.4725 - val_accuracy: 0.8878\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5498 - accuracy: 0.7322 - val_loss: 0.5554 - val_accuracy: 0.7959\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5593 - accuracy: 0.7329 - val_loss: 0.3859 - val_accuracy: 0.8959\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.670178\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.666413\n",
            "[2]\tvalidation_0-auc:0.667791\n",
            "[3]\tvalidation_0-auc:0.657711\n",
            "[4]\tvalidation_0-auc:0.665076\n",
            "[5]\tvalidation_0-auc:0.677995\n",
            "[6]\tvalidation_0-auc:0.671597\n",
            "[7]\tvalidation_0-auc:0.669375\n",
            "[8]\tvalidation_0-auc:0.670095\n",
            "[9]\tvalidation_0-auc:0.668779\n",
            "[10]\tvalidation_0-auc:0.665569\n",
            "[11]\tvalidation_0-auc:0.67421\n",
            "[12]\tvalidation_0-auc:0.672832\n",
            "[13]\tvalidation_0-auc:0.676905\n",
            "[14]\tvalidation_0-auc:0.675527\n",
            "[15]\tvalidation_0-auc:0.676226\n",
            "[16]\tvalidation_0-auc:0.67635\n",
            "[17]\tvalidation_0-auc:0.675876\n",
            "[18]\tvalidation_0-auc:0.677255\n",
            "[19]\tvalidation_0-auc:0.677152\n",
            "[20]\tvalidation_0-auc:0.677255\n",
            "[21]\tvalidation_0-auc:0.68141\n",
            "[22]\tvalidation_0-auc:0.68104\n",
            "[23]\tvalidation_0-auc:0.68141\n",
            "[24]\tvalidation_0-auc:0.682028\n",
            "[25]\tvalidation_0-auc:0.681061\n",
            "[26]\tvalidation_0-auc:0.680937\n",
            "[27]\tvalidation_0-auc:0.683838\n",
            "[28]\tvalidation_0-auc:0.683344\n",
            "[29]\tvalidation_0-auc:0.685813\n",
            "[30]\tvalidation_0-auc:0.684208\n",
            "[31]\tvalidation_0-auc:0.686019\n",
            "[32]\tvalidation_0-auc:0.682707\n",
            "[33]\tvalidation_0-auc:0.683118\n",
            "[34]\tvalidation_0-auc:0.683241\n",
            "[35]\tvalidation_0-auc:0.683571\n",
            "[36]\tvalidation_0-auc:0.683818\n",
            "[37]\tvalidation_0-auc:0.683838\n",
            "[38]\tvalidation_0-auc:0.684908\n",
            "[39]\tvalidation_0-auc:0.68534\n",
            "[40]\tvalidation_0-auc:0.684414\n",
            "[41]\tvalidation_0-auc:0.685648\n",
            "[42]\tvalidation_0-auc:0.685566\n",
            "[43]\tvalidation_0-auc:0.686615\n",
            "[44]\tvalidation_0-auc:0.686759\n",
            "[45]\tvalidation_0-auc:0.686142\n",
            "[46]\tvalidation_0-auc:0.687089\n",
            "[47]\tvalidation_0-auc:0.685772\n",
            "[48]\tvalidation_0-auc:0.68499\n",
            "[49]\tvalidation_0-auc:0.685196\n",
            "[50]\tvalidation_0-auc:0.685895\n",
            "[51]\tvalidation_0-auc:0.682521\n",
            "[52]\tvalidation_0-auc:0.682686\n",
            "[53]\tvalidation_0-auc:0.680917\n",
            "[54]\tvalidation_0-auc:0.680094\n",
            "[55]\tvalidation_0-auc:0.679641\n",
            "[56]\tvalidation_0-auc:0.680382\n",
            "[57]\tvalidation_0-auc:0.679189\n",
            "[58]\tvalidation_0-auc:0.678448\n",
            "[59]\tvalidation_0-auc:0.679559\n",
            "[60]\tvalidation_0-auc:0.680217\n",
            "[61]\tvalidation_0-auc:0.679271\n",
            "[62]\tvalidation_0-auc:0.680258\n",
            "[63]\tvalidation_0-auc:0.6803\n",
            "[64]\tvalidation_0-auc:0.680217\n",
            "[65]\tvalidation_0-auc:0.679888\n",
            "[66]\tvalidation_0-auc:0.677831\n",
            "[67]\tvalidation_0-auc:0.677049\n",
            "[68]\tvalidation_0-auc:0.67779\n",
            "[69]\tvalidation_0-auc:0.679724\n",
            "[70]\tvalidation_0-auc:0.679271\n",
            "[71]\tvalidation_0-auc:0.678695\n",
            "[72]\tvalidation_0-auc:0.678818\n",
            "[73]\tvalidation_0-auc:0.677872\n",
            "[74]\tvalidation_0-auc:0.678201\n",
            "[75]\tvalidation_0-auc:0.678613\n",
            "[76]\tvalidation_0-auc:0.681246\n",
            "[77]\tvalidation_0-auc:0.681452\n",
            "[78]\tvalidation_0-auc:0.683097\n",
            "[79]\tvalidation_0-auc:0.682439\n",
            "[80]\tvalidation_0-auc:0.682192\n",
            "[81]\tvalidation_0-auc:0.683962\n",
            "[82]\tvalidation_0-auc:0.685319\n",
            "[83]\tvalidation_0-auc:0.683056\n",
            "[84]\tvalidation_0-auc:0.683756\n",
            "[85]\tvalidation_0-auc:0.681575\n",
            "[86]\tvalidation_0-auc:0.68141\n",
            "[87]\tvalidation_0-auc:0.680382\n",
            "[88]\tvalidation_0-auc:0.678942\n",
            "[89]\tvalidation_0-auc:0.679065\n",
            "[90]\tvalidation_0-auc:0.67853\n",
            "[91]\tvalidation_0-auc:0.678736\n",
            "[92]\tvalidation_0-auc:0.677913\n",
            "[93]\tvalidation_0-auc:0.674827\n",
            "[94]\tvalidation_0-auc:0.676391\n",
            "[95]\tvalidation_0-auc:0.676679\n",
            "[96]\tvalidation_0-auc:0.678777\n",
            "Stopping. Best iteration:\n",
            "[46]\tvalidation_0-auc:0.687089\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6077 - accuracy: 0.7152 - val_loss: 0.4218 - val_accuracy: 0.8775\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5654 - accuracy: 0.7289 - val_loss: 0.3743 - val_accuracy: 0.8775\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5424 - accuracy: 0.7406 - val_loss: 0.3757 - val_accuracy: 0.8490\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5090 - accuracy: 0.7632 - val_loss: 0.4012 - val_accuracy: 0.8775\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4938 - accuracy: 0.7728 - val_loss: 0.4426 - val_accuracy: 0.8446\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.5897 - accuracy: 0.7152 - val_loss: 0.4029 - val_accuracy: 0.8775\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5118 - accuracy: 0.7694 - val_loss: 0.4759 - val_accuracy: 0.8468\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4857 - accuracy: 0.7769 - val_loss: 0.3759 - val_accuracy: 0.8906\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4536 - accuracy: 0.7859 - val_loss: 0.3696 - val_accuracy: 0.8906\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4293 - accuracy: 0.8202 - val_loss: 0.3914 - val_accuracy: 0.8840\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.662585\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.663275\n",
            "[2]\tvalidation_0-auc:0.667884\n",
            "[3]\tvalidation_0-auc:0.670244\n",
            "[4]\tvalidation_0-auc:0.633194\n",
            "[5]\tvalidation_0-auc:0.633639\n",
            "[6]\tvalidation_0-auc:0.642835\n",
            "[7]\tvalidation_0-auc:0.643859\n",
            "[8]\tvalidation_0-auc:0.641031\n",
            "[9]\tvalidation_0-auc:0.659623\n",
            "[10]\tvalidation_0-auc:0.665568\n",
            "[11]\tvalidation_0-auc:0.670155\n",
            "[12]\tvalidation_0-auc:0.676122\n",
            "[13]\tvalidation_0-auc:0.668374\n",
            "[14]\tvalidation_0-auc:0.67423\n",
            "[15]\tvalidation_0-auc:0.677859\n",
            "[16]\tvalidation_0-auc:0.677146\n",
            "[17]\tvalidation_0-auc:0.670556\n",
            "[18]\tvalidation_0-auc:0.67561\n",
            "[19]\tvalidation_0-auc:0.677169\n",
            "[20]\tvalidation_0-auc:0.676545\n",
            "[21]\tvalidation_0-auc:0.675944\n",
            "[22]\tvalidation_0-auc:0.679328\n",
            "[23]\tvalidation_0-auc:0.677926\n",
            "[24]\tvalidation_0-auc:0.679262\n",
            "[25]\tvalidation_0-auc:0.682713\n",
            "[26]\tvalidation_0-auc:0.683581\n",
            "[27]\tvalidation_0-auc:0.684873\n",
            "[28]\tvalidation_0-auc:0.686209\n",
            "[29]\tvalidation_0-auc:0.682045\n",
            "[30]\tvalidation_0-auc:0.68367\n",
            "[31]\tvalidation_0-auc:0.678082\n",
            "[32]\tvalidation_0-auc:0.678883\n",
            "[33]\tvalidation_0-auc:0.682646\n",
            "[34]\tvalidation_0-auc:0.681488\n",
            "[35]\tvalidation_0-auc:0.683002\n",
            "[36]\tvalidation_0-auc:0.685006\n",
            "[37]\tvalidation_0-auc:0.681911\n",
            "[38]\tvalidation_0-auc:0.683247\n",
            "[39]\tvalidation_0-auc:0.682357\n",
            "[40]\tvalidation_0-auc:0.680843\n",
            "[41]\tvalidation_0-auc:0.679818\n",
            "[42]\tvalidation_0-auc:0.680152\n",
            "[43]\tvalidation_0-auc:0.683091\n",
            "[44]\tvalidation_0-auc:0.68445\n",
            "[45]\tvalidation_0-auc:0.687255\n",
            "[46]\tvalidation_0-auc:0.688881\n",
            "[47]\tvalidation_0-auc:0.688836\n",
            "[48]\tvalidation_0-auc:0.690862\n",
            "[49]\tvalidation_0-auc:0.693\n",
            "[50]\tvalidation_0-auc:0.691575\n",
            "[51]\tvalidation_0-auc:0.691085\n",
            "[52]\tvalidation_0-auc:0.690773\n",
            "[53]\tvalidation_0-auc:0.690907\n",
            "[54]\tvalidation_0-auc:0.691174\n",
            "[55]\tvalidation_0-auc:0.690239\n",
            "[56]\tvalidation_0-auc:0.692599\n",
            "[57]\tvalidation_0-auc:0.694024\n",
            "[58]\tvalidation_0-auc:0.696206\n",
            "[59]\tvalidation_0-auc:0.697097\n",
            "[60]\tvalidation_0-auc:0.699412\n",
            "[61]\tvalidation_0-auc:0.700525\n",
            "[62]\tvalidation_0-auc:0.703732\n",
            "[63]\tvalidation_0-auc:0.699947\n",
            "[64]\tvalidation_0-auc:0.700347\n",
            "[65]\tvalidation_0-auc:0.699902\n",
            "[66]\tvalidation_0-auc:0.699412\n",
            "[67]\tvalidation_0-auc:0.699947\n",
            "[68]\tvalidation_0-auc:0.703821\n",
            "[69]\tvalidation_0-auc:0.705914\n",
            "[70]\tvalidation_0-auc:0.70725\n",
            "[71]\tvalidation_0-auc:0.707294\n",
            "[72]\tvalidation_0-auc:0.707918\n",
            "[73]\tvalidation_0-auc:0.70774\n",
            "[74]\tvalidation_0-auc:0.70892\n",
            "[75]\tvalidation_0-auc:0.708875\n",
            "[76]\tvalidation_0-auc:0.708697\n",
            "[77]\tvalidation_0-auc:0.710924\n",
            "[78]\tvalidation_0-auc:0.709209\n",
            "[79]\tvalidation_0-auc:0.709387\n",
            "[80]\tvalidation_0-auc:0.70725\n",
            "[81]\tvalidation_0-auc:0.707695\n",
            "[82]\tvalidation_0-auc:0.707962\n",
            "[83]\tvalidation_0-auc:0.708986\n",
            "[84]\tvalidation_0-auc:0.709788\n",
            "[85]\tvalidation_0-auc:0.709031\n",
            "[86]\tvalidation_0-auc:0.707561\n",
            "[87]\tvalidation_0-auc:0.708318\n",
            "[88]\tvalidation_0-auc:0.708096\n",
            "[89]\tvalidation_0-auc:0.707339\n",
            "[90]\tvalidation_0-auc:0.705246\n",
            "[91]\tvalidation_0-auc:0.7048\n",
            "[92]\tvalidation_0-auc:0.703776\n",
            "[93]\tvalidation_0-auc:0.703197\n",
            "[94]\tvalidation_0-auc:0.702841\n",
            "[95]\tvalidation_0-auc:0.703153\n",
            "[96]\tvalidation_0-auc:0.703865\n",
            "[97]\tvalidation_0-auc:0.702886\n",
            "[98]\tvalidation_0-auc:0.702218\n",
            "[99]\tvalidation_0-auc:0.703554\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.8857142857142857 |         0.0         |         0.0         |         0.0         |\n",
            "|      GRU 0.15     | 0.8959183673469387 |  0.8571428571428571 | 0.10714285714285714 | 0.19047619047619047 |\n",
            "|    XGBoost 0.15   | 0.8755102040816326 |  0.4074074074074074 | 0.19642857142857142 |  0.2650602409638554 |\n",
            "|    Logreg 0.15    | 0.8959183673469387 |  0.8571428571428571 | 0.10714285714285714 | 0.19047619047619047 |\n",
            "|      SVM 0.15     |        0.9         |         1.0         |        0.125        |  0.2222222222222222 |\n",
            "|   LSTM beta 0.15  | 0.8446389496717724 | 0.32558139534883723 |         0.25        |  0.2828282828282828 |\n",
            "|   GRU beta 0.15   | 0.8840262582056893 |  0.5454545454545454 | 0.32142857142857145 | 0.40449438202247195 |\n",
            "| XGBoost beta 0.15 | 0.8096280087527352 |  0.2876712328767123 |        0.375        |  0.3255813953488372 |\n",
            "|  logreg beta 0.15 | 0.8905908096280087 |  0.6153846153846154 |  0.2857142857142857 |  0.3902439024390244 |\n",
            "|   svm beta 0.15   | 0.8905908096280087 |  0.6153846153846154 |  0.2857142857142857 |  0.3902439024390244 |\n",
            "+-------------------+--------------------+---------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uH7Ob-cZPyM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "123335f7-fd5a-4521-929b-7d48c4ddd16c"
      },
      "source": [
        "Result_purging.to_csv('SYY_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.529412</td>\n",
              "      <td>0.887755</td>\n",
              "      <td>0.246575</td>\n",
              "      <td>0.160714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.828571</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.276190</td>\n",
              "      <td>0.789796</td>\n",
              "      <td>0.360248</td>\n",
              "      <td>0.517857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.877551</td>\n",
              "      <td>0.318182</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.350000</td>\n",
              "      <td>0.848980</td>\n",
              "      <td>0.362069</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>0.831510</td>\n",
              "      <td>0.280374</td>\n",
              "      <td>0.267857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.275510</td>\n",
              "      <td>0.781182</td>\n",
              "      <td>0.350649</td>\n",
              "      <td>0.482143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.227723</td>\n",
              "      <td>0.757112</td>\n",
              "      <td>0.292994</td>\n",
              "      <td>0.410714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.486486</td>\n",
              "      <td>0.875274</td>\n",
              "      <td>0.387097</td>\n",
              "      <td>0.321429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.276316</td>\n",
              "      <td>0.803063</td>\n",
              "      <td>0.318182</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.885714</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.895918</td>\n",
              "      <td>0.190476</td>\n",
              "      <td>0.107143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.407407</td>\n",
              "      <td>0.875510</td>\n",
              "      <td>0.265060</td>\n",
              "      <td>0.196429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.895918</td>\n",
              "      <td>0.190476</td>\n",
              "      <td>0.107143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.325581</td>\n",
              "      <td>0.844639</td>\n",
              "      <td>0.282828</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.884026</td>\n",
              "      <td>0.404494</td>\n",
              "      <td>0.321429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.287671</td>\n",
              "      <td>0.809628</td>\n",
              "      <td>0.325581</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.890591</td>\n",
              "      <td>0.390244</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>SYY</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.890591</td>\n",
              "      <td>0.390244</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  SYY  0.529412  0.887755  0.246575  0.160714\n",
              "1            GRU 0.1  SYY  0.300000  0.828571  0.333333  0.375000\n",
              "2        XGBoost 0.1  SYY  0.276190  0.789796  0.360248  0.517857\n",
              "3         Logreg 0.1  SYY  0.437500  0.877551  0.318182  0.250000\n",
              "4            SVM 0.1  SYY  0.350000  0.848980  0.362069  0.375000\n",
              "5      LSTM beta 0.1  SYY  0.294118  0.831510  0.280374  0.267857\n",
              "6       GRU beta 0.1  SYY  0.275510  0.781182  0.350649  0.482143\n",
              "7   XGBoost beta 0.1  SYY  0.227723  0.757112  0.292994  0.410714\n",
              "8    logreg beta 0.1  SYY  0.486486  0.875274  0.387097  0.321429\n",
              "9       svm beta 0.1  SYY  0.276316  0.803063  0.318182  0.375000\n",
              "0          LSTM 0.15  SYY  0.000000  0.885714  0.000000  0.000000\n",
              "1           GRU 0.15  SYY  0.857143  0.895918  0.190476  0.107143\n",
              "2       XGBoost 0.15  SYY  0.407407  0.875510  0.265060  0.196429\n",
              "3        Logreg 0.15  SYY  0.857143  0.895918  0.190476  0.107143\n",
              "4           SVM 0.15  SYY  1.000000  0.900000  0.222222  0.125000\n",
              "5     LSTM beta 0.15  SYY  0.325581  0.844639  0.282828  0.250000\n",
              "6      GRU beta 0.15  SYY  0.545455  0.884026  0.404494  0.321429\n",
              "7  XGBoost beta 0.15  SYY  0.287671  0.809628  0.325581  0.375000\n",
              "8   logreg beta 0.15  SYY  0.615385  0.890591  0.390244  0.285714\n",
              "9      svm beta 0.15  SYY  0.615385  0.890591  0.390244  0.285714"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fS9sETFZPyN"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('SYY_logreg_beta_p.csv')"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOWgCHkdZPyO"
      },
      "source": [
        ""
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB5tpi2kZu09"
      },
      "source": [
        "## TWTR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_hU15yoZu1D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c9922b61-6c0b-4c17-f8e1-535428e6988f"
      },
      "source": [
        "dfs = pd.read_csv(\"TWTR.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "dfs"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1988</td>\n",
              "      <td>US1.TWTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>60.84</td>\n",
              "      <td>62.140</td>\n",
              "      <td>60.570</td>\n",
              "      <td>61.970</td>\n",
              "      <td>291952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1987</td>\n",
              "      <td>US1.TWTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>60.37</td>\n",
              "      <td>61.220</td>\n",
              "      <td>59.880</td>\n",
              "      <td>60.380</td>\n",
              "      <td>307226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1986</td>\n",
              "      <td>US1.TWTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>63.17</td>\n",
              "      <td>63.345</td>\n",
              "      <td>60.020</td>\n",
              "      <td>60.060</td>\n",
              "      <td>285767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1985</td>\n",
              "      <td>US1.TWTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>64.00</td>\n",
              "      <td>64.290</td>\n",
              "      <td>62.170</td>\n",
              "      <td>62.440</td>\n",
              "      <td>332449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1984</td>\n",
              "      <td>US1.TWTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>66.62</td>\n",
              "      <td>67.000</td>\n",
              "      <td>65.220</td>\n",
              "      <td>65.365</td>\n",
              "      <td>331665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1984</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.TWTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20131113</td>\n",
              "      <td>0</td>\n",
              "      <td>41.03</td>\n",
              "      <td>42.870</td>\n",
              "      <td>40.760</td>\n",
              "      <td>42.430</td>\n",
              "      <td>7606967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.TWTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20131112</td>\n",
              "      <td>0</td>\n",
              "      <td>43.66</td>\n",
              "      <td>43.780</td>\n",
              "      <td>41.830</td>\n",
              "      <td>41.920</td>\n",
              "      <td>5902473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1986</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.TWTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20131111</td>\n",
              "      <td>0</td>\n",
              "      <td>40.50</td>\n",
              "      <td>43.000</td>\n",
              "      <td>39.400</td>\n",
              "      <td>43.000</td>\n",
              "      <td>15755276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1987</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.TWTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20131108</td>\n",
              "      <td>0</td>\n",
              "      <td>45.93</td>\n",
              "      <td>46.940</td>\n",
              "      <td>40.685</td>\n",
              "      <td>41.650</td>\n",
              "      <td>26066094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1988</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.TWTR</td>\n",
              "      <td>D</td>\n",
              "      <td>20131107</td>\n",
              "      <td>0</td>\n",
              "      <td>45.98</td>\n",
              "      <td>50.090</td>\n",
              "      <td>44.000</td>\n",
              "      <td>44.910</td>\n",
              "      <td>103209765</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1989 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  <TICKER> <PER>    <DATE>  ...  <HIGH>   <LOW>  <CLOSE>      <VOL>\n",
              "0      1988  US1.TWTR     D  20211001  ...  62.140  60.570   61.970     291952\n",
              "1      1987  US1.TWTR     D  20210930  ...  61.220  59.880   60.380     307226\n",
              "2      1986  US1.TWTR     D  20210929  ...  63.345  60.020   60.060     285767\n",
              "3      1985  US1.TWTR     D  20210928  ...  64.290  62.170   62.440     332449\n",
              "4      1984  US1.TWTR     D  20210927  ...  67.000  65.220   65.365     331665\n",
              "...     ...       ...   ...       ...  ...     ...     ...      ...        ...\n",
              "1984      4  US1.TWTR     D  20131113  ...  42.870  40.760   42.430    7606967\n",
              "1985      3  US1.TWTR     D  20131112  ...  43.780  41.830   41.920    5902473\n",
              "1986      2  US1.TWTR     D  20131111  ...  43.000  39.400   43.000   15755276\n",
              "1987      1  US1.TWTR     D  20131108  ...  46.940  40.685   41.650   26066094\n",
              "1988      0  US1.TWTR     D  20131107  ...  50.090  44.000   44.910  103209765\n",
              "\n",
              "[1989 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "755vVQDpZu1E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5cfbd41d-984b-41c7-c99b-fcb6dd028d20"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"4ca805b6-98af-4fba-a241-9a5d10e4c45f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"4ca805b6-98af-4fba-a241-9a5d10e4c45f\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '4ca805b6-98af-4fba-a241-9a5d10e4c45f',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [39.68, 40.8, 40.35, 40.03, 39.7, 40.24, 41.18, 41.33, 42.43, 42.51, 41.4, 43.4, 43.23, 42.93, 43.23, 43.25, 42.76, 42.61, 43.2, 43.25, 43.25, 44.25, 45.41, 45.31, 43.344, 41.92, 42.63, 42.48, 41.68, 42.13, 41.46, 41.01, 42.17, 42.8, 42.28, 41.7, 40.58, 40.12, 40.65, 41.83, 40.46, 41.54, 42.06, 41.73, 41.34, 40.36, 42.86, 42.1, 42.31, 41.01, 41.5, 41.53, 38.11, 38.74, 37.895, 37.58, 36.77, 37.68, 37.7087, 38.01, 38.69, 37.83, 37.19, 37.46, 37.64, 36.45, 36.26, 36.0311, 36.22, 36.1, 34.9, 34.74, 35.22, 34.72, 35.58, 35.0, 35.44, 36.28, 36.64, 36.44, 36.15, 36.35, 37.5, 37.21, 37.64, 37.94, 36.6, 36.33, 36.105, 34.41, 36.43, 37.14, 36.83, 37.3, 37.4, 37.18, 38.59, 37.47, 37.14, 37.49, 38.3, 37.93, 36.9302, 36.58, 38.47, 38.79, 38.6, 38.61, 40.24, 40.8001, 39.95, 39.3, 39.89, 39.81, 38.67, 38.49, 39.28, 39.76, 34.39, 34.4, 34.46, 34.4542, 34.7, 34.36, 34.57, 34.74, 35.11, 34.86, 34.7, 34.41, 34.38, 33.75, 33.435, 32.88, 32.86, 32.27, 33.05, 32.58, 33.02, 32.62, 32.58, 31.29, 31.09, 31.25, 31.03, 31.3, 31.16, 30.87, 30.04, 30.12, 30.8, 31.04, 30.5, 30.63, 30.78, 30.4, 31.01, 32.0, 31.71, 30.77, 31.39, 31.64, 31.23, 30.96, 31.115, 30.39, 30.23, 29.99, 30.8, 34.16, 34.37, 33.95, 33.18, 33.54, 32.25, 31.64, 33.14, 32.91, 31.6, 30.975, 32.26, 33.27, 32.86, 32.46, 33.0, 32.35, 32.85, 33.0808, 32.25, 31.805, 31.34, 29.95, 28.01, 28.81, 28.7311, 28.43, 28.68, 28.65, 26.45, 27.3, 29.2801, 32.92, 33.74, 33.43, 35.87, 35.88, 36.2434, 34.445, 33.43, 32.8406, 32.96, 32.57, 33.65, 31.45, 31.3, 32.72, 32.63, 32.82, 31.0, 31.61, 31.05, 31.98, 33.69, 33.14, 32.925, 32.49, 32.01, 34.07, 34.17, 34.98, 34.4, 34.015, 34.29, 34.62, 34.75, 33.86, 32.41, 32.38, 31.8, 27.54, 28.78, 29.18, 28.81, 29.28, 29.55, 29.87, 28.61, 27.995, 27.0, 26.8, 29.28, 28.47, 28.4, 28.23, 29.01, 28.19, 28.3, 28.47, 29.42, 29.02, 29.11, 28.59, 28.5, 29.86, 29.53, 29.22, 28.855, 30.12, 30.38, 29.75, 30.895, 30.54, 30.49, 30.82, 32.73, 34.84, 35.19, 35.65, 35.35, 35.49, 35.89, 34.28, 33.88, 33.82, 33.68, 32.61, 32.74, 32.83, 32.39, 33.175, 32.8, 32.01, 31.96, 31.84, 32.67, 32.97, 31.95, 32.8316, 31.92, 31.83, 31.38, 34.12, 42.94, 44.22, 42.17, 43.3, 43.42, 43.44, 43.34, 44.7, 44.26, 44.47, 45.25, 43.87, 43.74, 44.1163, 46.65, 45.08, 43.89, 44.97, 43.69, 44.79, 43.69, 44.83, 44.19, 45.9, 45.23, 46.12, 44.9, 46.02, 45.8, 46.77, 44.08, 43.48, 41.42, 41.2, 39.69, 40.09, 39.79, 37.92, 36.64, 34.69, 34.36, 34.01, 33.64, 33.53, 33.43, 32.86, 33.64, 32.63, 32.555, 32.77, 32.74, 33.39, 32.735, 32.87, 32.46, 31.855, 31.33, 31.06, 30.67, 30.56, 30.27, 30.31, 29.005, 30.26, 29.88, 30.4678, 31.22, 31.92, 31.54, 31.545, 31.83, 28.58, 28.7501, 28.99, 29.39, 29.54, 28.01, 28.1, 28.65, 28.2499, 27.55, 28.05, 29.005, 28.44, 28.065, 31.905, 31.03, 31.2, 32.73, 31.35, 34.98, 35.57, 35.815, 36.6, 34.095, 35.49, 35.34, 34.86, 35.76, 34.4148, 34.59, 32.995, 32.2651, 31.86, 31.32, 32.1675, 32.67, 32.11, 33.37, 32.83, 33.05, 33.6, 33.75, 33.43, 30.95, 31.5, 30.17, 26.92, 25.25, 25.15, 25.925, 27.15, 25.8186, 25.62, 25.18, 24.2711, 22.17, 22.36, 22.8, 23.34, 23.65, 24.0302, 24.55, 24.68, 25.415, 24.34, 24.24, 24.17, 24.605, 24.32, 23.985, 24.45, 24.505, 24.015, 24.31, 24.23, 24.255, 24.4501, 25.05, 25.19, 25.07, 24.695, 22.22, 22.57, 21.67, 21.645, 22.05, 21.1, 21.0, 21.085, 20.78, 20.4, 20.72, 20.5795, 20.785, 21.8201, 21.82, 22.4198, 22.27, 21.885, 21.135, 20.76, 20.35, 19.91, 20.045, 20.16, 20.325, 19.9, 19.6, 19.655, 19.39, 19.88, 19.7, 20.6, 20.6186, 21.25, 21.68, 20.315, 17.16, 17.25, 17.36, 17.86, 17.8901, 18.03, 18.28, 18.34, 18.63, 18.475, 17.735, 17.41, 17.67, 17.855, 18.25, 17.755, 17.61, 17.1, 16.88, 16.85, 16.95, 16.585, 16.985, 17.6, 17.5853, 17.63, 17.77, 17.59, 18.01, 18.21, 18.1962, 18.18, 17.665, 17.45, 17.23, 16.8201, 16.641, 16.86, 16.9, 16.925, 16.93, 16.77, 16.64, 16.89, 16.98, 16.635, 16.105, 16.0, 15.8676, 16.145, 15.95, 16.085, 15.935, 15.755, 16.145, 16.145, 16.39, 16.29, 16.18, 16.07, 16.2062, 16.06, 16.75, 16.84, 19.63, 19.97, 20.0, 20.11, 20.525, 20.11, 19.97, 19.93, 19.635, 19.32, 19.26, 18.64, 18.075, 18.005, 17.925, 17.82, 17.6, 17.86, 17.6446, 17.93, 18.125, 18.295, 18.495, 18.155, 17.77, 16.9, 17.07, 16.66, 16.835, 16.76, 16.98, 17.05, 16.91, 17.59, 17.43, 17.57, 18.24, 18.3, 18.52, 18.3, 18.44, 18.23, 17.95, 17.98, 18.145, 18.445, 18.37, 18.505, 18.285, 19.49, 19.2252, 18.61, 18.38, 18.54, 18.36, 18.31, 18.68, 18.49, 18.56, 18.24, 17.535, 16.475, 16.6, 15.815, 14.67, 14.71, 14.6375, 14.65, 14.535, 14.44, 14.41, 14.305, 14.41, 14.31, 14.35, 14.29, 14.39, 14.52, 14.68, 14.84, 14.9401, 14.91, 15.04, 14.95, 14.98, 15.14, 14.93, 14.98, 14.53, 15.09, 15.075, 15.19, 15.04, 15.31, 15.22, 15.11, 15.21, 15.23, 15.19, 15.57, 15.74, 15.78, 15.8, 15.77, 16.05, 15.96, 16.028, 16.0657, 16.43, 16.62, 16.35, 16.74, 16.5122, 15.81, 15.5749, 16.4001, 18.71, 18.25, 17.945, 17.61, 17.78, 17.23, 17.615, 16.94, 16.57, 16.8128, 16.73, 16.51, 16.6, 16.58, 16.795, 17.1151, 16.955, 17.24, 17.37, 17.29, 17.365, 17.51, 17.175, 17.1, 16.86, 16.43, 16.29, 16.39, 16.38, 16.605, 16.495, 16.4, 17.0761, 17.91, 18.24, 18.6299, 18.79, 18.93, 19.35, 18.93, 19.64, 19.65, 19.48, 18.23, 18.23, 17.9399, 18.02, 18.49, 18.2, 18.3, 18.05, 18.23, 18.631, 18.61, 18.73, 18.55, 18.62, 18.9889, 19.15, 18.545, 18.3799, 19.1364, 18.37, 18.4, 18.03, 17.57, 17.6142, 17.4832, 17.95, 17.67, 17.39, 17.295, 17.251, 18.035, 18.09, 16.9, 17.06, 16.83, 16.72, 16.8799, 17.79, 18.05, 17.9948, 17.57, 19.815, 19.86, 24.8601, 23.505, 23.99, 23.05, 23.005, 22.955, 23.73, 23.39, 22.63, 18.625, 18.49, 18.39, 18.36, 19.1, 18.295, 18.07, 17.755, 18.145, 18.1092, 18.69, 19.86, 19.92, 19.54, 19.495, 19.215, 18.38, 18.479, 18.295, 18.31, 18.24, 18.7, 18.5475, 18.98, 18.985, 20.17, 20.395, 20.86, 19.535, 19.77, 19.045, 18.675, 18.2, 18.255, 18.13, 17.61, 16.41, 16.641, 16.635, 16.3, 15.7601, 18.44, 18.65, 18.37, 18.4, 18.56, 18.34, 18.66, 18.065, 17.97, 17.732, 18.1, 17.72, 18.085, 17.37, 17.19, 17.135, 17.2701, 16.89, 16.83, 16.42, 15.84, 16.43, 17.03, 16.13, 16.31, 16.35, 16.07, 15.8701, 15.97, 15.38, 14.56, 14.015, 14.605, 14.945, 15.005, 15.28, 15.19, 15.19, 15.02, 15.23, 15.0501, 14.28, 14.4, 14.03, 14.4, 14.43, 14.14, 14.14, 14.34, 14.28, 14.105, 14.09, 14.6, 14.62, 14.195, 14.4, 14.11, 14.845, 14.0, 14.39, 14.61, 14.64, 14.86, 17.75, 17.075, 17.235, 17.52, 17.405, 16.9199, 17.31, 17.59, 17.53, 17.36, 16.56, 16.51, 16.65, 16.97, 17.2699, 17.05, 17.1, 15.98, 16.54, 16.35, 15.965, 15.61, 15.905, 16.0183, 16.8597, 16.885, 16.835, 16.84, 16.705, 16.1899, 17.13, 16.812, 16.62, 17.6599, 18.34, 19.155, 19.3524, 19.31, 18.55, 17.85, 18.12, 17.941, 17.59, 18.0, 18.32, 18.31, 18.32, 18.44, 17.45, 16.36, 15.8878, 14.32, 14.971, 14.395, 14.89, 15.72, 16.92, 16.569, 16.09, 17.9, 16.8, 16.516, 16.78, 17.01, 17.01, 17.84, 17.82, 17.3878, 16.69, 17.94, 19.005, 18.68, 19.6258, 19.63, 19.98, 20.26, 21.39, 21.9, 22.54, 23.14, 22.22, 22.4778, 22.5202, 22.95, 22.6601, 22.57, 22.15, 22.98, 23.31, 24.295, 23.96, 24.92, 24.85, 25.94, 24.32, 25.0, 24.47, 25.03, 25.91, 25.415, 25.54, 25.39, 25.68, 26.05, 25.52, 25.19, 26.28, 26.3153, 25.8999, 25.26, 25.41, 25.1775, 26.12, 26.52, 27.0505, 27.08, 28.26, 28.66, 29.36, 29.14, 29.19, 28.46, 29.0505, 30.87, 31.29, 30.89, 30.29, 29.16, 29.29, 30.91, 30.92, 31.144, 29.73, 29.381, 29.051, 28.7401, 30.85, 30.32, 29.83, 27.6314, 28.17, 26.3199, 24.667, 26.9399, 25.58, 25.24, 25.2921, 26.605, 26.805, 26.835, 27.36, 27.92, 27.41, 27.746, 27.17, 26.889, 27.39, 27.7001, 27.17, 27.18, 28.16, 28.28, 27.825, 27.02, 27.78, 26.84, 26.46, 25.03, 24.48, 25.26, 25.86, 26.0, 27.5949, 28.3, 29.06, 29.0799, 28.55, 29.41, 29.63, 29.5, 27.02, 27.55, 28.48, 29.34, 29.25, 31.005, 31.48, 31.235, 36.573, 34.7201, 35.4, 36.19, 36.08, 36.61, 35.805, 35.65, 36.1, 35.6608, 36.7042, 35.8, 34.925, 34.3551, 34.7285, 35.525, 35.4155, 35.7, 35.42, 36.23, 34.22, 35.25, 35.16, 35.18, 35.37, 35.57, 35.87, 34.66, 34.68, 34.82, 34.66, 35.88, 35.83, 35.85, 35.88, 36.455, 36.99, 36.7199, 37.0, 36.4, 36.61, 36.66, 36.83, 36.42, 36.51, 36.59, 36.69, 36.7953, 37.49, 37.28, 37.11, 37.32, 37.71, 37.48, 37.31, 37.5901, 37.71, 37.2528, 37.42, 37.87, 37.8338, 38.96, 38.49, 42.26, 51.66, 50.84, 51.42, 51.716, 51.34, 51.39, 50.65, 52.04, 51.3, 51.195, 51.61, 51.93, 52.17, 52.28, 52.86, 50.8199, 50.4001, 50.47, 50.0849, 49.88, 50.0001, 49.93, 49.4949, 51.48, 48.4701, 48.45, 47.93, 47.22, 46.93, 46.42, 46.635, 47.05, 46.28, 45.84, 47.5738, 46.74, 47.36, 47.5602, 47.6899, 48.15, 48.1, 49.43, 48.5462, 48.67, 48.43, 49.09, 48.71, 47.79, 48.03, 48.53, 47.95, 47.5076, 46.25, 47.3, 48.03, 41.2801, 40.72, 39.8, 37.45, 37.55, 36.67, 37.14, 38.9, 40.0999, 39.43, 39.0799, 37.81, 37.56, 37.28, 36.92, 39.84, 39.65, 39.35, 40.2, 39.11, 37.28, 38.76, 36.38, 36.55, 35.86, 35.85, 36.41, 37.59, 37.63, 37.57, 38.44, 37.06, 36.72, 35.57, 35.12, 36.8401, 37.099, 36.69, 36.3549, 37.0436, 36.28, 38.47, 38.79, 39.06, 38.92, 39.03, 41.54, 41.14, 39.74, 40.2, 40.01, 39.8, 39.7, 40.6, 40.49, 41.81, 40.06, 42.54, 39.6, 39.57, 40.2901, 40.85, 40.37, 40.89, 40.22, 41.45, 41.81, 42.07, 43.78, 48.56, 49.9199, 49.6699, 49.08, 50.64, 50.67, 48.7699, 48.25, 49.97, 48.58, 48.48, 50.38, 55.29, 55.41, 53.49, 53.49, 53.935, 51.85, 50.03, 51.571, 51.73, 51.88, 51.43, 52.94, 52.16, 51.9405, 52.59, 50.89, 50.69, 50.84, 49.38, 52.075, 52.6499, 52.89, 50.61, 51.9875, 50.68, 50.2131, 49.29, 51.01, 49.69, 49.41, 48.06, 48.194, 46.09, 46.0, 45.1, 45.0599, 45.08, 45.12, 44.75, 45.33, 44.15, 43.81, 43.27, 43.12, 42.9957, 43.45, 43.82, 43.45, 44.11, 45.18, 46.29, 38.6, 37.94, 38.17, 38.7, 37.74, 37.64, 38.04, 37.04, 36.88, 37.42, 37.88, 38.2901, 38.31, 37.83, 38.06, 37.41, 40.23, 41.2501, 41.76, 42.06, 40.94, 40.885, 41.45, 39.43, 38.49, 39.5049, 39.17, 38.91, 38.7436, 38.02, 38.025, 36.87, 36.8, 35.54, 35.37, 34.46, 33.33, 33.88, 32.91, 32.5801, 31.755, 32.44, 33.98, 33.7362, 30.5, 30.46, 31.48, 31.7, 31.7, 32.07, 32.26, 32.77, 32.85, 33.36, 33.93, 31.99, 31.955, 30.68, 31.87, 38.72, 39.02, 39.09, 38.725, 43.075, 40.57, 41.587, 44.67, 45.93, 45.97, 46.12, 45.03, 44.44, 45.52, 40.87, 40.06, 41.26, 42.5099, 41.79, 42.43, 43.12, 44.01, 45.69, 46.64, 46.69, 47.25, 46.19, 44.43, 47.9, 48.8, 50.27, 50.14, 51.27, 51.12, 52.08, 51.95, 53.5599, 54.44, 54.0201, 53.87, 53.52, 54.84, 54.36, 54.27, 53.69, 54.9, 55.77, 55.8799, 54.96, 55.77, 55.87, 56.63, 55.43, 58.251, 57.5599, 56.3, 56.7, 53.96, 52.86, 54.31, 50.02, 65.64, 66.34, 65.2301, 64.5, 63.46, 59.45, 60.458, 57.902, 61.72, 62.81, 62.44, 62.53, 62.2, 60.57, 61.5699, 58.1701, 57.7901, 56.99, 57.05, 59.29, 61.42, 66.29, 68.9701, 67.4, 63.65, 60.51, 63.7899, 73.21, 70.24, 64.6, 59.9377, 57.43, 55.44, 56.41, 56.66, 59.1501, 55.33, 52.311, 52.0, 49.19, 44.96, 45.35, 43.6499, 41.44, 40.74, 41.49, 40.89, 40.15, 39.06, 40.99, 42.09, 41.03, 41.7299, 41.01, 43.94, 44.71, 42.43, 41.92, 43.0, 41.65, 44.91]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4ca805b6-98af-4fba-a241-9a5d10e4c45f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"313d600f-6f13-4639-ad8b-7e76b12b6c55\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"313d600f-6f13-4639-ad8b-7e76b12b6c55\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '313d600f-6f13-4639-ad8b-7e76b12b6c55',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('313d600f-6f13-4639-ad8b-7e76b12b6c55');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM_vTGO7Zu1E"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnDjbfCWZu1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "627da571-9e5a-490d-e3f7-38c70b50f505"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .2, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1400, test_end=1600)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"TWTR\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 3s 17ms/step - loss: 0.6579 - accuracy: 0.6596 - val_loss: 0.8832 - val_accuracy: 0.1895\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.6427 - accuracy: 0.6596 - val_loss: 0.8943 - val_accuracy: 0.1895\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6085 - accuracy: 0.6651 - val_loss: 0.6610 - val_accuracy: 0.7579\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.5810 - accuracy: 0.6945 - val_loss: 0.8017 - val_accuracy: 0.5053\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5583 - accuracy: 0.7055 - val_loss: 0.7446 - val_accuracy: 0.6579\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 2s 15ms/step - loss: 0.6376 - accuracy: 0.6596 - val_loss: 0.8685 - val_accuracy: 0.2211\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5728 - accuracy: 0.7055 - val_loss: 0.7198 - val_accuracy: 0.5579\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5323 - accuracy: 0.7229 - val_loss: 0.8954 - val_accuracy: 0.4737\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5131 - accuracy: 0.7376 - val_loss: 0.8236 - val_accuracy: 0.5000\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5130 - accuracy: 0.7431 - val_loss: 0.7306 - val_accuracy: 0.5789\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.747024\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.736923\n",
            "[2]\tvalidation_0-auc:0.770112\n",
            "[3]\tvalidation_0-auc:0.76921\n",
            "[4]\tvalidation_0-auc:0.769751\n",
            "[5]\tvalidation_0-auc:0.768849\n",
            "[6]\tvalidation_0-auc:0.772457\n",
            "[7]\tvalidation_0-auc:0.772637\n",
            "[8]\tvalidation_0-auc:0.771916\n",
            "[9]\tvalidation_0-auc:0.770833\n",
            "[10]\tvalidation_0-auc:0.771014\n",
            "[11]\tvalidation_0-auc:0.775703\n",
            "[12]\tvalidation_0-auc:0.780483\n",
            "[13]\tvalidation_0-auc:0.780483\n",
            "[14]\tvalidation_0-auc:0.781566\n",
            "[15]\tvalidation_0-auc:0.784091\n",
            "[16]\tvalidation_0-auc:0.78382\n",
            "[17]\tvalidation_0-auc:0.784001\n",
            "[18]\tvalidation_0-auc:0.784181\n",
            "[19]\tvalidation_0-auc:0.78373\n",
            "[20]\tvalidation_0-auc:0.783911\n",
            "[21]\tvalidation_0-auc:0.783189\n",
            "[22]\tvalidation_0-auc:0.787698\n",
            "[23]\tvalidation_0-auc:0.786255\n",
            "[24]\tvalidation_0-auc:0.787338\n",
            "[25]\tvalidation_0-auc:0.786797\n",
            "[26]\tvalidation_0-auc:0.790494\n",
            "[27]\tvalidation_0-auc:0.790675\n",
            "[28]\tvalidation_0-auc:0.790133\n",
            "[29]\tvalidation_0-auc:0.789953\n",
            "[30]\tvalidation_0-auc:0.790314\n",
            "[31]\tvalidation_0-auc:0.790675\n",
            "[32]\tvalidation_0-auc:0.791667\n",
            "[33]\tvalidation_0-auc:0.791667\n",
            "[34]\tvalidation_0-auc:0.791486\n",
            "[35]\tvalidation_0-auc:0.788961\n",
            "[36]\tvalidation_0-auc:0.788961\n",
            "[37]\tvalidation_0-auc:0.788961\n",
            "[38]\tvalidation_0-auc:0.788961\n",
            "[39]\tvalidation_0-auc:0.785985\n",
            "[40]\tvalidation_0-auc:0.785624\n",
            "[41]\tvalidation_0-auc:0.785985\n",
            "[42]\tvalidation_0-auc:0.785444\n",
            "[43]\tvalidation_0-auc:0.786165\n",
            "[44]\tvalidation_0-auc:0.784903\n",
            "[45]\tvalidation_0-auc:0.785083\n",
            "[46]\tvalidation_0-auc:0.785083\n",
            "[47]\tvalidation_0-auc:0.785804\n",
            "[48]\tvalidation_0-auc:0.784361\n",
            "[49]\tvalidation_0-auc:0.783911\n",
            "[50]\tvalidation_0-auc:0.783189\n",
            "[51]\tvalidation_0-auc:0.784001\n",
            "[52]\tvalidation_0-auc:0.784181\n",
            "[53]\tvalidation_0-auc:0.781385\n",
            "[54]\tvalidation_0-auc:0.781025\n",
            "[55]\tvalidation_0-auc:0.782648\n",
            "[56]\tvalidation_0-auc:0.783009\n",
            "[57]\tvalidation_0-auc:0.782828\n",
            "[58]\tvalidation_0-auc:0.783189\n",
            "[59]\tvalidation_0-auc:0.784993\n",
            "[60]\tvalidation_0-auc:0.782287\n",
            "[61]\tvalidation_0-auc:0.781566\n",
            "[62]\tvalidation_0-auc:0.781746\n",
            "[63]\tvalidation_0-auc:0.783009\n",
            "[64]\tvalidation_0-auc:0.78373\n",
            "[65]\tvalidation_0-auc:0.784091\n",
            "[66]\tvalidation_0-auc:0.782287\n",
            "[67]\tvalidation_0-auc:0.781205\n",
            "[68]\tvalidation_0-auc:0.781746\n",
            "[69]\tvalidation_0-auc:0.780664\n",
            "[70]\tvalidation_0-auc:0.781205\n",
            "[71]\tvalidation_0-auc:0.779762\n",
            "[72]\tvalidation_0-auc:0.77886\n",
            "[73]\tvalidation_0-auc:0.77904\n",
            "[74]\tvalidation_0-auc:0.779582\n",
            "[75]\tvalidation_0-auc:0.780303\n",
            "[76]\tvalidation_0-auc:0.77904\n",
            "[77]\tvalidation_0-auc:0.779582\n",
            "[78]\tvalidation_0-auc:0.779401\n",
            "[79]\tvalidation_0-auc:0.77868\n",
            "[80]\tvalidation_0-auc:0.777958\n",
            "[81]\tvalidation_0-auc:0.779401\n",
            "[82]\tvalidation_0-auc:0.781385\n",
            "Stopping. Best iteration:\n",
            "[32]\tvalidation_0-auc:0.791667\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 3s 16ms/step - loss: 0.6514 - accuracy: 0.6556 - val_loss: 1.0318 - val_accuracy: 0.1592\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.6440 - accuracy: 0.6632 - val_loss: 0.9005 - val_accuracy: 0.1592\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.6273 - accuracy: 0.6708 - val_loss: 0.8932 - val_accuracy: 0.1592\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.6073 - accuracy: 0.6812 - val_loss: 0.8250 - val_accuracy: 0.3503\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5793 - accuracy: 0.7039 - val_loss: 0.7008 - val_accuracy: 0.5669\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 2s 15ms/step - loss: 0.6408 - accuracy: 0.6632 - val_loss: 0.8861 - val_accuracy: 0.1592\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.6066 - accuracy: 0.6963 - val_loss: 0.6937 - val_accuracy: 0.5414\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5585 - accuracy: 0.7209 - val_loss: 0.7338 - val_accuracy: 0.5032\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5338 - accuracy: 0.7247 - val_loss: 0.5741 - val_accuracy: 0.7134\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5242 - accuracy: 0.7408 - val_loss: 0.7219 - val_accuracy: 0.5669\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.707576\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.764545\n",
            "[2]\tvalidation_0-auc:0.776364\n",
            "[3]\tvalidation_0-auc:0.777273\n",
            "[4]\tvalidation_0-auc:0.782879\n",
            "[5]\tvalidation_0-auc:0.784849\n",
            "[6]\tvalidation_0-auc:0.796212\n",
            "[7]\tvalidation_0-auc:0.790152\n",
            "[8]\tvalidation_0-auc:0.785\n",
            "[9]\tvalidation_0-auc:0.793333\n",
            "[10]\tvalidation_0-auc:0.795152\n",
            "[11]\tvalidation_0-auc:0.790758\n",
            "[12]\tvalidation_0-auc:0.795303\n",
            "[13]\tvalidation_0-auc:0.796364\n",
            "[14]\tvalidation_0-auc:0.793333\n",
            "[15]\tvalidation_0-auc:0.796212\n",
            "[16]\tvalidation_0-auc:0.793788\n",
            "[17]\tvalidation_0-auc:0.795606\n",
            "[18]\tvalidation_0-auc:0.799697\n",
            "[19]\tvalidation_0-auc:0.801061\n",
            "[20]\tvalidation_0-auc:0.803788\n",
            "[21]\tvalidation_0-auc:0.80197\n",
            "[22]\tvalidation_0-auc:0.799545\n",
            "[23]\tvalidation_0-auc:0.803788\n",
            "[24]\tvalidation_0-auc:0.801061\n",
            "[25]\tvalidation_0-auc:0.800909\n",
            "[26]\tvalidation_0-auc:0.805152\n",
            "[27]\tvalidation_0-auc:0.804848\n",
            "[28]\tvalidation_0-auc:0.804242\n",
            "[29]\tvalidation_0-auc:0.804242\n",
            "[30]\tvalidation_0-auc:0.803636\n",
            "[31]\tvalidation_0-auc:0.804697\n",
            "[32]\tvalidation_0-auc:0.805909\n",
            "[33]\tvalidation_0-auc:0.804091\n",
            "[34]\tvalidation_0-auc:0.804394\n",
            "[35]\tvalidation_0-auc:0.805606\n",
            "[36]\tvalidation_0-auc:0.805909\n",
            "[37]\tvalidation_0-auc:0.808636\n",
            "[38]\tvalidation_0-auc:0.805909\n",
            "[39]\tvalidation_0-auc:0.805303\n",
            "[40]\tvalidation_0-auc:0.806212\n",
            "[41]\tvalidation_0-auc:0.805606\n",
            "[42]\tvalidation_0-auc:0.805\n",
            "[43]\tvalidation_0-auc:0.806818\n",
            "[44]\tvalidation_0-auc:0.80197\n",
            "[45]\tvalidation_0-auc:0.802879\n",
            "[46]\tvalidation_0-auc:0.803182\n",
            "[47]\tvalidation_0-auc:0.803788\n",
            "[48]\tvalidation_0-auc:0.803182\n",
            "[49]\tvalidation_0-auc:0.803182\n",
            "[50]\tvalidation_0-auc:0.802576\n",
            "[51]\tvalidation_0-auc:0.806515\n",
            "[52]\tvalidation_0-auc:0.807727\n",
            "[53]\tvalidation_0-auc:0.807424\n",
            "[54]\tvalidation_0-auc:0.809697\n",
            "[55]\tvalidation_0-auc:0.809697\n",
            "[56]\tvalidation_0-auc:0.808485\n",
            "[57]\tvalidation_0-auc:0.80697\n",
            "[58]\tvalidation_0-auc:0.808788\n",
            "[59]\tvalidation_0-auc:0.809697\n",
            "[60]\tvalidation_0-auc:0.809091\n",
            "[61]\tvalidation_0-auc:0.804242\n",
            "[62]\tvalidation_0-auc:0.805758\n",
            "[63]\tvalidation_0-auc:0.806364\n",
            "[64]\tvalidation_0-auc:0.807273\n",
            "[65]\tvalidation_0-auc:0.807879\n",
            "[66]\tvalidation_0-auc:0.80697\n",
            "[67]\tvalidation_0-auc:0.807273\n",
            "[68]\tvalidation_0-auc:0.805758\n",
            "[69]\tvalidation_0-auc:0.803939\n",
            "[70]\tvalidation_0-auc:0.803939\n",
            "[71]\tvalidation_0-auc:0.804848\n",
            "[72]\tvalidation_0-auc:0.804848\n",
            "[73]\tvalidation_0-auc:0.806061\n",
            "[74]\tvalidation_0-auc:0.804848\n",
            "[75]\tvalidation_0-auc:0.808788\n",
            "[76]\tvalidation_0-auc:0.808788\n",
            "[77]\tvalidation_0-auc:0.808485\n",
            "[78]\tvalidation_0-auc:0.809697\n",
            "[79]\tvalidation_0-auc:0.806667\n",
            "[80]\tvalidation_0-auc:0.808788\n",
            "[81]\tvalidation_0-auc:0.803939\n",
            "[82]\tvalidation_0-auc:0.806667\n",
            "[83]\tvalidation_0-auc:0.806667\n",
            "[84]\tvalidation_0-auc:0.807879\n",
            "[85]\tvalidation_0-auc:0.806364\n",
            "[86]\tvalidation_0-auc:0.802121\n",
            "[87]\tvalidation_0-auc:0.803939\n",
            "[88]\tvalidation_0-auc:0.806364\n",
            "[89]\tvalidation_0-auc:0.808788\n",
            "[90]\tvalidation_0-auc:0.807879\n",
            "[91]\tvalidation_0-auc:0.807879\n",
            "[92]\tvalidation_0-auc:0.809394\n",
            "[93]\tvalidation_0-auc:0.808485\n",
            "[94]\tvalidation_0-auc:0.807576\n",
            "[95]\tvalidation_0-auc:0.807273\n",
            "[96]\tvalidation_0-auc:0.807273\n",
            "[97]\tvalidation_0-auc:0.806364\n",
            "[98]\tvalidation_0-auc:0.804848\n",
            "[99]\tvalidation_0-auc:0.805758\n",
            "end training. \n",
            "\n",
            "+------------------+---------------------+--------------------+---------------------+--------------------+\n",
            "|      Model       |       Accuracy      |     Precision      |        Recall       |      F1 score      |\n",
            "+------------------+---------------------+--------------------+---------------------+--------------------+\n",
            "|     LSTM 0.1     |  0.6578947368421053 | 0.9405940594059405 |  0.6168831168831169 | 0.7450980392156862 |\n",
            "|     GRU 0.1      |  0.5789473684210527 | 0.9302325581395349 |  0.5194805194805194 | 0.6666666666666666 |\n",
            "|   XGBoost 0.1    |  0.631578947368421  |        0.92        |  0.5974025974025974 | 0.7244094488188978 |\n",
            "|    Logreg 0.1    | 0.47368421052631576 |        0.95        | 0.37012987012987014 | 0.5327102803738318 |\n",
            "|     SVM 0.1      |         0.6         | 0.9333333333333333 |  0.5454545454545454 | 0.6885245901639344 |\n",
            "|  LSTM beta 0.1   |  0.5668789808917197 | 0.9210526315789473 |  0.5303030303030303 | 0.673076923076923  |\n",
            "|   GRU beta 0.1   |  0.5668789808917197 | 0.9210526315789473 |  0.5303030303030303 | 0.673076923076923  |\n",
            "| XGBoost beta 0.1 |  0.6305732484076433 | 0.9404761904761905 |  0.5984848484848485 | 0.7314814814814815 |\n",
            "| logreg beta 0.1  |  0.535031847133758  | 0.9402985074626866 |  0.4772727272727273 | 0.6331658291457287 |\n",
            "|   svm beta 0.1   |  0.6242038216560509 | 0.9620253164556962 |  0.5757575757575758 | 0.7203791469194313 |\n",
            "+------------------+---------------------+--------------------+---------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 3s 17ms/step - loss: 0.6460 - accuracy: 0.6679 - val_loss: 0.8769 - val_accuracy: 0.2895\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6002 - accuracy: 0.6844 - val_loss: 0.8887 - val_accuracy: 0.4211\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6113 - accuracy: 0.6661 - val_loss: 0.8197 - val_accuracy: 0.4053\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.5831 - accuracy: 0.6927 - val_loss: 0.7710 - val_accuracy: 0.5421\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.5980 - accuracy: 0.6789 - val_loss: 0.8354 - val_accuracy: 0.3421\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 3s 15ms/step - loss: 0.6355 - accuracy: 0.6578 - val_loss: 0.8223 - val_accuracy: 0.2895\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5836 - accuracy: 0.6927 - val_loss: 0.7270 - val_accuracy: 0.5316\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5544 - accuracy: 0.7046 - val_loss: 0.6303 - val_accuracy: 0.6368\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.5626 - accuracy: 0.6991 - val_loss: 0.6528 - val_accuracy: 0.6263\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5517 - accuracy: 0.7110 - val_loss: 0.7371 - val_accuracy: 0.5368\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.737912\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.740067\n",
            "[2]\tvalidation_0-auc:0.742492\n",
            "[3]\tvalidation_0-auc:0.741481\n",
            "[4]\tvalidation_0-auc:0.747475\n",
            "[5]\tvalidation_0-auc:0.747879\n",
            "[6]\tvalidation_0-auc:0.747609\n",
            "[7]\tvalidation_0-auc:0.736364\n",
            "[8]\tvalidation_0-auc:0.752593\n",
            "[9]\tvalidation_0-auc:0.747071\n",
            "[10]\tvalidation_0-auc:0.754478\n",
            "[11]\tvalidation_0-auc:0.755556\n",
            "[12]\tvalidation_0-auc:0.759057\n",
            "[13]\tvalidation_0-auc:0.760404\n",
            "[14]\tvalidation_0-auc:0.76\n",
            "[15]\tvalidation_0-auc:0.76229\n",
            "[16]\tvalidation_0-auc:0.763906\n",
            "[17]\tvalidation_0-auc:0.775892\n",
            "[18]\tvalidation_0-auc:0.775421\n",
            "[19]\tvalidation_0-auc:0.775017\n",
            "[20]\tvalidation_0-auc:0.778114\n",
            "[21]\tvalidation_0-auc:0.779394\n",
            "[22]\tvalidation_0-auc:0.777374\n",
            "[23]\tvalidation_0-auc:0.778316\n",
            "[24]\tvalidation_0-auc:0.777778\n",
            "[25]\tvalidation_0-auc:0.776768\n",
            "[26]\tvalidation_0-auc:0.777037\n",
            "[27]\tvalidation_0-auc:0.778114\n",
            "[28]\tvalidation_0-auc:0.778721\n",
            "[29]\tvalidation_0-auc:0.783906\n",
            "[30]\tvalidation_0-auc:0.782694\n",
            "[31]\tvalidation_0-auc:0.780471\n",
            "[32]\tvalidation_0-auc:0.782155\n",
            "[33]\tvalidation_0-auc:0.784983\n",
            "[34]\tvalidation_0-auc:0.785455\n",
            "[35]\tvalidation_0-auc:0.78963\n",
            "[36]\tvalidation_0-auc:0.790572\n",
            "[37]\tvalidation_0-auc:0.79165\n",
            "[38]\tvalidation_0-auc:0.792054\n",
            "[39]\tvalidation_0-auc:0.791111\n",
            "[40]\tvalidation_0-auc:0.793064\n",
            "[41]\tvalidation_0-auc:0.793064\n",
            "[42]\tvalidation_0-auc:0.793333\n",
            "[43]\tvalidation_0-auc:0.794007\n",
            "[44]\tvalidation_0-auc:0.793872\n",
            "[45]\tvalidation_0-auc:0.793333\n",
            "[46]\tvalidation_0-auc:0.794949\n",
            "[47]\tvalidation_0-auc:0.797104\n",
            "[48]\tvalidation_0-auc:0.795421\n",
            "[49]\tvalidation_0-auc:0.797172\n",
            "[50]\tvalidation_0-auc:0.798114\n",
            "[51]\tvalidation_0-auc:0.800404\n",
            "[52]\tvalidation_0-auc:0.799192\n",
            "[53]\tvalidation_0-auc:0.800135\n",
            "[54]\tvalidation_0-auc:0.803232\n",
            "[55]\tvalidation_0-auc:0.804175\n",
            "[56]\tvalidation_0-auc:0.80404\n",
            "[57]\tvalidation_0-auc:0.802424\n",
            "[58]\tvalidation_0-auc:0.80404\n",
            "[59]\tvalidation_0-auc:0.802963\n",
            "[60]\tvalidation_0-auc:0.802963\n",
            "[61]\tvalidation_0-auc:0.802424\n",
            "[62]\tvalidation_0-auc:0.802963\n",
            "[63]\tvalidation_0-auc:0.804983\n",
            "[64]\tvalidation_0-auc:0.809293\n",
            "[65]\tvalidation_0-auc:0.810101\n",
            "[66]\tvalidation_0-auc:0.808754\n",
            "[67]\tvalidation_0-auc:0.809158\n",
            "[68]\tvalidation_0-auc:0.808889\n",
            "[69]\tvalidation_0-auc:0.809024\n",
            "[70]\tvalidation_0-auc:0.808619\n",
            "[71]\tvalidation_0-auc:0.809158\n",
            "[72]\tvalidation_0-auc:0.811717\n",
            "[73]\tvalidation_0-auc:0.812525\n",
            "[74]\tvalidation_0-auc:0.812795\n",
            "[75]\tvalidation_0-auc:0.815354\n",
            "[76]\tvalidation_0-auc:0.815488\n",
            "[77]\tvalidation_0-auc:0.813737\n",
            "[78]\tvalidation_0-auc:0.810505\n",
            "[79]\tvalidation_0-auc:0.810236\n",
            "[80]\tvalidation_0-auc:0.808619\n",
            "[81]\tvalidation_0-auc:0.809966\n",
            "[82]\tvalidation_0-auc:0.806734\n",
            "[83]\tvalidation_0-auc:0.805926\n",
            "[84]\tvalidation_0-auc:0.805657\n",
            "[85]\tvalidation_0-auc:0.804175\n",
            "[86]\tvalidation_0-auc:0.804579\n",
            "[87]\tvalidation_0-auc:0.803906\n",
            "[88]\tvalidation_0-auc:0.799192\n",
            "[89]\tvalidation_0-auc:0.798384\n",
            "[90]\tvalidation_0-auc:0.801077\n",
            "[91]\tvalidation_0-auc:0.803367\n",
            "[92]\tvalidation_0-auc:0.80431\n",
            "[93]\tvalidation_0-auc:0.805522\n",
            "[94]\tvalidation_0-auc:0.806195\n",
            "[95]\tvalidation_0-auc:0.806195\n",
            "[96]\tvalidation_0-auc:0.807542\n",
            "[97]\tvalidation_0-auc:0.807946\n",
            "[98]\tvalidation_0-auc:0.807407\n",
            "[99]\tvalidation_0-auc:0.806061\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 3s 17ms/step - loss: 0.6306 - accuracy: 0.6746 - val_loss: 0.6512 - val_accuracy: 0.6433\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5934 - accuracy: 0.7001 - val_loss: 0.5639 - val_accuracy: 0.8025\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5789 - accuracy: 0.7228 - val_loss: 0.6863 - val_accuracy: 0.5669\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5431 - accuracy: 0.7304 - val_loss: 0.5361 - val_accuracy: 0.7898\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5461 - accuracy: 0.7360 - val_loss: 0.6307 - val_accuracy: 0.6306\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 2s 16ms/step - loss: 0.6191 - accuracy: 0.6746 - val_loss: 0.5926 - val_accuracy: 0.7006\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5542 - accuracy: 0.7304 - val_loss: 0.5655 - val_accuracy: 0.6688\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5238 - accuracy: 0.7483 - val_loss: 0.5201 - val_accuracy: 0.7070\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5195 - accuracy: 0.7512 - val_loss: 0.4870 - val_accuracy: 0.7452\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5104 - accuracy: 0.7483 - val_loss: 0.4696 - val_accuracy: 0.7962\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.771118\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.801388\n",
            "[2]\tvalidation_0-auc:0.813053\n",
            "[3]\tvalidation_0-auc:0.808427\n",
            "[4]\tvalidation_0-auc:0.799879\n",
            "[5]\tvalidation_0-auc:0.800583\n",
            "[6]\tvalidation_0-auc:0.801488\n",
            "[7]\tvalidation_0-auc:0.798572\n",
            "[8]\tvalidation_0-auc:0.786102\n",
            "[9]\tvalidation_0-auc:0.780068\n",
            "[10]\tvalidation_0-auc:0.763978\n",
            "[11]\tvalidation_0-auc:0.7678\n",
            "[12]\tvalidation_0-auc:0.760559\n",
            "[13]\tvalidation_0-auc:0.754525\n",
            "[14]\tvalidation_0-auc:0.754123\n",
            "[15]\tvalidation_0-auc:0.746078\n",
            "[16]\tvalidation_0-auc:0.739843\n",
            "[17]\tvalidation_0-auc:0.740648\n",
            "[18]\tvalidation_0-auc:0.740547\n",
            "[19]\tvalidation_0-auc:0.73934\n",
            "[20]\tvalidation_0-auc:0.736726\n",
            "[21]\tvalidation_0-auc:0.73572\n",
            "[22]\tvalidation_0-auc:0.734714\n",
            "[23]\tvalidation_0-auc:0.733709\n",
            "[24]\tvalidation_0-auc:0.73029\n",
            "[25]\tvalidation_0-auc:0.729284\n",
            "[26]\tvalidation_0-auc:0.730893\n",
            "[27]\tvalidation_0-auc:0.726368\n",
            "[28]\tvalidation_0-auc:0.717317\n",
            "[29]\tvalidation_0-auc:0.7143\n",
            "[30]\tvalidation_0-auc:0.715708\n",
            "[31]\tvalidation_0-auc:0.715507\n",
            "[32]\tvalidation_0-auc:0.711685\n",
            "[33]\tvalidation_0-auc:0.710076\n",
            "[34]\tvalidation_0-auc:0.711685\n",
            "[35]\tvalidation_0-auc:0.711887\n",
            "[36]\tvalidation_0-auc:0.710881\n",
            "[37]\tvalidation_0-auc:0.706255\n",
            "[38]\tvalidation_0-auc:0.701428\n",
            "[39]\tvalidation_0-auc:0.701227\n",
            "[40]\tvalidation_0-auc:0.697204\n",
            "[41]\tvalidation_0-auc:0.697808\n",
            "[42]\tvalidation_0-auc:0.700221\n",
            "[43]\tvalidation_0-auc:0.694791\n",
            "[44]\tvalidation_0-auc:0.693986\n",
            "[45]\tvalidation_0-auc:0.693383\n",
            "[46]\tvalidation_0-auc:0.69288\n",
            "[47]\tvalidation_0-auc:0.69288\n",
            "[48]\tvalidation_0-auc:0.690064\n",
            "[49]\tvalidation_0-auc:0.68564\n",
            "[50]\tvalidation_0-auc:0.682723\n",
            "[51]\tvalidation_0-auc:0.682723\n",
            "[52]\tvalidation_0-auc:0.686344\n",
            "Stopping. Best iteration:\n",
            "[2]\tvalidation_0-auc:0.813053\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+---------------------+--------------------+---------------------+---------------------+\n",
            "|      Model       |       Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+------------------+---------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.2     | 0.34210526315789475 |        1.0         | 0.07407407407407407 | 0.13793103448275862 |\n",
            "|     GRU 0.2      |  0.5368421052631579 | 0.9607843137254902 |  0.362962962962963  |  0.5268817204301075 |\n",
            "|   XGBoost 0.2    |         0.6         | 0.927536231884058  |  0.4740740740740741 |  0.627450980392157  |\n",
            "|    Logreg 0.2    |  0.5842105263157895 |       0.9375       |  0.4444444444444444 |  0.6030150753768844 |\n",
            "|     SVM 0.2      | 0.49473684210526314 | 0.975609756097561  |  0.2962962962962963 | 0.45454545454545453 |\n",
            "|  LSTM beta 0.2   |  0.6305732484076433 | 0.9230769230769231 |  0.5309734513274337 |  0.6741573033707865 |\n",
            "|   GRU beta 0.2   |  0.7961783439490446 | 0.8857142857142857 |  0.8230088495575221 |  0.8532110091743119 |\n",
            "| XGBoost beta 0.2 |  0.7261146496815286 | 0.8571428571428571 |  0.7433628318584071 |  0.7962085308056872 |\n",
            "| logreg beta 0.2  |  0.7452229299363057 | 0.8924731182795699 |  0.7345132743362832 |  0.8058252427184466 |\n",
            "|   svm beta 0.2   |  0.7579617834394905 | 0.8440366972477065 |  0.8141592920353983 |  0.8288288288288288 |\n",
            "+------------------+---------------------+--------------------+---------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 3s 17ms/step - loss: 0.6743 - accuracy: 0.6046 - val_loss: 0.7600 - val_accuracy: 0.2895\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6242 - accuracy: 0.6413 - val_loss: 0.6967 - val_accuracy: 0.5947\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6137 - accuracy: 0.6486 - val_loss: 0.7008 - val_accuracy: 0.5947\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6049 - accuracy: 0.6578 - val_loss: 0.7376 - val_accuracy: 0.5579\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.5983 - accuracy: 0.6578 - val_loss: 0.6723 - val_accuracy: 0.6842\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 2s 15ms/step - loss: 0.6616 - accuracy: 0.6174 - val_loss: 0.7266 - val_accuracy: 0.3947\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.6021 - accuracy: 0.6844 - val_loss: 0.6936 - val_accuracy: 0.5789\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5627 - accuracy: 0.6872 - val_loss: 0.6254 - val_accuracy: 0.6737\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5726 - accuracy: 0.6972 - val_loss: 0.6815 - val_accuracy: 0.5632\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5659 - accuracy: 0.7083 - val_loss: 0.6082 - val_accuracy: 0.7105\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.773266\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.763502\n",
            "[2]\tvalidation_0-auc:0.76633\n",
            "[3]\tvalidation_0-auc:0.767205\n",
            "[4]\tvalidation_0-auc:0.768552\n",
            "[5]\tvalidation_0-auc:0.764781\n",
            "[6]\tvalidation_0-auc:0.76404\n",
            "[7]\tvalidation_0-auc:0.763569\n",
            "[8]\tvalidation_0-auc:0.766061\n",
            "[9]\tvalidation_0-auc:0.765791\n",
            "[10]\tvalidation_0-auc:0.767273\n",
            "[11]\tvalidation_0-auc:0.765791\n",
            "[12]\tvalidation_0-auc:0.766195\n",
            "[13]\tvalidation_0-auc:0.765455\n",
            "[14]\tvalidation_0-auc:0.764175\n",
            "[15]\tvalidation_0-auc:0.76404\n",
            "[16]\tvalidation_0-auc:0.764983\n",
            "[17]\tvalidation_0-auc:0.767407\n",
            "[18]\tvalidation_0-auc:0.767071\n",
            "[19]\tvalidation_0-auc:0.766128\n",
            "[20]\tvalidation_0-auc:0.765859\n",
            "[21]\tvalidation_0-auc:0.765926\n",
            "[22]\tvalidation_0-auc:0.765522\n",
            "[23]\tvalidation_0-auc:0.76431\n",
            "[24]\tvalidation_0-auc:0.767946\n",
            "[25]\tvalidation_0-auc:0.770236\n",
            "[26]\tvalidation_0-auc:0.77037\n",
            "[27]\tvalidation_0-auc:0.770505\n",
            "[28]\tvalidation_0-auc:0.770438\n",
            "[29]\tvalidation_0-auc:0.770438\n",
            "[30]\tvalidation_0-auc:0.769428\n",
            "[31]\tvalidation_0-auc:0.769562\n",
            "[32]\tvalidation_0-auc:0.768889\n",
            "[33]\tvalidation_0-auc:0.767946\n",
            "[34]\tvalidation_0-auc:0.767542\n",
            "[35]\tvalidation_0-auc:0.766734\n",
            "[36]\tvalidation_0-auc:0.764983\n",
            "[37]\tvalidation_0-auc:0.766667\n",
            "[38]\tvalidation_0-auc:0.768081\n",
            "[39]\tvalidation_0-auc:0.767946\n",
            "[40]\tvalidation_0-auc:0.76835\n",
            "[41]\tvalidation_0-auc:0.769158\n",
            "[42]\tvalidation_0-auc:0.767677\n",
            "[43]\tvalidation_0-auc:0.767542\n",
            "[44]\tvalidation_0-auc:0.767677\n",
            "[45]\tvalidation_0-auc:0.767677\n",
            "[46]\tvalidation_0-auc:0.766599\n",
            "[47]\tvalidation_0-auc:0.766599\n",
            "[48]\tvalidation_0-auc:0.764444\n",
            "[49]\tvalidation_0-auc:0.772795\n",
            "[50]\tvalidation_0-auc:0.771717\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.773266\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 3s 17ms/step - loss: 0.6690 - accuracy: 0.6093 - val_loss: 0.6410 - val_accuracy: 0.7197\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.6355 - accuracy: 0.6358 - val_loss: 0.5534 - val_accuracy: 0.7197\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.6281 - accuracy: 0.6471 - val_loss: 0.5660 - val_accuracy: 0.7580\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.6086 - accuracy: 0.6840 - val_loss: 0.5776 - val_accuracy: 0.7643\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5698 - accuracy: 0.7067 - val_loss: 0.4468 - val_accuracy: 0.8726\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 2s 15ms/step - loss: 0.6389 - accuracy: 0.6500 - val_loss: 0.5271 - val_accuracy: 0.7325\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5653 - accuracy: 0.7096 - val_loss: 0.4277 - val_accuracy: 0.8790\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5403 - accuracy: 0.7275 - val_loss: 0.4539 - val_accuracy: 0.8471\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5360 - accuracy: 0.7275 - val_loss: 0.4162 - val_accuracy: 0.8790\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5187 - accuracy: 0.7313 - val_loss: 0.4454 - val_accuracy: 0.8408\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.761263\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.756235\n",
            "[2]\tvalidation_0-auc:0.757039\n",
            "[3]\tvalidation_0-auc:0.761062\n",
            "[4]\tvalidation_0-auc:0.76066\n",
            "[5]\tvalidation_0-auc:0.760056\n",
            "[6]\tvalidation_0-auc:0.739139\n",
            "[7]\tvalidation_0-auc:0.73924\n",
            "[8]\tvalidation_0-auc:0.739441\n",
            "[9]\tvalidation_0-auc:0.725463\n",
            "[10]\tvalidation_0-auc:0.724658\n",
            "[11]\tvalidation_0-auc:0.723954\n",
            "[12]\tvalidation_0-auc:0.712591\n",
            "[13]\tvalidation_0-auc:0.709875\n",
            "[14]\tvalidation_0-auc:0.709574\n",
            "[15]\tvalidation_0-auc:0.719027\n",
            "[16]\tvalidation_0-auc:0.714099\n",
            "[17]\tvalidation_0-auc:0.713898\n",
            "[18]\tvalidation_0-auc:0.699517\n",
            "[19]\tvalidation_0-auc:0.702534\n",
            "[20]\tvalidation_0-auc:0.699517\n",
            "[21]\tvalidation_0-auc:0.700925\n",
            "[22]\tvalidation_0-auc:0.697104\n",
            "[23]\tvalidation_0-auc:0.67317\n",
            "[24]\tvalidation_0-auc:0.660499\n",
            "[25]\tvalidation_0-auc:0.659091\n",
            "[26]\tvalidation_0-auc:0.659393\n",
            "[27]\tvalidation_0-auc:0.652957\n",
            "[28]\tvalidation_0-auc:0.647124\n",
            "[29]\tvalidation_0-auc:0.643303\n",
            "[30]\tvalidation_0-auc:0.647124\n",
            "[31]\tvalidation_0-auc:0.632643\n",
            "[32]\tvalidation_0-auc:0.63043\n",
            "[33]\tvalidation_0-auc:0.629224\n",
            "[34]\tvalidation_0-auc:0.630632\n",
            "[35]\tvalidation_0-auc:0.627816\n",
            "[36]\tvalidation_0-auc:0.629023\n",
            "[37]\tvalidation_0-auc:0.628218\n",
            "[38]\tvalidation_0-auc:0.634453\n",
            "[39]\tvalidation_0-auc:0.63566\n",
            "[40]\tvalidation_0-auc:0.635257\n",
            "[41]\tvalidation_0-auc:0.634252\n",
            "[42]\tvalidation_0-auc:0.636263\n",
            "[43]\tvalidation_0-auc:0.63566\n",
            "[44]\tvalidation_0-auc:0.634654\n",
            "[45]\tvalidation_0-auc:0.633447\n",
            "[46]\tvalidation_0-auc:0.625201\n",
            "[47]\tvalidation_0-auc:0.624799\n",
            "[48]\tvalidation_0-auc:0.629224\n",
            "[49]\tvalidation_0-auc:0.631637\n",
            "[50]\tvalidation_0-auc:0.630028\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.761263\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.6842105263157895 | 0.8504672897196262 | 0.674074074074074  | 0.7520661157024793 |\n",
            "|      GRU 0.15     | 0.7105263157894737 | 0.8571428571428571 | 0.7111111111111111 | 0.777327935222672  |\n",
            "|    XGBoost 0.15   | 0.7105263157894737 | 0.8703703703703703 | 0.6962962962962963 | 0.7736625514403292 |\n",
            "|    Logreg 0.15    | 0.6421052631578947 | 0.8764044943820225 | 0.5777777777777777 | 0.6964285714285714 |\n",
            "|      SVM 0.15     | 0.7052631578947368 | 0.8691588785046729 | 0.6888888888888889 | 0.768595041322314  |\n",
            "|   LSTM beta 0.15  | 0.8726114649681529 | 0.8780487804878049 | 0.9557522123893806 | 0.9152542372881357 |\n",
            "|   GRU beta 0.15   | 0.8407643312101911 | 0.9150943396226415 | 0.8584070796460177 | 0.8858447488584474 |\n",
            "| XGBoost beta 0.15 | 0.7961783439490446 | 0.8292682926829268 | 0.9026548672566371 | 0.8644067796610169 |\n",
            "|  logreg beta 0.15 | 0.8535031847133758 | 0.8813559322033898 | 0.9203539823008849 | 0.9004329004329005 |\n",
            "|   svm beta 0.15   | 0.802547770700637  | 0.8306451612903226 | 0.911504424778761  | 0.8691983122362869 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnVh3UGsZu1E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "bfc156cc-7037-4917-a2f3-e420a501403d"
      },
      "source": [
        "Result_cross.to_csv('SYY_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.940594</td>\n",
              "      <td>0.657895</td>\n",
              "      <td>0.745098</td>\n",
              "      <td>0.616883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.930233</td>\n",
              "      <td>0.578947</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.519481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.631579</td>\n",
              "      <td>0.724409</td>\n",
              "      <td>0.597403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.473684</td>\n",
              "      <td>0.532710</td>\n",
              "      <td>0.370130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.688525</td>\n",
              "      <td>0.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.921053</td>\n",
              "      <td>0.566879</td>\n",
              "      <td>0.673077</td>\n",
              "      <td>0.530303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.921053</td>\n",
              "      <td>0.566879</td>\n",
              "      <td>0.673077</td>\n",
              "      <td>0.530303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.940476</td>\n",
              "      <td>0.630573</td>\n",
              "      <td>0.731481</td>\n",
              "      <td>0.598485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.940299</td>\n",
              "      <td>0.535032</td>\n",
              "      <td>0.633166</td>\n",
              "      <td>0.477273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.962025</td>\n",
              "      <td>0.624204</td>\n",
              "      <td>0.720379</td>\n",
              "      <td>0.575758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.342105</td>\n",
              "      <td>0.137931</td>\n",
              "      <td>0.074074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.960784</td>\n",
              "      <td>0.536842</td>\n",
              "      <td>0.526882</td>\n",
              "      <td>0.362963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.927536</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.627451</td>\n",
              "      <td>0.474074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.584211</td>\n",
              "      <td>0.603015</td>\n",
              "      <td>0.444444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.975610</td>\n",
              "      <td>0.494737</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.296296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>0.630573</td>\n",
              "      <td>0.674157</td>\n",
              "      <td>0.530973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.885714</td>\n",
              "      <td>0.796178</td>\n",
              "      <td>0.853211</td>\n",
              "      <td>0.823009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.726115</td>\n",
              "      <td>0.796209</td>\n",
              "      <td>0.743363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.892473</td>\n",
              "      <td>0.745223</td>\n",
              "      <td>0.805825</td>\n",
              "      <td>0.734513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.844037</td>\n",
              "      <td>0.757962</td>\n",
              "      <td>0.828829</td>\n",
              "      <td>0.814159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.850467</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.752066</td>\n",
              "      <td>0.674074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.710526</td>\n",
              "      <td>0.777328</td>\n",
              "      <td>0.711111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.870370</td>\n",
              "      <td>0.710526</td>\n",
              "      <td>0.773663</td>\n",
              "      <td>0.696296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.876404</td>\n",
              "      <td>0.642105</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.577778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.869159</td>\n",
              "      <td>0.705263</td>\n",
              "      <td>0.768595</td>\n",
              "      <td>0.688889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.878049</td>\n",
              "      <td>0.872611</td>\n",
              "      <td>0.915254</td>\n",
              "      <td>0.955752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.915094</td>\n",
              "      <td>0.840764</td>\n",
              "      <td>0.885845</td>\n",
              "      <td>0.858407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.829268</td>\n",
              "      <td>0.796178</td>\n",
              "      <td>0.864407</td>\n",
              "      <td>0.902655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.881356</td>\n",
              "      <td>0.853503</td>\n",
              "      <td>0.900433</td>\n",
              "      <td>0.920354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.830645</td>\n",
              "      <td>0.802548</td>\n",
              "      <td>0.869198</td>\n",
              "      <td>0.911504</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  TWTR  0.940594  0.657895  0.745098  0.616883\n",
              "1            GRU 0.1  TWTR  0.930233  0.578947  0.666667  0.519481\n",
              "2        XGBoost 0.1  TWTR  0.920000  0.631579  0.724409  0.597403\n",
              "3         Logreg 0.1  TWTR  0.950000  0.473684  0.532710  0.370130\n",
              "4            SVM 0.1  TWTR  0.933333  0.600000  0.688525  0.545455\n",
              "5      LSTM beta 0.1  TWTR  0.921053  0.566879  0.673077  0.530303\n",
              "6       GRU beta 0.1  TWTR  0.921053  0.566879  0.673077  0.530303\n",
              "7   XGBoost beta 0.1  TWTR  0.940476  0.630573  0.731481  0.598485\n",
              "8    logreg beta 0.1  TWTR  0.940299  0.535032  0.633166  0.477273\n",
              "9       svm beta 0.1  TWTR  0.962025  0.624204  0.720379  0.575758\n",
              "0           LSTM 0.2  TWTR  1.000000  0.342105  0.137931  0.074074\n",
              "1            GRU 0.2  TWTR  0.960784  0.536842  0.526882  0.362963\n",
              "2        XGBoost 0.2  TWTR  0.927536  0.600000  0.627451  0.474074\n",
              "3         Logreg 0.2  TWTR  0.937500  0.584211  0.603015  0.444444\n",
              "4            SVM 0.2  TWTR  0.975610  0.494737  0.454545  0.296296\n",
              "5      LSTM beta 0.2  TWTR  0.923077  0.630573  0.674157  0.530973\n",
              "6       GRU beta 0.2  TWTR  0.885714  0.796178  0.853211  0.823009\n",
              "7   XGBoost beta 0.2  TWTR  0.857143  0.726115  0.796209  0.743363\n",
              "8    logreg beta 0.2  TWTR  0.892473  0.745223  0.805825  0.734513\n",
              "9       svm beta 0.2  TWTR  0.844037  0.757962  0.828829  0.814159\n",
              "0          LSTM 0.15  TWTR  0.850467  0.684211  0.752066  0.674074\n",
              "1           GRU 0.15  TWTR  0.857143  0.710526  0.777328  0.711111\n",
              "2       XGBoost 0.15  TWTR  0.870370  0.710526  0.773663  0.696296\n",
              "3        Logreg 0.15  TWTR  0.876404  0.642105  0.696429  0.577778\n",
              "4           SVM 0.15  TWTR  0.869159  0.705263  0.768595  0.688889\n",
              "5     LSTM beta 0.15  TWTR  0.878049  0.872611  0.915254  0.955752\n",
              "6      GRU beta 0.15  TWTR  0.915094  0.840764  0.885845  0.858407\n",
              "7  XGBoost beta 0.15  TWTR  0.829268  0.796178  0.864407  0.902655\n",
              "8   logreg beta 0.15  TWTR  0.881356  0.853503  0.900433  0.920354\n",
              "9      svm beta 0.15  TWTR  0.830645  0.802548  0.869198  0.911504"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNVLyy9fZu1E"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1410:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1410:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1410:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1410:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1410:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1443:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1443:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1443:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1443:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1443:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_logreg_beta.csv')"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imedSl2KZu1E"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyRtwuO2Zu1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fde3252e-1538-4318-ae2c-5df8f8591778"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1400, test_end=1600)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"TWTR\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 3s 17ms/step - loss: 0.6502 - accuracy: 0.6569 - val_loss: 0.7937 - val_accuracy: 0.1895\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6389 - accuracy: 0.6596 - val_loss: 0.9699 - val_accuracy: 0.1895\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6087 - accuracy: 0.6679 - val_loss: 0.7394 - val_accuracy: 0.5895\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5751 - accuracy: 0.6927 - val_loss: 0.8066 - val_accuracy: 0.4684\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.5642 - accuracy: 0.7174 - val_loss: 0.9575 - val_accuracy: 0.4158\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 2s 15ms/step - loss: 0.6334 - accuracy: 0.6633 - val_loss: 0.7936 - val_accuracy: 0.3105\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5589 - accuracy: 0.7101 - val_loss: 0.8960 - val_accuracy: 0.3632\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5152 - accuracy: 0.7450 - val_loss: 0.7147 - val_accuracy: 0.6211\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5069 - accuracy: 0.7468 - val_loss: 0.8108 - val_accuracy: 0.5789\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5059 - accuracy: 0.7339 - val_loss: 1.0659 - val_accuracy: 0.4579\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.747024\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.736923\n",
            "[2]\tvalidation_0-auc:0.770112\n",
            "[3]\tvalidation_0-auc:0.76921\n",
            "[4]\tvalidation_0-auc:0.769751\n",
            "[5]\tvalidation_0-auc:0.768849\n",
            "[6]\tvalidation_0-auc:0.772457\n",
            "[7]\tvalidation_0-auc:0.772637\n",
            "[8]\tvalidation_0-auc:0.771916\n",
            "[9]\tvalidation_0-auc:0.770833\n",
            "[10]\tvalidation_0-auc:0.771014\n",
            "[11]\tvalidation_0-auc:0.775703\n",
            "[12]\tvalidation_0-auc:0.780483\n",
            "[13]\tvalidation_0-auc:0.780483\n",
            "[14]\tvalidation_0-auc:0.781566\n",
            "[15]\tvalidation_0-auc:0.784091\n",
            "[16]\tvalidation_0-auc:0.78382\n",
            "[17]\tvalidation_0-auc:0.784001\n",
            "[18]\tvalidation_0-auc:0.784181\n",
            "[19]\tvalidation_0-auc:0.78373\n",
            "[20]\tvalidation_0-auc:0.783911\n",
            "[21]\tvalidation_0-auc:0.783189\n",
            "[22]\tvalidation_0-auc:0.787698\n",
            "[23]\tvalidation_0-auc:0.786255\n",
            "[24]\tvalidation_0-auc:0.787338\n",
            "[25]\tvalidation_0-auc:0.786797\n",
            "[26]\tvalidation_0-auc:0.790494\n",
            "[27]\tvalidation_0-auc:0.790675\n",
            "[28]\tvalidation_0-auc:0.790133\n",
            "[29]\tvalidation_0-auc:0.789953\n",
            "[30]\tvalidation_0-auc:0.790314\n",
            "[31]\tvalidation_0-auc:0.790675\n",
            "[32]\tvalidation_0-auc:0.791667\n",
            "[33]\tvalidation_0-auc:0.791667\n",
            "[34]\tvalidation_0-auc:0.791486\n",
            "[35]\tvalidation_0-auc:0.788961\n",
            "[36]\tvalidation_0-auc:0.788961\n",
            "[37]\tvalidation_0-auc:0.788961\n",
            "[38]\tvalidation_0-auc:0.788961\n",
            "[39]\tvalidation_0-auc:0.785985\n",
            "[40]\tvalidation_0-auc:0.785624\n",
            "[41]\tvalidation_0-auc:0.785985\n",
            "[42]\tvalidation_0-auc:0.785444\n",
            "[43]\tvalidation_0-auc:0.786165\n",
            "[44]\tvalidation_0-auc:0.784903\n",
            "[45]\tvalidation_0-auc:0.785083\n",
            "[46]\tvalidation_0-auc:0.785083\n",
            "[47]\tvalidation_0-auc:0.785804\n",
            "[48]\tvalidation_0-auc:0.784361\n",
            "[49]\tvalidation_0-auc:0.783911\n",
            "[50]\tvalidation_0-auc:0.783189\n",
            "[51]\tvalidation_0-auc:0.784001\n",
            "[52]\tvalidation_0-auc:0.784181\n",
            "[53]\tvalidation_0-auc:0.781385\n",
            "[54]\tvalidation_0-auc:0.781025\n",
            "[55]\tvalidation_0-auc:0.782648\n",
            "[56]\tvalidation_0-auc:0.783009\n",
            "[57]\tvalidation_0-auc:0.782828\n",
            "[58]\tvalidation_0-auc:0.783189\n",
            "[59]\tvalidation_0-auc:0.784993\n",
            "[60]\tvalidation_0-auc:0.782287\n",
            "[61]\tvalidation_0-auc:0.781566\n",
            "[62]\tvalidation_0-auc:0.781746\n",
            "[63]\tvalidation_0-auc:0.783009\n",
            "[64]\tvalidation_0-auc:0.78373\n",
            "[65]\tvalidation_0-auc:0.784091\n",
            "[66]\tvalidation_0-auc:0.782287\n",
            "[67]\tvalidation_0-auc:0.781205\n",
            "[68]\tvalidation_0-auc:0.781746\n",
            "[69]\tvalidation_0-auc:0.780664\n",
            "[70]\tvalidation_0-auc:0.781205\n",
            "[71]\tvalidation_0-auc:0.779762\n",
            "[72]\tvalidation_0-auc:0.77886\n",
            "[73]\tvalidation_0-auc:0.77904\n",
            "[74]\tvalidation_0-auc:0.779582\n",
            "[75]\tvalidation_0-auc:0.780303\n",
            "[76]\tvalidation_0-auc:0.77904\n",
            "[77]\tvalidation_0-auc:0.779582\n",
            "[78]\tvalidation_0-auc:0.779401\n",
            "[79]\tvalidation_0-auc:0.77868\n",
            "[80]\tvalidation_0-auc:0.777958\n",
            "[81]\tvalidation_0-auc:0.779401\n",
            "[82]\tvalidation_0-auc:0.781385\n",
            "Stopping. Best iteration:\n",
            "[32]\tvalidation_0-auc:0.791667\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 3s 17ms/step - loss: 0.6521 - accuracy: 0.6594 - val_loss: 0.9859 - val_accuracy: 0.1592\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.6451 - accuracy: 0.6632 - val_loss: 0.9293 - val_accuracy: 0.1592\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.6340 - accuracy: 0.6613 - val_loss: 0.8992 - val_accuracy: 0.1592\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.6209 - accuracy: 0.6689 - val_loss: 0.9445 - val_accuracy: 0.1911\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5669 - accuracy: 0.7190 - val_loss: 0.7739 - val_accuracy: 0.4841\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 2s 15ms/step - loss: 0.6469 - accuracy: 0.6613 - val_loss: 0.9362 - val_accuracy: 0.1592\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.6041 - accuracy: 0.6802 - val_loss: 0.7555 - val_accuracy: 0.4777\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5592 - accuracy: 0.7162 - val_loss: 0.7417 - val_accuracy: 0.5414\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.5528 - accuracy: 0.7105 - val_loss: 0.6568 - val_accuracy: 0.5924\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5479 - accuracy: 0.7351 - val_loss: 0.6204 - val_accuracy: 0.6497\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.707576\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.764545\n",
            "[2]\tvalidation_0-auc:0.776364\n",
            "[3]\tvalidation_0-auc:0.777273\n",
            "[4]\tvalidation_0-auc:0.782879\n",
            "[5]\tvalidation_0-auc:0.784849\n",
            "[6]\tvalidation_0-auc:0.796212\n",
            "[7]\tvalidation_0-auc:0.790152\n",
            "[8]\tvalidation_0-auc:0.785\n",
            "[9]\tvalidation_0-auc:0.793333\n",
            "[10]\tvalidation_0-auc:0.795152\n",
            "[11]\tvalidation_0-auc:0.790758\n",
            "[12]\tvalidation_0-auc:0.795303\n",
            "[13]\tvalidation_0-auc:0.796364\n",
            "[14]\tvalidation_0-auc:0.793333\n",
            "[15]\tvalidation_0-auc:0.796212\n",
            "[16]\tvalidation_0-auc:0.793788\n",
            "[17]\tvalidation_0-auc:0.795606\n",
            "[18]\tvalidation_0-auc:0.799697\n",
            "[19]\tvalidation_0-auc:0.801061\n",
            "[20]\tvalidation_0-auc:0.803788\n",
            "[21]\tvalidation_0-auc:0.80197\n",
            "[22]\tvalidation_0-auc:0.799545\n",
            "[23]\tvalidation_0-auc:0.803788\n",
            "[24]\tvalidation_0-auc:0.801061\n",
            "[25]\tvalidation_0-auc:0.800909\n",
            "[26]\tvalidation_0-auc:0.805152\n",
            "[27]\tvalidation_0-auc:0.804848\n",
            "[28]\tvalidation_0-auc:0.804242\n",
            "[29]\tvalidation_0-auc:0.804242\n",
            "[30]\tvalidation_0-auc:0.803636\n",
            "[31]\tvalidation_0-auc:0.804697\n",
            "[32]\tvalidation_0-auc:0.805909\n",
            "[33]\tvalidation_0-auc:0.804091\n",
            "[34]\tvalidation_0-auc:0.804394\n",
            "[35]\tvalidation_0-auc:0.805606\n",
            "[36]\tvalidation_0-auc:0.805909\n",
            "[37]\tvalidation_0-auc:0.808636\n",
            "[38]\tvalidation_0-auc:0.805909\n",
            "[39]\tvalidation_0-auc:0.805303\n",
            "[40]\tvalidation_0-auc:0.806212\n",
            "[41]\tvalidation_0-auc:0.805606\n",
            "[42]\tvalidation_0-auc:0.805\n",
            "[43]\tvalidation_0-auc:0.806818\n",
            "[44]\tvalidation_0-auc:0.80197\n",
            "[45]\tvalidation_0-auc:0.802879\n",
            "[46]\tvalidation_0-auc:0.803182\n",
            "[47]\tvalidation_0-auc:0.803788\n",
            "[48]\tvalidation_0-auc:0.803182\n",
            "[49]\tvalidation_0-auc:0.803182\n",
            "[50]\tvalidation_0-auc:0.802576\n",
            "[51]\tvalidation_0-auc:0.806515\n",
            "[52]\tvalidation_0-auc:0.807727\n",
            "[53]\tvalidation_0-auc:0.807424\n",
            "[54]\tvalidation_0-auc:0.809697\n",
            "[55]\tvalidation_0-auc:0.809697\n",
            "[56]\tvalidation_0-auc:0.808485\n",
            "[57]\tvalidation_0-auc:0.80697\n",
            "[58]\tvalidation_0-auc:0.808788\n",
            "[59]\tvalidation_0-auc:0.809697\n",
            "[60]\tvalidation_0-auc:0.809091\n",
            "[61]\tvalidation_0-auc:0.804242\n",
            "[62]\tvalidation_0-auc:0.805758\n",
            "[63]\tvalidation_0-auc:0.806364\n",
            "[64]\tvalidation_0-auc:0.807273\n",
            "[65]\tvalidation_0-auc:0.807879\n",
            "[66]\tvalidation_0-auc:0.80697\n",
            "[67]\tvalidation_0-auc:0.807273\n",
            "[68]\tvalidation_0-auc:0.805758\n",
            "[69]\tvalidation_0-auc:0.803939\n",
            "[70]\tvalidation_0-auc:0.803939\n",
            "[71]\tvalidation_0-auc:0.804848\n",
            "[72]\tvalidation_0-auc:0.804848\n",
            "[73]\tvalidation_0-auc:0.806061\n",
            "[74]\tvalidation_0-auc:0.804848\n",
            "[75]\tvalidation_0-auc:0.808788\n",
            "[76]\tvalidation_0-auc:0.808788\n",
            "[77]\tvalidation_0-auc:0.808485\n",
            "[78]\tvalidation_0-auc:0.809697\n",
            "[79]\tvalidation_0-auc:0.806667\n",
            "[80]\tvalidation_0-auc:0.808788\n",
            "[81]\tvalidation_0-auc:0.803939\n",
            "[82]\tvalidation_0-auc:0.806667\n",
            "[83]\tvalidation_0-auc:0.806667\n",
            "[84]\tvalidation_0-auc:0.807879\n",
            "[85]\tvalidation_0-auc:0.806364\n",
            "[86]\tvalidation_0-auc:0.802121\n",
            "[87]\tvalidation_0-auc:0.803939\n",
            "[88]\tvalidation_0-auc:0.806364\n",
            "[89]\tvalidation_0-auc:0.808788\n",
            "[90]\tvalidation_0-auc:0.807879\n",
            "[91]\tvalidation_0-auc:0.807879\n",
            "[92]\tvalidation_0-auc:0.809394\n",
            "[93]\tvalidation_0-auc:0.808485\n",
            "[94]\tvalidation_0-auc:0.807576\n",
            "[95]\tvalidation_0-auc:0.807273\n",
            "[96]\tvalidation_0-auc:0.807273\n",
            "[97]\tvalidation_0-auc:0.806364\n",
            "[98]\tvalidation_0-auc:0.804848\n",
            "[99]\tvalidation_0-auc:0.805758\n",
            "end training. \n",
            "\n",
            "+------------------+---------------------+--------------------+---------------------+--------------------+\n",
            "|      Model       |       Accuracy      |     Precision      |        Recall       |      F1 score      |\n",
            "+------------------+---------------------+--------------------+---------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.41578947368421054 | 0.9777777777777777 |  0.2857142857142857 | 0.4422110552763818 |\n",
            "|     GRU 0.1      | 0.45789473684210524 | 0.9811320754716981 | 0.33766233766233766 | 0.5024154589371981 |\n",
            "|   XGBoost 0.1    |  0.631578947368421  |        0.92        |  0.5974025974025974 | 0.7244094488188978 |\n",
            "|    Logreg 0.1    | 0.47368421052631576 |        0.95        | 0.37012987012987014 | 0.5327102803738318 |\n",
            "|     SVM 0.1      |         0.6         | 0.9333333333333333 |  0.5454545454545454 | 0.6885245901639344 |\n",
            "|  LSTM beta 0.1   |  0.4840764331210191 | 0.9047619047619048 |  0.4318181818181818 | 0.5846153846153846 |\n",
            "|   GRU beta 0.1   |  0.6496815286624203 | 0.9325842696629213 |  0.6287878787878788 | 0.7511312217194569 |\n",
            "| XGBoost beta 0.1 |  0.6305732484076433 | 0.9404761904761905 |  0.5984848484848485 | 0.7314814814814815 |\n",
            "| logreg beta 0.1  |  0.535031847133758  | 0.9402985074626866 |  0.4772727272727273 | 0.6331658291457287 |\n",
            "|   svm beta 0.1   |  0.6242038216560509 | 0.9620253164556962 |  0.5757575757575758 | 0.7203791469194313 |\n",
            "+------------------+---------------------+--------------------+---------------------+--------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 3s 17ms/step - loss: 0.6501 - accuracy: 0.6651 - val_loss: 0.8603 - val_accuracy: 0.2895\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6323 - accuracy: 0.6725 - val_loss: 0.7920 - val_accuracy: 0.2895\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6030 - accuracy: 0.6917 - val_loss: 0.7186 - val_accuracy: 0.4579\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.5997 - accuracy: 0.6752 - val_loss: 0.7937 - val_accuracy: 0.4947\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.5778 - accuracy: 0.6881 - val_loss: 0.8070 - val_accuracy: 0.4789\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 2s 15ms/step - loss: 0.6447 - accuracy: 0.6633 - val_loss: 0.8308 - val_accuracy: 0.2895\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 0.5929 - accuracy: 0.6936 - val_loss: 0.7956 - val_accuracy: 0.4737\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5818 - accuracy: 0.7028 - val_loss: 0.5981 - val_accuracy: 0.7105\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5671 - accuracy: 0.7009 - val_loss: 0.6607 - val_accuracy: 0.5947\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5506 - accuracy: 0.7174 - val_loss: 0.7357 - val_accuracy: 0.5368\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.737912\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.740067\n",
            "[2]\tvalidation_0-auc:0.742492\n",
            "[3]\tvalidation_0-auc:0.741481\n",
            "[4]\tvalidation_0-auc:0.747475\n",
            "[5]\tvalidation_0-auc:0.747879\n",
            "[6]\tvalidation_0-auc:0.747609\n",
            "[7]\tvalidation_0-auc:0.736364\n",
            "[8]\tvalidation_0-auc:0.752593\n",
            "[9]\tvalidation_0-auc:0.747071\n",
            "[10]\tvalidation_0-auc:0.754478\n",
            "[11]\tvalidation_0-auc:0.755556\n",
            "[12]\tvalidation_0-auc:0.759057\n",
            "[13]\tvalidation_0-auc:0.760404\n",
            "[14]\tvalidation_0-auc:0.76\n",
            "[15]\tvalidation_0-auc:0.76229\n",
            "[16]\tvalidation_0-auc:0.763906\n",
            "[17]\tvalidation_0-auc:0.775892\n",
            "[18]\tvalidation_0-auc:0.775421\n",
            "[19]\tvalidation_0-auc:0.775017\n",
            "[20]\tvalidation_0-auc:0.778114\n",
            "[21]\tvalidation_0-auc:0.779394\n",
            "[22]\tvalidation_0-auc:0.777374\n",
            "[23]\tvalidation_0-auc:0.778316\n",
            "[24]\tvalidation_0-auc:0.777778\n",
            "[25]\tvalidation_0-auc:0.776768\n",
            "[26]\tvalidation_0-auc:0.777037\n",
            "[27]\tvalidation_0-auc:0.778114\n",
            "[28]\tvalidation_0-auc:0.778721\n",
            "[29]\tvalidation_0-auc:0.783906\n",
            "[30]\tvalidation_0-auc:0.782694\n",
            "[31]\tvalidation_0-auc:0.780471\n",
            "[32]\tvalidation_0-auc:0.782155\n",
            "[33]\tvalidation_0-auc:0.784983\n",
            "[34]\tvalidation_0-auc:0.785455\n",
            "[35]\tvalidation_0-auc:0.78963\n",
            "[36]\tvalidation_0-auc:0.790572\n",
            "[37]\tvalidation_0-auc:0.79165\n",
            "[38]\tvalidation_0-auc:0.792054\n",
            "[39]\tvalidation_0-auc:0.791111\n",
            "[40]\tvalidation_0-auc:0.793064\n",
            "[41]\tvalidation_0-auc:0.793064\n",
            "[42]\tvalidation_0-auc:0.793333\n",
            "[43]\tvalidation_0-auc:0.794007\n",
            "[44]\tvalidation_0-auc:0.793872\n",
            "[45]\tvalidation_0-auc:0.793333\n",
            "[46]\tvalidation_0-auc:0.794949\n",
            "[47]\tvalidation_0-auc:0.797104\n",
            "[48]\tvalidation_0-auc:0.795421\n",
            "[49]\tvalidation_0-auc:0.797172\n",
            "[50]\tvalidation_0-auc:0.798114\n",
            "[51]\tvalidation_0-auc:0.800404\n",
            "[52]\tvalidation_0-auc:0.799192\n",
            "[53]\tvalidation_0-auc:0.800135\n",
            "[54]\tvalidation_0-auc:0.803232\n",
            "[55]\tvalidation_0-auc:0.804175\n",
            "[56]\tvalidation_0-auc:0.80404\n",
            "[57]\tvalidation_0-auc:0.802424\n",
            "[58]\tvalidation_0-auc:0.80404\n",
            "[59]\tvalidation_0-auc:0.802963\n",
            "[60]\tvalidation_0-auc:0.802963\n",
            "[61]\tvalidation_0-auc:0.802424\n",
            "[62]\tvalidation_0-auc:0.802963\n",
            "[63]\tvalidation_0-auc:0.804983\n",
            "[64]\tvalidation_0-auc:0.809293\n",
            "[65]\tvalidation_0-auc:0.810101\n",
            "[66]\tvalidation_0-auc:0.808754\n",
            "[67]\tvalidation_0-auc:0.809158\n",
            "[68]\tvalidation_0-auc:0.808889\n",
            "[69]\tvalidation_0-auc:0.809024\n",
            "[70]\tvalidation_0-auc:0.808619\n",
            "[71]\tvalidation_0-auc:0.809158\n",
            "[72]\tvalidation_0-auc:0.811717\n",
            "[73]\tvalidation_0-auc:0.812525\n",
            "[74]\tvalidation_0-auc:0.812795\n",
            "[75]\tvalidation_0-auc:0.815354\n",
            "[76]\tvalidation_0-auc:0.815488\n",
            "[77]\tvalidation_0-auc:0.813737\n",
            "[78]\tvalidation_0-auc:0.810505\n",
            "[79]\tvalidation_0-auc:0.810236\n",
            "[80]\tvalidation_0-auc:0.808619\n",
            "[81]\tvalidation_0-auc:0.809966\n",
            "[82]\tvalidation_0-auc:0.806734\n",
            "[83]\tvalidation_0-auc:0.805926\n",
            "[84]\tvalidation_0-auc:0.805657\n",
            "[85]\tvalidation_0-auc:0.804175\n",
            "[86]\tvalidation_0-auc:0.804579\n",
            "[87]\tvalidation_0-auc:0.803906\n",
            "[88]\tvalidation_0-auc:0.799192\n",
            "[89]\tvalidation_0-auc:0.798384\n",
            "[90]\tvalidation_0-auc:0.801077\n",
            "[91]\tvalidation_0-auc:0.803367\n",
            "[92]\tvalidation_0-auc:0.80431\n",
            "[93]\tvalidation_0-auc:0.805522\n",
            "[94]\tvalidation_0-auc:0.806195\n",
            "[95]\tvalidation_0-auc:0.806195\n",
            "[96]\tvalidation_0-auc:0.807542\n",
            "[97]\tvalidation_0-auc:0.807946\n",
            "[98]\tvalidation_0-auc:0.807407\n",
            "[99]\tvalidation_0-auc:0.806061\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 3s 17ms/step - loss: 0.6486 - accuracy: 0.6660 - val_loss: 0.8175 - val_accuracy: 0.2803\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.6116 - accuracy: 0.6831 - val_loss: 0.6231 - val_accuracy: 0.6879\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5768 - accuracy: 0.7029 - val_loss: 0.5793 - val_accuracy: 0.7197\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5493 - accuracy: 0.7427 - val_loss: 0.5937 - val_accuracy: 0.6688\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5388 - accuracy: 0.7550 - val_loss: 0.5258 - val_accuracy: 0.7197\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 2s 15ms/step - loss: 0.6383 - accuracy: 0.6764 - val_loss: 0.7698 - val_accuracy: 0.2866\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5495 - accuracy: 0.7342 - val_loss: 0.5100 - val_accuracy: 0.7134\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5304 - accuracy: 0.7502 - val_loss: 0.5020 - val_accuracy: 0.8025\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5107 - accuracy: 0.7465 - val_loss: 0.5517 - val_accuracy: 0.6943\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5196 - accuracy: 0.7502 - val_loss: 0.4712 - val_accuracy: 0.7962\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.771118\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.801388\n",
            "[2]\tvalidation_0-auc:0.813053\n",
            "[3]\tvalidation_0-auc:0.808427\n",
            "[4]\tvalidation_0-auc:0.799879\n",
            "[5]\tvalidation_0-auc:0.800583\n",
            "[6]\tvalidation_0-auc:0.801488\n",
            "[7]\tvalidation_0-auc:0.798572\n",
            "[8]\tvalidation_0-auc:0.786102\n",
            "[9]\tvalidation_0-auc:0.780068\n",
            "[10]\tvalidation_0-auc:0.763978\n",
            "[11]\tvalidation_0-auc:0.7678\n",
            "[12]\tvalidation_0-auc:0.760559\n",
            "[13]\tvalidation_0-auc:0.754525\n",
            "[14]\tvalidation_0-auc:0.754123\n",
            "[15]\tvalidation_0-auc:0.746078\n",
            "[16]\tvalidation_0-auc:0.739843\n",
            "[17]\tvalidation_0-auc:0.740648\n",
            "[18]\tvalidation_0-auc:0.740547\n",
            "[19]\tvalidation_0-auc:0.73934\n",
            "[20]\tvalidation_0-auc:0.736726\n",
            "[21]\tvalidation_0-auc:0.73572\n",
            "[22]\tvalidation_0-auc:0.734714\n",
            "[23]\tvalidation_0-auc:0.733709\n",
            "[24]\tvalidation_0-auc:0.73029\n",
            "[25]\tvalidation_0-auc:0.729284\n",
            "[26]\tvalidation_0-auc:0.730893\n",
            "[27]\tvalidation_0-auc:0.726368\n",
            "[28]\tvalidation_0-auc:0.717317\n",
            "[29]\tvalidation_0-auc:0.7143\n",
            "[30]\tvalidation_0-auc:0.715708\n",
            "[31]\tvalidation_0-auc:0.715507\n",
            "[32]\tvalidation_0-auc:0.711685\n",
            "[33]\tvalidation_0-auc:0.710076\n",
            "[34]\tvalidation_0-auc:0.711685\n",
            "[35]\tvalidation_0-auc:0.711887\n",
            "[36]\tvalidation_0-auc:0.710881\n",
            "[37]\tvalidation_0-auc:0.706255\n",
            "[38]\tvalidation_0-auc:0.701428\n",
            "[39]\tvalidation_0-auc:0.701227\n",
            "[40]\tvalidation_0-auc:0.697204\n",
            "[41]\tvalidation_0-auc:0.697808\n",
            "[42]\tvalidation_0-auc:0.700221\n",
            "[43]\tvalidation_0-auc:0.694791\n",
            "[44]\tvalidation_0-auc:0.693986\n",
            "[45]\tvalidation_0-auc:0.693383\n",
            "[46]\tvalidation_0-auc:0.69288\n",
            "[47]\tvalidation_0-auc:0.69288\n",
            "[48]\tvalidation_0-auc:0.690064\n",
            "[49]\tvalidation_0-auc:0.68564\n",
            "[50]\tvalidation_0-auc:0.682723\n",
            "[51]\tvalidation_0-auc:0.682723\n",
            "[52]\tvalidation_0-auc:0.686344\n",
            "Stopping. Best iteration:\n",
            "[2]\tvalidation_0-auc:0.813053\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+---------------------+--------------------+--------------------+---------------------+\n",
            "|      Model       |       Accuracy      |     Precision      |       Recall       |       F1 score      |\n",
            "+------------------+---------------------+--------------------+--------------------+---------------------+\n",
            "|     LSTM 0.2     |  0.4789473684210526 |        0.95        | 0.2814814814814815 | 0.43428571428571433 |\n",
            "|     GRU 0.2      |  0.5368421052631579 | 0.9607843137254902 | 0.362962962962963  |  0.5268817204301075 |\n",
            "|   XGBoost 0.2    |         0.6         | 0.927536231884058  | 0.4740740740740741 |  0.627450980392157  |\n",
            "|    Logreg 0.2    |  0.5842105263157895 |       0.9375       | 0.4444444444444444 |  0.6030150753768844 |\n",
            "|     SVM 0.2      | 0.49473684210526314 | 0.975609756097561  | 0.2962962962962963 | 0.45454545454545453 |\n",
            "|  LSTM beta 0.2   |  0.7197452229299363 | 0.8556701030927835 | 0.7345132743362832 |  0.7904761904761903 |\n",
            "|   GRU beta 0.2   |  0.7961783439490446 | 0.8932038834951457 | 0.8141592920353983 |  0.851851851851852  |\n",
            "| XGBoost beta 0.2 |  0.7261146496815286 | 0.8571428571428571 | 0.7433628318584071 |  0.7962085308056872 |\n",
            "| logreg beta 0.2  |  0.7452229299363057 | 0.8924731182795699 | 0.7345132743362832 |  0.8058252427184466 |\n",
            "|   svm beta 0.2   |  0.7579617834394905 | 0.8440366972477065 | 0.8141592920353983 |  0.8288288288288288 |\n",
            "+------------------+---------------------+--------------------+--------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 3s 17ms/step - loss: 0.6688 - accuracy: 0.6046 - val_loss: 0.8539 - val_accuracy: 0.2895\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6126 - accuracy: 0.6716 - val_loss: 0.6189 - val_accuracy: 0.7526\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6086 - accuracy: 0.6835 - val_loss: 0.6708 - val_accuracy: 0.6632\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.6024 - accuracy: 0.6780 - val_loss: 0.6730 - val_accuracy: 0.6842\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 0.5952 - accuracy: 0.6697 - val_loss: 0.7991 - val_accuracy: 0.6053\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 3s 15ms/step - loss: 0.6626 - accuracy: 0.6092 - val_loss: 0.6534 - val_accuracy: 0.6158\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5948 - accuracy: 0.6807 - val_loss: 0.6003 - val_accuracy: 0.6947\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5753 - accuracy: 0.6936 - val_loss: 0.6765 - val_accuracy: 0.5737\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5589 - accuracy: 0.7083 - val_loss: 0.6204 - val_accuracy: 0.6737\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 1s 10ms/step - loss: 0.5647 - accuracy: 0.6963 - val_loss: 0.5639 - val_accuracy: 0.7368\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.773266\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.763502\n",
            "[2]\tvalidation_0-auc:0.76633\n",
            "[3]\tvalidation_0-auc:0.767205\n",
            "[4]\tvalidation_0-auc:0.768552\n",
            "[5]\tvalidation_0-auc:0.764781\n",
            "[6]\tvalidation_0-auc:0.76404\n",
            "[7]\tvalidation_0-auc:0.763569\n",
            "[8]\tvalidation_0-auc:0.766061\n",
            "[9]\tvalidation_0-auc:0.765791\n",
            "[10]\tvalidation_0-auc:0.767273\n",
            "[11]\tvalidation_0-auc:0.765791\n",
            "[12]\tvalidation_0-auc:0.766195\n",
            "[13]\tvalidation_0-auc:0.765455\n",
            "[14]\tvalidation_0-auc:0.764175\n",
            "[15]\tvalidation_0-auc:0.76404\n",
            "[16]\tvalidation_0-auc:0.764983\n",
            "[17]\tvalidation_0-auc:0.767407\n",
            "[18]\tvalidation_0-auc:0.767071\n",
            "[19]\tvalidation_0-auc:0.766128\n",
            "[20]\tvalidation_0-auc:0.765859\n",
            "[21]\tvalidation_0-auc:0.765926\n",
            "[22]\tvalidation_0-auc:0.765522\n",
            "[23]\tvalidation_0-auc:0.76431\n",
            "[24]\tvalidation_0-auc:0.767946\n",
            "[25]\tvalidation_0-auc:0.770236\n",
            "[26]\tvalidation_0-auc:0.77037\n",
            "[27]\tvalidation_0-auc:0.770505\n",
            "[28]\tvalidation_0-auc:0.770438\n",
            "[29]\tvalidation_0-auc:0.770438\n",
            "[30]\tvalidation_0-auc:0.769428\n",
            "[31]\tvalidation_0-auc:0.769562\n",
            "[32]\tvalidation_0-auc:0.768889\n",
            "[33]\tvalidation_0-auc:0.767946\n",
            "[34]\tvalidation_0-auc:0.767542\n",
            "[35]\tvalidation_0-auc:0.766734\n",
            "[36]\tvalidation_0-auc:0.764983\n",
            "[37]\tvalidation_0-auc:0.766667\n",
            "[38]\tvalidation_0-auc:0.768081\n",
            "[39]\tvalidation_0-auc:0.767946\n",
            "[40]\tvalidation_0-auc:0.76835\n",
            "[41]\tvalidation_0-auc:0.769158\n",
            "[42]\tvalidation_0-auc:0.767677\n",
            "[43]\tvalidation_0-auc:0.767542\n",
            "[44]\tvalidation_0-auc:0.767677\n",
            "[45]\tvalidation_0-auc:0.767677\n",
            "[46]\tvalidation_0-auc:0.766599\n",
            "[47]\tvalidation_0-auc:0.766599\n",
            "[48]\tvalidation_0-auc:0.764444\n",
            "[49]\tvalidation_0-auc:0.772795\n",
            "[50]\tvalidation_0-auc:0.771717\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.773266\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 3s 17ms/step - loss: 0.6621 - accuracy: 0.6112 - val_loss: 0.6617 - val_accuracy: 0.7325\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.6361 - accuracy: 0.6490 - val_loss: 0.5589 - val_accuracy: 0.7197\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 12ms/step - loss: 0.6287 - accuracy: 0.6424 - val_loss: 0.6078 - val_accuracy: 0.7134\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 12ms/step - loss: 0.6051 - accuracy: 0.6575 - val_loss: 0.5682 - val_accuracy: 0.7962\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.5962 - accuracy: 0.6708 - val_loss: 0.6745 - val_accuracy: 0.6178\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 2s 16ms/step - loss: 0.6581 - accuracy: 0.6206 - val_loss: 0.6400 - val_accuracy: 0.6306\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5831 - accuracy: 0.6897 - val_loss: 0.4488 - val_accuracy: 0.8662\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5465 - accuracy: 0.7181 - val_loss: 0.4820 - val_accuracy: 0.8471\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5366 - accuracy: 0.7275 - val_loss: 0.4380 - val_accuracy: 0.8217\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.5256 - accuracy: 0.7294 - val_loss: 0.4415 - val_accuracy: 0.8471\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.761263\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.756235\n",
            "[2]\tvalidation_0-auc:0.757039\n",
            "[3]\tvalidation_0-auc:0.761062\n",
            "[4]\tvalidation_0-auc:0.76066\n",
            "[5]\tvalidation_0-auc:0.760056\n",
            "[6]\tvalidation_0-auc:0.739139\n",
            "[7]\tvalidation_0-auc:0.73924\n",
            "[8]\tvalidation_0-auc:0.739441\n",
            "[9]\tvalidation_0-auc:0.725463\n",
            "[10]\tvalidation_0-auc:0.724658\n",
            "[11]\tvalidation_0-auc:0.723954\n",
            "[12]\tvalidation_0-auc:0.712591\n",
            "[13]\tvalidation_0-auc:0.709875\n",
            "[14]\tvalidation_0-auc:0.709574\n",
            "[15]\tvalidation_0-auc:0.719027\n",
            "[16]\tvalidation_0-auc:0.714099\n",
            "[17]\tvalidation_0-auc:0.713898\n",
            "[18]\tvalidation_0-auc:0.699517\n",
            "[19]\tvalidation_0-auc:0.702534\n",
            "[20]\tvalidation_0-auc:0.699517\n",
            "[21]\tvalidation_0-auc:0.700925\n",
            "[22]\tvalidation_0-auc:0.697104\n",
            "[23]\tvalidation_0-auc:0.67317\n",
            "[24]\tvalidation_0-auc:0.660499\n",
            "[25]\tvalidation_0-auc:0.659091\n",
            "[26]\tvalidation_0-auc:0.659393\n",
            "[27]\tvalidation_0-auc:0.652957\n",
            "[28]\tvalidation_0-auc:0.647124\n",
            "[29]\tvalidation_0-auc:0.643303\n",
            "[30]\tvalidation_0-auc:0.647124\n",
            "[31]\tvalidation_0-auc:0.632643\n",
            "[32]\tvalidation_0-auc:0.63043\n",
            "[33]\tvalidation_0-auc:0.629224\n",
            "[34]\tvalidation_0-auc:0.630632\n",
            "[35]\tvalidation_0-auc:0.627816\n",
            "[36]\tvalidation_0-auc:0.629023\n",
            "[37]\tvalidation_0-auc:0.628218\n",
            "[38]\tvalidation_0-auc:0.634453\n",
            "[39]\tvalidation_0-auc:0.63566\n",
            "[40]\tvalidation_0-auc:0.635257\n",
            "[41]\tvalidation_0-auc:0.634252\n",
            "[42]\tvalidation_0-auc:0.636263\n",
            "[43]\tvalidation_0-auc:0.63566\n",
            "[44]\tvalidation_0-auc:0.634654\n",
            "[45]\tvalidation_0-auc:0.633447\n",
            "[46]\tvalidation_0-auc:0.625201\n",
            "[47]\tvalidation_0-auc:0.624799\n",
            "[48]\tvalidation_0-auc:0.629224\n",
            "[49]\tvalidation_0-auc:0.631637\n",
            "[50]\tvalidation_0-auc:0.630028\n",
            "Stopping. Best iteration:\n",
            "[0]\tvalidation_0-auc:0.761263\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.15     | 0.6052631578947368 | 0.8947368421052632 | 0.5037037037037037 | 0.6445497630331753 |\n",
            "|      GRU 0.15     | 0.7368421052631579 |        0.84        | 0.7777777777777778 | 0.8076923076923077 |\n",
            "|    XGBoost 0.15   | 0.7105263157894737 | 0.8703703703703703 | 0.6962962962962963 | 0.7736625514403292 |\n",
            "|    Logreg 0.15    | 0.6421052631578947 | 0.8764044943820225 | 0.5777777777777777 | 0.6964285714285714 |\n",
            "|      SVM 0.15     | 0.7052631578947368 | 0.8691588785046729 | 0.6888888888888889 | 0.768595041322314  |\n",
            "|   LSTM beta 0.15  | 0.6178343949044586 | 0.8955223880597015 | 0.5309734513274337 | 0.6666666666666667 |\n",
            "|   GRU beta 0.15   | 0.8471337579617835 | 0.8617886178861789 | 0.9380530973451328 | 0.8983050847457628 |\n",
            "| XGBoost beta 0.15 | 0.7961783439490446 | 0.8292682926829268 | 0.9026548672566371 | 0.8644067796610169 |\n",
            "|  logreg beta 0.15 | 0.8535031847133758 | 0.8813559322033898 | 0.9203539823008849 | 0.9004329004329005 |\n",
            "|   svm beta 0.15   | 0.802547770700637  | 0.8306451612903226 | 0.911504424778761  | 0.8691983122362869 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh3w1JW7Zu1E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f150f6a5-1a94-4698-8397-b9d8e75a2bee"
      },
      "source": [
        "Result_purging.to_csv('TWTR_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.977778</td>\n",
              "      <td>0.415789</td>\n",
              "      <td>0.442211</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.981132</td>\n",
              "      <td>0.457895</td>\n",
              "      <td>0.502415</td>\n",
              "      <td>0.337662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.631579</td>\n",
              "      <td>0.724409</td>\n",
              "      <td>0.597403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.473684</td>\n",
              "      <td>0.532710</td>\n",
              "      <td>0.370130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.688525</td>\n",
              "      <td>0.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.904762</td>\n",
              "      <td>0.484076</td>\n",
              "      <td>0.584615</td>\n",
              "      <td>0.431818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.932584</td>\n",
              "      <td>0.649682</td>\n",
              "      <td>0.751131</td>\n",
              "      <td>0.628788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.940476</td>\n",
              "      <td>0.630573</td>\n",
              "      <td>0.731481</td>\n",
              "      <td>0.598485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.940299</td>\n",
              "      <td>0.535032</td>\n",
              "      <td>0.633166</td>\n",
              "      <td>0.477273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.962025</td>\n",
              "      <td>0.624204</td>\n",
              "      <td>0.720379</td>\n",
              "      <td>0.575758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.478947</td>\n",
              "      <td>0.434286</td>\n",
              "      <td>0.281481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.960784</td>\n",
              "      <td>0.536842</td>\n",
              "      <td>0.526882</td>\n",
              "      <td>0.362963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.927536</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.627451</td>\n",
              "      <td>0.474074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.584211</td>\n",
              "      <td>0.603015</td>\n",
              "      <td>0.444444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.975610</td>\n",
              "      <td>0.494737</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.296296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.855670</td>\n",
              "      <td>0.719745</td>\n",
              "      <td>0.790476</td>\n",
              "      <td>0.734513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.893204</td>\n",
              "      <td>0.796178</td>\n",
              "      <td>0.851852</td>\n",
              "      <td>0.814159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.726115</td>\n",
              "      <td>0.796209</td>\n",
              "      <td>0.743363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.892473</td>\n",
              "      <td>0.745223</td>\n",
              "      <td>0.805825</td>\n",
              "      <td>0.734513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.844037</td>\n",
              "      <td>0.757962</td>\n",
              "      <td>0.828829</td>\n",
              "      <td>0.814159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.894737</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>0.644550</td>\n",
              "      <td>0.503704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.840000</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.807692</td>\n",
              "      <td>0.777778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.870370</td>\n",
              "      <td>0.710526</td>\n",
              "      <td>0.773663</td>\n",
              "      <td>0.696296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.876404</td>\n",
              "      <td>0.642105</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.577778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.869159</td>\n",
              "      <td>0.705263</td>\n",
              "      <td>0.768595</td>\n",
              "      <td>0.688889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.895522</td>\n",
              "      <td>0.617834</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.530973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.861789</td>\n",
              "      <td>0.847134</td>\n",
              "      <td>0.898305</td>\n",
              "      <td>0.938053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.829268</td>\n",
              "      <td>0.796178</td>\n",
              "      <td>0.864407</td>\n",
              "      <td>0.902655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.881356</td>\n",
              "      <td>0.853503</td>\n",
              "      <td>0.900433</td>\n",
              "      <td>0.920354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>TWTR</td>\n",
              "      <td>0.830645</td>\n",
              "      <td>0.802548</td>\n",
              "      <td>0.869198</td>\n",
              "      <td>0.911504</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model  Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1  TWTR  0.977778  0.415789  0.442211  0.285714\n",
              "1            GRU 0.1  TWTR  0.981132  0.457895  0.502415  0.337662\n",
              "2        XGBoost 0.1  TWTR  0.920000  0.631579  0.724409  0.597403\n",
              "3         Logreg 0.1  TWTR  0.950000  0.473684  0.532710  0.370130\n",
              "4            SVM 0.1  TWTR  0.933333  0.600000  0.688525  0.545455\n",
              "5      LSTM beta 0.1  TWTR  0.904762  0.484076  0.584615  0.431818\n",
              "6       GRU beta 0.1  TWTR  0.932584  0.649682  0.751131  0.628788\n",
              "7   XGBoost beta 0.1  TWTR  0.940476  0.630573  0.731481  0.598485\n",
              "8    logreg beta 0.1  TWTR  0.940299  0.535032  0.633166  0.477273\n",
              "9       svm beta 0.1  TWTR  0.962025  0.624204  0.720379  0.575758\n",
              "0           LSTM 0.2  TWTR  0.950000  0.478947  0.434286  0.281481\n",
              "1            GRU 0.2  TWTR  0.960784  0.536842  0.526882  0.362963\n",
              "2        XGBoost 0.2  TWTR  0.927536  0.600000  0.627451  0.474074\n",
              "3         Logreg 0.2  TWTR  0.937500  0.584211  0.603015  0.444444\n",
              "4            SVM 0.2  TWTR  0.975610  0.494737  0.454545  0.296296\n",
              "5      LSTM beta 0.2  TWTR  0.855670  0.719745  0.790476  0.734513\n",
              "6       GRU beta 0.2  TWTR  0.893204  0.796178  0.851852  0.814159\n",
              "7   XGBoost beta 0.2  TWTR  0.857143  0.726115  0.796209  0.743363\n",
              "8    logreg beta 0.2  TWTR  0.892473  0.745223  0.805825  0.734513\n",
              "9       svm beta 0.2  TWTR  0.844037  0.757962  0.828829  0.814159\n",
              "0          LSTM 0.15  TWTR  0.894737  0.605263  0.644550  0.503704\n",
              "1           GRU 0.15  TWTR  0.840000  0.736842  0.807692  0.777778\n",
              "2       XGBoost 0.15  TWTR  0.870370  0.710526  0.773663  0.696296\n",
              "3        Logreg 0.15  TWTR  0.876404  0.642105  0.696429  0.577778\n",
              "4           SVM 0.15  TWTR  0.869159  0.705263  0.768595  0.688889\n",
              "5     LSTM beta 0.15  TWTR  0.895522  0.617834  0.666667  0.530973\n",
              "6      GRU beta 0.15  TWTR  0.861789  0.847134  0.898305  0.938053\n",
              "7  XGBoost beta 0.15  TWTR  0.829268  0.796178  0.864407  0.902655\n",
              "8   logreg beta 0.15  TWTR  0.881356  0.853503  0.900433  0.920354\n",
              "9      svm beta 0.15  TWTR  0.830645  0.802548  0.869198  0.911504"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qYmyD2kZu1G"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1410:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1410:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1410:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1410:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1410:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1443:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1443:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1443:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1443:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1443:1600].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('TWTR_logreg_beta_p.csv')"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjDPxMfoZu1G"
      },
      "source": [
        ""
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f65PzbZEaAC6"
      },
      "source": [
        "## WMT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx_ja3kTaAC7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "outputId": "a485a7ec-d0c3-48dc-db53-5cb6c2abc7d3"
      },
      "source": [
        "dfs = pd.read_csv(\"WMT.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "dfs"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.WMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>139.34</td>\n",
              "      <td>139.58</td>\n",
              "      <td>135.940</td>\n",
              "      <td>136.95</td>\n",
              "      <td>382464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.WMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>140.80</td>\n",
              "      <td>141.70</td>\n",
              "      <td>139.335</td>\n",
              "      <td>139.35</td>\n",
              "      <td>295652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.WMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>140.91</td>\n",
              "      <td>141.82</td>\n",
              "      <td>140.330</td>\n",
              "      <td>140.43</td>\n",
              "      <td>208623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2763</td>\n",
              "      <td>US1.WMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>142.02</td>\n",
              "      <td>142.02</td>\n",
              "      <td>139.990</td>\n",
              "      <td>140.51</td>\n",
              "      <td>313123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2762</td>\n",
              "      <td>US1.WMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>142.89</td>\n",
              "      <td>143.51</td>\n",
              "      <td>141.850</td>\n",
              "      <td>142.26</td>\n",
              "      <td>306424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2762</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.WMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>54.35</td>\n",
              "      <td>54.59</td>\n",
              "      <td>54.020</td>\n",
              "      <td>54.41</td>\n",
              "      <td>8421572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2763</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.WMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>54.60</td>\n",
              "      <td>54.82</td>\n",
              "      <td>54.040</td>\n",
              "      <td>54.37</td>\n",
              "      <td>7316985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.WMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>53.92</td>\n",
              "      <td>54.63</td>\n",
              "      <td>53.900</td>\n",
              "      <td>54.53</td>\n",
              "      <td>9474898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.WMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>53.90</td>\n",
              "      <td>54.25</td>\n",
              "      <td>53.830</td>\n",
              "      <td>53.99</td>\n",
              "      <td>9078140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.WMT</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>53.34</td>\n",
              "      <td>53.70</td>\n",
              "      <td>53.140</td>\n",
              "      <td>53.57</td>\n",
              "      <td>6732639</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2767 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...  <HIGH>    <LOW>  <CLOSE>    <VOL>\n",
              "0      2766  US1.WMT     D  20211001  ...  139.58  135.940   136.95   382464\n",
              "1      2765  US1.WMT     D  20210930  ...  141.70  139.335   139.35   295652\n",
              "2      2764  US1.WMT     D  20210929  ...  141.82  140.330   140.43   208623\n",
              "3      2763  US1.WMT     D  20210928  ...  142.02  139.990   140.51   313123\n",
              "4      2762  US1.WMT     D  20210927  ...  143.51  141.850   142.26   306424\n",
              "...     ...      ...   ...       ...  ...     ...      ...      ...      ...\n",
              "2762      4  US1.WMT     D  20101008  ...   54.59   54.020    54.41  8421572\n",
              "2763      3  US1.WMT     D  20101007  ...   54.82   54.040    54.37  7316985\n",
              "2764      2  US1.WMT     D  20101006  ...   54.63   53.900    54.53  9474898\n",
              "2765      1  US1.WMT     D  20101005  ...   54.25   53.830    53.99  9078140\n",
              "2766      0  US1.WMT     D  20101004  ...   53.70   53.140    53.57  6732639\n",
              "\n",
              "[2767 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUZOa1N2aAC7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "06a1e38f-7312-406b-e9df-5a13a80f1397"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"c270df30-89f8-4e78-84e7-e849668a1ba7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"c270df30-89f8-4e78-84e7-e849668a1ba7\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'c270df30-89f8-4e78-84e7-e849668a1ba7',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [118.15, 116.32, 116.14, 117.85, 118.71, 118.44, 118.32, 118.48, 118.47, 117.62, 116.93, 117.07, 117.16, 116.48, 115.51, 117.43, 116.98, 115.97, 116.03, 116.36, 114.72, 115.45, 115.89, 114.65, 114.19, 114.05, 112.65, 112.35, 111.99, 110.82, 111.91, 112.01, 112.11, 113.79, 112.97, 112.7, 106.23, 107.43, 105.19, 107.28, 108.52, 108.23, 107.27, 105.82, 109.43, 109.41, 110.38, 112.06, 112.27, 113.02, 112.25, 111.96, 112.06, 112.81, 113.92, 114.72, 114.6, 114.74, 114.98, 114.59, 113.91, 112.98, 112.92, 112.7499, 112.0, 112.4, 111.61, 110.63, 110.44, 110.11, 110.17, 110.72, 111.26, 111.14, 110.3, 109.62, 109.67, 109.15, 109.07, 108.65, 108.82, 107.91, 107.51, 106.0258, 105.11, 104.42, 102.56, 101.95, 101.44, 102.19, 102.13, 102.42, 102.68, 101.85, 102.23, 101.12, 101.5, 100.84, 101.27, 99.9, 100.32, 99.87, 101.95, 99.54, 100.31, 101.31, 102.46, 102.07, 101.15, 101.34, 102.82, 101.54, 101.51, 103.52, 103.55, 103.06, 102.38, 103.15, 103.13, 102.93, 102.44, 101.57, 100.8, 99.61, 98.69, 99.22, 98.84, 98.12, 97.16, 96.95, 97.81, 97.52, 97.16, 97.17, 98.33, 98.17, 98.29, 99.05, 98.64, 99.88, 99.67, 98.26, 98.23, 99.03, 98.4, 98.45, 97.575, 97.44, 98.26, 98.34, 97.84, 97.95, 98.99, 98.12, 98.69, 99.13, 99.54, 99.38, 99.88, 102.24, 99.96, 98.52, 97.93, 96.96, 96.17, 95.6, 96.72, 95.65, 95.61, 94.76, 93.81, 96.08, 94.8, 96.69, 97.05, 96.92, 98.36, 98.71, 97.5, 97.71, 96.75, 96.34, 96.25, 94.93, 94.82, 94.94, 94.9, 95.21, 94.53, 93.42, 92.88, 93.32, 93.15, 92.15, 91.61, 90.41, 85.6, 87.08, 87.26, 90.58, 91.07, 90.76, 91.86, 92.95, 93.09, 93.86, 93.92, 93.2, 94.8285, 95.83, 98.75, 97.59, 97.29, 97.46, 95.05, 95.14, 95.15, 94.16, 94.14, 96.78, 97.7, 99.55, 101.58, 102.95, 103.88, 105.55, 104.89, 104.34, 103.32, 102.89, 101.22, 100.58, 100.265, 102.44, 99.8, 98.95, 99.15, 97.54, 97.84, 97.14, 97.15, 96.19, 96.56, 95.83, 93.85, 94.78, 93.92, 95.78, 97.09, 94.7, 93.31, 94.21, 94.07, 95.16, 94.42, 93.93, 94.16, 94.6, 95.1, 94.93, 95.84, 95.76, 95.2401, 95.44, 94.82, 94.58, 95.13, 95.97, 96.63, 96.91, 95.84, 96.45, 96.6, 95.37, 95.84, 96.11, 95.65, 96.12, 94.51, 94.95, 95.19, 95.67, 96.08, 96.0, 97.85, 98.64, 90.25, 90.847, 89.65, 90.19, 89.02, 90.05, 89.78, 89.68, 89.61, 88.7785, 88.23, 89.21, 88.87, 88.14, 88.25, 87.92, 87.95, 87.64, 88.07, 87.74, 88.08, 88.2, 87.63, 87.69, 86.5, 86.54, 87.21, 85.94, 84.5, 84.57, 84.41, 83.98, 85.66, 85.86, 86.88, 85.94, 86.46, 84.64, 84.22, 83.63, 83.6, 83.09, 83.65, 83.82, 84.08, 84.11, 84.3, 84.32, 84.95, 84.55, 84.64, 85.42, 82.99, 82.55, 84.13, 82.41, 82.46, 82.85, 83.0, 83.4001, 84.5, 83.6, 84.34, 86.09, 84.51, 84.42, 83.37, 82.67, 83.05, 85.75, 85.46, 87.55, 86.225, 86.34, 87.38, 88.45, 87.29, 87.945, 87.19, 86.51, 86.11, 86.98, 87.88, 87.57, 87.88, 86.84, 86.02, 85.44, 85.9, 86.45, 86.3, 86.7, 87.82, 87.24, 86.8, 85.59, 88.96, 87.68, 86.05, 87.51, 85.41, 87.15, 88.19, 87.97, 87.46, 89.55, 87.52, 87.69, 88.29, 88.075, 88.68, 87.89, 87.74, 89.07, 89.98, 88.76, 89.05, 90.005, 91.53, 93.09, 92.89, 92.8, 91.5499, 94.11, 104.72, 103.26, 101.71, 100.98, 99.55, 99.4, 100.02, 102.91, 100.9, 100.08, 104.41, 105.54, 106.61, 107.76, 109.57, 108.39, 106.6, 105.81, 105.92, 105.43, 104.59, 104.34, 102.69, 100.685, 100.88, 100.01, 99.69, 100.4, 101.57, 100.12, 99.535, 99.44, 98.56, 98.88, 99.39, 99.25, 99.16, 98.22, 98.06, 98.8, 98.79, 97.87, 97.11, 97.14, 97.8, 96.7, 96.95, 96.55, 96.8, 97.3, 97.83, 97.01, 97.34, 97.26, 97.58, 96.79, 96.62, 96.62, 96.41, 96.5, 97.5, 97.46, 99.63, 89.83, 91.1, 90.99, 90.9201, 90.32, 90.25, 88.93, 88.71, 89.66, 88.81, 87.96, 87.3, 86.94, 88.17, 88.62, 88.5, 87.99, 88.7, 87.435, 86.43, 86.23, 85.985, 85.74, 86.64, 86.09, 85.685, 84.15, 80.51, 78.99, 79.45, 79.13, 79.24, 78.46, 78.14, 78.91, 79.29, 79.41, 79.15, 79.52, 80.0, 80.51, 80.07, 80.01, 80.33, 79.7, 79.85, 79.65, 79.07, 78.91, 80.13, 80.1, 79.83, 78.35, 78.07, 78.55, 78.81, 77.99, 78.67, 78.32, 79.96, 80.01, 79.74, 79.36, 79.7, 80.95, 80.79, 80.67, 80.41, 80.67, 81.59, 81.61, 81.29, 80.47, 80.87, 80.5328, 80.5, 79.99, 79.81, 79.79, 78.9, 78.51, 76.9, 76.15, 76.04, 75.86, 76.21, 76.4, 76.36, 75.04, 73.98, 73.47, 73.23, 75.32, 75.48, 75.32, 75.59, 75.675, 75.9, 76.51, 76.04, 75.5, 74.82, 75.53, 76.25, 75.56, 75.48, 75.26, 78.91, 79.91, 79.52, 79.23, 79.4, 78.95, 79.15, 78.93, 80.26, 79.53, 79.8, 78.6, 78.16, 78.12, 78.3, 78.16, 78.48, 78.53, 78.81, 77.54, 75.1391, 75.12, 76.29, 75.71, 76.12, 76.71, 76.72, 76.1, 76.48, 76.33, 75.74, 75.53, 75.25, 75.15, 75.43, 75.44, 75.04, 74.8, 74.96, 74.77, 74.06, 73.89, 73.5, 73.19, 73.43, 73.44, 73.05, 72.9043, 71.42, 71.63, 71.98, 71.84, 72.07, 71.581, 70.74, 70.32, 69.645, 69.63, 69.89, 70.26, 69.9003, 69.98, 70.2, 70.46, 70.56, 70.72, 69.96, 70.13, 69.88, 69.81, 69.88, 69.91, 70.0, 70.74, 70.43, 70.84, 71.74, 72.39, 71.2746, 71.68, 71.45, 69.35, 68.85, 68.6801, 68.66, 67.76, 68.02, 69.07, 67.81, 66.9, 66.41, 66.48, 66.67, 66.23, 66.75, 66.41, 65.64, 66.72, 66.9, 67.39, 66.67, 67.16, 67.6, 68.11, 68.43, 67.1, 67.97, 68.53, 68.24, 68.69, 68.26, 69.21, 69.07, 68.64, 69.14, 69.27, 69.31, 69.71, 69.53, 69.58, 71.25, 71.8, 71.59, 70.94, 71.08, 71.35, 71.81, 71.66, 70.1, 70.35, 70.59, 70.36, 69.94, 70.88, 70.71, 70.46, 71.39, 71.2, 71.0, 70.81, 70.1, 69.37, 68.54, 69.19, 71.37, 71.41, 70.48, 71.22, 71.4, 71.13, 69.79, 69.78, 69.17, 69.63, 69.45, 69.29, 70.03, 70.01, 69.85, 69.59, 69.38, 69.2, 68.36, 68.74, 68.91, 68.86, 68.23, 68.46, 68.25, 67.47, 67.4, 67.96, 68.7, 69.36, 71.65, 71.77, 72.02, 72.11, 70.72, 71.76, 72.3, 71.63, 72.36, 72.24, 72.19, 71.98, 72.09, 72.86, 72.4, 71.52, 71.47, 71.94, 70.34, 71.795, 72.05, 73.03, 72.49, 72.84, 71.41, 71.32, 71.41, 71.16, 71.22, 72.25, 71.97, 72.71, 72.75, 74.26, 72.91, 72.89, 73.3, 73.88, 73.81, 73.96, 73.545, 73.34, 73.74, 73.31, 72.91, 73.12, 73.79, 72.96, 73.23, 73.33, 73.74, 73.76, 73.54, 73.53, 73.79, 73.68, 73.83, 73.64, 73.7, 73.61, 73.26, 74.04, 73.79, 73.4, 73.84, 73.14, 72.81, 73.03, 72.44, 71.51, 71.52, 71.92, 72.08, 71.75, 71.47, 71.09, 70.95, 71.29, 71.13, 70.95, 70.53, 71.145, 71.09, 71.28, 71.02, 71.07, 70.87, 70.93, 70.5, 70.77, 70.76, 70.81, 70.4503, 70.22, 69.53, 69.86, 69.2, 63.13, 65.06, 66.0, 64.94, 66.87, 66.41, 68.79, 68.96, 68.25, 67.21, 67.17, 67.07, 67.58, 66.88, 68.95, 69.44, 69.31, 69.495, 68.72, 68.48, 69.22, 69.75, 69.81, 69.08, 68.76, 69.17, 68.82, 67.41, 68.06, 68.23, 69.03, 68.62, 69.09, 69.08, 68.5, 68.79, 68.04, 68.11, 68.01, 67.47, 67.88, 67.98, 66.95, 67.44, 68.0, 68.08, 67.37, 67.14, 67.41, 67.53, 68.04, 67.85, 66.77, 66.13, 66.18, 66.43, 66.3, 66.49, 68.04, 67.1, 66.495, 65.62, 64.66, 64.0901, 66.14, 65.89, 66.17, 65.3, 65.78, 65.82, 66.93, 66.99, 66.41, 66.27, 66.85, 67.48, 66.36, 64.19, 63.95, 63.94, 63.45, 62.68, 61.85, 60.85, 62.57, 61.92, 63.05, 61.92, 63.61, 64.23, 63.52, 65.01, 63.6, 62.94, 61.47, 61.3, 61.67, 61.58, 60.74, 60.83, 61.08, 60.55, 59.57, 58.83, 58.98, 60.29, 59.67, 60.39, 59.34, 59.56, 59.13, 59.63, 60.5, 59.67, 59.05, 58.379, 58.97, 58.87, 59.89, 60.25, 59.9122, 60.26, 60.09, 60.7, 60.92, 59.94, 57.88, 56.44, 56.9566, 57.57, 58.7, 58.51, 58.79, 58.61, 58.38, 58.13, 57.6, 57.25, 57.95, 57.6199, 57.48, 58.03, 58.3011, 58.89, 58.64, 58.76, 58.8501, 58.87, 59.34, 60.03, 66.71, 66.92, 66.69, 66.87, 66.35, 65.69, 65.86, 64.98, 64.26, 64.83, 63.77, 63.68, 63.77, 63.81, 63.73, 63.6, 63.72, 63.38, 64.48, 64.68, 64.32, 64.29, 64.63, 64.14, 65.13, 66.38, 63.89, 64.86, 64.45, 63.85, 64.73, 64.94, 66.11, 64.83, 63.1, 63.95, 66.5699, 68.44, 68.59, 69.48, 71.9, 72.38, 72.12, 72.6, 71.94, 71.49, 71.25, 72.78, 73.53, 72.25, 72.19, 72.01, 72.17, 72.24, 72.095, 71.39, 71.59, 72.51, 73.16, 72.75, 73.09, 73.39, 73.83, 73.65, 73.78, 73.88, 73.135, 72.76, 73.04, 73.8, 72.54, 71.84, 71.89, 70.92, 71.43, 72.12, 71.86, 72.405, 72.56, 72.78, 72.71, 72.97, 72.73, 72.34, 71.94, 72.43, 72.95, 72.92, 72.45, 72.62, 73.03, 74.17, 74.88, 74.51, 74.71, 74.28, 74.845, 75.18, 74.9285, 75.88, 76.12, 75.88, 76.41, 79.9, 79.23, 78.71, 78.14, 78.96, 78.12, 78.51, 78.04, 77.64, 78.13, 79.19, 78.58, 78.03, 77.86, 79.1, 79.38, 79.84, 79.2, 78.42, 78.04, 78.11, 77.88, 79.25, 79.735, 80.17, 80.31, 80.66, 80.83, 81.0, 80.47, 80.98, 80.73, 80.71, 82.26, 82.55, 81.33, 81.88, 81.32, 83.07, 83.33, 83.265, 81.55, 82.48, 82.62, 83.29, 81.925, 81.89, 80.69, 82.14, 82.91, 82.59, 83.58, 82.57, 83.36, 83.94, 83.91, 83.79, 83.55, 84.58, 84.625, 84.32, 83.54, 86.28, 85.95, 85.8, 85.86, 86.34, 87.28, 85.89, 87.32, 87.274, 86.64, 86.24, 85.71, 84.98, 87.71, 86.85, 87.54, 88.62, 88.51, 88.29, 86.65, 86.68, 86.76, 87.38, 86.58, 89.31, 90.06, 89.36, 90.46, 88.61, 86.29, 85.68, 85.9, 85.89, 86.79, 86.655, 86.93, 86.27, 86.69, 86.4, 85.17, 85.94, 84.22, 82.98, 83.92, 83.82, 83.81, 83.0, 83.55, 84.25, 84.11, 84.74, 84.92, 86.4, 86.23, 87.68, 84.98, 84.99, 85.4, 84.62, 84.58, 85.0, 83.82, 83.57, 82.96, 82.94, 79.2, 79.03, 79.45, 78.76, 77.8, 77.7, 77.25, 76.25, 76.24, 76.44, 76.38, 76.36, 76.57, 76.38, 76.22, 76.03, 76.03, 75.12, 74.14, 73.81, 75.19, 77.95, 77.56, 78.28, 77.87, 78.24, 77.31, 77.37, 77.32, 76.22, 76.12, 76.49, 76.08, 76.48, 76.14, 77.1, 75.63, 76.32, 76.86, 76.22, 76.23, 76.31, 75.8, 75.8, 76.09, 76.51, 76.72, 76.53, 77.46, 76.55, 76.0, 75.75, 75.53, 75.91, 75.84, 75.51, 75.72, 75.73, 75.54, 74.95, 74.88, 74.47, 73.92, 74.37, 74.02, 74.22, 74.36, 74.66, 73.94, 74.19, 73.34, 73.55, 73.55, 73.6, 74.78, 75.45, 75.7, 75.98, 76.34, 76.99, 76.63, 76.77, 77.06, 76.61, 76.85, 76.86, 76.55, 76.81, 77.06, 77.21, 76.67, 76.06, 75.8, 75.61, 75.26, 75.03, 75.33, 74.92, 75.62, 75.97, 75.79, 75.69, 75.87, 75.72, 75.0, 75.35, 75.28, 75.71, 76.17, 76.63, 77.01, 77.21, 77.33, 77.13, 76.72, 76.76, 76.76, 75.98, 75.54, 75.57, 75.62, 75.39, 75.65, 75.7, 76.61, 77.02, 76.84, 78.73, 79.15, 79.16, 79.18, 78.7, 78.0, 78.03, 78.65, 79.18, 79.68, 79.7, 79.66, 79.75, 78.65, 78.31, 78.05, 77.58, 77.59, 77.66, 77.21, 76.88, 77.37, 76.51, 76.88, 77.96, 78.19, 77.32, 77.31, 77.45, 77.18, 76.77, 76.43, 76.01, 76.15, 76.23, 76.88, 76.76, 76.47, 75.38, 74.38, 74.76, 74.68, 74.29, 74.95, 75.52, 74.92, 74.42, 74.56, 74.898, 74.8, 75.1, 74.11, 74.73, 74.56, 74.79, 73.35, 73.35, 73.12, 73.5299, 74.86, 75.35, 75.8, 75.36, 74.97, 74.79, 73.74, 73.76, 72.82, 72.88, 72.73, 72.66, 74.69, 74.74, 74.11, 74.67, 74.13, 74.45, 74.95, 75.36, 75.84, 76.15, 76.76, 77.66, 77.94, 77.47, 78.04, 78.1, 77.805, 78.44, 78.18, 78.64, 78.91, 78.69, 78.63, 78.49, 78.38, 78.01, 77.87, 77.5, 77.24, 77.95, 77.24, 77.73, 78.07, 78.5, 79.08, 79.08, 79.96, 79.94, 79.44, 80.22, 81.21, 81.1097, 81.0099, 80.935, 80.68, 80.44, 79.81, 78.86, 78.91, 79.25, 79.21, 79.25, 79.09, 78.88, 78.72, 79.0, 77.95, 77.51, 78.15, 77.43, 77.34, 77.07, 76.78, 76.93, 77.07, 77.17, 76.07, 76.42, 75.89, 76.33, 75.16]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c270df30-89f8-4e78-84e7-e849668a1ba7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"6083e24f-7365-442c-ac7a-5d3953a1f5f3\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"6083e24f-7365-442c-ac7a-5d3953a1f5f3\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '6083e24f-7365-442c-ac7a-5d3953a1f5f3',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6083e24f-7365-442c-ac7a-5d3953a1f5f3');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqbGAFB4aAC7"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUFREgSWaAC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f09eccf-761d-4f6a-9885-171e98909795"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.05, .1]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"WMT\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.05\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6861 - accuracy: 0.5799 - val_loss: 0.6709 - val_accuracy: 0.6122\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6804 - accuracy: 0.5799 - val_loss: 0.6695 - val_accuracy: 0.6122\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6725 - accuracy: 0.5899 - val_loss: 0.6320 - val_accuracy: 0.7347\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6304 - accuracy: 0.6624 - val_loss: 0.5589 - val_accuracy: 0.7367\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6359 - accuracy: 0.6591 - val_loss: 0.6473 - val_accuracy: 0.6122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6839 - accuracy: 0.5624 - val_loss: 0.6674 - val_accuracy: 0.6122\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6490 - accuracy: 0.6383 - val_loss: 0.6232 - val_accuracy: 0.6755\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5970 - accuracy: 0.7047 - val_loss: 0.5493 - val_accuracy: 0.7347\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5816 - accuracy: 0.7007 - val_loss: 0.5040 - val_accuracy: 0.7694\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5782 - accuracy: 0.7047 - val_loss: 0.5118 - val_accuracy: 0.7816\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.780088\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.799561\n",
            "[2]\tvalidation_0-auc:0.799588\n",
            "[3]\tvalidation_0-auc:0.810228\n",
            "[4]\tvalidation_0-auc:0.810009\n",
            "[5]\tvalidation_0-auc:0.809395\n",
            "[6]\tvalidation_0-auc:0.811246\n",
            "[7]\tvalidation_0-auc:0.811675\n",
            "[8]\tvalidation_0-auc:0.810272\n",
            "[9]\tvalidation_0-auc:0.812763\n",
            "[10]\tvalidation_0-auc:0.812614\n",
            "[11]\tvalidation_0-auc:0.812632\n",
            "[12]\tvalidation_0-auc:0.813561\n",
            "[13]\tvalidation_0-auc:0.814298\n",
            "[14]\tvalidation_0-auc:0.813018\n",
            "[15]\tvalidation_0-auc:0.812675\n",
            "[16]\tvalidation_0-auc:0.813535\n",
            "[17]\tvalidation_0-auc:0.812342\n",
            "[18]\tvalidation_0-auc:0.812553\n",
            "[19]\tvalidation_0-auc:0.813079\n",
            "[20]\tvalidation_0-auc:0.812465\n",
            "[21]\tvalidation_0-auc:0.813272\n",
            "[22]\tvalidation_0-auc:0.812746\n",
            "[23]\tvalidation_0-auc:0.81243\n",
            "[24]\tvalidation_0-auc:0.811044\n",
            "[25]\tvalidation_0-auc:0.810947\n",
            "[26]\tvalidation_0-auc:0.811193\n",
            "[27]\tvalidation_0-auc:0.812035\n",
            "[28]\tvalidation_0-auc:0.811386\n",
            "[29]\tvalidation_0-auc:0.811246\n",
            "[30]\tvalidation_0-auc:0.810763\n",
            "[31]\tvalidation_0-auc:0.811009\n",
            "[32]\tvalidation_0-auc:0.810281\n",
            "[33]\tvalidation_0-auc:0.809956\n",
            "[34]\tvalidation_0-auc:0.810281\n",
            "[35]\tvalidation_0-auc:0.810298\n",
            "[36]\tvalidation_0-auc:0.810298\n",
            "[37]\tvalidation_0-auc:0.809912\n",
            "[38]\tvalidation_0-auc:0.809982\n",
            "[39]\tvalidation_0-auc:0.810307\n",
            "[40]\tvalidation_0-auc:0.8105\n",
            "[41]\tvalidation_0-auc:0.810167\n",
            "[42]\tvalidation_0-auc:0.810061\n",
            "[43]\tvalidation_0-auc:0.810061\n",
            "[44]\tvalidation_0-auc:0.809149\n",
            "[45]\tvalidation_0-auc:0.808833\n",
            "[46]\tvalidation_0-auc:0.809193\n",
            "[47]\tvalidation_0-auc:0.808719\n",
            "[48]\tvalidation_0-auc:0.808842\n",
            "[49]\tvalidation_0-auc:0.809193\n",
            "[50]\tvalidation_0-auc:0.809184\n",
            "[51]\tvalidation_0-auc:0.808886\n",
            "[52]\tvalidation_0-auc:0.80857\n",
            "[53]\tvalidation_0-auc:0.80857\n",
            "[54]\tvalidation_0-auc:0.808518\n",
            "[55]\tvalidation_0-auc:0.807956\n",
            "[56]\tvalidation_0-auc:0.8075\n",
            "[57]\tvalidation_0-auc:0.806877\n",
            "[58]\tvalidation_0-auc:0.807193\n",
            "[59]\tvalidation_0-auc:0.807158\n",
            "[60]\tvalidation_0-auc:0.806807\n",
            "[61]\tvalidation_0-auc:0.806877\n",
            "[62]\tvalidation_0-auc:0.806965\n",
            "[63]\tvalidation_0-auc:0.806719\n",
            "Stopping. Best iteration:\n",
            "[13]\tvalidation_0-auc:0.814298\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6851 - accuracy: 0.5724 - val_loss: 0.6595 - val_accuracy: 0.6565\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6822 - accuracy: 0.5800 - val_loss: 0.6647 - val_accuracy: 0.6565\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6791 - accuracy: 0.5786 - val_loss: 0.6719 - val_accuracy: 0.6565\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6652 - accuracy: 0.5916 - val_loss: 0.6058 - val_accuracy: 0.6586\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6403 - accuracy: 0.6294 - val_loss: 0.5837 - val_accuracy: 0.7505\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6820 - accuracy: 0.5717 - val_loss: 0.6739 - val_accuracy: 0.6958\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6544 - accuracy: 0.6102 - val_loss: 0.6170 - val_accuracy: 0.6915\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6077 - accuracy: 0.6685 - val_loss: 0.5232 - val_accuracy: 0.7659\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5927 - accuracy: 0.7062 - val_loss: 0.5444 - val_accuracy: 0.7177\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5970 - accuracy: 0.6870 - val_loss: 0.5271 - val_accuracy: 0.7527\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.721826\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.750318\n",
            "[2]\tvalidation_0-auc:0.736369\n",
            "[3]\tvalidation_0-auc:0.73448\n",
            "[4]\tvalidation_0-auc:0.742144\n",
            "[5]\tvalidation_0-auc:0.749618\n",
            "[6]\tvalidation_0-auc:0.746635\n",
            "[7]\tvalidation_0-auc:0.759841\n",
            "[8]\tvalidation_0-auc:0.753471\n",
            "[9]\tvalidation_0-auc:0.760457\n",
            "[10]\tvalidation_0-auc:0.763503\n",
            "[11]\tvalidation_0-auc:0.767176\n",
            "[12]\tvalidation_0-auc:0.769671\n",
            "[13]\tvalidation_0-auc:0.765021\n",
            "[14]\tvalidation_0-auc:0.768079\n",
            "[15]\tvalidation_0-auc:0.768853\n",
            "[16]\tvalidation_0-auc:0.767803\n",
            "[17]\tvalidation_0-auc:0.769183\n",
            "[18]\tvalidation_0-auc:0.769926\n",
            "[19]\tvalidation_0-auc:0.769936\n",
            "[20]\tvalidation_0-auc:0.769735\n",
            "[21]\tvalidation_0-auc:0.772887\n",
            "[22]\tvalidation_0-auc:0.773301\n",
            "[23]\tvalidation_0-auc:0.773493\n",
            "[24]\tvalidation_0-auc:0.772134\n",
            "[25]\tvalidation_0-auc:0.77241\n",
            "[26]\tvalidation_0-auc:0.772155\n",
            "[27]\tvalidation_0-auc:0.771433\n",
            "[28]\tvalidation_0-auc:0.772495\n",
            "[29]\tvalidation_0-auc:0.774915\n",
            "[30]\tvalidation_0-auc:0.774745\n",
            "[31]\tvalidation_0-auc:0.776221\n",
            "[32]\tvalidation_0-auc:0.77603\n",
            "[33]\tvalidation_0-auc:0.776019\n",
            "[34]\tvalidation_0-auc:0.775786\n",
            "[35]\tvalidation_0-auc:0.775764\n",
            "[36]\tvalidation_0-auc:0.776306\n",
            "[37]\tvalidation_0-auc:0.775584\n",
            "[38]\tvalidation_0-auc:0.774586\n",
            "[39]\tvalidation_0-auc:0.77448\n",
            "[40]\tvalidation_0-auc:0.773928\n",
            "[41]\tvalidation_0-auc:0.774289\n",
            "[42]\tvalidation_0-auc:0.773949\n",
            "[43]\tvalidation_0-auc:0.775828\n",
            "[44]\tvalidation_0-auc:0.775446\n",
            "[45]\tvalidation_0-auc:0.776369\n",
            "[46]\tvalidation_0-auc:0.776285\n",
            "[47]\tvalidation_0-auc:0.77603\n",
            "[48]\tvalidation_0-auc:0.775679\n",
            "[49]\tvalidation_0-auc:0.776019\n",
            "[50]\tvalidation_0-auc:0.774342\n",
            "[51]\tvalidation_0-auc:0.775287\n",
            "[52]\tvalidation_0-auc:0.775096\n",
            "[53]\tvalidation_0-auc:0.775308\n",
            "[54]\tvalidation_0-auc:0.777229\n",
            "[55]\tvalidation_0-auc:0.777675\n",
            "[56]\tvalidation_0-auc:0.777208\n",
            "[57]\tvalidation_0-auc:0.77638\n",
            "[58]\tvalidation_0-auc:0.776868\n",
            "[59]\tvalidation_0-auc:0.776529\n",
            "[60]\tvalidation_0-auc:0.775637\n",
            "[61]\tvalidation_0-auc:0.777059\n",
            "[62]\tvalidation_0-auc:0.77672\n",
            "[63]\tvalidation_0-auc:0.775913\n",
            "[64]\tvalidation_0-auc:0.779352\n",
            "[65]\tvalidation_0-auc:0.778822\n",
            "[66]\tvalidation_0-auc:0.778758\n",
            "[67]\tvalidation_0-auc:0.778482\n",
            "[68]\tvalidation_0-auc:0.778758\n",
            "[69]\tvalidation_0-auc:0.778163\n",
            "[70]\tvalidation_0-auc:0.777972\n",
            "[71]\tvalidation_0-auc:0.7781\n",
            "[72]\tvalidation_0-auc:0.779544\n",
            "[73]\tvalidation_0-auc:0.779363\n",
            "[74]\tvalidation_0-auc:0.779448\n",
            "[75]\tvalidation_0-auc:0.780913\n",
            "[76]\tvalidation_0-auc:0.781783\n",
            "[77]\tvalidation_0-auc:0.780594\n",
            "[78]\tvalidation_0-auc:0.780807\n",
            "[79]\tvalidation_0-auc:0.780807\n",
            "[80]\tvalidation_0-auc:0.780552\n",
            "[81]\tvalidation_0-auc:0.780478\n",
            "[82]\tvalidation_0-auc:0.779904\n",
            "[83]\tvalidation_0-auc:0.778609\n",
            "[84]\tvalidation_0-auc:0.778206\n",
            "[85]\tvalidation_0-auc:0.778015\n",
            "[86]\tvalidation_0-auc:0.778079\n",
            "[87]\tvalidation_0-auc:0.777739\n",
            "[88]\tvalidation_0-auc:0.776996\n",
            "[89]\tvalidation_0-auc:0.776614\n",
            "[90]\tvalidation_0-auc:0.776614\n",
            "[91]\tvalidation_0-auc:0.77604\n",
            "[92]\tvalidation_0-auc:0.775701\n",
            "[93]\tvalidation_0-auc:0.775637\n",
            "[94]\tvalidation_0-auc:0.773832\n",
            "[95]\tvalidation_0-auc:0.773535\n",
            "[96]\tvalidation_0-auc:0.773132\n",
            "[97]\tvalidation_0-auc:0.772728\n",
            "[98]\tvalidation_0-auc:0.773811\n",
            "[99]\tvalidation_0-auc:0.773365\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Model       |      Accuracy      |     Precision      |       Recall       |      F1 score      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     LSTM 0.05     | 0.6122448979591837 |        0.0         |        0.0         |        0.0         |\n",
            "|      GRU 0.05     | 0.7816326530612245 | 0.7243243243243244 | 0.7052631578947368 | 0.7146666666666667 |\n",
            "|    XGBoost 0.05   | 0.7489795918367347 | 0.6830601092896175 | 0.6578947368421053 | 0.6702412868632708 |\n",
            "|    Logreg 0.05    | 0.7489795918367347 | 0.7375886524822695 | 0.5473684210526316 | 0.6283987915407855 |\n",
            "|      SVM 0.05     | 0.763265306122449  | 0.7102272727272727 | 0.6578947368421053 | 0.6830601092896175 |\n",
            "|   LSTM beta 0.05  |  0.75054704595186  | 0.6524822695035462 | 0.5859872611464968 | 0.6174496644295302 |\n",
            "|   GRU beta 0.05   | 0.7527352297592997 | 0.6264367816091954 | 0.6942675159235668 | 0.6586102719033233 |\n",
            "| XGBoost beta 0.05 | 0.7286652078774617 | 0.5854922279792746 | 0.7197452229299363 | 0.6457142857142857 |\n",
            "|  logreg beta 0.05 | 0.7614879649890591 | 0.6739130434782609 | 0.5923566878980892 | 0.6305084745762711 |\n",
            "|   svm beta 0.05   | 0.7417943107221007 | 0.6031746031746031 | 0.7261146496815286 | 0.6589595375722542 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6689 - accuracy: 0.6168 - val_loss: 0.6683 - val_accuracy: 0.6102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6622 - accuracy: 0.6235 - val_loss: 0.6557 - val_accuracy: 0.6918\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6665 - accuracy: 0.6188 - val_loss: 0.6709 - val_accuracy: 0.6102\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6656 - accuracy: 0.6235 - val_loss: 0.6682 - val_accuracy: 0.6102\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6637 - accuracy: 0.6235 - val_loss: 0.6680 - val_accuracy: 0.6102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6676 - accuracy: 0.6215 - val_loss: 0.6627 - val_accuracy: 0.6102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6429 - accuracy: 0.6376 - val_loss: 0.6202 - val_accuracy: 0.6755\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5953 - accuracy: 0.6758 - val_loss: 0.6145 - val_accuracy: 0.6755\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5926 - accuracy: 0.6960 - val_loss: 0.6028 - val_accuracy: 0.6878\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5739 - accuracy: 0.6966 - val_loss: 0.6035 - val_accuracy: 0.7143\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.692693\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.69208\n",
            "[2]\tvalidation_0-auc:0.703742\n",
            "[3]\tvalidation_0-auc:0.702884\n",
            "[4]\tvalidation_0-auc:0.704723\n",
            "[5]\tvalidation_0-auc:0.700397\n",
            "[6]\tvalidation_0-auc:0.70038\n",
            "[7]\tvalidation_0-auc:0.701895\n",
            "[8]\tvalidation_0-auc:0.70045\n",
            "[9]\tvalidation_0-auc:0.700257\n",
            "[10]\tvalidation_0-auc:0.698778\n",
            "[11]\tvalidation_0-auc:0.697272\n",
            "[12]\tvalidation_0-auc:0.69806\n",
            "[13]\tvalidation_0-auc:0.698603\n",
            "[14]\tvalidation_0-auc:0.699163\n",
            "[15]\tvalidation_0-auc:0.699329\n",
            "[16]\tvalidation_0-auc:0.699312\n",
            "[17]\tvalidation_0-auc:0.699618\n",
            "[18]\tvalidation_0-auc:0.698498\n",
            "[19]\tvalidation_0-auc:0.69841\n",
            "[20]\tvalidation_0-auc:0.697929\n",
            "[21]\tvalidation_0-auc:0.69806\n",
            "[22]\tvalidation_0-auc:0.698165\n",
            "[23]\tvalidation_0-auc:0.70073\n",
            "[24]\tvalidation_0-auc:0.700739\n",
            "[25]\tvalidation_0-auc:0.699898\n",
            "[26]\tvalidation_0-auc:0.700546\n",
            "[27]\tvalidation_0-auc:0.700695\n",
            "[28]\tvalidation_0-auc:0.701185\n",
            "[29]\tvalidation_0-auc:0.701002\n",
            "[30]\tvalidation_0-auc:0.700573\n",
            "[31]\tvalidation_0-auc:0.700818\n",
            "[32]\tvalidation_0-auc:0.699968\n",
            "[33]\tvalidation_0-auc:0.700196\n",
            "[34]\tvalidation_0-auc:0.699399\n",
            "[35]\tvalidation_0-auc:0.699487\n",
            "[36]\tvalidation_0-auc:0.699373\n",
            "[37]\tvalidation_0-auc:0.699084\n",
            "[38]\tvalidation_0-auc:0.69827\n",
            "[39]\tvalidation_0-auc:0.697228\n",
            "[40]\tvalidation_0-auc:0.697228\n",
            "[41]\tvalidation_0-auc:0.697823\n",
            "[42]\tvalidation_0-auc:0.697648\n",
            "[43]\tvalidation_0-auc:0.699434\n",
            "[44]\tvalidation_0-auc:0.699469\n",
            "[45]\tvalidation_0-auc:0.698979\n",
            "[46]\tvalidation_0-auc:0.700074\n",
            "[47]\tvalidation_0-auc:0.699776\n",
            "[48]\tvalidation_0-auc:0.699758\n",
            "[49]\tvalidation_0-auc:0.699671\n",
            "[50]\tvalidation_0-auc:0.699513\n",
            "[51]\tvalidation_0-auc:0.699828\n",
            "[52]\tvalidation_0-auc:0.69869\n",
            "[53]\tvalidation_0-auc:0.699356\n",
            "[54]\tvalidation_0-auc:0.699023\n",
            "Stopping. Best iteration:\n",
            "[4]\tvalidation_0-auc:0.704723\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6671 - accuracy: 0.6225 - val_loss: 0.6793 - val_accuracy: 0.5821\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6337 - accuracy: 0.6534 - val_loss: 0.6648 - val_accuracy: 0.5799\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5882 - accuracy: 0.7111 - val_loss: 0.6534 - val_accuracy: 0.6280\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5655 - accuracy: 0.7275 - val_loss: 0.6944 - val_accuracy: 0.6214\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5872 - accuracy: 0.7021 - val_loss: 0.6721 - val_accuracy: 0.6149\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6333 - accuracy: 0.6555 - val_loss: 0.6476 - val_accuracy: 0.5952\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5408 - accuracy: 0.7536 - val_loss: 0.5971 - val_accuracy: 0.6761\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5190 - accuracy: 0.7660 - val_loss: 0.5970 - val_accuracy: 0.6565\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5127 - accuracy: 0.7653 - val_loss: 0.6079 - val_accuracy: 0.6849\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.4873 - accuracy: 0.7852 - val_loss: 0.6543 - val_accuracy: 0.6827\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.701571\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.709404\n",
            "[2]\tvalidation_0-auc:0.714453\n",
            "[3]\tvalidation_0-auc:0.714965\n",
            "[4]\tvalidation_0-auc:0.715634\n",
            "[5]\tvalidation_0-auc:0.711786\n",
            "[6]\tvalidation_0-auc:0.714551\n",
            "[7]\tvalidation_0-auc:0.713705\n",
            "[8]\tvalidation_0-auc:0.714965\n",
            "[9]\tvalidation_0-auc:0.714866\n",
            "[10]\tvalidation_0-auc:0.714335\n",
            "[11]\tvalidation_0-auc:0.71216\n",
            "[12]\tvalidation_0-auc:0.711648\n",
            "[13]\tvalidation_0-auc:0.71281\n",
            "[14]\tvalidation_0-auc:0.715969\n",
            "[15]\tvalidation_0-auc:0.716254\n",
            "[16]\tvalidation_0-auc:0.718144\n",
            "[17]\tvalidation_0-auc:0.717799\n",
            "[18]\tvalidation_0-auc:0.71653\n",
            "[19]\tvalidation_0-auc:0.718891\n",
            "[20]\tvalidation_0-auc:0.717425\n",
            "[21]\tvalidation_0-auc:0.718626\n",
            "[22]\tvalidation_0-auc:0.716657\n",
            "[23]\tvalidation_0-auc:0.715329\n",
            "[24]\tvalidation_0-auc:0.71715\n",
            "[25]\tvalidation_0-auc:0.715516\n",
            "[26]\tvalidation_0-auc:0.714788\n",
            "[27]\tvalidation_0-auc:0.71401\n",
            "[28]\tvalidation_0-auc:0.712819\n",
            "[29]\tvalidation_0-auc:0.713922\n",
            "[30]\tvalidation_0-auc:0.709365\n",
            "[31]\tvalidation_0-auc:0.709493\n",
            "[32]\tvalidation_0-auc:0.71031\n",
            "[33]\tvalidation_0-auc:0.710418\n",
            "[34]\tvalidation_0-auc:0.710674\n",
            "[35]\tvalidation_0-auc:0.70965\n",
            "[36]\tvalidation_0-auc:0.709296\n",
            "[37]\tvalidation_0-auc:0.71092\n",
            "[38]\tvalidation_0-auc:0.71031\n",
            "[39]\tvalidation_0-auc:0.710211\n",
            "[40]\tvalidation_0-auc:0.71213\n",
            "[41]\tvalidation_0-auc:0.710566\n",
            "[42]\tvalidation_0-auc:0.710448\n",
            "[43]\tvalidation_0-auc:0.710369\n",
            "[44]\tvalidation_0-auc:0.711756\n",
            "[45]\tvalidation_0-auc:0.711619\n",
            "[46]\tvalidation_0-auc:0.71152\n",
            "[47]\tvalidation_0-auc:0.711107\n",
            "[48]\tvalidation_0-auc:0.712111\n",
            "[49]\tvalidation_0-auc:0.712288\n",
            "[50]\tvalidation_0-auc:0.711855\n",
            "[51]\tvalidation_0-auc:0.712406\n",
            "[52]\tvalidation_0-auc:0.711934\n",
            "[53]\tvalidation_0-auc:0.713056\n",
            "[54]\tvalidation_0-auc:0.713065\n",
            "[55]\tvalidation_0-auc:0.712947\n",
            "[56]\tvalidation_0-auc:0.713046\n",
            "[57]\tvalidation_0-auc:0.713095\n",
            "[58]\tvalidation_0-auc:0.713597\n",
            "[59]\tvalidation_0-auc:0.713479\n",
            "[60]\tvalidation_0-auc:0.713892\n",
            "[61]\tvalidation_0-auc:0.714522\n",
            "[62]\tvalidation_0-auc:0.714719\n",
            "[63]\tvalidation_0-auc:0.714187\n",
            "[64]\tvalidation_0-auc:0.714404\n",
            "[65]\tvalidation_0-auc:0.713459\n",
            "[66]\tvalidation_0-auc:0.714246\n",
            "[67]\tvalidation_0-auc:0.713006\n",
            "[68]\tvalidation_0-auc:0.713557\n",
            "[69]\tvalidation_0-auc:0.71277\n",
            "Stopping. Best iteration:\n",
            "[19]\tvalidation_0-auc:0.718891\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |      F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n",
            "|     LSTM 0.1     | 0.610204081632653  |        0.0         |         0.0         |        0.0         |\n",
            "|     GRU 0.1      | 0.7142857142857143 | 0.6783216783216783 |  0.5078534031413613 | 0.5808383233532934 |\n",
            "|   XGBoost 0.1    | 0.6918367346938775 | 0.6219512195121951 |  0.5340314136125655 | 0.5746478873239438 |\n",
            "|    Logreg 0.1    | 0.7020408163265306 | 0.7227722772277227 | 0.38219895287958117 | 0.5000000000000001 |\n",
            "|     SVM 0.1      | 0.6877551020408164 | 0.6376811594202898 |  0.4607329842931937 | 0.5349544072948328 |\n",
            "|  LSTM beta 0.1   | 0.6148796498905909 | 0.5449101796407185 | 0.47643979057591623 | 0.5083798882681564 |\n",
            "|   GRU beta 0.1   | 0.6827133479212254 | 0.6885245901639344 |  0.4397905759162304 | 0.536741214057508  |\n",
            "| XGBoost beta 0.1 | 0.6673960612691466 | 0.6181818181818182 |  0.5340314136125655 | 0.5730337078651685 |\n",
            "| logreg beta 0.1  | 0.6717724288840262 | 0.6357615894039735 |  0.5026178010471204 | 0.5614035087719298 |\n",
            "|   svm beta 0.1   | 0.649890590809628  | 0.5987261146496815 | 0.49214659685863876 | 0.5402298850574713 |\n",
            "+------------------+--------------------+--------------------+---------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE1IawbDaAC7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "a4fa95b8-aff2-4121-b5e3-ef45d81b8295"
      },
      "source": [
        "Result_cross.to_csv('WMT_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.612245</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.724324</td>\n",
              "      <td>0.781633</td>\n",
              "      <td>0.714667</td>\n",
              "      <td>0.705263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.683060</td>\n",
              "      <td>0.748980</td>\n",
              "      <td>0.670241</td>\n",
              "      <td>0.657895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.737589</td>\n",
              "      <td>0.748980</td>\n",
              "      <td>0.628399</td>\n",
              "      <td>0.547368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.710227</td>\n",
              "      <td>0.763265</td>\n",
              "      <td>0.683060</td>\n",
              "      <td>0.657895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.652482</td>\n",
              "      <td>0.750547</td>\n",
              "      <td>0.617450</td>\n",
              "      <td>0.585987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.626437</td>\n",
              "      <td>0.752735</td>\n",
              "      <td>0.658610</td>\n",
              "      <td>0.694268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.585492</td>\n",
              "      <td>0.728665</td>\n",
              "      <td>0.645714</td>\n",
              "      <td>0.719745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.673913</td>\n",
              "      <td>0.761488</td>\n",
              "      <td>0.630508</td>\n",
              "      <td>0.592357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.603175</td>\n",
              "      <td>0.741794</td>\n",
              "      <td>0.658960</td>\n",
              "      <td>0.726115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.610204</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.678322</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.580838</td>\n",
              "      <td>0.507853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.621951</td>\n",
              "      <td>0.691837</td>\n",
              "      <td>0.574648</td>\n",
              "      <td>0.534031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.722772</td>\n",
              "      <td>0.702041</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.382199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.637681</td>\n",
              "      <td>0.687755</td>\n",
              "      <td>0.534954</td>\n",
              "      <td>0.460733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.544910</td>\n",
              "      <td>0.614880</td>\n",
              "      <td>0.508380</td>\n",
              "      <td>0.476440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.688525</td>\n",
              "      <td>0.682713</td>\n",
              "      <td>0.536741</td>\n",
              "      <td>0.439791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.618182</td>\n",
              "      <td>0.667396</td>\n",
              "      <td>0.573034</td>\n",
              "      <td>0.534031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.635762</td>\n",
              "      <td>0.671772</td>\n",
              "      <td>0.561404</td>\n",
              "      <td>0.502618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.598726</td>\n",
              "      <td>0.649891</td>\n",
              "      <td>0.540230</td>\n",
              "      <td>0.492147</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0          LSTM 0.05  WMT  0.000000  0.612245  0.000000  0.000000\n",
              "1           GRU 0.05  WMT  0.724324  0.781633  0.714667  0.705263\n",
              "2       XGBoost 0.05  WMT  0.683060  0.748980  0.670241  0.657895\n",
              "3        Logreg 0.05  WMT  0.737589  0.748980  0.628399  0.547368\n",
              "4           SVM 0.05  WMT  0.710227  0.763265  0.683060  0.657895\n",
              "5     LSTM beta 0.05  WMT  0.652482  0.750547  0.617450  0.585987\n",
              "6      GRU beta 0.05  WMT  0.626437  0.752735  0.658610  0.694268\n",
              "7  XGBoost beta 0.05  WMT  0.585492  0.728665  0.645714  0.719745\n",
              "8   logreg beta 0.05  WMT  0.673913  0.761488  0.630508  0.592357\n",
              "9      svm beta 0.05  WMT  0.603175  0.741794  0.658960  0.726115\n",
              "0           LSTM 0.1  WMT  0.000000  0.610204  0.000000  0.000000\n",
              "1            GRU 0.1  WMT  0.678322  0.714286  0.580838  0.507853\n",
              "2        XGBoost 0.1  WMT  0.621951  0.691837  0.574648  0.534031\n",
              "3         Logreg 0.1  WMT  0.722772  0.702041  0.500000  0.382199\n",
              "4            SVM 0.1  WMT  0.637681  0.687755  0.534954  0.460733\n",
              "5      LSTM beta 0.1  WMT  0.544910  0.614880  0.508380  0.476440\n",
              "6       GRU beta 0.1  WMT  0.688525  0.682713  0.536741  0.439791\n",
              "7   XGBoost beta 0.1  WMT  0.618182  0.667396  0.573034  0.534031\n",
              "8    logreg beta 0.1  WMT  0.635762  0.671772  0.561404  0.502618\n",
              "9       svm beta 0.1  WMT  0.598726  0.649891  0.540230  0.492147"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQXUsFNmaAC7"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_logreg_beta.csv')"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9Ztn64XaAC7"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjUwAKiUaAC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce0306b8-7156-4b10-aede-ff03a723c55d"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"WMT\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.05\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6844 - accuracy: 0.5758 - val_loss: 0.6729 - val_accuracy: 0.6122\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6824 - accuracy: 0.5805 - val_loss: 0.6668 - val_accuracy: 0.6122\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6778 - accuracy: 0.5785 - val_loss: 0.6389 - val_accuracy: 0.6388\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6411 - accuracy: 0.6356 - val_loss: 0.5801 - val_accuracy: 0.7143\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6557 - accuracy: 0.6383 - val_loss: 0.5826 - val_accuracy: 0.7122\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6825 - accuracy: 0.5725 - val_loss: 0.6552 - val_accuracy: 0.6122\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6351 - accuracy: 0.6463 - val_loss: 0.5589 - val_accuracy: 0.7592\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5898 - accuracy: 0.6980 - val_loss: 0.5163 - val_accuracy: 0.7531\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5675 - accuracy: 0.7121 - val_loss: 0.5506 - val_accuracy: 0.7204\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5772 - accuracy: 0.7154 - val_loss: 0.5109 - val_accuracy: 0.7755\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.780088\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.799561\n",
            "[2]\tvalidation_0-auc:0.799588\n",
            "[3]\tvalidation_0-auc:0.810228\n",
            "[4]\tvalidation_0-auc:0.810009\n",
            "[5]\tvalidation_0-auc:0.809395\n",
            "[6]\tvalidation_0-auc:0.811246\n",
            "[7]\tvalidation_0-auc:0.811675\n",
            "[8]\tvalidation_0-auc:0.810272\n",
            "[9]\tvalidation_0-auc:0.812763\n",
            "[10]\tvalidation_0-auc:0.812614\n",
            "[11]\tvalidation_0-auc:0.812632\n",
            "[12]\tvalidation_0-auc:0.813561\n",
            "[13]\tvalidation_0-auc:0.814298\n",
            "[14]\tvalidation_0-auc:0.813018\n",
            "[15]\tvalidation_0-auc:0.812675\n",
            "[16]\tvalidation_0-auc:0.813535\n",
            "[17]\tvalidation_0-auc:0.812342\n",
            "[18]\tvalidation_0-auc:0.812553\n",
            "[19]\tvalidation_0-auc:0.813079\n",
            "[20]\tvalidation_0-auc:0.812465\n",
            "[21]\tvalidation_0-auc:0.813272\n",
            "[22]\tvalidation_0-auc:0.812746\n",
            "[23]\tvalidation_0-auc:0.81243\n",
            "[24]\tvalidation_0-auc:0.811044\n",
            "[25]\tvalidation_0-auc:0.810947\n",
            "[26]\tvalidation_0-auc:0.811193\n",
            "[27]\tvalidation_0-auc:0.812035\n",
            "[28]\tvalidation_0-auc:0.811386\n",
            "[29]\tvalidation_0-auc:0.811246\n",
            "[30]\tvalidation_0-auc:0.810763\n",
            "[31]\tvalidation_0-auc:0.811009\n",
            "[32]\tvalidation_0-auc:0.810281\n",
            "[33]\tvalidation_0-auc:0.809956\n",
            "[34]\tvalidation_0-auc:0.810281\n",
            "[35]\tvalidation_0-auc:0.810298\n",
            "[36]\tvalidation_0-auc:0.810298\n",
            "[37]\tvalidation_0-auc:0.809912\n",
            "[38]\tvalidation_0-auc:0.809982\n",
            "[39]\tvalidation_0-auc:0.810307\n",
            "[40]\tvalidation_0-auc:0.8105\n",
            "[41]\tvalidation_0-auc:0.810167\n",
            "[42]\tvalidation_0-auc:0.810061\n",
            "[43]\tvalidation_0-auc:0.810061\n",
            "[44]\tvalidation_0-auc:0.809149\n",
            "[45]\tvalidation_0-auc:0.808833\n",
            "[46]\tvalidation_0-auc:0.809193\n",
            "[47]\tvalidation_0-auc:0.808719\n",
            "[48]\tvalidation_0-auc:0.808842\n",
            "[49]\tvalidation_0-auc:0.809193\n",
            "[50]\tvalidation_0-auc:0.809184\n",
            "[51]\tvalidation_0-auc:0.808886\n",
            "[52]\tvalidation_0-auc:0.80857\n",
            "[53]\tvalidation_0-auc:0.80857\n",
            "[54]\tvalidation_0-auc:0.808518\n",
            "[55]\tvalidation_0-auc:0.807956\n",
            "[56]\tvalidation_0-auc:0.8075\n",
            "[57]\tvalidation_0-auc:0.806877\n",
            "[58]\tvalidation_0-auc:0.807193\n",
            "[59]\tvalidation_0-auc:0.807158\n",
            "[60]\tvalidation_0-auc:0.806807\n",
            "[61]\tvalidation_0-auc:0.806877\n",
            "[62]\tvalidation_0-auc:0.806965\n",
            "[63]\tvalidation_0-auc:0.806719\n",
            "Stopping. Best iteration:\n",
            "[13]\tvalidation_0-auc:0.814298\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6845 - accuracy: 0.5683 - val_loss: 0.6581 - val_accuracy: 0.6565\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6862 - accuracy: 0.5793 - val_loss: 0.6622 - val_accuracy: 0.6565\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6825 - accuracy: 0.5800 - val_loss: 0.6553 - val_accuracy: 0.6565\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6780 - accuracy: 0.5800 - val_loss: 0.6508 - val_accuracy: 0.6565\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6674 - accuracy: 0.5875 - val_loss: 0.6534 - val_accuracy: 0.6455\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6735 - accuracy: 0.5806 - val_loss: 0.6139 - val_accuracy: 0.6565\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6237 - accuracy: 0.6596 - val_loss: 0.5942 - val_accuracy: 0.7177\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6059 - accuracy: 0.6925 - val_loss: 0.5264 - val_accuracy: 0.7309\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5757 - accuracy: 0.6994 - val_loss: 0.5607 - val_accuracy: 0.7090\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5752 - accuracy: 0.7193 - val_loss: 0.6041 - val_accuracy: 0.6937\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.721826\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.750318\n",
            "[2]\tvalidation_0-auc:0.736369\n",
            "[3]\tvalidation_0-auc:0.73448\n",
            "[4]\tvalidation_0-auc:0.742144\n",
            "[5]\tvalidation_0-auc:0.749618\n",
            "[6]\tvalidation_0-auc:0.746635\n",
            "[7]\tvalidation_0-auc:0.759841\n",
            "[8]\tvalidation_0-auc:0.753471\n",
            "[9]\tvalidation_0-auc:0.760457\n",
            "[10]\tvalidation_0-auc:0.763503\n",
            "[11]\tvalidation_0-auc:0.767176\n",
            "[12]\tvalidation_0-auc:0.769671\n",
            "[13]\tvalidation_0-auc:0.765021\n",
            "[14]\tvalidation_0-auc:0.768079\n",
            "[15]\tvalidation_0-auc:0.768853\n",
            "[16]\tvalidation_0-auc:0.767803\n",
            "[17]\tvalidation_0-auc:0.769183\n",
            "[18]\tvalidation_0-auc:0.769926\n",
            "[19]\tvalidation_0-auc:0.769936\n",
            "[20]\tvalidation_0-auc:0.769735\n",
            "[21]\tvalidation_0-auc:0.772887\n",
            "[22]\tvalidation_0-auc:0.773301\n",
            "[23]\tvalidation_0-auc:0.773493\n",
            "[24]\tvalidation_0-auc:0.772134\n",
            "[25]\tvalidation_0-auc:0.77241\n",
            "[26]\tvalidation_0-auc:0.772155\n",
            "[27]\tvalidation_0-auc:0.771433\n",
            "[28]\tvalidation_0-auc:0.772495\n",
            "[29]\tvalidation_0-auc:0.774915\n",
            "[30]\tvalidation_0-auc:0.774745\n",
            "[31]\tvalidation_0-auc:0.776221\n",
            "[32]\tvalidation_0-auc:0.77603\n",
            "[33]\tvalidation_0-auc:0.776019\n",
            "[34]\tvalidation_0-auc:0.775786\n",
            "[35]\tvalidation_0-auc:0.775764\n",
            "[36]\tvalidation_0-auc:0.776306\n",
            "[37]\tvalidation_0-auc:0.775584\n",
            "[38]\tvalidation_0-auc:0.774586\n",
            "[39]\tvalidation_0-auc:0.77448\n",
            "[40]\tvalidation_0-auc:0.773928\n",
            "[41]\tvalidation_0-auc:0.774289\n",
            "[42]\tvalidation_0-auc:0.773949\n",
            "[43]\tvalidation_0-auc:0.775828\n",
            "[44]\tvalidation_0-auc:0.775446\n",
            "[45]\tvalidation_0-auc:0.776369\n",
            "[46]\tvalidation_0-auc:0.776285\n",
            "[47]\tvalidation_0-auc:0.77603\n",
            "[48]\tvalidation_0-auc:0.775679\n",
            "[49]\tvalidation_0-auc:0.776019\n",
            "[50]\tvalidation_0-auc:0.774342\n",
            "[51]\tvalidation_0-auc:0.775287\n",
            "[52]\tvalidation_0-auc:0.775096\n",
            "[53]\tvalidation_0-auc:0.775308\n",
            "[54]\tvalidation_0-auc:0.777229\n",
            "[55]\tvalidation_0-auc:0.777675\n",
            "[56]\tvalidation_0-auc:0.777208\n",
            "[57]\tvalidation_0-auc:0.77638\n",
            "[58]\tvalidation_0-auc:0.776868\n",
            "[59]\tvalidation_0-auc:0.776529\n",
            "[60]\tvalidation_0-auc:0.775637\n",
            "[61]\tvalidation_0-auc:0.777059\n",
            "[62]\tvalidation_0-auc:0.77672\n",
            "[63]\tvalidation_0-auc:0.775913\n",
            "[64]\tvalidation_0-auc:0.779352\n",
            "[65]\tvalidation_0-auc:0.778822\n",
            "[66]\tvalidation_0-auc:0.778758\n",
            "[67]\tvalidation_0-auc:0.778482\n",
            "[68]\tvalidation_0-auc:0.778758\n",
            "[69]\tvalidation_0-auc:0.778163\n",
            "[70]\tvalidation_0-auc:0.777972\n",
            "[71]\tvalidation_0-auc:0.7781\n",
            "[72]\tvalidation_0-auc:0.779544\n",
            "[73]\tvalidation_0-auc:0.779363\n",
            "[74]\tvalidation_0-auc:0.779448\n",
            "[75]\tvalidation_0-auc:0.780913\n",
            "[76]\tvalidation_0-auc:0.781783\n",
            "[77]\tvalidation_0-auc:0.780594\n",
            "[78]\tvalidation_0-auc:0.780807\n",
            "[79]\tvalidation_0-auc:0.780807\n",
            "[80]\tvalidation_0-auc:0.780552\n",
            "[81]\tvalidation_0-auc:0.780478\n",
            "[82]\tvalidation_0-auc:0.779904\n",
            "[83]\tvalidation_0-auc:0.778609\n",
            "[84]\tvalidation_0-auc:0.778206\n",
            "[85]\tvalidation_0-auc:0.778015\n",
            "[86]\tvalidation_0-auc:0.778079\n",
            "[87]\tvalidation_0-auc:0.777739\n",
            "[88]\tvalidation_0-auc:0.776996\n",
            "[89]\tvalidation_0-auc:0.776614\n",
            "[90]\tvalidation_0-auc:0.776614\n",
            "[91]\tvalidation_0-auc:0.77604\n",
            "[92]\tvalidation_0-auc:0.775701\n",
            "[93]\tvalidation_0-auc:0.775637\n",
            "[94]\tvalidation_0-auc:0.773832\n",
            "[95]\tvalidation_0-auc:0.773535\n",
            "[96]\tvalidation_0-auc:0.773132\n",
            "[97]\tvalidation_0-auc:0.772728\n",
            "[98]\tvalidation_0-auc:0.773811\n",
            "[99]\tvalidation_0-auc:0.773365\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+---------------------+--------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |        Recall       |      F1 score      |\n",
            "+-------------------+--------------------+---------------------+---------------------+--------------------+\n",
            "|     LSTM 0.05     | 0.7122448979591837 |  0.6033755274261603 |  0.7526315789473684 | 0.6697892271662763 |\n",
            "|      GRU 0.05     | 0.7755102040816326 |  0.7061855670103093 |  0.7210526315789474 | 0.7135416666666666 |\n",
            "|    XGBoost 0.05   | 0.7489795918367347 |  0.6830601092896175 |  0.6578947368421053 | 0.6702412868632708 |\n",
            "|    Logreg 0.05    | 0.7489795918367347 |  0.7375886524822695 |  0.5473684210526316 | 0.6283987915407855 |\n",
            "|      SVM 0.05     | 0.763265306122449  |  0.7102272727272727 |  0.6578947368421053 | 0.6830601092896175 |\n",
            "|   LSTM beta 0.05  | 0.6455142231947484 | 0.48427672955974843 | 0.49044585987261147 | 0.4873417721518987 |\n",
            "|   GRU beta 0.05   | 0.6936542669584245 |  0.5320754716981132 |  0.8980891719745223 | 0.6682464454976302 |\n",
            "| XGBoost beta 0.05 | 0.7286652078774617 |  0.5854922279792746 |  0.7197452229299363 | 0.6457142857142857 |\n",
            "|  logreg beta 0.05 | 0.7614879649890591 |  0.6739130434782609 |  0.5923566878980892 | 0.6305084745762711 |\n",
            "|   svm beta 0.05   | 0.7417943107221007 |  0.6031746031746031 |  0.7261146496815286 | 0.6589595375722542 |\n",
            "+-------------------+--------------------+---------------------+---------------------+--------------------+\n",
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6686 - accuracy: 0.6242 - val_loss: 0.6706 - val_accuracy: 0.6102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6643 - accuracy: 0.6235 - val_loss: 0.6664 - val_accuracy: 0.6102\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6548 - accuracy: 0.6315 - val_loss: 0.6318 - val_accuracy: 0.6306\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6246 - accuracy: 0.6517 - val_loss: 0.6217 - val_accuracy: 0.6571\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.6065 - accuracy: 0.6772 - val_loss: 0.6206 - val_accuracy: 0.7061\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6639 - accuracy: 0.6188 - val_loss: 0.6602 - val_accuracy: 0.6102\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6338 - accuracy: 0.6537 - val_loss: 0.6135 - val_accuracy: 0.6735\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6044 - accuracy: 0.6906 - val_loss: 0.6405 - val_accuracy: 0.6429\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5948 - accuracy: 0.6906 - val_loss: 0.6051 - val_accuracy: 0.7000\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.5668 - accuracy: 0.7221 - val_loss: 0.6099 - val_accuracy: 0.6837\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.692693\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.69208\n",
            "[2]\tvalidation_0-auc:0.703742\n",
            "[3]\tvalidation_0-auc:0.702884\n",
            "[4]\tvalidation_0-auc:0.704723\n",
            "[5]\tvalidation_0-auc:0.700397\n",
            "[6]\tvalidation_0-auc:0.70038\n",
            "[7]\tvalidation_0-auc:0.701895\n",
            "[8]\tvalidation_0-auc:0.70045\n",
            "[9]\tvalidation_0-auc:0.700257\n",
            "[10]\tvalidation_0-auc:0.698778\n",
            "[11]\tvalidation_0-auc:0.697272\n",
            "[12]\tvalidation_0-auc:0.69806\n",
            "[13]\tvalidation_0-auc:0.698603\n",
            "[14]\tvalidation_0-auc:0.699163\n",
            "[15]\tvalidation_0-auc:0.699329\n",
            "[16]\tvalidation_0-auc:0.699312\n",
            "[17]\tvalidation_0-auc:0.699618\n",
            "[18]\tvalidation_0-auc:0.698498\n",
            "[19]\tvalidation_0-auc:0.69841\n",
            "[20]\tvalidation_0-auc:0.697929\n",
            "[21]\tvalidation_0-auc:0.69806\n",
            "[22]\tvalidation_0-auc:0.698165\n",
            "[23]\tvalidation_0-auc:0.70073\n",
            "[24]\tvalidation_0-auc:0.700739\n",
            "[25]\tvalidation_0-auc:0.699898\n",
            "[26]\tvalidation_0-auc:0.700546\n",
            "[27]\tvalidation_0-auc:0.700695\n",
            "[28]\tvalidation_0-auc:0.701185\n",
            "[29]\tvalidation_0-auc:0.701002\n",
            "[30]\tvalidation_0-auc:0.700573\n",
            "[31]\tvalidation_0-auc:0.700818\n",
            "[32]\tvalidation_0-auc:0.699968\n",
            "[33]\tvalidation_0-auc:0.700196\n",
            "[34]\tvalidation_0-auc:0.699399\n",
            "[35]\tvalidation_0-auc:0.699487\n",
            "[36]\tvalidation_0-auc:0.699373\n",
            "[37]\tvalidation_0-auc:0.699084\n",
            "[38]\tvalidation_0-auc:0.69827\n",
            "[39]\tvalidation_0-auc:0.697228\n",
            "[40]\tvalidation_0-auc:0.697228\n",
            "[41]\tvalidation_0-auc:0.697823\n",
            "[42]\tvalidation_0-auc:0.697648\n",
            "[43]\tvalidation_0-auc:0.699434\n",
            "[44]\tvalidation_0-auc:0.699469\n",
            "[45]\tvalidation_0-auc:0.698979\n",
            "[46]\tvalidation_0-auc:0.700074\n",
            "[47]\tvalidation_0-auc:0.699776\n",
            "[48]\tvalidation_0-auc:0.699758\n",
            "[49]\tvalidation_0-auc:0.699671\n",
            "[50]\tvalidation_0-auc:0.699513\n",
            "[51]\tvalidation_0-auc:0.699828\n",
            "[52]\tvalidation_0-auc:0.69869\n",
            "[53]\tvalidation_0-auc:0.699356\n",
            "[54]\tvalidation_0-auc:0.699023\n",
            "Stopping. Best iteration:\n",
            "[4]\tvalidation_0-auc:0.704723\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6637 - accuracy: 0.6205 - val_loss: 0.6849 - val_accuracy: 0.6039\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6407 - accuracy: 0.6342 - val_loss: 0.6680 - val_accuracy: 0.5821\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5969 - accuracy: 0.7001 - val_loss: 0.6694 - val_accuracy: 0.5864\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5831 - accuracy: 0.7261 - val_loss: 0.6495 - val_accuracy: 0.6193\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5523 - accuracy: 0.7289 - val_loss: 0.7050 - val_accuracy: 0.6171\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6407 - accuracy: 0.6294 - val_loss: 0.6456 - val_accuracy: 0.6302\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5476 - accuracy: 0.7570 - val_loss: 0.6291 - val_accuracy: 0.6718\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5263 - accuracy: 0.7714 - val_loss: 0.5969 - val_accuracy: 0.6718\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5248 - accuracy: 0.7653 - val_loss: 0.6314 - val_accuracy: 0.6586\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4959 - accuracy: 0.7797 - val_loss: 0.6115 - val_accuracy: 0.6608\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.701571\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.709404\n",
            "[2]\tvalidation_0-auc:0.714453\n",
            "[3]\tvalidation_0-auc:0.714965\n",
            "[4]\tvalidation_0-auc:0.715634\n",
            "[5]\tvalidation_0-auc:0.711786\n",
            "[6]\tvalidation_0-auc:0.714551\n",
            "[7]\tvalidation_0-auc:0.713705\n",
            "[8]\tvalidation_0-auc:0.714965\n",
            "[9]\tvalidation_0-auc:0.714866\n",
            "[10]\tvalidation_0-auc:0.714335\n",
            "[11]\tvalidation_0-auc:0.71216\n",
            "[12]\tvalidation_0-auc:0.711648\n",
            "[13]\tvalidation_0-auc:0.71281\n",
            "[14]\tvalidation_0-auc:0.715969\n",
            "[15]\tvalidation_0-auc:0.716254\n",
            "[16]\tvalidation_0-auc:0.718144\n",
            "[17]\tvalidation_0-auc:0.717799\n",
            "[18]\tvalidation_0-auc:0.71653\n",
            "[19]\tvalidation_0-auc:0.718891\n",
            "[20]\tvalidation_0-auc:0.717425\n",
            "[21]\tvalidation_0-auc:0.718626\n",
            "[22]\tvalidation_0-auc:0.716657\n",
            "[23]\tvalidation_0-auc:0.715329\n",
            "[24]\tvalidation_0-auc:0.71715\n",
            "[25]\tvalidation_0-auc:0.715516\n",
            "[26]\tvalidation_0-auc:0.714788\n",
            "[27]\tvalidation_0-auc:0.71401\n",
            "[28]\tvalidation_0-auc:0.712819\n",
            "[29]\tvalidation_0-auc:0.713922\n",
            "[30]\tvalidation_0-auc:0.709365\n",
            "[31]\tvalidation_0-auc:0.709493\n",
            "[32]\tvalidation_0-auc:0.71031\n",
            "[33]\tvalidation_0-auc:0.710418\n",
            "[34]\tvalidation_0-auc:0.710674\n",
            "[35]\tvalidation_0-auc:0.70965\n",
            "[36]\tvalidation_0-auc:0.709296\n",
            "[37]\tvalidation_0-auc:0.71092\n",
            "[38]\tvalidation_0-auc:0.71031\n",
            "[39]\tvalidation_0-auc:0.710211\n",
            "[40]\tvalidation_0-auc:0.71213\n",
            "[41]\tvalidation_0-auc:0.710566\n",
            "[42]\tvalidation_0-auc:0.710448\n",
            "[43]\tvalidation_0-auc:0.710369\n",
            "[44]\tvalidation_0-auc:0.711756\n",
            "[45]\tvalidation_0-auc:0.711619\n",
            "[46]\tvalidation_0-auc:0.71152\n",
            "[47]\tvalidation_0-auc:0.711107\n",
            "[48]\tvalidation_0-auc:0.712111\n",
            "[49]\tvalidation_0-auc:0.712288\n",
            "[50]\tvalidation_0-auc:0.711855\n",
            "[51]\tvalidation_0-auc:0.712406\n",
            "[52]\tvalidation_0-auc:0.711934\n",
            "[53]\tvalidation_0-auc:0.713056\n",
            "[54]\tvalidation_0-auc:0.713065\n",
            "[55]\tvalidation_0-auc:0.712947\n",
            "[56]\tvalidation_0-auc:0.713046\n",
            "[57]\tvalidation_0-auc:0.713095\n",
            "[58]\tvalidation_0-auc:0.713597\n",
            "[59]\tvalidation_0-auc:0.713479\n",
            "[60]\tvalidation_0-auc:0.713892\n",
            "[61]\tvalidation_0-auc:0.714522\n",
            "[62]\tvalidation_0-auc:0.714719\n",
            "[63]\tvalidation_0-auc:0.714187\n",
            "[64]\tvalidation_0-auc:0.714404\n",
            "[65]\tvalidation_0-auc:0.713459\n",
            "[66]\tvalidation_0-auc:0.714246\n",
            "[67]\tvalidation_0-auc:0.713006\n",
            "[68]\tvalidation_0-auc:0.713557\n",
            "[69]\tvalidation_0-auc:0.71277\n",
            "Stopping. Best iteration:\n",
            "[19]\tvalidation_0-auc:0.718891\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |     Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.7061224489795919 | 0.6459627329192547 |  0.5445026178010471 |  0.5909090909090909 |\n",
            "|     GRU 0.1      | 0.6836734693877551 | 0.6071428571428571 |  0.5340314136125655 |  0.5682451253481894 |\n",
            "|   XGBoost 0.1    | 0.6918367346938775 | 0.6219512195121951 |  0.5340314136125655 |  0.5746478873239438 |\n",
            "|    Logreg 0.1    | 0.7020408163265306 | 0.7227722772277227 | 0.38219895287958117 |  0.5000000000000001 |\n",
            "|     SVM 0.1      | 0.6877551020408164 | 0.6376811594202898 |  0.4607329842931937 |  0.5349544072948328 |\n",
            "|  LSTM beta 0.1   | 0.6170678336980306 | 0.5579710144927537 |  0.4031413612565445 | 0.46808510638297873 |\n",
            "|   GRU beta 0.1   | 0.6608315098468271 | 0.608433734939759  |  0.5287958115183246 |  0.5658263305322129 |\n",
            "| XGBoost beta 0.1 | 0.6673960612691466 | 0.6181818181818182 |  0.5340314136125655 |  0.5730337078651685 |\n",
            "| logreg beta 0.1  | 0.6717724288840262 | 0.6357615894039735 |  0.5026178010471204 |  0.5614035087719298 |\n",
            "|   svm beta 0.1   | 0.649890590809628  | 0.5987261146496815 | 0.49214659685863876 |  0.5402298850574713 |\n",
            "+------------------+--------------------+--------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-St_8xjOaAC8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "c764a66b-30ba-4c8a-a425-de910c88d2f2"
      },
      "source": [
        "Result_purging.to_csv('WMT_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.603376</td>\n",
              "      <td>0.712245</td>\n",
              "      <td>0.669789</td>\n",
              "      <td>0.752632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.706186</td>\n",
              "      <td>0.775510</td>\n",
              "      <td>0.713542</td>\n",
              "      <td>0.721053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.683060</td>\n",
              "      <td>0.748980</td>\n",
              "      <td>0.670241</td>\n",
              "      <td>0.657895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.737589</td>\n",
              "      <td>0.748980</td>\n",
              "      <td>0.628399</td>\n",
              "      <td>0.547368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.710227</td>\n",
              "      <td>0.763265</td>\n",
              "      <td>0.683060</td>\n",
              "      <td>0.657895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.484277</td>\n",
              "      <td>0.645514</td>\n",
              "      <td>0.487342</td>\n",
              "      <td>0.490446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.532075</td>\n",
              "      <td>0.693654</td>\n",
              "      <td>0.668246</td>\n",
              "      <td>0.898089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.585492</td>\n",
              "      <td>0.728665</td>\n",
              "      <td>0.645714</td>\n",
              "      <td>0.719745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.673913</td>\n",
              "      <td>0.761488</td>\n",
              "      <td>0.630508</td>\n",
              "      <td>0.592357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.05</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.603175</td>\n",
              "      <td>0.741794</td>\n",
              "      <td>0.658960</td>\n",
              "      <td>0.726115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.645963</td>\n",
              "      <td>0.706122</td>\n",
              "      <td>0.590909</td>\n",
              "      <td>0.544503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.607143</td>\n",
              "      <td>0.683673</td>\n",
              "      <td>0.568245</td>\n",
              "      <td>0.534031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.621951</td>\n",
              "      <td>0.691837</td>\n",
              "      <td>0.574648</td>\n",
              "      <td>0.534031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.722772</td>\n",
              "      <td>0.702041</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.382199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.637681</td>\n",
              "      <td>0.687755</td>\n",
              "      <td>0.534954</td>\n",
              "      <td>0.460733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.557971</td>\n",
              "      <td>0.617068</td>\n",
              "      <td>0.468085</td>\n",
              "      <td>0.403141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.608434</td>\n",
              "      <td>0.660832</td>\n",
              "      <td>0.565826</td>\n",
              "      <td>0.528796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.618182</td>\n",
              "      <td>0.667396</td>\n",
              "      <td>0.573034</td>\n",
              "      <td>0.534031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.635762</td>\n",
              "      <td>0.671772</td>\n",
              "      <td>0.561404</td>\n",
              "      <td>0.502618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>WMT</td>\n",
              "      <td>0.598726</td>\n",
              "      <td>0.649891</td>\n",
              "      <td>0.540230</td>\n",
              "      <td>0.492147</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0          LSTM 0.05  WMT  0.603376  0.712245  0.669789  0.752632\n",
              "1           GRU 0.05  WMT  0.706186  0.775510  0.713542  0.721053\n",
              "2       XGBoost 0.05  WMT  0.683060  0.748980  0.670241  0.657895\n",
              "3        Logreg 0.05  WMT  0.737589  0.748980  0.628399  0.547368\n",
              "4           SVM 0.05  WMT  0.710227  0.763265  0.683060  0.657895\n",
              "5     LSTM beta 0.05  WMT  0.484277  0.645514  0.487342  0.490446\n",
              "6      GRU beta 0.05  WMT  0.532075  0.693654  0.668246  0.898089\n",
              "7  XGBoost beta 0.05  WMT  0.585492  0.728665  0.645714  0.719745\n",
              "8   logreg beta 0.05  WMT  0.673913  0.761488  0.630508  0.592357\n",
              "9      svm beta 0.05  WMT  0.603175  0.741794  0.658960  0.726115\n",
              "0           LSTM 0.1  WMT  0.645963  0.706122  0.590909  0.544503\n",
              "1            GRU 0.1  WMT  0.607143  0.683673  0.568245  0.534031\n",
              "2        XGBoost 0.1  WMT  0.621951  0.691837  0.574648  0.534031\n",
              "3         Logreg 0.1  WMT  0.722772  0.702041  0.500000  0.382199\n",
              "4            SVM 0.1  WMT  0.637681  0.687755  0.534954  0.460733\n",
              "5      LSTM beta 0.1  WMT  0.557971  0.617068  0.468085  0.403141\n",
              "6       GRU beta 0.1  WMT  0.608434  0.660832  0.565826  0.528796\n",
              "7   XGBoost beta 0.1  WMT  0.618182  0.667396  0.573034  0.534031\n",
              "8    logreg beta 0.1  WMT  0.635762  0.671772  0.561404  0.502618\n",
              "9       svm beta 0.1  WMT  0.598726  0.649891  0.540230  0.492147"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t5nYxWsaAC8"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WMT_logreg_beta_p.csv')"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3kQ1guHaT_S"
      },
      "source": [
        "## WU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZVmjjLlaT_S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "outputId": "d5518968-743e-4071-8107-51db45d132a6"
      },
      "source": [
        "dfs = pd.read_csv(\"WU.csv\")\n",
        "dfs = dfs[::-1].reset_index()\n",
        "dfs"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>&lt;TICKER&gt;</th>\n",
              "      <th>&lt;PER&gt;</th>\n",
              "      <th>&lt;DATE&gt;</th>\n",
              "      <th>&lt;TIME&gt;</th>\n",
              "      <th>&lt;OPEN&gt;</th>\n",
              "      <th>&lt;HIGH&gt;</th>\n",
              "      <th>&lt;LOW&gt;</th>\n",
              "      <th>&lt;CLOSE&gt;</th>\n",
              "      <th>&lt;VOL&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2768</td>\n",
              "      <td>US1.WU</td>\n",
              "      <td>D</td>\n",
              "      <td>20211001</td>\n",
              "      <td>0</td>\n",
              "      <td>20.50</td>\n",
              "      <td>20.675</td>\n",
              "      <td>20.160</td>\n",
              "      <td>20.530</td>\n",
              "      <td>111302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2767</td>\n",
              "      <td>US1.WU</td>\n",
              "      <td>D</td>\n",
              "      <td>20210930</td>\n",
              "      <td>0</td>\n",
              "      <td>20.83</td>\n",
              "      <td>20.870</td>\n",
              "      <td>20.215</td>\n",
              "      <td>20.215</td>\n",
              "      <td>155988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2766</td>\n",
              "      <td>US1.WU</td>\n",
              "      <td>D</td>\n",
              "      <td>20210929</td>\n",
              "      <td>0</td>\n",
              "      <td>20.86</td>\n",
              "      <td>20.950</td>\n",
              "      <td>20.660</td>\n",
              "      <td>20.790</td>\n",
              "      <td>65787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2765</td>\n",
              "      <td>US1.WU</td>\n",
              "      <td>D</td>\n",
              "      <td>20210928</td>\n",
              "      <td>0</td>\n",
              "      <td>20.82</td>\n",
              "      <td>21.130</td>\n",
              "      <td>20.755</td>\n",
              "      <td>20.785</td>\n",
              "      <td>160902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2764</td>\n",
              "      <td>US1.WU</td>\n",
              "      <td>D</td>\n",
              "      <td>20210927</td>\n",
              "      <td>0</td>\n",
              "      <td>20.94</td>\n",
              "      <td>20.940</td>\n",
              "      <td>20.450</td>\n",
              "      <td>20.660</td>\n",
              "      <td>157910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>4</td>\n",
              "      <td>US1.WU</td>\n",
              "      <td>D</td>\n",
              "      <td>20101008</td>\n",
              "      <td>0</td>\n",
              "      <td>17.62</td>\n",
              "      <td>17.780</td>\n",
              "      <td>17.500</td>\n",
              "      <td>17.730</td>\n",
              "      <td>2250995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>3</td>\n",
              "      <td>US1.WU</td>\n",
              "      <td>D</td>\n",
              "      <td>20101007</td>\n",
              "      <td>0</td>\n",
              "      <td>17.77</td>\n",
              "      <td>17.820</td>\n",
              "      <td>17.540</td>\n",
              "      <td>17.590</td>\n",
              "      <td>3231681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>2</td>\n",
              "      <td>US1.WU</td>\n",
              "      <td>D</td>\n",
              "      <td>20101006</td>\n",
              "      <td>0</td>\n",
              "      <td>17.99</td>\n",
              "      <td>18.000</td>\n",
              "      <td>17.570</td>\n",
              "      <td>17.670</td>\n",
              "      <td>3513358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>1</td>\n",
              "      <td>US1.WU</td>\n",
              "      <td>D</td>\n",
              "      <td>20101005</td>\n",
              "      <td>0</td>\n",
              "      <td>17.56</td>\n",
              "      <td>17.990</td>\n",
              "      <td>17.400</td>\n",
              "      <td>17.950</td>\n",
              "      <td>5231417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>0</td>\n",
              "      <td>US1.WU</td>\n",
              "      <td>D</td>\n",
              "      <td>20101004</td>\n",
              "      <td>0</td>\n",
              "      <td>17.62</td>\n",
              "      <td>17.790</td>\n",
              "      <td>17.360</td>\n",
              "      <td>17.410</td>\n",
              "      <td>4081478</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2769 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index <TICKER> <PER>    <DATE>  ...  <HIGH>   <LOW>  <CLOSE>    <VOL>\n",
              "0      2768   US1.WU     D  20211001  ...  20.675  20.160   20.530   111302\n",
              "1      2767   US1.WU     D  20210930  ...  20.870  20.215   20.215   155988\n",
              "2      2766   US1.WU     D  20210929  ...  20.950  20.660   20.790    65787\n",
              "3      2765   US1.WU     D  20210928  ...  21.130  20.755   20.785   160902\n",
              "4      2764   US1.WU     D  20210927  ...  20.940  20.450   20.660   157910\n",
              "...     ...      ...   ...       ...  ...     ...     ...      ...      ...\n",
              "2764      4   US1.WU     D  20101008  ...  17.780  17.500   17.730  2250995\n",
              "2765      3   US1.WU     D  20101007  ...  17.820  17.540   17.590  3231681\n",
              "2766      2   US1.WU     D  20101006  ...  18.000  17.570   17.670  3513358\n",
              "2767      1   US1.WU     D  20101005  ...  17.990  17.400   17.950  5231417\n",
              "2768      0   US1.WU     D  20101004  ...  17.790  17.360   17.410  4081478\n",
              "\n",
              "[2769 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9P6ZUYWaT_S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f716b0f3-7c90-4e5d-86f3-1dfb4cf1d87e"
      },
      "source": [
        "visualize(dfs, '<CLOSE>', '<DATE>', 0.15, 500, 2000)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"57a4a394-f37e-41d4-93dd-02dbc68d616f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"57a4a394-f37e-41d4-93dd-02dbc68d616f\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '57a4a394-f37e-41d4-93dd-02dbc68d616f',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [23.16, 23.32, 23.3, 22.89, 22.77, 23.22, 23.18, 23.05, 22.445, 22.33, 21.91, 22.4, 22.5, 22.7, 22.8, 22.86, 22.76, 23.35, 23.75, 23.72, 23.22, 23.31, 22.85, 22.59, 22.26, 21.93, 22.12, 22.12, 21.96, 21.85, 21.68, 21.4, 21.9, 21.76, 21.39, 21.38, 21.1, 20.88, 20.75, 21.41, 21.29, 21.59, 21.605, 21.4, 21.38, 21.06, 21.83, 21.1, 20.99, 20.9, 21.13, 21.07, 20.85, 20.88, 20.66, 20.4, 20.4, 20.51, 20.395, 20.53, 20.755, 20.495, 20.42, 20.54, 20.64, 20.36, 20.35, 20.3, 20.17, 19.98, 19.88, 19.68, 19.61, 19.94, 19.94, 20.09, 19.88, 19.71, 19.57, 20.05, 20.09, 20.06, 20.14, 19.89, 20.01, 20.16, 19.96, 19.85, 20.38, 19.39, 19.39, 19.32, 19.36, 19.33, 19.35, 19.48, 19.47, 19.41, 19.22, 19.32, 19.6, 19.44, 19.32, 19.06, 19.4, 19.2, 19.21, 19.2, 19.225, 19.29, 19.275, 19.32, 19.42, 19.4, 19.69, 19.19, 19.44, 19.4, 19.27, 19.29, 19.305, 19.24, 19.17, 19.12, 19.16, 19.37, 19.38, 19.47, 19.4, 19.02, 19.07, 18.93, 18.655, 18.46, 18.62, 18.575, 18.57, 18.21, 18.25, 18.32, 18.0, 18.07, 18.18, 18.38, 18.19, 18.4, 18.19, 18.16, 17.925, 17.71, 18.14, 18.07, 18.23, 18.22, 17.87, 17.8, 17.73, 17.71, 17.83, 17.63, 17.85, 17.85, 17.8, 17.75, 17.96, 18.0, 17.86, 18.015, 18.32, 18.6, 18.51, 18.41, 18.47, 18.25, 18.31, 18.13, 18.17, 18.22, 18.175, 18.07, 18.1, 18.26, 17.92, 17.75, 17.7, 17.5, 17.52, 17.41, 17.33, 17.07, 17.02, 17.09, 16.87, 17.14, 17.065, 17.035, 17.08, 17.01, 16.55, 16.81, 17.02, 17.49, 17.65, 17.83, 18.175, 18.69, 18.53, 18.37, 18.29, 18.15, 18.48, 18.36, 18.72, 18.71, 18.68, 18.85, 18.64, 18.525, 18.46, 18.43, 18.62, 18.71, 18.89, 18.76, 18.865, 18.89, 18.57, 18.82, 18.97, 18.49, 18.48, 18.45, 17.92, 18.06, 18.04, 17.87, 17.58, 17.62, 17.93, 17.66, 17.93, 18.015, 18.18, 18.34, 18.42, 18.39, 18.04, 17.92, 17.67, 17.77, 18.17, 18.02, 18.175, 18.6, 19.085, 18.905, 19.01, 19.07, 18.94, 19.07, 18.76, 18.78, 19.03, 18.72, 18.6, 18.66, 18.52, 18.74, 18.75, 19.29, 19.16, 19.14, 18.89, 18.96, 18.71, 18.86, 18.93, 18.82, 18.89, 18.82, 18.84, 18.83, 18.89, 18.965, 18.89, 18.965, 18.92, 18.86, 18.91, 18.81, 18.745, 18.83, 18.92, 19.14, 19.17, 19.08, 19.4, 20.58, 20.15, 20.15, 20.11, 20.26, 20.41, 20.38, 20.22, 20.22, 20.28, 20.28, 20.16, 20.405, 20.27, 20.36, 20.4, 20.3, 20.45, 20.3, 20.25, 20.31, 20.11, 20.43, 20.34, 20.31, 20.31, 20.625, 20.79, 21.15, 21.08, 21.19, 21.0, 20.75, 20.7, 20.67, 20.97, 21.11, 20.91, 20.81, 20.78, 20.945, 20.625, 20.36, 20.065, 19.89, 19.94, 19.7, 19.98, 20.06, 20.11, 20.2, 20.15, 20.09, 20.11, 20.02, 20.03, 19.75, 19.88, 20.05, 19.91, 19.72, 19.78, 19.6, 19.4, 19.255, 19.91, 19.745, 19.34, 19.585, 19.155, 19.02, 19.135, 19.08, 19.41, 19.38, 19.41, 19.145, 18.89, 19.0, 18.91, 19.025, 18.71, 18.65, 19.07, 19.1, 18.69, 18.92, 19.23, 19.23, 18.88, 19.205, 19.0, 19.35, 19.67, 19.75, 19.64, 20.06, 20.01, 20.33, 20.385, 20.135, 20.155, 19.665, 19.95, 19.9, 19.9, 19.86, 19.655, 19.82, 19.98, 20.305, 20.25, 20.1, 20.09, 20.24, 20.26, 20.2, 20.3, 19.76, 19.77, 19.35, 18.82, 19.49, 19.8, 19.56, 20.28, 20.69, 20.79, 20.61, 20.83, 21.17, 20.79, 20.525, 20.735, 20.55, 20.25, 20.21, 20.23, 20.6101, 21.11, 21.18, 21.235, 21.045, 21.49, 20.56, 19.46, 19.075, 19.08, 19.015, 19.06, 18.8, 18.88, 19.08, 19.26, 19.48, 19.6, 19.57, 19.47, 19.35, 19.35, 19.49, 19.53, 19.39, 19.52, 19.4, 19.55, 19.71, 19.71, 19.68, 19.7, 19.69, 19.48, 19.65, 19.54, 19.71, 19.69, 19.67, 19.78, 19.61, 19.645, 19.53, 20.13, 20.08, 19.76, 19.16, 19.44, 19.5, 20.08, 19.98, 19.86, 19.71, 20.26, 20.24, 20.025, 20.1, 20.21, 20.305, 19.86, 19.795, 19.71, 19.75, 19.78, 19.75, 19.72, 19.59, 19.52, 19.51, 19.66, 19.42, 19.39, 19.41, 19.19, 19.06, 19.0, 18.73, 19.02, 18.98, 18.915, 18.96, 19.05, 19.11, 18.9, 18.94, 19.115, 19.225, 18.89, 18.7, 18.54, 18.51, 18.685, 18.78, 18.92, 18.94, 19.08, 19.03, 19.01, 18.97, 18.97, 19.025, 18.87, 18.87, 19.015, 19.39, 19.235, 19.14, 18.9, 18.9101, 19.27, 19.3, 19.37, 19.71, 19.58, 19.5, 19.73, 19.74, 19.655, 19.71, 19.49, 19.48, 19.17, 19.09, 19.085, 19.17, 18.95, 18.995, 19.025, 18.94, 19.02, 18.8, 18.71, 18.655, 18.485, 18.69, 19.15, 19.05, 18.86, 18.97, 18.88, 19.01, 19.16, 18.79, 18.84, 18.96, 19.1, 19.08, 18.99, 19.24, 19.46, 19.265, 19.0, 18.94, 19.13, 19.215, 19.4, 19.275, 19.44, 19.02, 19.17, 19.0625, 19.31, 19.27, 18.83, 19.13, 19.01, 18.86, 18.905, 19.15, 19.41, 19.29, 19.46, 19.65, 19.49, 19.54, 19.38, 19.39, 19.27, 20.14, 19.975, 19.86, 19.985, 20.1, 20.13, 20.085, 19.765, 19.98, 19.77, 19.675, 19.495, 19.35, 19.42, 19.52, 19.43, 19.36, 19.59, 19.67, 19.72, 19.78, 20.355, 20.5, 20.249, 20.15, 20.03, 20.25, 20.14, 20.135, 20.09, 20.21, 20.41, 20.21, 20.18, 20.27, 19.575, 19.68, 19.68, 19.82, 19.91, 19.91, 19.965, 19.92, 19.91, 19.65, 20.13, 20.18, 19.93, 20.0, 19.82, 19.815, 19.56, 19.59, 19.69, 19.54, 19.75, 20.38, 19.9, 19.83, 19.83, 19.97, 19.72, 19.54, 19.579, 19.6, 19.59, 19.59, 20.07, 20.4, 20.72, 20.98, 21.13, 21.85, 22.01, 22.17, 22.12, 22.315, 22.23, 22.545, 22.56, 22.305, 22.42, 21.96, 21.73, 21.84, 21.74, 22.04, 21.8475, 21.79, 21.67, 21.7, 21.62, 21.255, 21.34, 21.53, 21.79, 21.79, 22.0, 22.14, 22.14, 21.635, 20.89, 20.48, 20.73, 21.03, 21.16, 21.2, 21.48, 21.465, 21.26, 21.05, 20.49, 20.62, 20.56, 20.58, 20.58, 20.43, 19.95, 19.87, 20.43, 20.36, 19.64, 19.77, 19.9, 19.74, 20.07, 19.94, 19.895, 19.8, 20.355, 20.33, 20.005, 20.0, 20.01, 19.77, 19.84, 20.125, 19.93, 20.06, 19.91, 20.29, 20.42, 20.405, 20.3, 20.33, 20.62, 20.83, 20.485, 20.5, 20.25, 20.27, 20.29, 20.31, 20.2, 19.83, 20.0, 20.33, 20.32, 20.3, 20.58, 20.96, 20.76, 21.31, 21.775, 21.76, 21.61, 21.42, 21.51, 21.56, 21.46, 21.63, 21.48, 21.45, 21.36, 21.41, 21.19, 21.07, 21.03, 20.83, 21.02, 20.97, 21.09, 20.94, 20.96, 20.85, 21.0, 20.695, 19.92, 19.63, 19.955, 19.995, 20.19, 20.39, 20.4, 20.2, 20.26, 20.17, 20.18, 20.095, 20.12, 20.13, 20.03, 19.88, 19.97, 19.59, 19.56, 19.0, 19.18, 19.15, 19.35, 19.175, 18.81, 18.71, 18.11, 18.62, 19.37, 19.09, 18.94, 18.985, 19.02, 18.805, 18.86, 18.93, 19.07, 19.36, 19.65, 19.77, 19.61, 19.59, 19.43, 19.48, 19.46, 19.45, 19.47, 19.265, 19.4, 19.26, 19.0, 19.245, 18.89, 19.07, 19.05, 19.23, 19.17, 19.26, 19.15, 19.36, 19.05, 19.01, 18.85, 19.005, 19.83, 20.11, 20.0, 20.12, 20.53, 20.23, 19.98, 20.07, 20.08, 20.185, 20.12, 20.11, 19.99, 19.96, 20.06, 19.88, 19.72, 19.6, 19.45, 19.705, 19.295, 19.39, 19.48, 19.28, 19.33, 19.135, 18.85, 18.79, 18.8, 18.89, 19.1, 19.15, 19.0, 18.64, 18.48, 19.09, 19.1, 18.76, 19.22, 19.105, 19.505, 19.26, 19.1, 18.465, 18.63, 18.25, 18.46, 18.35, 18.13, 17.95, 18.37, 18.21, 18.205, 18.48, 18.395, 17.55, 16.88, 17.01, 17.01, 17.405, 17.57, 17.55, 17.28, 17.08, 17.63, 17.85, 17.11, 16.86, 17.29, 16.93, 17.41, 16.92, 16.51, 16.445, 16.63, 17.13, 16.6, 17.04, 16.88, 16.905, 17.05, 17.155, 17.56, 17.62, 17.92, 18.24, 18.43, 18.245, 18.5, 18.51, 18.26, 18.01, 17.815, 18.13, 18.51, 18.09, 18.15, 18.28, 18.9, 18.88, 18.92, 19.03, 18.9, 18.66, 18.94, 19.21, 18.88, 18.78, 18.8, 18.82, 18.95, 19.07, 19.07, 19.5, 19.03, 18.73, 18.54, 18.78, 19.28, 19.42, 19.43, 19.87, 19.58, 19.27, 19.42, 19.39, 19.24, 19.0, 19.59, 19.25, 19.65, 19.78, 19.65, 19.075, 19.04, 19.08, 19.09, 18.945, 18.71, 18.94, 19.13, 19.14, 19.195, 18.81, 18.615, 18.74, 18.545, 18.205, 18.35, 18.1, 17.86, 18.22, 18.26, 18.45, 18.5, 18.77, 18.56, 18.89, 18.94, 18.67, 18.12, 18.3, 18.42, 18.225, 18.6, 18.25, 18.46, 18.05, 17.72, 18.44, 18.55, 18.6, 18.3, 17.81, 17.78, 18.75, 19.5, 19.95, 20.37, 20.42, 20.35, 20.37, 20.42, 20.41, 20.55, 20.39, 20.38, 20.33, 20.18, 20.21, 20.25, 19.02, 19.035, 18.63, 18.345, 18.66, 18.91, 19.1, 19.09, 19.205, 19.2, 19.27, 18.96, 19.17, 19.085, 19.035, 18.91, 18.87, 19.09, 18.955, 18.98, 20.39, 20.32, 20.355, 20.61, 20.87, 21.135, 21.425, 21.47, 21.28, 21.28, 21.14, 21.3, 21.2, 21.46, 21.865, 21.79, 21.57, 21.515, 21.75, 21.9, 22.235, 22.122, 21.95, 21.95, 21.95, 22.19, 22.24, 22.57, 22.51, 22.49, 22.43, 21.98, 21.715, 21.96, 21.86, 21.75, 21.75, 21.93, 21.71, 21.86, 20.96, 21.18, 21.25, 20.29, 20.38, 20.46, 20.36, 20.58, 20.97, 20.815, 20.7, 20.72, 20.62, 20.75, 20.78, 20.66, 20.51, 20.63, 20.62, 20.565, 20.07, 20.24, 20.91, 20.56, 20.79, 20.42, 19.7, 19.39, 19.39, 19.71, 19.73, 19.21, 19.25, 19.34, 19.3, 19.605, 19.52, 19.59, 19.28, 19.38, 19.565, 19.43, 19.485, 19.65, 19.71, 20.05, 19.53, 19.6, 19.6, 19.325, 19.26, 19.45, 19.26, 19.3, 19.3, 19.01, 18.3, 18.29, 18.405, 17.61, 17.73, 17.89, 17.68, 17.675, 17.41, 16.99, 17.13, 16.97, 17.38, 17.72, 17.61, 17.6, 17.16, 17.01, 17.21, 17.01, 17.27, 17.81, 17.83, 17.93, 17.9, 17.565, 17.72, 17.77, 17.93, 17.91, 18.02, 18.045, 18.09, 18.07, 18.11, 18.05, 17.99, 17.78, 17.47, 16.96, 17.07, 17.03, 17.78, 17.85, 18.335, 18.28, 18.5, 18.48, 18.47, 18.27, 18.1501, 18.58, 18.28, 18.3, 18.23, 18.23, 18.28, 18.22, 18.13, 18.21, 18.12, 18.12, 18.39, 18.315, 18.28, 17.88, 17.945, 17.54, 17.19, 17.1, 16.96, 16.7, 16.42, 16.47, 16.345, 16.47, 16.41, 16.22, 16.55, 16.15, 15.9, 15.83, 15.845, 15.7, 15.51, 15.675, 15.8, 16.33, 15.98, 16.24, 16.23, 16.09, 15.92, 16.05, 16.17, 16.349, 16.24, 16.43, 16.27, 16.49, 16.73, 16.84, 16.95, 16.73, 16.83, 16.77, 16.88, 17.23, 17.07, 17.31, 17.54, 17.48, 17.49, 17.46, 17.47, 17.495, 17.52, 17.525, 17.48, 17.49, 17.74, 17.68, 17.55, 17.54, 17.33, 17.35, 17.4, 17.105, 17.21, 16.99, 16.9, 17.01, 16.91, 17.03, 16.79, 17.46, 17.69, 17.51, 17.57, 17.52, 17.71, 17.61, 17.59, 17.52, 17.51, 17.33, 17.57, 17.27, 17.46, 17.74, 17.29, 17.38, 17.345, 17.455, 17.39, 17.3, 17.39, 17.33, 17.24, 17.01, 16.78, 16.71, 16.7, 16.51, 16.35, 16.4, 16.27, 15.99, 16.08, 15.99, 16.34, 16.525, 16.45, 16.25, 16.24, 16.25, 15.96, 16.015, 16.18, 16.03, 15.88, 15.77, 15.765, 15.74, 15.61, 15.72, 16.04, 15.89, 16.04, 16.0, 16.38, 16.499, 16.35, 16.26, 16.26, 16.12, 16.27, 16.3, 15.84, 15.87, 15.66, 15.62, 15.555, 15.82, 15.74, 15.65, 15.48, 15.26, 16.04, 15.74, 15.83, 15.8, 16.06, 16.39, 16.115, 16.01, 16.39, 16.52, 16.78, 16.765, 16.37, 16.21, 16.33, 16.01, 16.36, 16.45, 16.19, 16.5, 16.32, 16.43, 16.22, 15.92, 16.06, 16.65, 16.84, 16.75, 16.71, 16.8, 16.86, 16.75, 16.6, 16.74, 16.61, 16.39, 16.13, 16.35, 16.03, 15.99, 16.05, 16.22, 16.27, 16.12, 16.34, 15.88, 15.635, 15.63, 15.3, 15.22, 15.22, 15.15, 15.39, 15.66, 15.54, 15.91, 15.51, 15.74, 16.13, 16.31, 16.16, 16.45, 16.71, 16.7, 16.51, 16.39, 16.76, 17.05, 17.41, 17.43, 17.05, 16.92, 17.04, 17.24, 17.235, 17.21, 17.25, 17.14, 17.08, 17.02, 16.89, 17.0, 16.48, 16.46, 16.38, 16.77, 16.57, 16.82, 16.7, 16.76, 16.44, 16.69, 16.86, 16.69, 16.71, 16.82, 16.8, 16.68, 16.85, 16.65, 16.4, 16.59, 16.73, 16.69, 17.45, 17.5, 17.48, 17.255, 17.19, 17.4, 17.24, 17.27, 17.23, 17.47, 17.02, 16.845, 19.24, 19.3, 19.3, 19.38, 19.259]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('57a4a394-f37e-41d4-93dd-02dbc68d616f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"54fb2f3d-82d3-4873-8d15-5e324d67f7c0\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"54fb2f3d-82d3-4873-8d15-5e324d67f7c0\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '54fb2f3d-82d3-4873-8d15-5e324d67f7c0',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2016\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2015\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2014\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\"], \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('54fb2f3d-82d3-4873-8d15-5e324d67f7c0');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Myc10MEbaT_S"
      },
      "source": [
        "### Cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJpJDHsGaT_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b2a1503-8670-4cd9-83a9-4d79ad187b48"
      },
      "source": [
        "col_name = '<CLOSE>'\n",
        "threshholds = [.1, .2, .15]\n",
        "Result_cross = pd.DataFrame()\n",
        "Result_purging = pd.DataFrame()\n",
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=False)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"WU\", step_sizes=4, th=th)\n",
        "  Result_cross = Result_cross.append(result)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6835 - accuracy: 0.5779 - val_loss: 0.6447 - val_accuracy: 0.7306\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6794 - accuracy: 0.5879 - val_loss: 0.6169 - val_accuracy: 0.7306\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6703 - accuracy: 0.5886 - val_loss: 0.6526 - val_accuracy: 0.7306\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6704 - accuracy: 0.5940 - val_loss: 0.6364 - val_accuracy: 0.7306\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6520 - accuracy: 0.6289 - val_loss: 0.5490 - val_accuracy: 0.7286\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6797 - accuracy: 0.5852 - val_loss: 0.6497 - val_accuracy: 0.7694\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6433 - accuracy: 0.6389 - val_loss: 0.5618 - val_accuracy: 0.7327\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6037 - accuracy: 0.6698 - val_loss: 0.5523 - val_accuracy: 0.7163\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5932 - accuracy: 0.6765 - val_loss: 0.4846 - val_accuracy: 0.7776\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.5905 - accuracy: 0.6705 - val_loss: 0.5475 - val_accuracy: 0.7122\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.781615\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.777499\n",
            "[2]\tvalidation_0-auc:0.779086\n",
            "[3]\tvalidation_0-auc:0.783361\n",
            "[4]\tvalidation_0-auc:0.782483\n",
            "[5]\tvalidation_0-auc:0.782218\n",
            "[6]\tvalidation_0-auc:0.784429\n",
            "[7]\tvalidation_0-auc:0.782916\n",
            "[8]\tvalidation_0-auc:0.782652\n",
            "[9]\tvalidation_0-auc:0.780049\n",
            "[10]\tvalidation_0-auc:0.780695\n",
            "[11]\tvalidation_0-auc:0.779943\n",
            "[12]\tvalidation_0-auc:0.782123\n",
            "[13]\tvalidation_0-auc:0.783477\n",
            "[14]\tvalidation_0-auc:0.78553\n",
            "[15]\tvalidation_0-auc:0.783985\n",
            "[16]\tvalidation_0-auc:0.784556\n",
            "[17]\tvalidation_0-auc:0.784313\n",
            "[18]\tvalidation_0-auc:0.7859\n",
            "[19]\tvalidation_0-auc:0.785435\n",
            "[20]\tvalidation_0-auc:0.78626\n",
            "[21]\tvalidation_0-auc:0.785202\n",
            "[22]\tvalidation_0-auc:0.784345\n",
            "[23]\tvalidation_0-auc:0.783509\n",
            "[24]\tvalidation_0-auc:0.783932\n",
            "[25]\tvalidation_0-auc:0.784112\n",
            "[26]\tvalidation_0-auc:0.785943\n",
            "[27]\tvalidation_0-auc:0.787085\n",
            "[28]\tvalidation_0-auc:0.786927\n",
            "[29]\tvalidation_0-auc:0.786482\n",
            "[30]\tvalidation_0-auc:0.786112\n",
            "[31]\tvalidation_0-auc:0.786472\n",
            "[32]\tvalidation_0-auc:0.786355\n",
            "[33]\tvalidation_0-auc:0.786927\n",
            "[34]\tvalidation_0-auc:0.786863\n",
            "[35]\tvalidation_0-auc:0.786249\n",
            "[36]\tvalidation_0-auc:0.786175\n",
            "[37]\tvalidation_0-auc:0.785519\n",
            "[38]\tvalidation_0-auc:0.78607\n",
            "[39]\tvalidation_0-auc:0.786958\n",
            "[40]\tvalidation_0-auc:0.786958\n",
            "[41]\tvalidation_0-auc:0.78717\n",
            "[42]\tvalidation_0-auc:0.787128\n",
            "[43]\tvalidation_0-auc:0.787445\n",
            "[44]\tvalidation_0-auc:0.786091\n",
            "[45]\tvalidation_0-auc:0.785985\n",
            "[46]\tvalidation_0-auc:0.786101\n",
            "[47]\tvalidation_0-auc:0.785974\n",
            "[48]\tvalidation_0-auc:0.785033\n",
            "[49]\tvalidation_0-auc:0.78571\n",
            "[50]\tvalidation_0-auc:0.78571\n",
            "[51]\tvalidation_0-auc:0.784906\n",
            "[52]\tvalidation_0-auc:0.783996\n",
            "[53]\tvalidation_0-auc:0.783594\n",
            "[54]\tvalidation_0-auc:0.784334\n",
            "[55]\tvalidation_0-auc:0.784355\n",
            "[56]\tvalidation_0-auc:0.783869\n",
            "[57]\tvalidation_0-auc:0.782938\n",
            "[58]\tvalidation_0-auc:0.781668\n",
            "[59]\tvalidation_0-auc:0.780948\n",
            "[60]\tvalidation_0-auc:0.780906\n",
            "[61]\tvalidation_0-auc:0.780822\n",
            "[62]\tvalidation_0-auc:0.780864\n",
            "[63]\tvalidation_0-auc:0.780885\n",
            "[64]\tvalidation_0-auc:0.780589\n",
            "[65]\tvalidation_0-auc:0.779446\n",
            "[66]\tvalidation_0-auc:0.77915\n",
            "[67]\tvalidation_0-auc:0.779573\n",
            "[68]\tvalidation_0-auc:0.777806\n",
            "[69]\tvalidation_0-auc:0.777002\n",
            "[70]\tvalidation_0-auc:0.777679\n",
            "[71]\tvalidation_0-auc:0.777996\n",
            "[72]\tvalidation_0-auc:0.777869\n",
            "[73]\tvalidation_0-auc:0.777023\n",
            "[74]\tvalidation_0-auc:0.775732\n",
            "[75]\tvalidation_0-auc:0.773976\n",
            "[76]\tvalidation_0-auc:0.774187\n",
            "[77]\tvalidation_0-auc:0.773849\n",
            "[78]\tvalidation_0-auc:0.774505\n",
            "[79]\tvalidation_0-auc:0.77569\n",
            "[80]\tvalidation_0-auc:0.776981\n",
            "[81]\tvalidation_0-auc:0.777743\n",
            "[82]\tvalidation_0-auc:0.77751\n",
            "[83]\tvalidation_0-auc:0.777869\n",
            "[84]\tvalidation_0-auc:0.778208\n",
            "[85]\tvalidation_0-auc:0.780007\n",
            "[86]\tvalidation_0-auc:0.780091\n",
            "[87]\tvalidation_0-auc:0.779774\n",
            "[88]\tvalidation_0-auc:0.77897\n",
            "[89]\tvalidation_0-auc:0.778949\n",
            "[90]\tvalidation_0-auc:0.777594\n",
            "[91]\tvalidation_0-auc:0.777594\n",
            "[92]\tvalidation_0-auc:0.778547\n",
            "[93]\tvalidation_0-auc:0.778991\n",
            "Stopping. Best iteration:\n",
            "[43]\tvalidation_0-auc:0.787445\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6824 - accuracy: 0.5731 - val_loss: 0.6134 - val_accuracy: 0.7112\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6789 - accuracy: 0.5937 - val_loss: 0.6388 - val_accuracy: 0.7112\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6759 - accuracy: 0.5937 - val_loss: 0.6142 - val_accuracy: 0.7112\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6618 - accuracy: 0.6040 - val_loss: 0.6021 - val_accuracy: 0.8009\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6327 - accuracy: 0.6198 - val_loss: 0.4553 - val_accuracy: 0.8162\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6703 - accuracy: 0.5909 - val_loss: 0.5610 - val_accuracy: 0.7549\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6420 - accuracy: 0.6431 - val_loss: 0.4911 - val_accuracy: 0.8009\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6163 - accuracy: 0.6706 - val_loss: 0.4913 - val_accuracy: 0.8206\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5978 - accuracy: 0.6946 - val_loss: 0.4734 - val_accuracy: 0.8009\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5858 - accuracy: 0.6925 - val_loss: 0.4624 - val_accuracy: 0.7856\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.763124\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.767564\n",
            "[2]\tvalidation_0-auc:0.774476\n",
            "[3]\tvalidation_0-auc:0.768124\n",
            "[4]\tvalidation_0-auc:0.770151\n",
            "[5]\tvalidation_0-auc:0.815082\n",
            "[6]\tvalidation_0-auc:0.782063\n",
            "[7]\tvalidation_0-auc:0.819569\n",
            "[8]\tvalidation_0-auc:0.825699\n",
            "[9]\tvalidation_0-auc:0.825874\n",
            "[10]\tvalidation_0-auc:0.827168\n",
            "[11]\tvalidation_0-auc:0.834347\n",
            "[12]\tvalidation_0-auc:0.836247\n",
            "[13]\tvalidation_0-auc:0.838869\n",
            "[14]\tvalidation_0-auc:0.84\n",
            "[15]\tvalidation_0-auc:0.840967\n",
            "[16]\tvalidation_0-auc:0.838566\n",
            "[17]\tvalidation_0-auc:0.83662\n",
            "[18]\tvalidation_0-auc:0.838357\n",
            "[19]\tvalidation_0-auc:0.839709\n",
            "[20]\tvalidation_0-auc:0.839406\n",
            "[21]\tvalidation_0-auc:0.839883\n",
            "[22]\tvalidation_0-auc:0.839767\n",
            "[23]\tvalidation_0-auc:0.840303\n",
            "[24]\tvalidation_0-auc:0.840746\n",
            "[25]\tvalidation_0-auc:0.842051\n",
            "[26]\tvalidation_0-auc:0.841504\n",
            "[27]\tvalidation_0-auc:0.842949\n",
            "[28]\tvalidation_0-auc:0.843741\n",
            "[29]\tvalidation_0-auc:0.843625\n",
            "[30]\tvalidation_0-auc:0.844184\n",
            "[31]\tvalidation_0-auc:0.844627\n",
            "[32]\tvalidation_0-auc:0.846655\n",
            "[33]\tvalidation_0-auc:0.846165\n",
            "[34]\tvalidation_0-auc:0.844814\n",
            "[35]\tvalidation_0-auc:0.843322\n",
            "[36]\tvalidation_0-auc:0.843846\n",
            "[37]\tvalidation_0-auc:0.842075\n",
            "[38]\tvalidation_0-auc:0.843497\n",
            "[39]\tvalidation_0-auc:0.845198\n",
            "[40]\tvalidation_0-auc:0.841772\n",
            "[41]\tvalidation_0-auc:0.842401\n",
            "[42]\tvalidation_0-auc:0.8431\n",
            "[43]\tvalidation_0-auc:0.83958\n",
            "[44]\tvalidation_0-auc:0.83965\n",
            "[45]\tvalidation_0-auc:0.838788\n",
            "[46]\tvalidation_0-auc:0.836876\n",
            "[47]\tvalidation_0-auc:0.838112\n",
            "[48]\tvalidation_0-auc:0.837191\n",
            "[49]\tvalidation_0-auc:0.836702\n",
            "[50]\tvalidation_0-auc:0.839009\n",
            "[51]\tvalidation_0-auc:0.839406\n",
            "[52]\tvalidation_0-auc:0.841131\n",
            "[53]\tvalidation_0-auc:0.837261\n",
            "[54]\tvalidation_0-auc:0.837751\n",
            "[55]\tvalidation_0-auc:0.839371\n",
            "[56]\tvalidation_0-auc:0.84049\n",
            "[57]\tvalidation_0-auc:0.841154\n",
            "[58]\tvalidation_0-auc:0.839219\n",
            "[59]\tvalidation_0-auc:0.839312\n",
            "[60]\tvalidation_0-auc:0.841876\n",
            "[61]\tvalidation_0-auc:0.842156\n",
            "[62]\tvalidation_0-auc:0.839662\n",
            "[63]\tvalidation_0-auc:0.839918\n",
            "[64]\tvalidation_0-auc:0.840338\n",
            "[65]\tvalidation_0-auc:0.841084\n",
            "[66]\tvalidation_0-auc:0.839895\n",
            "[67]\tvalidation_0-auc:0.836305\n",
            "[68]\tvalidation_0-auc:0.836189\n",
            "[69]\tvalidation_0-auc:0.839755\n",
            "[70]\tvalidation_0-auc:0.836445\n",
            "[71]\tvalidation_0-auc:0.837168\n",
            "[72]\tvalidation_0-auc:0.837448\n",
            "[73]\tvalidation_0-auc:0.836026\n",
            "[74]\tvalidation_0-auc:0.835909\n",
            "[75]\tvalidation_0-auc:0.836655\n",
            "[76]\tvalidation_0-auc:0.836492\n",
            "[77]\tvalidation_0-auc:0.833951\n",
            "[78]\tvalidation_0-auc:0.8312\n",
            "[79]\tvalidation_0-auc:0.833112\n",
            "[80]\tvalidation_0-auc:0.835303\n",
            "[81]\tvalidation_0-auc:0.834953\n",
            "[82]\tvalidation_0-auc:0.834697\n",
            "Stopping. Best iteration:\n",
            "[32]\tvalidation_0-auc:0.846655\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.7285714285714285 | 0.49572649572649574 |  0.4393939393939394 |  0.465863453815261  |\n",
            "|     GRU 0.1      | 0.7122448979591837 | 0.47513812154696133 |  0.6515151515151515 |  0.549520766773163  |\n",
            "|   XGBoost 0.1    | 0.746938775510204  |        0.525        |  0.6363636363636364 |  0.5753424657534246 |\n",
            "|    Logreg 0.1    | 0.763265306122449  |  0.5754716981132075 |  0.4621212121212121 |  0.5126050420168067 |\n",
            "|     SVM 0.1      | 0.7285714285714285 | 0.49612403100775193 | 0.48484848484848486 | 0.49042145593869735 |\n",
            "|  LSTM beta 0.1   | 0.8161925601750547 |  0.8157894736842105 |  0.4696969696969697 |  0.5961538461538461 |\n",
            "|   GRU beta 0.1   | 0.7855579868708972 |  0.6103896103896104 |  0.7121212121212122 |  0.6573426573426574 |\n",
            "| XGBoost beta 0.1 | 0.8008752735229759 |  0.6541353383458647 |  0.6590909090909091 |  0.6566037735849056 |\n",
            "| logreg beta 0.1  | 0.8336980306345733 |         0.78        |  0.5909090909090909 |  0.6724137931034482 |\n",
            "|   svm beta 0.1   | 0.8424507658643327 |  0.7307692307692307 |  0.7196969696969697 |  0.7251908396946564 |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6952 - accuracy: 0.5054 - val_loss: 0.6929 - val_accuracy: 0.4714\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6937 - accuracy: 0.5040 - val_loss: 0.6661 - val_accuracy: 0.7592\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6904 - accuracy: 0.5289 - val_loss: 0.7005 - val_accuracy: 0.4510\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6892 - accuracy: 0.5544 - val_loss: 0.6852 - val_accuracy: 0.5571\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6793 - accuracy: 0.5805 - val_loss: 0.6534 - val_accuracy: 0.7184\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6943 - accuracy: 0.5141 - val_loss: 0.6606 - val_accuracy: 0.7633\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6863 - accuracy: 0.5510 - val_loss: 0.7098 - val_accuracy: 0.3796\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6749 - accuracy: 0.5664 - val_loss: 0.6300 - val_accuracy: 0.6653\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6661 - accuracy: 0.5906 - val_loss: 0.6091 - val_accuracy: 0.6959\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6720 - accuracy: 0.5752 - val_loss: 0.6244 - val_accuracy: 0.6898\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.662017\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.678433\n",
            "[2]\tvalidation_0-auc:0.68198\n",
            "[3]\tvalidation_0-auc:0.682002\n",
            "[4]\tvalidation_0-auc:0.699112\n",
            "[5]\tvalidation_0-auc:0.698528\n",
            "[6]\tvalidation_0-auc:0.695906\n",
            "[7]\tvalidation_0-auc:0.69562\n",
            "[8]\tvalidation_0-auc:0.695476\n",
            "[9]\tvalidation_0-auc:0.69757\n",
            "[10]\tvalidation_0-auc:0.695465\n",
            "[11]\tvalidation_0-auc:0.69682\n",
            "[12]\tvalidation_0-auc:0.698693\n",
            "[13]\tvalidation_0-auc:0.699553\n",
            "[14]\tvalidation_0-auc:0.700368\n",
            "[15]\tvalidation_0-auc:0.697713\n",
            "[16]\tvalidation_0-auc:0.697052\n",
            "[17]\tvalidation_0-auc:0.695829\n",
            "[18]\tvalidation_0-auc:0.694826\n",
            "[19]\tvalidation_0-auc:0.693791\n",
            "[20]\tvalidation_0-auc:0.695234\n",
            "[21]\tvalidation_0-auc:0.692292\n",
            "[22]\tvalidation_0-auc:0.691962\n",
            "[23]\tvalidation_0-auc:0.689781\n",
            "[24]\tvalidation_0-auc:0.68707\n",
            "[25]\tvalidation_0-auc:0.688007\n",
            "[26]\tvalidation_0-auc:0.687026\n",
            "[27]\tvalidation_0-auc:0.686233\n",
            "[28]\tvalidation_0-auc:0.684955\n",
            "[29]\tvalidation_0-auc:0.681804\n",
            "[30]\tvalidation_0-auc:0.681716\n",
            "[31]\tvalidation_0-auc:0.682047\n",
            "[32]\tvalidation_0-auc:0.682024\n",
            "[33]\tvalidation_0-auc:0.680019\n",
            "[34]\tvalidation_0-auc:0.678896\n",
            "[35]\tvalidation_0-auc:0.677904\n",
            "[36]\tvalidation_0-auc:0.677904\n",
            "[37]\tvalidation_0-auc:0.678896\n",
            "[38]\tvalidation_0-auc:0.679986\n",
            "[39]\tvalidation_0-auc:0.683941\n",
            "[40]\tvalidation_0-auc:0.683986\n",
            "[41]\tvalidation_0-auc:0.683369\n",
            "[42]\tvalidation_0-auc:0.68198\n",
            "[43]\tvalidation_0-auc:0.68154\n",
            "[44]\tvalidation_0-auc:0.680526\n",
            "[45]\tvalidation_0-auc:0.68046\n",
            "[46]\tvalidation_0-auc:0.680802\n",
            "[47]\tvalidation_0-auc:0.680691\n",
            "[48]\tvalidation_0-auc:0.681187\n",
            "[49]\tvalidation_0-auc:0.683115\n",
            "[50]\tvalidation_0-auc:0.683302\n",
            "[51]\tvalidation_0-auc:0.682157\n",
            "[52]\tvalidation_0-auc:0.682113\n",
            "[53]\tvalidation_0-auc:0.67808\n",
            "[54]\tvalidation_0-auc:0.679325\n",
            "[55]\tvalidation_0-auc:0.675778\n",
            "[56]\tvalidation_0-auc:0.675778\n",
            "[57]\tvalidation_0-auc:0.675381\n",
            "[58]\tvalidation_0-auc:0.676219\n",
            "[59]\tvalidation_0-auc:0.675954\n",
            "[60]\tvalidation_0-auc:0.675315\n",
            "[61]\tvalidation_0-auc:0.675073\n",
            "[62]\tvalidation_0-auc:0.67309\n",
            "[63]\tvalidation_0-auc:0.6745\n",
            "[64]\tvalidation_0-auc:0.675315\n",
            "Stopping. Best iteration:\n",
            "[14]\tvalidation_0-auc:0.700368\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 4s 16ms/step - loss: 0.6935 - accuracy: 0.5216 - val_loss: 0.6642 - val_accuracy: 0.7287\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6762 - accuracy: 0.5772 - val_loss: 0.6714 - val_accuracy: 0.5164\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6516 - accuracy: 0.6246 - val_loss: 0.6671 - val_accuracy: 0.5558\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6345 - accuracy: 0.6452 - val_loss: 0.5440 - val_accuracy: 0.7287\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6195 - accuracy: 0.6664 - val_loss: 0.5768 - val_accuracy: 0.6324\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6940 - accuracy: 0.5257 - val_loss: 0.6409 - val_accuracy: 0.7702\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6525 - accuracy: 0.6294 - val_loss: 0.6163 - val_accuracy: 0.6630\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6388 - accuracy: 0.6266 - val_loss: 0.5216 - val_accuracy: 0.7374\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5922 - accuracy: 0.6754 - val_loss: 0.6309 - val_accuracy: 0.5974\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5939 - accuracy: 0.6692 - val_loss: 0.6172 - val_accuracy: 0.6280\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.619503\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.619575\n",
            "[2]\tvalidation_0-auc:0.62994\n",
            "[3]\tvalidation_0-auc:0.634082\n",
            "[4]\tvalidation_0-auc:0.630643\n",
            "[5]\tvalidation_0-auc:0.628935\n",
            "[6]\tvalidation_0-auc:0.645258\n",
            "[7]\tvalidation_0-auc:0.647123\n",
            "[8]\tvalidation_0-auc:0.647632\n",
            "[9]\tvalidation_0-auc:0.653395\n",
            "[10]\tvalidation_0-auc:0.666969\n",
            "[11]\tvalidation_0-auc:0.666533\n",
            "[12]\tvalidation_0-auc:0.671982\n",
            "[13]\tvalidation_0-auc:0.672043\n",
            "[14]\tvalidation_0-auc:0.688378\n",
            "[15]\tvalidation_0-auc:0.683486\n",
            "[16]\tvalidation_0-auc:0.678994\n",
            "[17]\tvalidation_0-auc:0.683219\n",
            "[18]\tvalidation_0-auc:0.690872\n",
            "[19]\tvalidation_0-auc:0.683655\n",
            "[20]\tvalidation_0-auc:0.685072\n",
            "[21]\tvalidation_0-auc:0.689274\n",
            "[22]\tvalidation_0-auc:0.690872\n",
            "[23]\tvalidation_0-auc:0.687748\n",
            "[24]\tvalidation_0-auc:0.685908\n",
            "[25]\tvalidation_0-auc:0.683583\n",
            "[26]\tvalidation_0-auc:0.683341\n",
            "[27]\tvalidation_0-auc:0.682372\n",
            "[28]\tvalidation_0-auc:0.684152\n",
            "[29]\tvalidation_0-auc:0.684782\n",
            "[30]\tvalidation_0-auc:0.684854\n",
            "[31]\tvalidation_0-auc:0.68437\n",
            "[32]\tvalidation_0-auc:0.684406\n",
            "[33]\tvalidation_0-auc:0.684576\n",
            "[34]\tvalidation_0-auc:0.684164\n",
            "[35]\tvalidation_0-auc:0.682614\n",
            "[36]\tvalidation_0-auc:0.683655\n",
            "[37]\tvalidation_0-auc:0.683994\n",
            "[38]\tvalidation_0-auc:0.683098\n",
            "[39]\tvalidation_0-auc:0.683195\n",
            "[40]\tvalidation_0-auc:0.685254\n",
            "[41]\tvalidation_0-auc:0.68552\n",
            "[42]\tvalidation_0-auc:0.68569\n",
            "[43]\tvalidation_0-auc:0.685678\n",
            "[44]\tvalidation_0-auc:0.686646\n",
            "[45]\tvalidation_0-auc:0.686671\n",
            "[46]\tvalidation_0-auc:0.687106\n",
            "[47]\tvalidation_0-auc:0.686864\n",
            "[48]\tvalidation_0-auc:0.687155\n",
            "[49]\tvalidation_0-auc:0.687615\n",
            "[50]\tvalidation_0-auc:0.688027\n",
            "[51]\tvalidation_0-auc:0.6873\n",
            "[52]\tvalidation_0-auc:0.686259\n",
            "[53]\tvalidation_0-auc:0.689068\n",
            "[54]\tvalidation_0-auc:0.688342\n",
            "[55]\tvalidation_0-auc:0.689044\n",
            "[56]\tvalidation_0-auc:0.687082\n",
            "[57]\tvalidation_0-auc:0.685871\n",
            "[58]\tvalidation_0-auc:0.687349\n",
            "[59]\tvalidation_0-auc:0.687446\n",
            "[60]\tvalidation_0-auc:0.688705\n",
            "[61]\tvalidation_0-auc:0.687785\n",
            "[62]\tvalidation_0-auc:0.687058\n",
            "[63]\tvalidation_0-auc:0.686598\n",
            "[64]\tvalidation_0-auc:0.686525\n",
            "[65]\tvalidation_0-auc:0.687446\n",
            "[66]\tvalidation_0-auc:0.687349\n",
            "[67]\tvalidation_0-auc:0.686477\n",
            "[68]\tvalidation_0-auc:0.687203\n",
            "Stopping. Best iteration:\n",
            "[18]\tvalidation_0-auc:0.690872\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.2     | 0.7183673469387755 |  0.4426229508196721 | 0.43548387096774194 | 0.43902439024390244 |\n",
            "|     GRU 0.2      | 0.689795918367347  | 0.40789473684210525 |         0.5         |  0.4492753623188405 |\n",
            "|   XGBoost 0.2    | 0.5959183673469388 |        0.352        |  0.7096774193548387 |  0.4705882352941176 |\n",
            "|    Logreg 0.2    | 0.673469387755102  |  0.4052631578947368 |  0.6209677419354839 |  0.4904458598726114 |\n",
            "|     SVM 0.2      |        0.6         | 0.35714285714285715 |  0.7258064516129032 |  0.4787234042553191 |\n",
            "|  LSTM beta 0.2   | 0.6323851203501094 |         0.4         |  0.7096774193548387 |  0.5116279069767441 |\n",
            "|   GRU beta 0.2   | 0.6280087527352297 | 0.39351851851851855 |  0.6854838709677419 |         0.5         |\n",
            "| XGBoost beta 0.2 | 0.6236323851203501 | 0.38571428571428573 |  0.6532258064516129 | 0.48502994011976047 |\n",
            "| logreg beta 0.2  | 0.7724288840262582 |  0.5602409638554217 |         0.75        |  0.6413793103448276 |\n",
            "|   svm beta 0.2   | 0.7111597374179431 |  0.481651376146789  |  0.8467741935483871 |  0.6140350877192983 |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6951 - accuracy: 0.5040 - val_loss: 0.6359 - val_accuracy: 0.7939\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6940 - accuracy: 0.4980 - val_loss: 0.6714 - val_accuracy: 0.8041\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6899 - accuracy: 0.5396 - val_loss: 0.8218 - val_accuracy: 0.2061\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6905 - accuracy: 0.5530 - val_loss: 0.6643 - val_accuracy: 0.6469\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6774 - accuracy: 0.5591 - val_loss: 0.6323 - val_accuracy: 0.6898\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6944 - accuracy: 0.5148 - val_loss: 0.6877 - val_accuracy: 0.5837\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6824 - accuracy: 0.5436 - val_loss: 0.7565 - val_accuracy: 0.3265\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6706 - accuracy: 0.5698 - val_loss: 0.6933 - val_accuracy: 0.5020\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6646 - accuracy: 0.6000 - val_loss: 0.7366 - val_accuracy: 0.4531\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6672 - accuracy: 0.5832 - val_loss: 0.6048 - val_accuracy: 0.7061\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.715124\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.733195\n",
            "[2]\tvalidation_0-auc:0.738031\n",
            "[3]\tvalidation_0-auc:0.739431\n",
            "[4]\tvalidation_0-auc:0.754664\n",
            "[5]\tvalidation_0-auc:0.756509\n",
            "[6]\tvalidation_0-auc:0.756051\n",
            "[7]\tvalidation_0-auc:0.75497\n",
            "[8]\tvalidation_0-auc:0.752043\n",
            "[9]\tvalidation_0-auc:0.755288\n",
            "[10]\tvalidation_0-auc:0.751483\n",
            "[11]\tvalidation_0-auc:0.752552\n",
            "[12]\tvalidation_0-auc:0.753112\n",
            "[13]\tvalidation_0-auc:0.751546\n",
            "[14]\tvalidation_0-auc:0.751572\n",
            "[15]\tvalidation_0-auc:0.747894\n",
            "[16]\tvalidation_0-auc:0.747156\n",
            "[17]\tvalidation_0-auc:0.745718\n",
            "[18]\tvalidation_0-auc:0.745196\n",
            "[19]\tvalidation_0-auc:0.743923\n",
            "[20]\tvalidation_0-auc:0.74615\n",
            "[21]\tvalidation_0-auc:0.742282\n",
            "[22]\tvalidation_0-auc:0.741849\n",
            "[23]\tvalidation_0-auc:0.739393\n",
            "[24]\tvalidation_0-auc:0.736033\n",
            "[25]\tvalidation_0-auc:0.737255\n",
            "[26]\tvalidation_0-auc:0.735969\n",
            "[27]\tvalidation_0-auc:0.736275\n",
            "[28]\tvalidation_0-auc:0.734824\n",
            "[29]\tvalidation_0-auc:0.72953\n",
            "[30]\tvalidation_0-auc:0.729326\n",
            "[31]\tvalidation_0-auc:0.730523\n",
            "[32]\tvalidation_0-auc:0.730523\n",
            "[33]\tvalidation_0-auc:0.728308\n",
            "[34]\tvalidation_0-auc:0.726985\n",
            "[35]\tvalidation_0-auc:0.726247\n",
            "[36]\tvalidation_0-auc:0.726272\n",
            "[37]\tvalidation_0-auc:0.726832\n",
            "[38]\tvalidation_0-auc:0.727634\n",
            "[39]\tvalidation_0-auc:0.733653\n",
            "[40]\tvalidation_0-auc:0.73359\n",
            "[41]\tvalidation_0-auc:0.733335\n",
            "[42]\tvalidation_0-auc:0.731808\n",
            "[43]\tvalidation_0-auc:0.731248\n",
            "[44]\tvalidation_0-auc:0.730675\n",
            "[45]\tvalidation_0-auc:0.730573\n",
            "[46]\tvalidation_0-auc:0.730917\n",
            "[47]\tvalidation_0-auc:0.730815\n",
            "[48]\tvalidation_0-auc:0.731439\n",
            "[49]\tvalidation_0-auc:0.732368\n",
            "[50]\tvalidation_0-auc:0.73247\n",
            "[51]\tvalidation_0-auc:0.731019\n",
            "[52]\tvalidation_0-auc:0.731121\n",
            "[53]\tvalidation_0-auc:0.727863\n",
            "[54]\tvalidation_0-auc:0.729759\n",
            "[55]\tvalidation_0-auc:0.726399\n",
            "Stopping. Best iteration:\n",
            "[5]\tvalidation_0-auc:0.756509\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6946 - accuracy: 0.5230 - val_loss: 0.6417 - val_accuracy: 0.7790\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6698 - accuracy: 0.5813 - val_loss: 0.8792 - val_accuracy: 0.2210\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6622 - accuracy: 0.6136 - val_loss: 0.7591 - val_accuracy: 0.3720\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6218 - accuracy: 0.6644 - val_loss: 0.6102 - val_accuracy: 0.6477\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6102 - accuracy: 0.6658 - val_loss: 0.7512 - val_accuracy: 0.3195\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6825 - accuracy: 0.5450 - val_loss: 0.5504 - val_accuracy: 0.8140\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6202 - accuracy: 0.6637 - val_loss: 0.6212 - val_accuracy: 0.6871\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6003 - accuracy: 0.6733 - val_loss: 0.5096 - val_accuracy: 0.7527\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5877 - accuracy: 0.6898 - val_loss: 0.5504 - val_accuracy: 0.7330\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5848 - accuracy: 0.6802 - val_loss: 0.5723 - val_accuracy: 0.7002\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.652798\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.652798\n",
            "[2]\tvalidation_0-auc:0.683822\n",
            "[3]\tvalidation_0-auc:0.688758\n",
            "[4]\tvalidation_0-auc:0.69439\n",
            "[5]\tvalidation_0-auc:0.693111\n",
            "[6]\tvalidation_0-auc:0.709019\n",
            "[7]\tvalidation_0-auc:0.706572\n",
            "[8]\tvalidation_0-auc:0.706767\n",
            "[9]\tvalidation_0-auc:0.713386\n",
            "[10]\tvalidation_0-auc:0.720339\n",
            "[11]\tvalidation_0-auc:0.718197\n",
            "[12]\tvalidation_0-auc:0.725261\n",
            "[13]\tvalidation_0-auc:0.72579\n",
            "[14]\tvalidation_0-auc:0.738124\n",
            "[15]\tvalidation_0-auc:0.732896\n",
            "[16]\tvalidation_0-auc:0.730045\n",
            "[17]\tvalidation_0-auc:0.733063\n",
            "[18]\tvalidation_0-auc:0.739571\n",
            "[19]\tvalidation_0-auc:0.736483\n",
            "[20]\tvalidation_0-auc:0.738528\n",
            "[21]\tvalidation_0-auc:0.743242\n",
            "[22]\tvalidation_0-auc:0.744465\n",
            "[23]\tvalidation_0-auc:0.741017\n",
            "[24]\tvalidation_0-auc:0.740683\n",
            "[25]\tvalidation_0-auc:0.738861\n",
            "[26]\tvalidation_0-auc:0.737207\n",
            "[27]\tvalidation_0-auc:0.736511\n",
            "[28]\tvalidation_0-auc:0.737916\n",
            "[29]\tvalidation_0-auc:0.737888\n",
            "[30]\tvalidation_0-auc:0.737805\n",
            "[31]\tvalidation_0-auc:0.73736\n",
            "[32]\tvalidation_0-auc:0.736845\n",
            "[33]\tvalidation_0-auc:0.737095\n",
            "[34]\tvalidation_0-auc:0.735705\n",
            "[35]\tvalidation_0-auc:0.735677\n",
            "[36]\tvalidation_0-auc:0.735566\n",
            "[37]\tvalidation_0-auc:0.735899\n",
            "[38]\tvalidation_0-auc:0.735899\n",
            "[39]\tvalidation_0-auc:0.736942\n",
            "[40]\tvalidation_0-auc:0.739251\n",
            "[41]\tvalidation_0-auc:0.739974\n",
            "[42]\tvalidation_0-auc:0.741337\n",
            "[43]\tvalidation_0-auc:0.740322\n",
            "[44]\tvalidation_0-auc:0.739543\n",
            "[45]\tvalidation_0-auc:0.739014\n",
            "[46]\tvalidation_0-auc:0.738959\n",
            "[47]\tvalidation_0-auc:0.738708\n",
            "[48]\tvalidation_0-auc:0.738625\n",
            "[49]\tvalidation_0-auc:0.738903\n",
            "[50]\tvalidation_0-auc:0.739098\n",
            "[51]\tvalidation_0-auc:0.739042\n",
            "[52]\tvalidation_0-auc:0.738764\n",
            "[53]\tvalidation_0-auc:0.742574\n",
            "[54]\tvalidation_0-auc:0.741573\n",
            "[55]\tvalidation_0-auc:0.742046\n",
            "[56]\tvalidation_0-auc:0.73996\n",
            "[57]\tvalidation_0-auc:0.737902\n",
            "[58]\tvalidation_0-auc:0.738291\n",
            "[59]\tvalidation_0-auc:0.738875\n",
            "[60]\tvalidation_0-auc:0.740572\n",
            "[61]\tvalidation_0-auc:0.73971\n",
            "[62]\tvalidation_0-auc:0.73843\n",
            "[63]\tvalidation_0-auc:0.738458\n",
            "[64]\tvalidation_0-auc:0.738208\n",
            "[65]\tvalidation_0-auc:0.738041\n",
            "[66]\tvalidation_0-auc:0.738291\n",
            "[67]\tvalidation_0-auc:0.736928\n",
            "[68]\tvalidation_0-auc:0.736901\n",
            "[69]\tvalidation_0-auc:0.735844\n",
            "[70]\tvalidation_0-auc:0.736456\n",
            "[71]\tvalidation_0-auc:0.735733\n",
            "[72]\tvalidation_0-auc:0.736678\n",
            "Stopping. Best iteration:\n",
            "[22]\tvalidation_0-auc:0.744465\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+---------------------+---------------------+--------------------+---------------------+\n",
            "|       Model       |       Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+-------------------+---------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.15     |  0.689795918367347  |  0.3473053892215569 | 0.5742574257425742 |  0.4328358208955224 |\n",
            "|      GRU 0.15     |  0.7061224489795919 | 0.37572254335260113 | 0.6435643564356436 |  0.4744525547445255 |\n",
            "|    XGBoost 0.15   |  0.5755102040816327 |  0.3054545454545455 | 0.8316831683168316 | 0.44680851063829785 |\n",
            "|    Logreg 0.15    |  0.6877551020408164 |  0.3631578947368421 | 0.6831683168316832 |  0.4742268041237113 |\n",
            "|      SVM 0.15     |  0.6061224489795919 | 0.31746031746031744 | 0.7920792079207921 | 0.45325779036827196 |\n",
            "|   LSTM beta 0.15  | 0.31947483588621445 |  0.2236842105263158 | 0.8415841584158416 | 0.35343035343035345 |\n",
            "|   GRU beta 0.15   |  0.700218818380744  |  0.4032258064516129 | 0.7425742574257426 |  0.5226480836236934 |\n",
            "| XGBoost beta 0.15 |  0.6564551422319475 | 0.36538461538461536 | 0.7524752475247525 | 0.49190938511326854 |\n",
            "|  logreg beta 0.15 |  0.7833698030634574 |  0.5060240963855421 | 0.8316831683168316 |  0.6292134831460674 |\n",
            "|   svm beta 0.15   |  0.7133479212253829 | 0.43119266055045874 | 0.9306930693069307 |  0.5893416927899687 |\n",
            "+-------------------+---------------------+---------------------+--------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9mZjIPEaT_T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "outputId": "207eb9dc-505b-44b7-ee8c-12b06fa9d499"
      },
      "source": [
        "Result_cross.to_csv('WU_Result_cross.csv')\n",
        "Result_cross"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.495726</td>\n",
              "      <td>0.728571</td>\n",
              "      <td>0.465863</td>\n",
              "      <td>0.439394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.475138</td>\n",
              "      <td>0.712245</td>\n",
              "      <td>0.549521</td>\n",
              "      <td>0.651515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.525000</td>\n",
              "      <td>0.746939</td>\n",
              "      <td>0.575342</td>\n",
              "      <td>0.636364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.575472</td>\n",
              "      <td>0.763265</td>\n",
              "      <td>0.512605</td>\n",
              "      <td>0.462121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.496124</td>\n",
              "      <td>0.728571</td>\n",
              "      <td>0.490421</td>\n",
              "      <td>0.484848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.815789</td>\n",
              "      <td>0.816193</td>\n",
              "      <td>0.596154</td>\n",
              "      <td>0.469697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.610390</td>\n",
              "      <td>0.785558</td>\n",
              "      <td>0.657343</td>\n",
              "      <td>0.712121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.654135</td>\n",
              "      <td>0.800875</td>\n",
              "      <td>0.656604</td>\n",
              "      <td>0.659091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.780000</td>\n",
              "      <td>0.833698</td>\n",
              "      <td>0.672414</td>\n",
              "      <td>0.590909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.730769</td>\n",
              "      <td>0.842451</td>\n",
              "      <td>0.725191</td>\n",
              "      <td>0.719697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.442623</td>\n",
              "      <td>0.718367</td>\n",
              "      <td>0.439024</td>\n",
              "      <td>0.435484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.407895</td>\n",
              "      <td>0.689796</td>\n",
              "      <td>0.449275</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.352000</td>\n",
              "      <td>0.595918</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.709677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.405263</td>\n",
              "      <td>0.673469</td>\n",
              "      <td>0.490446</td>\n",
              "      <td>0.620968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.478723</td>\n",
              "      <td>0.725806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.632385</td>\n",
              "      <td>0.511628</td>\n",
              "      <td>0.709677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.393519</td>\n",
              "      <td>0.628009</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.685484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.385714</td>\n",
              "      <td>0.623632</td>\n",
              "      <td>0.485030</td>\n",
              "      <td>0.653226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.560241</td>\n",
              "      <td>0.772429</td>\n",
              "      <td>0.641379</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.481651</td>\n",
              "      <td>0.711160</td>\n",
              "      <td>0.614035</td>\n",
              "      <td>0.846774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.347305</td>\n",
              "      <td>0.689796</td>\n",
              "      <td>0.432836</td>\n",
              "      <td>0.574257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.375723</td>\n",
              "      <td>0.706122</td>\n",
              "      <td>0.474453</td>\n",
              "      <td>0.643564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.305455</td>\n",
              "      <td>0.575510</td>\n",
              "      <td>0.446809</td>\n",
              "      <td>0.831683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.363158</td>\n",
              "      <td>0.687755</td>\n",
              "      <td>0.474227</td>\n",
              "      <td>0.683168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.317460</td>\n",
              "      <td>0.606122</td>\n",
              "      <td>0.453258</td>\n",
              "      <td>0.792079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.223684</td>\n",
              "      <td>0.319475</td>\n",
              "      <td>0.353430</td>\n",
              "      <td>0.841584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.403226</td>\n",
              "      <td>0.700219</td>\n",
              "      <td>0.522648</td>\n",
              "      <td>0.742574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.365385</td>\n",
              "      <td>0.656455</td>\n",
              "      <td>0.491909</td>\n",
              "      <td>0.752475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.506024</td>\n",
              "      <td>0.783370</td>\n",
              "      <td>0.629213</td>\n",
              "      <td>0.831683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.431193</td>\n",
              "      <td>0.713348</td>\n",
              "      <td>0.589342</td>\n",
              "      <td>0.930693</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1   WU  0.495726  0.728571  0.465863  0.439394\n",
              "1            GRU 0.1   WU  0.475138  0.712245  0.549521  0.651515\n",
              "2        XGBoost 0.1   WU  0.525000  0.746939  0.575342  0.636364\n",
              "3         Logreg 0.1   WU  0.575472  0.763265  0.512605  0.462121\n",
              "4            SVM 0.1   WU  0.496124  0.728571  0.490421  0.484848\n",
              "5      LSTM beta 0.1   WU  0.815789  0.816193  0.596154  0.469697\n",
              "6       GRU beta 0.1   WU  0.610390  0.785558  0.657343  0.712121\n",
              "7   XGBoost beta 0.1   WU  0.654135  0.800875  0.656604  0.659091\n",
              "8    logreg beta 0.1   WU  0.780000  0.833698  0.672414  0.590909\n",
              "9       svm beta 0.1   WU  0.730769  0.842451  0.725191  0.719697\n",
              "0           LSTM 0.2   WU  0.442623  0.718367  0.439024  0.435484\n",
              "1            GRU 0.2   WU  0.407895  0.689796  0.449275  0.500000\n",
              "2        XGBoost 0.2   WU  0.352000  0.595918  0.470588  0.709677\n",
              "3         Logreg 0.2   WU  0.405263  0.673469  0.490446  0.620968\n",
              "4            SVM 0.2   WU  0.357143  0.600000  0.478723  0.725806\n",
              "5      LSTM beta 0.2   WU  0.400000  0.632385  0.511628  0.709677\n",
              "6       GRU beta 0.2   WU  0.393519  0.628009  0.500000  0.685484\n",
              "7   XGBoost beta 0.2   WU  0.385714  0.623632  0.485030  0.653226\n",
              "8    logreg beta 0.2   WU  0.560241  0.772429  0.641379  0.750000\n",
              "9       svm beta 0.2   WU  0.481651  0.711160  0.614035  0.846774\n",
              "0          LSTM 0.15   WU  0.347305  0.689796  0.432836  0.574257\n",
              "1           GRU 0.15   WU  0.375723  0.706122  0.474453  0.643564\n",
              "2       XGBoost 0.15   WU  0.305455  0.575510  0.446809  0.831683\n",
              "3        Logreg 0.15   WU  0.363158  0.687755  0.474227  0.683168\n",
              "4           SVM 0.15   WU  0.317460  0.606122  0.453258  0.792079\n",
              "5     LSTM beta 0.15   WU  0.223684  0.319475  0.353430  0.841584\n",
              "6      GRU beta 0.15   WU  0.403226  0.700219  0.522648  0.742574\n",
              "7  XGBoost beta 0.15   WU  0.365385  0.656455  0.491909  0.752475\n",
              "8   logreg beta 0.15   WU  0.506024  0.783370  0.629213  0.831683\n",
              "9      svm beta 0.15   WU  0.431193  0.713348  0.589342  0.930693"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43TxeJH1aT_T"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_lstm.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_gru.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_svm.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_xgboost.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_logreg.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_lstm_beta.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_gru_beta.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_svm_beta.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_xgboost_beta.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_logreg_beta.csv')"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5OyCM2YaT_T"
      },
      "source": [
        "###Purging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgTYdmwYaT_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a82b36d-195f-4915-e986-8570c06edaab"
      },
      "source": [
        "for th in threshholds:\n",
        "  print('Threshhold = ', th)\n",
        "  true_label = labeling(dfs[col_name], th)\n",
        "  dfs = dfs.assign(true_labels=pd.Series(true_label).values)\n",
        "  # dfs = dfs.drop(dfs[dfs.true_labels == 0].index)\n",
        "  # dfs = dfs[::-1].reset_index()\n",
        "  #historical = Train_data(dfs[col_name], train_start=100, train_end=round(len(dfs.index)*0.8), test_end=len(dfs.index))\n",
        "  historical = Train_data(dfs[col_name], train_start=300, train_end=1800, test_end=2300)\n",
        "  historical.K_fold_purged(num_split=10, num_test=2, time_gaps=10, purging=True, emb=10)\n",
        "  historical.set_threshold(th=th)\n",
        "  result = final_result(historical, \"WU\", step_sizes=4, th= th)\n",
        "  Result_purging = Result_purging.append(result)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshhold =  0.1\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 15ms/step - loss: 0.6816 - accuracy: 0.5940 - val_loss: 0.6220 - val_accuracy: 0.7306\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6788 - accuracy: 0.5886 - val_loss: 0.6231 - val_accuracy: 0.7429\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6553 - accuracy: 0.6242 - val_loss: 0.5315 - val_accuracy: 0.7245\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6362 - accuracy: 0.6389 - val_loss: 0.5232 - val_accuracy: 0.7510\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6190 - accuracy: 0.6544 - val_loss: 0.5614 - val_accuracy: 0.7735\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6719 - accuracy: 0.5852 - val_loss: 0.6385 - val_accuracy: 0.7878\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6373 - accuracy: 0.6302 - val_loss: 0.6297 - val_accuracy: 0.6245\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6114 - accuracy: 0.6577 - val_loss: 0.5391 - val_accuracy: 0.7286\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6020 - accuracy: 0.6866 - val_loss: 0.4982 - val_accuracy: 0.7714\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6121 - accuracy: 0.6604 - val_loss: 0.5198 - val_accuracy: 0.7571\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.781615\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.777499\n",
            "[2]\tvalidation_0-auc:0.779086\n",
            "[3]\tvalidation_0-auc:0.783361\n",
            "[4]\tvalidation_0-auc:0.782483\n",
            "[5]\tvalidation_0-auc:0.782218\n",
            "[6]\tvalidation_0-auc:0.784429\n",
            "[7]\tvalidation_0-auc:0.782916\n",
            "[8]\tvalidation_0-auc:0.782652\n",
            "[9]\tvalidation_0-auc:0.780049\n",
            "[10]\tvalidation_0-auc:0.780695\n",
            "[11]\tvalidation_0-auc:0.779943\n",
            "[12]\tvalidation_0-auc:0.782123\n",
            "[13]\tvalidation_0-auc:0.783477\n",
            "[14]\tvalidation_0-auc:0.78553\n",
            "[15]\tvalidation_0-auc:0.783985\n",
            "[16]\tvalidation_0-auc:0.784556\n",
            "[17]\tvalidation_0-auc:0.784313\n",
            "[18]\tvalidation_0-auc:0.7859\n",
            "[19]\tvalidation_0-auc:0.785435\n",
            "[20]\tvalidation_0-auc:0.78626\n",
            "[21]\tvalidation_0-auc:0.785202\n",
            "[22]\tvalidation_0-auc:0.784345\n",
            "[23]\tvalidation_0-auc:0.783509\n",
            "[24]\tvalidation_0-auc:0.783932\n",
            "[25]\tvalidation_0-auc:0.784112\n",
            "[26]\tvalidation_0-auc:0.785943\n",
            "[27]\tvalidation_0-auc:0.787085\n",
            "[28]\tvalidation_0-auc:0.786927\n",
            "[29]\tvalidation_0-auc:0.786482\n",
            "[30]\tvalidation_0-auc:0.786112\n",
            "[31]\tvalidation_0-auc:0.786472\n",
            "[32]\tvalidation_0-auc:0.786355\n",
            "[33]\tvalidation_0-auc:0.786927\n",
            "[34]\tvalidation_0-auc:0.786863\n",
            "[35]\tvalidation_0-auc:0.786249\n",
            "[36]\tvalidation_0-auc:0.786175\n",
            "[37]\tvalidation_0-auc:0.785519\n",
            "[38]\tvalidation_0-auc:0.78607\n",
            "[39]\tvalidation_0-auc:0.786958\n",
            "[40]\tvalidation_0-auc:0.786958\n",
            "[41]\tvalidation_0-auc:0.78717\n",
            "[42]\tvalidation_0-auc:0.787128\n",
            "[43]\tvalidation_0-auc:0.787445\n",
            "[44]\tvalidation_0-auc:0.786091\n",
            "[45]\tvalidation_0-auc:0.785985\n",
            "[46]\tvalidation_0-auc:0.786101\n",
            "[47]\tvalidation_0-auc:0.785974\n",
            "[48]\tvalidation_0-auc:0.785033\n",
            "[49]\tvalidation_0-auc:0.78571\n",
            "[50]\tvalidation_0-auc:0.78571\n",
            "[51]\tvalidation_0-auc:0.784906\n",
            "[52]\tvalidation_0-auc:0.783996\n",
            "[53]\tvalidation_0-auc:0.783594\n",
            "[54]\tvalidation_0-auc:0.784334\n",
            "[55]\tvalidation_0-auc:0.784355\n",
            "[56]\tvalidation_0-auc:0.783869\n",
            "[57]\tvalidation_0-auc:0.782938\n",
            "[58]\tvalidation_0-auc:0.781668\n",
            "[59]\tvalidation_0-auc:0.780948\n",
            "[60]\tvalidation_0-auc:0.780906\n",
            "[61]\tvalidation_0-auc:0.780822\n",
            "[62]\tvalidation_0-auc:0.780864\n",
            "[63]\tvalidation_0-auc:0.780885\n",
            "[64]\tvalidation_0-auc:0.780589\n",
            "[65]\tvalidation_0-auc:0.779446\n",
            "[66]\tvalidation_0-auc:0.77915\n",
            "[67]\tvalidation_0-auc:0.779573\n",
            "[68]\tvalidation_0-auc:0.777806\n",
            "[69]\tvalidation_0-auc:0.777002\n",
            "[70]\tvalidation_0-auc:0.777679\n",
            "[71]\tvalidation_0-auc:0.777996\n",
            "[72]\tvalidation_0-auc:0.777869\n",
            "[73]\tvalidation_0-auc:0.777023\n",
            "[74]\tvalidation_0-auc:0.775732\n",
            "[75]\tvalidation_0-auc:0.773976\n",
            "[76]\tvalidation_0-auc:0.774187\n",
            "[77]\tvalidation_0-auc:0.773849\n",
            "[78]\tvalidation_0-auc:0.774505\n",
            "[79]\tvalidation_0-auc:0.77569\n",
            "[80]\tvalidation_0-auc:0.776981\n",
            "[81]\tvalidation_0-auc:0.777743\n",
            "[82]\tvalidation_0-auc:0.77751\n",
            "[83]\tvalidation_0-auc:0.777869\n",
            "[84]\tvalidation_0-auc:0.778208\n",
            "[85]\tvalidation_0-auc:0.780007\n",
            "[86]\tvalidation_0-auc:0.780091\n",
            "[87]\tvalidation_0-auc:0.779774\n",
            "[88]\tvalidation_0-auc:0.77897\n",
            "[89]\tvalidation_0-auc:0.778949\n",
            "[90]\tvalidation_0-auc:0.777594\n",
            "[91]\tvalidation_0-auc:0.777594\n",
            "[92]\tvalidation_0-auc:0.778547\n",
            "[93]\tvalidation_0-auc:0.778991\n",
            "Stopping. Best iteration:\n",
            "[43]\tvalidation_0-auc:0.787445\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 16ms/step - loss: 0.6799 - accuracy: 0.5896 - val_loss: 0.6495 - val_accuracy: 0.7112\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6784 - accuracy: 0.5937 - val_loss: 0.6223 - val_accuracy: 0.7112\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6729 - accuracy: 0.5916 - val_loss: 0.6034 - val_accuracy: 0.7112\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6520 - accuracy: 0.6102 - val_loss: 0.5342 - val_accuracy: 0.7877\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6333 - accuracy: 0.6349 - val_loss: 0.5269 - val_accuracy: 0.8228\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6736 - accuracy: 0.5868 - val_loss: 0.5570 - val_accuracy: 0.7746\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6315 - accuracy: 0.6404 - val_loss: 0.4548 - val_accuracy: 0.8162\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6038 - accuracy: 0.6658 - val_loss: 0.4487 - val_accuracy: 0.8271\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6016 - accuracy: 0.6788 - val_loss: 0.4504 - val_accuracy: 0.8009\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5902 - accuracy: 0.6857 - val_loss: 0.4373 - val_accuracy: 0.8118\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.763124\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.767564\n",
            "[2]\tvalidation_0-auc:0.774476\n",
            "[3]\tvalidation_0-auc:0.768124\n",
            "[4]\tvalidation_0-auc:0.770151\n",
            "[5]\tvalidation_0-auc:0.815082\n",
            "[6]\tvalidation_0-auc:0.782063\n",
            "[7]\tvalidation_0-auc:0.819569\n",
            "[8]\tvalidation_0-auc:0.825699\n",
            "[9]\tvalidation_0-auc:0.825874\n",
            "[10]\tvalidation_0-auc:0.827168\n",
            "[11]\tvalidation_0-auc:0.834347\n",
            "[12]\tvalidation_0-auc:0.836247\n",
            "[13]\tvalidation_0-auc:0.838869\n",
            "[14]\tvalidation_0-auc:0.84\n",
            "[15]\tvalidation_0-auc:0.840967\n",
            "[16]\tvalidation_0-auc:0.838566\n",
            "[17]\tvalidation_0-auc:0.83662\n",
            "[18]\tvalidation_0-auc:0.838357\n",
            "[19]\tvalidation_0-auc:0.839709\n",
            "[20]\tvalidation_0-auc:0.839406\n",
            "[21]\tvalidation_0-auc:0.839883\n",
            "[22]\tvalidation_0-auc:0.839767\n",
            "[23]\tvalidation_0-auc:0.840303\n",
            "[24]\tvalidation_0-auc:0.840746\n",
            "[25]\tvalidation_0-auc:0.842051\n",
            "[26]\tvalidation_0-auc:0.841504\n",
            "[27]\tvalidation_0-auc:0.842949\n",
            "[28]\tvalidation_0-auc:0.843741\n",
            "[29]\tvalidation_0-auc:0.843625\n",
            "[30]\tvalidation_0-auc:0.844184\n",
            "[31]\tvalidation_0-auc:0.844627\n",
            "[32]\tvalidation_0-auc:0.846655\n",
            "[33]\tvalidation_0-auc:0.846165\n",
            "[34]\tvalidation_0-auc:0.844814\n",
            "[35]\tvalidation_0-auc:0.843322\n",
            "[36]\tvalidation_0-auc:0.843846\n",
            "[37]\tvalidation_0-auc:0.842075\n",
            "[38]\tvalidation_0-auc:0.843497\n",
            "[39]\tvalidation_0-auc:0.845198\n",
            "[40]\tvalidation_0-auc:0.841772\n",
            "[41]\tvalidation_0-auc:0.842401\n",
            "[42]\tvalidation_0-auc:0.8431\n",
            "[43]\tvalidation_0-auc:0.83958\n",
            "[44]\tvalidation_0-auc:0.83965\n",
            "[45]\tvalidation_0-auc:0.838788\n",
            "[46]\tvalidation_0-auc:0.836876\n",
            "[47]\tvalidation_0-auc:0.838112\n",
            "[48]\tvalidation_0-auc:0.837191\n",
            "[49]\tvalidation_0-auc:0.836702\n",
            "[50]\tvalidation_0-auc:0.839009\n",
            "[51]\tvalidation_0-auc:0.839406\n",
            "[52]\tvalidation_0-auc:0.841131\n",
            "[53]\tvalidation_0-auc:0.837261\n",
            "[54]\tvalidation_0-auc:0.837751\n",
            "[55]\tvalidation_0-auc:0.839371\n",
            "[56]\tvalidation_0-auc:0.84049\n",
            "[57]\tvalidation_0-auc:0.841154\n",
            "[58]\tvalidation_0-auc:0.839219\n",
            "[59]\tvalidation_0-auc:0.839312\n",
            "[60]\tvalidation_0-auc:0.841876\n",
            "[61]\tvalidation_0-auc:0.842156\n",
            "[62]\tvalidation_0-auc:0.839662\n",
            "[63]\tvalidation_0-auc:0.839918\n",
            "[64]\tvalidation_0-auc:0.840338\n",
            "[65]\tvalidation_0-auc:0.841084\n",
            "[66]\tvalidation_0-auc:0.839895\n",
            "[67]\tvalidation_0-auc:0.836305\n",
            "[68]\tvalidation_0-auc:0.836189\n",
            "[69]\tvalidation_0-auc:0.839755\n",
            "[70]\tvalidation_0-auc:0.836445\n",
            "[71]\tvalidation_0-auc:0.837168\n",
            "[72]\tvalidation_0-auc:0.837448\n",
            "[73]\tvalidation_0-auc:0.836026\n",
            "[74]\tvalidation_0-auc:0.835909\n",
            "[75]\tvalidation_0-auc:0.836655\n",
            "[76]\tvalidation_0-auc:0.836492\n",
            "[77]\tvalidation_0-auc:0.833951\n",
            "[78]\tvalidation_0-auc:0.8312\n",
            "[79]\tvalidation_0-auc:0.833112\n",
            "[80]\tvalidation_0-auc:0.835303\n",
            "[81]\tvalidation_0-auc:0.834953\n",
            "[82]\tvalidation_0-auc:0.834697\n",
            "Stopping. Best iteration:\n",
            "[32]\tvalidation_0-auc:0.846655\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |        Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "|     LSTM 0.1     | 0.773469387755102  |  0.6082474226804123 | 0.44696969696969696 |  0.5152838427947598 |\n",
            "|     GRU 0.1      | 0.7571428571428571 |  0.5585585585585585 |  0.4696969696969697 |  0.5102880658436213 |\n",
            "|   XGBoost 0.1    | 0.746938775510204  |        0.525        |  0.6363636363636364 |  0.5753424657534246 |\n",
            "|    Logreg 0.1    | 0.763265306122449  |  0.5754716981132075 |  0.4621212121212121 |  0.5126050420168067 |\n",
            "|     SVM 0.1      | 0.7285714285714285 | 0.49612403100775193 | 0.48484848484848486 | 0.49042145593869735 |\n",
            "|  LSTM beta 0.1   | 0.8227571115973742 |  0.6688741721854304 |  0.7651515151515151 |  0.713780918727915  |\n",
            "|   GRU beta 0.1   | 0.811816192560175  |  0.7254901960784313 |  0.5606060606060606 |  0.6324786324786323 |\n",
            "| XGBoost beta 0.1 | 0.8008752735229759 |  0.6541353383458647 |  0.6590909090909091 |  0.6566037735849056 |\n",
            "| logreg beta 0.1  | 0.8336980306345733 |         0.78        |  0.5909090909090909 |  0.6724137931034482 |\n",
            "|   svm beta 0.1   | 0.8424507658643327 |  0.7307692307692307 |  0.7196969696969697 |  0.7251908396946564 |\n",
            "+------------------+--------------------+---------------------+---------------------+---------------------+\n",
            "Threshhold =  0.2\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6955 - accuracy: 0.5114 - val_loss: 0.6828 - val_accuracy: 0.7469\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6944 - accuracy: 0.5034 - val_loss: 0.6909 - val_accuracy: 0.7673\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6931 - accuracy: 0.5121 - val_loss: 0.6832 - val_accuracy: 0.7776\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6911 - accuracy: 0.5383 - val_loss: 0.6528 - val_accuracy: 0.7082\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6868 - accuracy: 0.5463 - val_loss: 0.6693 - val_accuracy: 0.6347\n",
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 14ms/step - loss: 0.6976 - accuracy: 0.4879 - val_loss: 0.7029 - val_accuracy: 0.2531\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6922 - accuracy: 0.5228 - val_loss: 0.7032 - val_accuracy: 0.2939\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6793 - accuracy: 0.5591 - val_loss: 0.6824 - val_accuracy: 0.5245\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6686 - accuracy: 0.5698 - val_loss: 0.6991 - val_accuracy: 0.5184\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6655 - accuracy: 0.5906 - val_loss: 0.6125 - val_accuracy: 0.6878\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.662017\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.678433\n",
            "[2]\tvalidation_0-auc:0.68198\n",
            "[3]\tvalidation_0-auc:0.682002\n",
            "[4]\tvalidation_0-auc:0.699112\n",
            "[5]\tvalidation_0-auc:0.698528\n",
            "[6]\tvalidation_0-auc:0.695906\n",
            "[7]\tvalidation_0-auc:0.69562\n",
            "[8]\tvalidation_0-auc:0.695476\n",
            "[9]\tvalidation_0-auc:0.69757\n",
            "[10]\tvalidation_0-auc:0.695465\n",
            "[11]\tvalidation_0-auc:0.69682\n",
            "[12]\tvalidation_0-auc:0.698693\n",
            "[13]\tvalidation_0-auc:0.699553\n",
            "[14]\tvalidation_0-auc:0.700368\n",
            "[15]\tvalidation_0-auc:0.697713\n",
            "[16]\tvalidation_0-auc:0.697052\n",
            "[17]\tvalidation_0-auc:0.695829\n",
            "[18]\tvalidation_0-auc:0.694826\n",
            "[19]\tvalidation_0-auc:0.693791\n",
            "[20]\tvalidation_0-auc:0.695234\n",
            "[21]\tvalidation_0-auc:0.692292\n",
            "[22]\tvalidation_0-auc:0.691962\n",
            "[23]\tvalidation_0-auc:0.689781\n",
            "[24]\tvalidation_0-auc:0.68707\n",
            "[25]\tvalidation_0-auc:0.688007\n",
            "[26]\tvalidation_0-auc:0.687026\n",
            "[27]\tvalidation_0-auc:0.686233\n",
            "[28]\tvalidation_0-auc:0.684955\n",
            "[29]\tvalidation_0-auc:0.681804\n",
            "[30]\tvalidation_0-auc:0.681716\n",
            "[31]\tvalidation_0-auc:0.682047\n",
            "[32]\tvalidation_0-auc:0.682024\n",
            "[33]\tvalidation_0-auc:0.680019\n",
            "[34]\tvalidation_0-auc:0.678896\n",
            "[35]\tvalidation_0-auc:0.677904\n",
            "[36]\tvalidation_0-auc:0.677904\n",
            "[37]\tvalidation_0-auc:0.678896\n",
            "[38]\tvalidation_0-auc:0.679986\n",
            "[39]\tvalidation_0-auc:0.683941\n",
            "[40]\tvalidation_0-auc:0.683986\n",
            "[41]\tvalidation_0-auc:0.683369\n",
            "[42]\tvalidation_0-auc:0.68198\n",
            "[43]\tvalidation_0-auc:0.68154\n",
            "[44]\tvalidation_0-auc:0.680526\n",
            "[45]\tvalidation_0-auc:0.68046\n",
            "[46]\tvalidation_0-auc:0.680802\n",
            "[47]\tvalidation_0-auc:0.680691\n",
            "[48]\tvalidation_0-auc:0.681187\n",
            "[49]\tvalidation_0-auc:0.683115\n",
            "[50]\tvalidation_0-auc:0.683302\n",
            "[51]\tvalidation_0-auc:0.682157\n",
            "[52]\tvalidation_0-auc:0.682113\n",
            "[53]\tvalidation_0-auc:0.67808\n",
            "[54]\tvalidation_0-auc:0.679325\n",
            "[55]\tvalidation_0-auc:0.675778\n",
            "[56]\tvalidation_0-auc:0.675778\n",
            "[57]\tvalidation_0-auc:0.675381\n",
            "[58]\tvalidation_0-auc:0.676219\n",
            "[59]\tvalidation_0-auc:0.675954\n",
            "[60]\tvalidation_0-auc:0.675315\n",
            "[61]\tvalidation_0-auc:0.675073\n",
            "[62]\tvalidation_0-auc:0.67309\n",
            "[63]\tvalidation_0-auc:0.6745\n",
            "[64]\tvalidation_0-auc:0.675315\n",
            "Stopping. Best iteration:\n",
            "[14]\tvalidation_0-auc:0.700368\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 15ms/step - loss: 0.6950 - accuracy: 0.5031 - val_loss: 0.6669 - val_accuracy: 0.7287\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6839 - accuracy: 0.5456 - val_loss: 0.6397 - val_accuracy: 0.7287\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6359 - accuracy: 0.6218 - val_loss: 0.5735 - val_accuracy: 0.7068\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6246 - accuracy: 0.6404 - val_loss: 0.6394 - val_accuracy: 0.6214\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6132 - accuracy: 0.6479 - val_loss: 0.5911 - val_accuracy: 0.6586\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6777 - accuracy: 0.5848 - val_loss: 0.5347 - val_accuracy: 0.7724\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.6298 - accuracy: 0.6328 - val_loss: 0.6369 - val_accuracy: 0.6389\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6188 - accuracy: 0.6603 - val_loss: 0.6045 - val_accuracy: 0.6696\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5979 - accuracy: 0.6678 - val_loss: 0.6020 - val_accuracy: 0.6718\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5908 - accuracy: 0.6726 - val_loss: 0.5163 - val_accuracy: 0.7177\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.619503\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.619575\n",
            "[2]\tvalidation_0-auc:0.62994\n",
            "[3]\tvalidation_0-auc:0.634082\n",
            "[4]\tvalidation_0-auc:0.630643\n",
            "[5]\tvalidation_0-auc:0.628935\n",
            "[6]\tvalidation_0-auc:0.645258\n",
            "[7]\tvalidation_0-auc:0.647123\n",
            "[8]\tvalidation_0-auc:0.647632\n",
            "[9]\tvalidation_0-auc:0.653395\n",
            "[10]\tvalidation_0-auc:0.666969\n",
            "[11]\tvalidation_0-auc:0.666533\n",
            "[12]\tvalidation_0-auc:0.671982\n",
            "[13]\tvalidation_0-auc:0.672043\n",
            "[14]\tvalidation_0-auc:0.688378\n",
            "[15]\tvalidation_0-auc:0.683486\n",
            "[16]\tvalidation_0-auc:0.678994\n",
            "[17]\tvalidation_0-auc:0.683219\n",
            "[18]\tvalidation_0-auc:0.690872\n",
            "[19]\tvalidation_0-auc:0.683655\n",
            "[20]\tvalidation_0-auc:0.685072\n",
            "[21]\tvalidation_0-auc:0.689274\n",
            "[22]\tvalidation_0-auc:0.690872\n",
            "[23]\tvalidation_0-auc:0.687748\n",
            "[24]\tvalidation_0-auc:0.685908\n",
            "[25]\tvalidation_0-auc:0.683583\n",
            "[26]\tvalidation_0-auc:0.683341\n",
            "[27]\tvalidation_0-auc:0.682372\n",
            "[28]\tvalidation_0-auc:0.684152\n",
            "[29]\tvalidation_0-auc:0.684782\n",
            "[30]\tvalidation_0-auc:0.684854\n",
            "[31]\tvalidation_0-auc:0.68437\n",
            "[32]\tvalidation_0-auc:0.684406\n",
            "[33]\tvalidation_0-auc:0.684576\n",
            "[34]\tvalidation_0-auc:0.684164\n",
            "[35]\tvalidation_0-auc:0.682614\n",
            "[36]\tvalidation_0-auc:0.683655\n",
            "[37]\tvalidation_0-auc:0.683994\n",
            "[38]\tvalidation_0-auc:0.683098\n",
            "[39]\tvalidation_0-auc:0.683195\n",
            "[40]\tvalidation_0-auc:0.685254\n",
            "[41]\tvalidation_0-auc:0.68552\n",
            "[42]\tvalidation_0-auc:0.68569\n",
            "[43]\tvalidation_0-auc:0.685678\n",
            "[44]\tvalidation_0-auc:0.686646\n",
            "[45]\tvalidation_0-auc:0.686671\n",
            "[46]\tvalidation_0-auc:0.687106\n",
            "[47]\tvalidation_0-auc:0.686864\n",
            "[48]\tvalidation_0-auc:0.687155\n",
            "[49]\tvalidation_0-auc:0.687615\n",
            "[50]\tvalidation_0-auc:0.688027\n",
            "[51]\tvalidation_0-auc:0.6873\n",
            "[52]\tvalidation_0-auc:0.686259\n",
            "[53]\tvalidation_0-auc:0.689068\n",
            "[54]\tvalidation_0-auc:0.688342\n",
            "[55]\tvalidation_0-auc:0.689044\n",
            "[56]\tvalidation_0-auc:0.687082\n",
            "[57]\tvalidation_0-auc:0.685871\n",
            "[58]\tvalidation_0-auc:0.687349\n",
            "[59]\tvalidation_0-auc:0.687446\n",
            "[60]\tvalidation_0-auc:0.688705\n",
            "[61]\tvalidation_0-auc:0.687785\n",
            "[62]\tvalidation_0-auc:0.687058\n",
            "[63]\tvalidation_0-auc:0.686598\n",
            "[64]\tvalidation_0-auc:0.686525\n",
            "[65]\tvalidation_0-auc:0.687446\n",
            "[66]\tvalidation_0-auc:0.687349\n",
            "[67]\tvalidation_0-auc:0.686477\n",
            "[68]\tvalidation_0-auc:0.687203\n",
            "Stopping. Best iteration:\n",
            "[18]\tvalidation_0-auc:0.690872\n",
            "\n",
            "end training. \n",
            "\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|      Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.2     | 0.6346938775510204 | 0.37777777777777777 | 0.6854838709677419 |  0.487106017191977  |\n",
            "|     GRU 0.2      | 0.6877551020408164 |  0.4064516129032258 | 0.5080645161290323 | 0.45161290322580644 |\n",
            "|   XGBoost 0.2    | 0.5959183673469388 |        0.352        | 0.7096774193548387 |  0.4705882352941176 |\n",
            "|    Logreg 0.2    | 0.673469387755102  |  0.4052631578947368 | 0.6209677419354839 |  0.4904458598726114 |\n",
            "|     SVM 0.2      |        0.6         | 0.35714285714285715 | 0.7258064516129032 |  0.4787234042553191 |\n",
            "|  LSTM beta 0.2   | 0.6586433260393874 | 0.43162393162393164 | 0.8145161290322581 |  0.5642458100558659 |\n",
            "|   GRU beta 0.2   | 0.7177242888402626 | 0.48175182481751827 | 0.532258064516129  |  0.5057471264367815 |\n",
            "| XGBoost beta 0.2 | 0.6236323851203501 | 0.38571428571428573 | 0.6532258064516129 | 0.48502994011976047 |\n",
            "| logreg beta 0.2  | 0.7724288840262582 |  0.5602409638554217 |        0.75        |  0.6413793103448276 |\n",
            "|   svm beta 0.2   | 0.7111597374179431 |  0.481651376146789  | 0.8467741935483871 |  0.6140350877192983 |\n",
            "+------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "Threshhold =  0.15\n",
            "start lstm training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 16ms/step - loss: 0.6975 - accuracy: 0.4846 - val_loss: 0.6968 - val_accuracy: 0.2061\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6936 - accuracy: 0.5134 - val_loss: 0.6493 - val_accuracy: 0.7939\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6945 - accuracy: 0.5148 - val_loss: 0.7180 - val_accuracy: 0.2061\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6862 - accuracy: 0.5430 - val_loss: 0.8655 - val_accuracy: 0.3143\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6969 - accuracy: 0.5268 - val_loss: 0.6553 - val_accuracy: 0.7939\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end training. \n",
            "\n",
            "start gru training...\n",
            "\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 3s 13ms/step - loss: 0.6959 - accuracy: 0.5054 - val_loss: 0.6454 - val_accuracy: 0.8020\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6919 - accuracy: 0.5268 - val_loss: 0.6711 - val_accuracy: 0.7102\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.6793 - accuracy: 0.5738 - val_loss: 0.6383 - val_accuracy: 0.7143\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6670 - accuracy: 0.5859 - val_loss: 0.5169 - val_accuracy: 0.8143\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.6720 - accuracy: 0.5819 - val_loss: 0.6622 - val_accuracy: 0.5918\n",
            "end training. \n",
            "\n",
            "start xgboost training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.715124\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.733195\n",
            "[2]\tvalidation_0-auc:0.738031\n",
            "[3]\tvalidation_0-auc:0.739431\n",
            "[4]\tvalidation_0-auc:0.754664\n",
            "[5]\tvalidation_0-auc:0.756509\n",
            "[6]\tvalidation_0-auc:0.756051\n",
            "[7]\tvalidation_0-auc:0.75497\n",
            "[8]\tvalidation_0-auc:0.752043\n",
            "[9]\tvalidation_0-auc:0.755288\n",
            "[10]\tvalidation_0-auc:0.751483\n",
            "[11]\tvalidation_0-auc:0.752552\n",
            "[12]\tvalidation_0-auc:0.753112\n",
            "[13]\tvalidation_0-auc:0.751546\n",
            "[14]\tvalidation_0-auc:0.751572\n",
            "[15]\tvalidation_0-auc:0.747894\n",
            "[16]\tvalidation_0-auc:0.747156\n",
            "[17]\tvalidation_0-auc:0.745718\n",
            "[18]\tvalidation_0-auc:0.745196\n",
            "[19]\tvalidation_0-auc:0.743923\n",
            "[20]\tvalidation_0-auc:0.74615\n",
            "[21]\tvalidation_0-auc:0.742282\n",
            "[22]\tvalidation_0-auc:0.741849\n",
            "[23]\tvalidation_0-auc:0.739393\n",
            "[24]\tvalidation_0-auc:0.736033\n",
            "[25]\tvalidation_0-auc:0.737255\n",
            "[26]\tvalidation_0-auc:0.735969\n",
            "[27]\tvalidation_0-auc:0.736275\n",
            "[28]\tvalidation_0-auc:0.734824\n",
            "[29]\tvalidation_0-auc:0.72953\n",
            "[30]\tvalidation_0-auc:0.729326\n",
            "[31]\tvalidation_0-auc:0.730523\n",
            "[32]\tvalidation_0-auc:0.730523\n",
            "[33]\tvalidation_0-auc:0.728308\n",
            "[34]\tvalidation_0-auc:0.726985\n",
            "[35]\tvalidation_0-auc:0.726247\n",
            "[36]\tvalidation_0-auc:0.726272\n",
            "[37]\tvalidation_0-auc:0.726832\n",
            "[38]\tvalidation_0-auc:0.727634\n",
            "[39]\tvalidation_0-auc:0.733653\n",
            "[40]\tvalidation_0-auc:0.73359\n",
            "[41]\tvalidation_0-auc:0.733335\n",
            "[42]\tvalidation_0-auc:0.731808\n",
            "[43]\tvalidation_0-auc:0.731248\n",
            "[44]\tvalidation_0-auc:0.730675\n",
            "[45]\tvalidation_0-auc:0.730573\n",
            "[46]\tvalidation_0-auc:0.730917\n",
            "[47]\tvalidation_0-auc:0.730815\n",
            "[48]\tvalidation_0-auc:0.731439\n",
            "[49]\tvalidation_0-auc:0.732368\n",
            "[50]\tvalidation_0-auc:0.73247\n",
            "[51]\tvalidation_0-auc:0.731019\n",
            "[52]\tvalidation_0-auc:0.731121\n",
            "[53]\tvalidation_0-auc:0.727863\n",
            "[54]\tvalidation_0-auc:0.729759\n",
            "[55]\tvalidation_0-auc:0.726399\n",
            "Stopping. Best iteration:\n",
            "[5]\tvalidation_0-auc:0.756509\n",
            "\n",
            "end training. \n",
            "\n",
            "start lstm beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 4s 16ms/step - loss: 0.6913 - accuracy: 0.5326 - val_loss: 0.7253 - val_accuracy: 0.3589\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6619 - accuracy: 0.5999 - val_loss: 0.7735 - val_accuracy: 0.3282\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6600 - accuracy: 0.5978 - val_loss: 0.6001 - val_accuracy: 0.6149\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.6305 - accuracy: 0.6479 - val_loss: 0.6890 - val_accuracy: 0.4617\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6157 - accuracy: 0.6706 - val_loss: 0.6680 - val_accuracy: 0.4880\n",
            "end training. \n",
            "\n",
            "start gru beta training...\n",
            "\n",
            "Epoch 1/5\n",
            "92/92 [==============================] - 3s 14ms/step - loss: 0.6849 - accuracy: 0.5587 - val_loss: 0.6643 - val_accuracy: 0.6214\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.6415 - accuracy: 0.6280 - val_loss: 0.5911 - val_accuracy: 0.7330\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5894 - accuracy: 0.6898 - val_loss: 0.4983 - val_accuracy: 0.7812\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.5809 - accuracy: 0.6911 - val_loss: 0.5966 - val_accuracy: 0.7002\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.5800 - accuracy: 0.7028 - val_loss: 0.6464 - val_accuracy: 0.6105\n",
            "end training. \n",
            "\n",
            "start xgboost beta training...\n",
            "\n",
            "[0]\tvalidation_0-auc:0.652798\n",
            "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
            "[1]\tvalidation_0-auc:0.652798\n",
            "[2]\tvalidation_0-auc:0.683822\n",
            "[3]\tvalidation_0-auc:0.688758\n",
            "[4]\tvalidation_0-auc:0.69439\n",
            "[5]\tvalidation_0-auc:0.693111\n",
            "[6]\tvalidation_0-auc:0.709019\n",
            "[7]\tvalidation_0-auc:0.706572\n",
            "[8]\tvalidation_0-auc:0.706767\n",
            "[9]\tvalidation_0-auc:0.713386\n",
            "[10]\tvalidation_0-auc:0.720339\n",
            "[11]\tvalidation_0-auc:0.718197\n",
            "[12]\tvalidation_0-auc:0.725261\n",
            "[13]\tvalidation_0-auc:0.72579\n",
            "[14]\tvalidation_0-auc:0.738124\n",
            "[15]\tvalidation_0-auc:0.732896\n",
            "[16]\tvalidation_0-auc:0.730045\n",
            "[17]\tvalidation_0-auc:0.733063\n",
            "[18]\tvalidation_0-auc:0.739571\n",
            "[19]\tvalidation_0-auc:0.736483\n",
            "[20]\tvalidation_0-auc:0.738528\n",
            "[21]\tvalidation_0-auc:0.743242\n",
            "[22]\tvalidation_0-auc:0.744465\n",
            "[23]\tvalidation_0-auc:0.741017\n",
            "[24]\tvalidation_0-auc:0.740683\n",
            "[25]\tvalidation_0-auc:0.738861\n",
            "[26]\tvalidation_0-auc:0.737207\n",
            "[27]\tvalidation_0-auc:0.736511\n",
            "[28]\tvalidation_0-auc:0.737916\n",
            "[29]\tvalidation_0-auc:0.737888\n",
            "[30]\tvalidation_0-auc:0.737805\n",
            "[31]\tvalidation_0-auc:0.73736\n",
            "[32]\tvalidation_0-auc:0.736845\n",
            "[33]\tvalidation_0-auc:0.737095\n",
            "[34]\tvalidation_0-auc:0.735705\n",
            "[35]\tvalidation_0-auc:0.735677\n",
            "[36]\tvalidation_0-auc:0.735566\n",
            "[37]\tvalidation_0-auc:0.735899\n",
            "[38]\tvalidation_0-auc:0.735899\n",
            "[39]\tvalidation_0-auc:0.736942\n",
            "[40]\tvalidation_0-auc:0.739251\n",
            "[41]\tvalidation_0-auc:0.739974\n",
            "[42]\tvalidation_0-auc:0.741337\n",
            "[43]\tvalidation_0-auc:0.740322\n",
            "[44]\tvalidation_0-auc:0.739543\n",
            "[45]\tvalidation_0-auc:0.739014\n",
            "[46]\tvalidation_0-auc:0.738959\n",
            "[47]\tvalidation_0-auc:0.738708\n",
            "[48]\tvalidation_0-auc:0.738625\n",
            "[49]\tvalidation_0-auc:0.738903\n",
            "[50]\tvalidation_0-auc:0.739098\n",
            "[51]\tvalidation_0-auc:0.739042\n",
            "[52]\tvalidation_0-auc:0.738764\n",
            "[53]\tvalidation_0-auc:0.742574\n",
            "[54]\tvalidation_0-auc:0.741573\n",
            "[55]\tvalidation_0-auc:0.742046\n",
            "[56]\tvalidation_0-auc:0.73996\n",
            "[57]\tvalidation_0-auc:0.737902\n",
            "[58]\tvalidation_0-auc:0.738291\n",
            "[59]\tvalidation_0-auc:0.738875\n",
            "[60]\tvalidation_0-auc:0.740572\n",
            "[61]\tvalidation_0-auc:0.73971\n",
            "[62]\tvalidation_0-auc:0.73843\n",
            "[63]\tvalidation_0-auc:0.738458\n",
            "[64]\tvalidation_0-auc:0.738208\n",
            "[65]\tvalidation_0-auc:0.738041\n",
            "[66]\tvalidation_0-auc:0.738291\n",
            "[67]\tvalidation_0-auc:0.736928\n",
            "[68]\tvalidation_0-auc:0.736901\n",
            "[69]\tvalidation_0-auc:0.735844\n",
            "[70]\tvalidation_0-auc:0.736456\n",
            "[71]\tvalidation_0-auc:0.735733\n",
            "[72]\tvalidation_0-auc:0.736678\n",
            "Stopping. Best iteration:\n",
            "[22]\tvalidation_0-auc:0.744465\n",
            "\n",
            "end training. \n",
            "\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|       Model       |      Accuracy      |      Precision      |       Recall       |       F1 score      |\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n",
            "|     LSTM 0.15     | 0.7938775510204081 |         0.0         |        0.0         |         0.0         |\n",
            "|      GRU 0.15     | 0.5918367346938775 |  0.3132075471698113 | 0.8217821782178217 | 0.45355191256830596 |\n",
            "|    XGBoost 0.15   | 0.5755102040816327 |  0.3054545454545455 | 0.8316831683168316 | 0.44680851063829785 |\n",
            "|    Logreg 0.15    | 0.6877551020408164 |  0.3631578947368421 | 0.6831683168316832 |  0.4742268041237113 |\n",
            "|      SVM 0.15     | 0.6061224489795919 | 0.31746031746031744 | 0.7920792079207921 | 0.45325779036827196 |\n",
            "|   LSTM beta 0.15  | 0.487964989059081  | 0.26501766784452296 | 0.7425742574257426 | 0.39062500000000006 |\n",
            "|   GRU beta 0.15   | 0.6105032822757112 |  0.3303964757709251 | 0.7425742574257426 |  0.4573170731707318 |\n",
            "| XGBoost beta 0.15 | 0.6564551422319475 | 0.36538461538461536 | 0.7524752475247525 | 0.49190938511326854 |\n",
            "|  logreg beta 0.15 | 0.7833698030634574 |  0.5060240963855421 | 0.8316831683168316 |  0.6292134831460674 |\n",
            "|   svm beta 0.15   | 0.7133479212253829 | 0.43119266055045874 | 0.9306930693069307 |  0.5893416927899687 |\n",
            "+-------------------+--------------------+---------------------+--------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dWGPyBdaT_T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "outputId": "283427bf-5603-4b6b-f78e-36b191e2c27c"
      },
      "source": [
        "Result_purging.to_csv('WU_Result_purging.csv')\n",
        "Result_purging"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Name</th>\n",
              "      <th>Perc</th>\n",
              "      <th>acc</th>\n",
              "      <th>f1</th>\n",
              "      <th>recal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.608247</td>\n",
              "      <td>0.773469</td>\n",
              "      <td>0.515284</td>\n",
              "      <td>0.446970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.558559</td>\n",
              "      <td>0.757143</td>\n",
              "      <td>0.510288</td>\n",
              "      <td>0.469697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.525000</td>\n",
              "      <td>0.746939</td>\n",
              "      <td>0.575342</td>\n",
              "      <td>0.636364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.575472</td>\n",
              "      <td>0.763265</td>\n",
              "      <td>0.512605</td>\n",
              "      <td>0.462121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.496124</td>\n",
              "      <td>0.728571</td>\n",
              "      <td>0.490421</td>\n",
              "      <td>0.484848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.668874</td>\n",
              "      <td>0.822757</td>\n",
              "      <td>0.713781</td>\n",
              "      <td>0.765152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.725490</td>\n",
              "      <td>0.811816</td>\n",
              "      <td>0.632479</td>\n",
              "      <td>0.560606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.654135</td>\n",
              "      <td>0.800875</td>\n",
              "      <td>0.656604</td>\n",
              "      <td>0.659091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.780000</td>\n",
              "      <td>0.833698</td>\n",
              "      <td>0.672414</td>\n",
              "      <td>0.590909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.1</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.730769</td>\n",
              "      <td>0.842451</td>\n",
              "      <td>0.725191</td>\n",
              "      <td>0.719697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.2</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.377778</td>\n",
              "      <td>0.634694</td>\n",
              "      <td>0.487106</td>\n",
              "      <td>0.685484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.2</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.406452</td>\n",
              "      <td>0.687755</td>\n",
              "      <td>0.451613</td>\n",
              "      <td>0.508065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.2</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.352000</td>\n",
              "      <td>0.595918</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.709677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.2</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.405263</td>\n",
              "      <td>0.673469</td>\n",
              "      <td>0.490446</td>\n",
              "      <td>0.620968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.2</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.478723</td>\n",
              "      <td>0.725806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.2</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.431624</td>\n",
              "      <td>0.658643</td>\n",
              "      <td>0.564246</td>\n",
              "      <td>0.814516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.2</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.481752</td>\n",
              "      <td>0.717724</td>\n",
              "      <td>0.505747</td>\n",
              "      <td>0.532258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.2</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.385714</td>\n",
              "      <td>0.623632</td>\n",
              "      <td>0.485030</td>\n",
              "      <td>0.653226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.2</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.560241</td>\n",
              "      <td>0.772429</td>\n",
              "      <td>0.641379</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.2</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.481651</td>\n",
              "      <td>0.711160</td>\n",
              "      <td>0.614035</td>\n",
              "      <td>0.846774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM 0.15</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.793878</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU 0.15</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.313208</td>\n",
              "      <td>0.591837</td>\n",
              "      <td>0.453552</td>\n",
              "      <td>0.821782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost 0.15</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.305455</td>\n",
              "      <td>0.575510</td>\n",
              "      <td>0.446809</td>\n",
              "      <td>0.831683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logreg 0.15</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.363158</td>\n",
              "      <td>0.687755</td>\n",
              "      <td>0.474227</td>\n",
              "      <td>0.683168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM 0.15</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.317460</td>\n",
              "      <td>0.606122</td>\n",
              "      <td>0.453258</td>\n",
              "      <td>0.792079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM beta 0.15</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.265018</td>\n",
              "      <td>0.487965</td>\n",
              "      <td>0.390625</td>\n",
              "      <td>0.742574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRU beta 0.15</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.330396</td>\n",
              "      <td>0.610503</td>\n",
              "      <td>0.457317</td>\n",
              "      <td>0.742574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost beta 0.15</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.365385</td>\n",
              "      <td>0.656455</td>\n",
              "      <td>0.491909</td>\n",
              "      <td>0.752475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logreg beta 0.15</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.506024</td>\n",
              "      <td>0.783370</td>\n",
              "      <td>0.629213</td>\n",
              "      <td>0.831683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>svm beta 0.15</td>\n",
              "      <td>WU</td>\n",
              "      <td>0.431193</td>\n",
              "      <td>0.713348</td>\n",
              "      <td>0.589342</td>\n",
              "      <td>0.930693</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Model Name      Perc       acc        f1     recal\n",
              "0           LSTM 0.1   WU  0.608247  0.773469  0.515284  0.446970\n",
              "1            GRU 0.1   WU  0.558559  0.757143  0.510288  0.469697\n",
              "2        XGBoost 0.1   WU  0.525000  0.746939  0.575342  0.636364\n",
              "3         Logreg 0.1   WU  0.575472  0.763265  0.512605  0.462121\n",
              "4            SVM 0.1   WU  0.496124  0.728571  0.490421  0.484848\n",
              "5      LSTM beta 0.1   WU  0.668874  0.822757  0.713781  0.765152\n",
              "6       GRU beta 0.1   WU  0.725490  0.811816  0.632479  0.560606\n",
              "7   XGBoost beta 0.1   WU  0.654135  0.800875  0.656604  0.659091\n",
              "8    logreg beta 0.1   WU  0.780000  0.833698  0.672414  0.590909\n",
              "9       svm beta 0.1   WU  0.730769  0.842451  0.725191  0.719697\n",
              "0           LSTM 0.2   WU  0.377778  0.634694  0.487106  0.685484\n",
              "1            GRU 0.2   WU  0.406452  0.687755  0.451613  0.508065\n",
              "2        XGBoost 0.2   WU  0.352000  0.595918  0.470588  0.709677\n",
              "3         Logreg 0.2   WU  0.405263  0.673469  0.490446  0.620968\n",
              "4            SVM 0.2   WU  0.357143  0.600000  0.478723  0.725806\n",
              "5      LSTM beta 0.2   WU  0.431624  0.658643  0.564246  0.814516\n",
              "6       GRU beta 0.2   WU  0.481752  0.717724  0.505747  0.532258\n",
              "7   XGBoost beta 0.2   WU  0.385714  0.623632  0.485030  0.653226\n",
              "8    logreg beta 0.2   WU  0.560241  0.772429  0.641379  0.750000\n",
              "9       svm beta 0.2   WU  0.481651  0.711160  0.614035  0.846774\n",
              "0          LSTM 0.15   WU  0.000000  0.793878  0.000000  0.000000\n",
              "1           GRU 0.15   WU  0.313208  0.591837  0.453552  0.821782\n",
              "2       XGBoost 0.15   WU  0.305455  0.575510  0.446809  0.831683\n",
              "3        Logreg 0.15   WU  0.363158  0.687755  0.474227  0.683168\n",
              "4           SVM 0.15   WU  0.317460  0.606122  0.453258  0.792079\n",
              "5     LSTM beta 0.15   WU  0.265018  0.487965  0.390625  0.742574\n",
              "6      GRU beta 0.15   WU  0.330396  0.610503  0.457317  0.742574\n",
              "7  XGBoost beta 0.15   WU  0.365385  0.656455  0.491909  0.752475\n",
              "8   logreg beta 0.15   WU  0.506024  0.783370  0.629213  0.831683\n",
              "9      svm beta 0.15   WU  0.431193  0.713348  0.589342  0.930693"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3CUgwa9aT_T"
      },
      "source": [
        "# Save predictions \n",
        "\n",
        "\n",
        "# 11 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_lstm_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_gru_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_svm_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_xgboost_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_predict\n",
        "new_dfs = dfs[1810:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_logreg_p.csv')\n",
        "\n",
        "\n",
        "# 44 days\n",
        "\n",
        "# LSTM\n",
        "pred = historical.LSTM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_lstm_beta_p.csv')\n",
        "# GRU\n",
        "pred = historical.GRU_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_gru_beta_p.csv')\n",
        "# SVM\n",
        "pred = historical.SVM_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_svm_beta_p.csv')\n",
        "# XGBoost\n",
        "pred = historical.XGboost_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_xgboost_beta_p.csv')\n",
        "# Logreg\n",
        "pred = historical.Logreg_beta_predict\n",
        "new_dfs = dfs[1843:2300].assign(labels=pd.Series(pred).values)\n",
        "new_dfs.to_csv('WU_logreg_beta_p.csv')"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG2mJtLFaT_U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}